{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_util\n",
    "import gym\n",
    "import load_policy\n",
    "import tempfile\n",
    "import time\n",
    "%autosave 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expert(expert_policy_file = \"experts/Humanoid-v1.pkl\", \n",
    "           envname = \"Humanoid-v1\", render = True, max_timesteps = None, num_rollouts = 20, verbose = False):\n",
    "    if verbose:\n",
    "        print('loading and building expert policy')\n",
    "    policy_fn = load_policy.load_policy( expert_policy_file)\n",
    "    if verbose:\n",
    "        print('loaded and built')\n",
    "\n",
    "    with tf.Session():\n",
    "        tf_util.initialize()\n",
    "\n",
    "        import gym\n",
    "        env = gym.make( envname)\n",
    "        max_steps =  max_timesteps or env.spec.timestep_limit\n",
    "\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "        for i in range(num_rollouts):\n",
    "            if verbose:\n",
    "                print('-----> iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action = policy_fn(obs[None,:])\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "                obs, r, done, _ = env.step(action)\n",
    "#                 print(\"obs\",np.array(obs).shape)\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if verbose:\n",
    "                    if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "\n",
    "        if verbose:\n",
    "            print('returns', returns)\n",
    "            print('mean return', np.mean(returns))\n",
    "            print('std of return', np.std(returns))\n",
    "\n",
    "        expert_data = {'observations': np.array(observations),\n",
    "                       'actions': np.array(actions)}\n",
    "        return expert_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:32:03,921] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:32:03,947] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "[2017-10-23 19:32:03,952] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken =  149.0485339164734 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "expert_data = expert(expert_policy_file = \"experts/Humanoid-v1.pkl\", \n",
    "           envname = \"Humanoid-v1\", render = False, max_timesteps = None, num_rollouts = 100, verbose = False)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time taken = \", (end-start),\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 376)\n",
      "(100000, 1, 17)\n"
     ]
    }
   ],
   "source": [
    "obs_shape = expert_data[\"observations\"].shape\n",
    "action_shape = expert_data[\"actions\"].shape\n",
    "print(obs_shape)\n",
    "print(action_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:34:41,458] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:34:41,483] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "[2017-10-23 19:34:41,488] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken =  23.435084581375122 s\n",
      "(15000, 376)\n",
      "(15000, 1, 17)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "val_data = expert(expert_policy_file = \"experts/Humanoid-v1.pkl\", \n",
    "           envname = \"Humanoid-v1\", render = False, max_timesteps = None, num_rollouts = 15, verbose = False)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time taken = \", (end-start),\"s\")\n",
    "print(val_data[\"observations\"].shape)\n",
    "print(val_data[\"actions\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immitation learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "    hidden1_units = 100\n",
    "    hidden2_units = 50\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([obs_shape[1], hidden1_units],\n",
    "                                stddev=1.0 / np.sqrt(float(obs_shape[1]))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(x, weights) + biases)\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                                stddev=1.0 / np.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    # Linear\n",
    "    with tf.name_scope('linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2_units, action_shape[2]],\n",
    "                                stddev=1.0 / np.sqrt(float(hidden2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([action_shape[2]]),\n",
    "                             name='biases')\n",
    "        output = tf.matmul(hidden2, weights) + biases\n",
    "#         print(output.get_shape)\n",
    "\n",
    "    return tf.expand_dims(output, 1)\n",
    "\n",
    "# def loss(pred, labels):\n",
    "#   labels = tf.to_int64(labels)\n",
    "#   loss = tf.square(tf.subtract(labels, pred), name='square_error')\n",
    "#   return tf.reduce_mean(loss, name='mean_square_error')\n",
    "\n",
    "# def training(loss, learning_rate):\n",
    "#   # Add a scalar summary for the snapshot loss.\n",
    "#   tf.summary.scalar('loss', loss)\n",
    "#   # Create the gradient descent optimizer with the given learning rate.\n",
    "#   optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#   # Create a variable to track the global step.\n",
    "#   global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "#   # Use the optimizer to apply the gradients that minimize the loss\n",
    "#   # (and also increment the global step counter) as a single training step.\n",
    "#   train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "#   return train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 376], name=\"input\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 1,17], name=\"prediction\")\n",
    "\n",
    "pred = deepnn(x)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.square(tf.subtract(y_, pred, name='square_error'))\n",
    "loss = tf.reduce_mean(loss, name='mean_square_error')\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 1e-3\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               100000, 0.96, staircase=True)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "# graph_location = tempfile.mkdtemp()\n",
    "# print('Saving graph to: %s' % graph_location)\n",
    "# train_writer = tf.summary.FileWriter(graph_location)\n",
    "# train_writer.add_graph(tf.get_default_graph())\n",
    "summary_op = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(expert_data, batch_size = 100, epochs = 50, train_from_start = False, verbose = True):\n",
    "    with tf.Session() as sess:\n",
    "        if train_from_start:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        else :\n",
    "            saver.restore(sess, \"./tmp/model.ckpt\")\n",
    "        train_writer = tf.summary.FileWriter('tensorboard/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('tensorboard/test')\n",
    "        for epoch in range(epochs):\n",
    "            for batch_indx in range(batch_size,obs_shape[0], batch_size):\n",
    "                batch_x = expert_data[\"observations\"][batch_indx-batch_size:batch_indx,:]\n",
    "                batch_y = expert_data[\"actions\"][batch_indx-batch_size:batch_indx,:,:]\n",
    "    #             print(batch_x.shape)\n",
    "\n",
    "                i = (epoch+1)*batch_indx\n",
    "                if i % 100 == 0:\n",
    "                    training_loss= loss.eval(feed_dict={x: batch_x, y_: batch_y})\n",
    "                    summary = summary_op.eval(feed_dict={x: batch_x, y_: batch_y})\n",
    "                    train_writer.add_summary(summary, i)\n",
    "                if verbose and (i %500 == 0):\n",
    "                    print('epoch %d,step %d, training loss %g' % (epoch,i, training_loss))\n",
    "                train_step.run(feed_dict={x: batch_x, y_: batch_y})\n",
    "\n",
    "            valid_loss = loss.eval(feed_dict={x:val_data[\"observations\"], y_:val_data[\"actions\"] })\n",
    "            summary = summary_op.eval(feed_dict={x:val_data[\"observations\"], y_:val_data[\"actions\"] })\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print('epoch %d,training loss %g ,test loss %g' % (epoch,training_loss,valid_loss))\n",
    "            save_path = saver.save(sess, \"./tmp/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,step 500, training loss 50.7585\n",
      "epoch 0,step 1000, training loss 19.1259\n",
      "epoch 0,step 1500, training loss 14.6506\n",
      "epoch 0,step 2000, training loss 6.25772\n",
      "epoch 0,step 2500, training loss 4.09501\n",
      "epoch 0,step 3000, training loss 3.55589\n",
      "epoch 0,step 3500, training loss 2.84298\n",
      "epoch 0,step 4000, training loss 1.69518\n",
      "epoch 0,step 4500, training loss 1.46496\n",
      "epoch 0,step 5000, training loss 1.80301\n",
      "epoch 0,step 5500, training loss 1.35236\n",
      "epoch 0,step 6000, training loss 1.45481\n",
      "epoch 0,step 6500, training loss 1.67892\n",
      "epoch 0,step 7000, training loss 1.2391\n",
      "epoch 0,step 7500, training loss 1.10266\n",
      "epoch 0,step 8000, training loss 1.12694\n",
      "epoch 0,step 8500, training loss 0.968804\n",
      "epoch 0,step 9000, training loss 0.788289\n",
      "epoch 0,step 9500, training loss 0.751538\n",
      "epoch 0,step 10000, training loss 0.76197\n",
      "epoch 0,step 10500, training loss 0.832579\n",
      "epoch 0,step 11000, training loss 0.755798\n",
      "epoch 0,step 11500, training loss 0.799145\n",
      "epoch 0,step 12000, training loss 0.682368\n",
      "epoch 0,step 12500, training loss 0.674536\n",
      "epoch 0,step 13000, training loss 0.730254\n",
      "epoch 0,step 13500, training loss 0.806541\n",
      "epoch 0,step 14000, training loss 0.63255\n",
      "epoch 0,step 14500, training loss 0.610725\n",
      "epoch 0,step 15000, training loss 0.633983\n",
      "epoch 0,step 15500, training loss 0.906086\n",
      "epoch 0,step 16000, training loss 0.601702\n",
      "epoch 0,step 16500, training loss 0.487938\n",
      "epoch 0,step 17000, training loss 0.692102\n",
      "epoch 0,step 17500, training loss 0.652832\n",
      "epoch 0,step 18000, training loss 0.464126\n",
      "epoch 0,step 18500, training loss 0.567434\n",
      "epoch 0,step 19000, training loss 0.630453\n",
      "epoch 0,step 19500, training loss 0.590447\n",
      "epoch 0,step 20000, training loss 0.444094\n",
      "epoch 0,step 20500, training loss 0.480826\n",
      "epoch 0,step 21000, training loss 0.482024\n",
      "epoch 0,step 21500, training loss 0.448605\n",
      "epoch 0,step 22000, training loss 0.490865\n",
      "epoch 0,step 22500, training loss 0.445489\n",
      "epoch 0,step 23000, training loss 0.502094\n",
      "epoch 0,step 23500, training loss 0.340549\n",
      "epoch 0,step 24000, training loss 0.392218\n",
      "epoch 0,step 24500, training loss 0.402661\n",
      "epoch 0,step 25000, training loss 0.467202\n",
      "epoch 0,step 25500, training loss 0.463707\n",
      "epoch 0,step 26000, training loss 0.497716\n",
      "epoch 0,step 26500, training loss 0.350401\n",
      "epoch 0,step 27000, training loss 0.385372\n",
      "epoch 0,step 27500, training loss 0.47954\n",
      "epoch 0,step 28000, training loss 0.374236\n",
      "epoch 0,step 28500, training loss 0.413935\n",
      "epoch 0,step 29000, training loss 0.455471\n",
      "epoch 0,step 29500, training loss 0.395752\n",
      "epoch 0,step 30000, training loss 0.435594\n",
      "epoch 0,step 30500, training loss 0.377902\n",
      "epoch 0,step 31000, training loss 0.320292\n",
      "epoch 0,step 31500, training loss 0.304205\n",
      "epoch 0,step 32000, training loss 0.447394\n",
      "epoch 0,step 32500, training loss 0.435155\n",
      "epoch 0,step 33000, training loss 0.322503\n",
      "epoch 0,step 33500, training loss 0.338526\n",
      "epoch 0,step 34000, training loss 0.359055\n",
      "epoch 0,step 34500, training loss 0.296898\n",
      "epoch 0,step 35000, training loss 0.304204\n",
      "epoch 0,step 35500, training loss 0.35076\n",
      "epoch 0,step 36000, training loss 0.331414\n",
      "epoch 0,step 36500, training loss 0.477977\n",
      "epoch 0,step 37000, training loss 0.415267\n",
      "epoch 0,step 37500, training loss 0.399996\n",
      "epoch 0,step 38000, training loss 0.318775\n",
      "epoch 0,step 38500, training loss 0.336074\n",
      "epoch 0,step 39000, training loss 0.314411\n",
      "epoch 0,step 39500, training loss 0.277519\n",
      "epoch 0,step 40000, training loss 0.259267\n",
      "epoch 0,step 40500, training loss 0.255072\n",
      "epoch 0,step 41000, training loss 0.29533\n",
      "epoch 0,step 41500, training loss 0.266851\n",
      "epoch 0,step 42000, training loss 0.273842\n",
      "epoch 0,step 42500, training loss 0.330003\n",
      "epoch 0,step 43000, training loss 0.343253\n",
      "epoch 0,step 43500, training loss 0.298382\n",
      "epoch 0,step 44000, training loss 0.292127\n",
      "epoch 0,step 44500, training loss 0.231323\n",
      "epoch 0,step 45000, training loss 0.244389\n",
      "epoch 0,step 45500, training loss 0.257238\n",
      "epoch 0,step 46000, training loss 0.32427\n",
      "epoch 0,step 46500, training loss 0.308914\n",
      "epoch 0,step 47000, training loss 0.348462\n",
      "epoch 0,step 47500, training loss 0.209348\n",
      "epoch 0,step 48000, training loss 0.261953\n",
      "epoch 0,step 48500, training loss 0.290862\n",
      "epoch 0,step 49000, training loss 0.326004\n",
      "epoch 0,step 49500, training loss 0.310167\n",
      "epoch 0,step 50000, training loss 0.230657\n",
      "epoch 0,step 50500, training loss 0.218332\n",
      "epoch 0,step 51000, training loss 0.236296\n",
      "epoch 0,step 51500, training loss 0.281791\n",
      "epoch 0,step 52000, training loss 0.207624\n",
      "epoch 0,step 52500, training loss 0.286227\n",
      "epoch 0,step 53000, training loss 0.276178\n",
      "epoch 0,step 53500, training loss 0.237522\n",
      "epoch 0,step 54000, training loss 0.232986\n",
      "epoch 0,step 54500, training loss 0.257516\n",
      "epoch 0,step 55000, training loss 0.297062\n",
      "epoch 0,step 55500, training loss 0.260195\n",
      "epoch 0,step 56000, training loss 0.288345\n",
      "epoch 0,step 56500, training loss 0.222242\n",
      "epoch 0,step 57000, training loss 0.21536\n",
      "epoch 0,step 57500, training loss 0.351005\n",
      "epoch 0,step 58000, training loss 0.180471\n",
      "epoch 0,step 58500, training loss 0.23592\n",
      "epoch 0,step 59000, training loss 0.169244\n",
      "epoch 0,step 59500, training loss 0.252588\n",
      "epoch 0,step 60000, training loss 0.265174\n",
      "epoch 0,step 60500, training loss 0.192197\n",
      "epoch 0,step 61000, training loss 0.218807\n",
      "epoch 0,step 61500, training loss 0.200667\n",
      "epoch 0,step 62000, training loss 0.290279\n",
      "epoch 0,step 62500, training loss 0.2846\n",
      "epoch 0,step 63000, training loss 0.263181\n",
      "epoch 0,step 63500, training loss 0.184714\n",
      "epoch 0,step 64000, training loss 0.213346\n",
      "epoch 0,step 64500, training loss 0.292925\n",
      "epoch 0,step 65000, training loss 0.224928\n",
      "epoch 0,step 65500, training loss 0.197427\n",
      "epoch 0,step 66000, training loss 0.190425\n",
      "epoch 0,step 66500, training loss 0.232566\n",
      "epoch 0,step 67000, training loss 0.259185\n",
      "epoch 0,step 67500, training loss 0.184253\n",
      "epoch 0,step 68000, training loss 0.212807\n",
      "epoch 0,step 68500, training loss 0.18409\n",
      "epoch 0,step 69000, training loss 0.2415\n",
      "epoch 0,step 69500, training loss 0.190851\n",
      "epoch 0,step 70000, training loss 0.187281\n",
      "epoch 0,step 70500, training loss 0.207426\n",
      "epoch 0,step 71000, training loss 0.193943\n",
      "epoch 0,step 71500, training loss 0.205014\n",
      "epoch 0,step 72000, training loss 0.229934\n",
      "epoch 0,step 72500, training loss 0.194952\n",
      "epoch 0,step 73000, training loss 0.180625\n",
      "epoch 0,step 73500, training loss 0.207289\n",
      "epoch 0,step 74000, training loss 0.268316\n",
      "epoch 0,step 74500, training loss 0.204853\n",
      "epoch 0,step 75000, training loss 0.376587\n",
      "epoch 0,step 75500, training loss 0.175795\n",
      "epoch 0,step 76000, training loss 0.209017\n",
      "epoch 0,step 76500, training loss 0.261979\n",
      "epoch 0,step 77000, training loss 0.190074\n",
      "epoch 0,step 77500, training loss 0.209258\n",
      "epoch 0,step 78000, training loss 0.187915\n",
      "epoch 0,step 78500, training loss 0.181811\n",
      "epoch 0,step 79000, training loss 0.207747\n",
      "epoch 0,step 79500, training loss 0.164612\n",
      "epoch 0,step 80000, training loss 0.170146\n",
      "epoch 0,step 80500, training loss 0.19798\n",
      "epoch 0,step 81000, training loss 0.18264\n",
      "epoch 0,step 81500, training loss 0.195934\n",
      "epoch 0,step 82000, training loss 0.186908\n",
      "epoch 0,step 82500, training loss 0.193577\n",
      "epoch 0,step 83000, training loss 0.173973\n",
      "epoch 0,step 83500, training loss 0.17991\n",
      "epoch 0,step 84000, training loss 0.183222\n",
      "epoch 0,step 84500, training loss 0.189695\n",
      "epoch 0,step 85000, training loss 0.202775\n",
      "epoch 0,step 85500, training loss 0.211254\n",
      "epoch 0,step 86000, training loss 0.214002\n",
      "epoch 0,step 86500, training loss 0.151906\n",
      "epoch 0,step 87000, training loss 0.228116\n",
      "epoch 0,step 87500, training loss 0.188141\n",
      "epoch 0,step 88000, training loss 0.190821\n",
      "epoch 0,step 88500, training loss 0.180795\n",
      "epoch 0,step 89000, training loss 0.148808\n",
      "epoch 0,step 89500, training loss 0.217586\n",
      "epoch 0,step 90000, training loss 0.269183\n",
      "epoch 0,step 90500, training loss 0.174189\n",
      "epoch 0,step 91000, training loss 0.173531\n",
      "epoch 0,step 91500, training loss 0.242186\n",
      "epoch 0,step 92000, training loss 0.178218\n",
      "epoch 0,step 92500, training loss 0.156544\n",
      "epoch 0,step 93000, training loss 0.138983\n",
      "epoch 0,step 93500, training loss 0.227278\n",
      "epoch 0,step 94000, training loss 0.157135\n",
      "epoch 0,step 94500, training loss 0.160222\n",
      "epoch 0,step 95000, training loss 0.145215\n",
      "epoch 0,step 95500, training loss 0.154777\n",
      "epoch 0,step 96000, training loss 0.188105\n",
      "epoch 0,step 96500, training loss 0.163433\n",
      "epoch 0,step 97000, training loss 0.178061\n",
      "epoch 0,step 97500, training loss 0.200116\n",
      "epoch 0,step 98000, training loss 0.156659\n",
      "epoch 0,step 98500, training loss 0.139686\n",
      "epoch 0,step 99000, training loss 0.153506\n",
      "epoch 0,step 99500, training loss 0.134599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss 0.253964 ,test loss 0.229334\n",
      "epoch 1,step 1000, training loss 0.214312\n",
      "epoch 1,step 2000, training loss 0.257825\n",
      "epoch 1,step 3000, training loss 0.215212\n",
      "epoch 1,step 4000, training loss 0.14842\n",
      "epoch 1,step 5000, training loss 0.137672\n",
      "epoch 1,step 6000, training loss 0.177503\n",
      "epoch 1,step 7000, training loss 0.156906\n",
      "epoch 1,step 8000, training loss 0.135412\n",
      "epoch 1,step 9000, training loss 0.146704\n",
      "epoch 1,step 10000, training loss 0.152\n",
      "epoch 1,step 11000, training loss 0.14181\n",
      "epoch 1,step 12000, training loss 0.208634\n",
      "epoch 1,step 13000, training loss 0.269657\n",
      "epoch 1,step 14000, training loss 0.187545\n",
      "epoch 1,step 15000, training loss 0.203567\n",
      "epoch 1,step 16000, training loss 0.17717\n",
      "epoch 1,step 17000, training loss 0.16808\n",
      "epoch 1,step 18000, training loss 0.134658\n",
      "epoch 1,step 19000, training loss 0.118711\n",
      "epoch 1,step 20000, training loss 0.176448\n",
      "epoch 1,step 21000, training loss 0.150967\n",
      "epoch 1,step 22000, training loss 0.160976\n",
      "epoch 1,step 23000, training loss 0.183496\n",
      "epoch 1,step 24000, training loss 0.175257\n",
      "epoch 1,step 25000, training loss 0.170487\n",
      "epoch 1,step 26000, training loss 0.134696\n",
      "epoch 1,step 27000, training loss 0.219996\n",
      "epoch 1,step 28000, training loss 0.148333\n",
      "epoch 1,step 29000, training loss 0.158144\n",
      "epoch 1,step 30000, training loss 0.151878\n",
      "epoch 1,step 31000, training loss 0.256968\n",
      "epoch 1,step 32000, training loss 0.173049\n",
      "epoch 1,step 33000, training loss 0.153134\n",
      "epoch 1,step 34000, training loss 0.188171\n",
      "epoch 1,step 35000, training loss 0.195772\n",
      "epoch 1,step 36000, training loss 0.132964\n",
      "epoch 1,step 37000, training loss 0.183241\n",
      "epoch 1,step 38000, training loss 0.206443\n",
      "epoch 1,step 39000, training loss 0.169171\n",
      "epoch 1,step 40000, training loss 0.137703\n",
      "epoch 1,step 41000, training loss 0.150191\n",
      "epoch 1,step 42000, training loss 0.150796\n",
      "epoch 1,step 43000, training loss 0.148232\n",
      "epoch 1,step 44000, training loss 0.159593\n",
      "epoch 1,step 45000, training loss 0.151213\n",
      "epoch 1,step 46000, training loss 0.173445\n",
      "epoch 1,step 47000, training loss 0.118888\n",
      "epoch 1,step 48000, training loss 0.128636\n",
      "epoch 1,step 49000, training loss 0.172179\n",
      "epoch 1,step 50000, training loss 0.161676\n",
      "epoch 1,step 51000, training loss 0.180497\n",
      "epoch 1,step 52000, training loss 0.191525\n",
      "epoch 1,step 53000, training loss 0.14554\n",
      "epoch 1,step 54000, training loss 0.155792\n",
      "epoch 1,step 55000, training loss 0.219186\n",
      "epoch 1,step 56000, training loss 0.141748\n",
      "epoch 1,step 57000, training loss 0.144362\n",
      "epoch 1,step 58000, training loss 0.163647\n",
      "epoch 1,step 59000, training loss 0.163484\n",
      "epoch 1,step 60000, training loss 0.152689\n",
      "epoch 1,step 61000, training loss 0.157683\n",
      "epoch 1,step 62000, training loss 0.131178\n",
      "epoch 1,step 63000, training loss 0.132461\n",
      "epoch 1,step 64000, training loss 0.206552\n",
      "epoch 1,step 65000, training loss 0.200515\n",
      "epoch 1,step 66000, training loss 0.151136\n",
      "epoch 1,step 67000, training loss 0.138316\n",
      "epoch 1,step 68000, training loss 0.163066\n",
      "epoch 1,step 69000, training loss 0.122083\n",
      "epoch 1,step 70000, training loss 0.135448\n",
      "epoch 1,step 71000, training loss 0.165155\n",
      "epoch 1,step 72000, training loss 0.140251\n",
      "epoch 1,step 73000, training loss 0.207286\n",
      "epoch 1,step 74000, training loss 0.202381\n",
      "epoch 1,step 75000, training loss 0.176244\n",
      "epoch 1,step 76000, training loss 0.161104\n",
      "epoch 1,step 77000, training loss 0.156619\n",
      "epoch 1,step 78000, training loss 0.15099\n",
      "epoch 1,step 79000, training loss 0.153231\n",
      "epoch 1,step 80000, training loss 0.1226\n",
      "epoch 1,step 81000, training loss 0.121743\n",
      "epoch 1,step 82000, training loss 0.144317\n",
      "epoch 1,step 83000, training loss 0.131646\n",
      "epoch 1,step 84000, training loss 0.130615\n",
      "epoch 1,step 85000, training loss 0.182083\n",
      "epoch 1,step 86000, training loss 0.173883\n",
      "epoch 1,step 87000, training loss 0.181482\n",
      "epoch 1,step 88000, training loss 0.148955\n",
      "epoch 1,step 89000, training loss 0.104087\n",
      "epoch 1,step 90000, training loss 0.124465\n",
      "epoch 1,step 91000, training loss 0.129933\n",
      "epoch 1,step 92000, training loss 0.163654\n",
      "epoch 1,step 93000, training loss 0.176733\n",
      "epoch 1,step 94000, training loss 0.16979\n",
      "epoch 1,step 95000, training loss 0.120209\n",
      "epoch 1,step 96000, training loss 0.146571\n",
      "epoch 1,step 97000, training loss 0.1832\n",
      "epoch 1,step 98000, training loss 0.190233\n",
      "epoch 1,step 99000, training loss 0.176051\n",
      "epoch 1,step 100000, training loss 0.133426\n",
      "epoch 1,step 101000, training loss 0.133114\n",
      "epoch 1,step 102000, training loss 0.12214\n",
      "epoch 1,step 103000, training loss 0.15843\n",
      "epoch 1,step 104000, training loss 0.117039\n",
      "epoch 1,step 105000, training loss 0.160226\n",
      "epoch 1,step 106000, training loss 0.146049\n",
      "epoch 1,step 107000, training loss 0.135015\n",
      "epoch 1,step 108000, training loss 0.127774\n",
      "epoch 1,step 109000, training loss 0.155206\n",
      "epoch 1,step 110000, training loss 0.190514\n",
      "epoch 1,step 111000, training loss 0.149057\n",
      "epoch 1,step 112000, training loss 0.173012\n",
      "epoch 1,step 113000, training loss 0.133627\n",
      "epoch 1,step 114000, training loss 0.119882\n",
      "epoch 1,step 115000, training loss 0.212603\n",
      "epoch 1,step 116000, training loss 0.109287\n",
      "epoch 1,step 117000, training loss 0.134326\n",
      "epoch 1,step 118000, training loss 0.102304\n",
      "epoch 1,step 119000, training loss 0.141476\n",
      "epoch 1,step 120000, training loss 0.148288\n",
      "epoch 1,step 121000, training loss 0.111715\n",
      "epoch 1,step 122000, training loss 0.122689\n",
      "epoch 1,step 123000, training loss 0.108735\n",
      "epoch 1,step 124000, training loss 0.163347\n",
      "epoch 1,step 125000, training loss 0.176549\n",
      "epoch 1,step 126000, training loss 0.151703\n",
      "epoch 1,step 127000, training loss 0.108732\n",
      "epoch 1,step 128000, training loss 0.123013\n",
      "epoch 1,step 129000, training loss 0.168538\n",
      "epoch 1,step 130000, training loss 0.131244\n",
      "epoch 1,step 131000, training loss 0.108822\n",
      "epoch 1,step 132000, training loss 0.110388\n",
      "epoch 1,step 133000, training loss 0.120715\n",
      "epoch 1,step 134000, training loss 0.156194\n",
      "epoch 1,step 135000, training loss 0.112548\n",
      "epoch 1,step 136000, training loss 0.137946\n",
      "epoch 1,step 137000, training loss 0.11767\n",
      "epoch 1,step 138000, training loss 0.144845\n",
      "epoch 1,step 139000, training loss 0.113909\n",
      "epoch 1,step 140000, training loss 0.109539\n",
      "epoch 1,step 141000, training loss 0.137818\n",
      "epoch 1,step 142000, training loss 0.129405\n",
      "epoch 1,step 143000, training loss 0.127372\n",
      "epoch 1,step 144000, training loss 0.137625\n",
      "epoch 1,step 145000, training loss 0.140209\n",
      "epoch 1,step 146000, training loss 0.12029\n",
      "epoch 1,step 147000, training loss 0.143914\n",
      "epoch 1,step 148000, training loss 0.179733\n",
      "epoch 1,step 149000, training loss 0.140904\n",
      "epoch 1,step 150000, training loss 0.25827\n",
      "epoch 1,step 151000, training loss 0.134189\n",
      "epoch 1,step 152000, training loss 0.148542\n",
      "epoch 1,step 153000, training loss 0.186125\n",
      "epoch 1,step 154000, training loss 0.115558\n",
      "epoch 1,step 155000, training loss 0.137648\n",
      "epoch 1,step 156000, training loss 0.120061\n",
      "epoch 1,step 157000, training loss 0.120527\n",
      "epoch 1,step 158000, training loss 0.135621\n",
      "epoch 1,step 159000, training loss 0.112457\n",
      "epoch 1,step 160000, training loss 0.107401\n",
      "epoch 1,step 161000, training loss 0.120266\n",
      "epoch 1,step 162000, training loss 0.114777\n",
      "epoch 1,step 163000, training loss 0.137687\n",
      "epoch 1,step 164000, training loss 0.131893\n",
      "epoch 1,step 165000, training loss 0.123689\n",
      "epoch 1,step 166000, training loss 0.122566\n",
      "epoch 1,step 167000, training loss 0.117273\n",
      "epoch 1,step 168000, training loss 0.110589\n",
      "epoch 1,step 169000, training loss 0.132623\n",
      "epoch 1,step 170000, training loss 0.124444\n",
      "epoch 1,step 171000, training loss 0.154627\n",
      "epoch 1,step 172000, training loss 0.148949\n",
      "epoch 1,step 173000, training loss 0.0959232\n",
      "epoch 1,step 174000, training loss 0.144633\n",
      "epoch 1,step 175000, training loss 0.124386\n",
      "epoch 1,step 176000, training loss 0.126862\n",
      "epoch 1,step 177000, training loss 0.118463\n",
      "epoch 1,step 178000, training loss 0.0953335\n",
      "epoch 1,step 179000, training loss 0.142897\n",
      "epoch 1,step 180000, training loss 0.189323\n",
      "epoch 1,step 181000, training loss 0.110576\n",
      "epoch 1,step 182000, training loss 0.11663\n",
      "epoch 1,step 183000, training loss 0.154655\n",
      "epoch 1,step 184000, training loss 0.117781\n",
      "epoch 1,step 185000, training loss 0.0964077\n",
      "epoch 1,step 186000, training loss 0.0976852\n",
      "epoch 1,step 187000, training loss 0.147447\n",
      "epoch 1,step 188000, training loss 0.101197\n",
      "epoch 1,step 189000, training loss 0.112357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,step 190000, training loss 0.0948727\n",
      "epoch 1,step 191000, training loss 0.105929\n",
      "epoch 1,step 192000, training loss 0.12464\n",
      "epoch 1,step 193000, training loss 0.11024\n",
      "epoch 1,step 194000, training loss 0.126548\n",
      "epoch 1,step 195000, training loss 0.144585\n",
      "epoch 1,step 196000, training loss 0.0994896\n",
      "epoch 1,step 197000, training loss 0.0985241\n",
      "epoch 1,step 198000, training loss 0.0995925\n",
      "epoch 1,step 199000, training loss 0.0861296\n",
      "epoch 1,training loss 0.163615 ,test loss 0.156869\n",
      "epoch 2,step 1500, training loss 0.142338\n",
      "epoch 2,step 3000, training loss 0.188631\n",
      "epoch 2,step 4500, training loss 0.151595\n",
      "epoch 2,step 6000, training loss 0.0998412\n",
      "epoch 2,step 7500, training loss 0.0889694\n",
      "epoch 2,step 9000, training loss 0.12076\n",
      "epoch 2,step 10500, training loss 0.107072\n",
      "epoch 2,step 12000, training loss 0.0919598\n",
      "epoch 2,step 13500, training loss 0.102995\n",
      "epoch 2,step 15000, training loss 0.115271\n",
      "epoch 2,step 16500, training loss 0.0983394\n",
      "epoch 2,step 18000, training loss 0.142935\n",
      "epoch 2,step 19500, training loss 0.208701\n",
      "epoch 2,step 21000, training loss 0.127169\n",
      "epoch 2,step 22500, training loss 0.137649\n",
      "epoch 2,step 24000, training loss 0.119177\n",
      "epoch 2,step 25500, training loss 0.101699\n",
      "epoch 2,step 27000, training loss 0.0923325\n",
      "epoch 2,step 28500, training loss 0.0783389\n",
      "epoch 2,step 30000, training loss 0.134606\n",
      "epoch 2,step 31500, training loss 0.100194\n",
      "epoch 2,step 33000, training loss 0.11335\n",
      "epoch 2,step 34500, training loss 0.131769\n",
      "epoch 2,step 36000, training loss 0.131999\n",
      "epoch 2,step 37500, training loss 0.125863\n",
      "epoch 2,step 39000, training loss 0.0911456\n",
      "epoch 2,step 40500, training loss 0.161721\n",
      "epoch 2,step 42000, training loss 0.105122\n",
      "epoch 2,step 43500, training loss 0.119594\n",
      "epoch 2,step 45000, training loss 0.102812\n",
      "epoch 2,step 46500, training loss 0.182077\n",
      "epoch 2,step 48000, training loss 0.132929\n",
      "epoch 2,step 49500, training loss 0.113649\n",
      "epoch 2,step 51000, training loss 0.126221\n",
      "epoch 2,step 52500, training loss 0.146298\n",
      "epoch 2,step 54000, training loss 0.104238\n",
      "epoch 2,step 55500, training loss 0.138168\n",
      "epoch 2,step 57000, training loss 0.14981\n",
      "epoch 2,step 58500, training loss 0.120528\n",
      "epoch 2,step 60000, training loss 0.0995608\n",
      "epoch 2,step 61500, training loss 0.110957\n",
      "epoch 2,step 63000, training loss 0.117895\n",
      "epoch 2,step 64500, training loss 0.122233\n",
      "epoch 2,step 66000, training loss 0.111707\n",
      "epoch 2,step 67500, training loss 0.101757\n",
      "epoch 2,step 69000, training loss 0.126286\n",
      "epoch 2,step 70500, training loss 0.081921\n",
      "epoch 2,step 72000, training loss 0.0911257\n",
      "epoch 2,step 73500, training loss 0.119787\n",
      "epoch 2,step 75000, training loss 0.107186\n",
      "epoch 2,step 76500, training loss 0.118621\n",
      "epoch 2,step 78000, training loss 0.140489\n",
      "epoch 2,step 79500, training loss 0.0927203\n",
      "epoch 2,step 81000, training loss 0.109079\n",
      "epoch 2,step 82500, training loss 0.154783\n",
      "epoch 2,step 84000, training loss 0.107124\n",
      "epoch 2,step 85500, training loss 0.100944\n",
      "epoch 2,step 87000, training loss 0.118184\n",
      "epoch 2,step 88500, training loss 0.1231\n",
      "epoch 2,step 90000, training loss 0.104974\n",
      "epoch 2,step 91500, training loss 0.120083\n",
      "epoch 2,step 93000, training loss 0.109574\n",
      "epoch 2,step 94500, training loss 0.111685\n",
      "epoch 2,step 96000, training loss 0.154619\n",
      "epoch 2,step 97500, training loss 0.157937\n",
      "epoch 2,step 99000, training loss 0.124002\n",
      "epoch 2,step 100500, training loss 0.109888\n",
      "epoch 2,step 102000, training loss 0.12765\n",
      "epoch 2,step 103500, training loss 0.0938272\n",
      "epoch 2,step 105000, training loss 0.113966\n",
      "epoch 2,step 106500, training loss 0.130218\n",
      "epoch 2,step 108000, training loss 0.0892955\n",
      "epoch 2,step 109500, training loss 0.140638\n",
      "epoch 2,step 111000, training loss 0.14452\n",
      "epoch 2,step 112500, training loss 0.120013\n",
      "epoch 2,step 114000, training loss 0.10695\n",
      "epoch 2,step 115500, training loss 0.10459\n",
      "epoch 2,step 117000, training loss 0.106917\n",
      "epoch 2,step 118500, training loss 0.111446\n",
      "epoch 2,step 120000, training loss 0.0866845\n",
      "epoch 2,step 121500, training loss 0.0844355\n",
      "epoch 2,step 123000, training loss 0.104776\n",
      "epoch 2,step 124500, training loss 0.106184\n",
      "epoch 2,step 126000, training loss 0.102361\n",
      "epoch 2,step 127500, training loss 0.137482\n",
      "epoch 2,step 129000, training loss 0.120692\n",
      "epoch 2,step 130500, training loss 0.131858\n",
      "epoch 2,step 132000, training loss 0.112737\n",
      "epoch 2,step 133500, training loss 0.0700658\n",
      "epoch 2,step 135000, training loss 0.0952557\n",
      "epoch 2,step 136500, training loss 0.0966871\n",
      "epoch 2,step 138000, training loss 0.117512\n",
      "epoch 2,step 139500, training loss 0.133321\n",
      "epoch 2,step 141000, training loss 0.131944\n",
      "epoch 2,step 142500, training loss 0.0828494\n",
      "epoch 2,step 144000, training loss 0.112996\n",
      "epoch 2,step 145500, training loss 0.108259\n",
      "epoch 2,step 147000, training loss 0.128349\n",
      "epoch 2,step 148500, training loss 0.12174\n",
      "epoch 2,step 150000, training loss 0.0929773\n",
      "epoch 2,step 151500, training loss 0.0970321\n",
      "epoch 2,step 153000, training loss 0.0828211\n",
      "epoch 2,step 154500, training loss 0.117855\n",
      "epoch 2,step 156000, training loss 0.0918467\n",
      "epoch 2,step 157500, training loss 0.116764\n",
      "epoch 2,step 159000, training loss 0.116529\n",
      "epoch 2,step 160500, training loss 0.111044\n",
      "epoch 2,step 162000, training loss 0.0953242\n",
      "epoch 2,step 163500, training loss 0.115813\n",
      "epoch 2,step 165000, training loss 0.149078\n",
      "epoch 2,step 166500, training loss 0.125126\n",
      "epoch 2,step 168000, training loss 0.156235\n",
      "epoch 2,step 169500, training loss 0.110031\n",
      "epoch 2,step 171000, training loss 0.0893811\n",
      "epoch 2,step 172500, training loss 0.162469\n",
      "epoch 2,step 174000, training loss 0.0867736\n",
      "epoch 2,step 175500, training loss 0.100727\n",
      "epoch 2,step 177000, training loss 0.0797627\n",
      "epoch 2,step 178500, training loss 0.10777\n",
      "epoch 2,step 180000, training loss 0.137954\n",
      "epoch 2,step 181500, training loss 0.0853639\n",
      "epoch 2,step 183000, training loss 0.0891673\n",
      "epoch 2,step 184500, training loss 0.0867754\n",
      "epoch 2,step 186000, training loss 0.12712\n",
      "epoch 2,step 187500, training loss 0.13755\n",
      "epoch 2,step 189000, training loss 0.119918\n",
      "epoch 2,step 190500, training loss 0.0922275\n",
      "epoch 2,step 192000, training loss 0.1008\n",
      "epoch 2,step 193500, training loss 0.121504\n",
      "epoch 2,step 195000, training loss 0.110991\n",
      "epoch 2,step 196500, training loss 0.0870994\n",
      "epoch 2,step 198000, training loss 0.0820807\n",
      "epoch 2,step 199500, training loss 0.0921314\n",
      "epoch 2,step 201000, training loss 0.131047\n",
      "epoch 2,step 202500, training loss 0.0850056\n",
      "epoch 2,step 204000, training loss 0.110973\n",
      "epoch 2,step 205500, training loss 0.0929196\n",
      "epoch 2,step 207000, training loss 0.111597\n",
      "epoch 2,step 208500, training loss 0.0916401\n",
      "epoch 2,step 210000, training loss 0.0820179\n",
      "epoch 2,step 211500, training loss 0.11085\n",
      "epoch 2,step 213000, training loss 0.100236\n",
      "epoch 2,step 214500, training loss 0.103748\n",
      "epoch 2,step 216000, training loss 0.0912567\n",
      "epoch 2,step 217500, training loss 0.1062\n",
      "epoch 2,step 219000, training loss 0.0965906\n",
      "epoch 2,step 220500, training loss 0.103639\n",
      "epoch 2,step 222000, training loss 0.146578\n",
      "epoch 2,step 223500, training loss 0.0959079\n",
      "epoch 2,step 225000, training loss 0.192586\n",
      "epoch 2,step 226500, training loss 0.0868968\n",
      "epoch 2,step 228000, training loss 0.109527\n",
      "epoch 2,step 229500, training loss 0.144977\n",
      "epoch 2,step 231000, training loss 0.082386\n",
      "epoch 2,step 232500, training loss 0.108961\n",
      "epoch 2,step 234000, training loss 0.0909503\n",
      "epoch 2,step 235500, training loss 0.0979244\n",
      "epoch 2,step 237000, training loss 0.106668\n",
      "epoch 2,step 238500, training loss 0.0900133\n",
      "epoch 2,step 240000, training loss 0.0829083\n",
      "epoch 2,step 241500, training loss 0.0975268\n",
      "epoch 2,step 243000, training loss 0.0939238\n",
      "epoch 2,step 244500, training loss 0.118535\n",
      "epoch 2,step 246000, training loss 0.100801\n",
      "epoch 2,step 247500, training loss 0.103829\n",
      "epoch 2,step 249000, training loss 0.0947423\n",
      "epoch 2,step 250500, training loss 0.0956743\n",
      "epoch 2,step 252000, training loss 0.0931042\n",
      "epoch 2,step 253500, training loss 0.112141\n",
      "epoch 2,step 255000, training loss 0.100143\n",
      "epoch 2,step 256500, training loss 0.123033\n",
      "epoch 2,step 258000, training loss 0.119234\n",
      "epoch 2,step 259500, training loss 0.0825555\n",
      "epoch 2,step 261000, training loss 0.120246\n",
      "epoch 2,step 262500, training loss 0.0994895\n",
      "epoch 2,step 264000, training loss 0.100702\n",
      "epoch 2,step 265500, training loss 0.0918293\n",
      "epoch 2,step 267000, training loss 0.0792922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2,step 268500, training loss 0.112287\n",
      "epoch 2,step 270000, training loss 0.160998\n",
      "epoch 2,step 271500, training loss 0.0901105\n",
      "epoch 2,step 273000, training loss 0.0946188\n",
      "epoch 2,step 274500, training loss 0.122079\n",
      "epoch 2,step 276000, training loss 0.104271\n",
      "epoch 2,step 277500, training loss 0.0798851\n",
      "epoch 2,step 279000, training loss 0.0835427\n",
      "epoch 2,step 280500, training loss 0.121609\n",
      "epoch 2,step 282000, training loss 0.0881794\n",
      "epoch 2,step 283500, training loss 0.0885611\n",
      "epoch 2,step 285000, training loss 0.0759322\n",
      "epoch 2,step 286500, training loss 0.086363\n",
      "epoch 2,step 288000, training loss 0.104937\n",
      "epoch 2,step 289500, training loss 0.0842712\n",
      "epoch 2,step 291000, training loss 0.0949635\n",
      "epoch 2,step 292500, training loss 0.115161\n",
      "epoch 2,step 294000, training loss 0.0817706\n",
      "epoch 2,step 295500, training loss 0.0769295\n",
      "epoch 2,step 297000, training loss 0.0686577\n",
      "epoch 2,step 298500, training loss 0.0746165\n",
      "epoch 2,training loss 0.138145 ,test loss 0.128189\n",
      "epoch 3,step 2000, training loss 0.116098\n",
      "epoch 3,step 4000, training loss 0.146982\n",
      "epoch 3,step 6000, training loss 0.120997\n",
      "epoch 3,step 8000, training loss 0.0747243\n",
      "epoch 3,step 10000, training loss 0.068908\n",
      "epoch 3,step 12000, training loss 0.0943224\n",
      "epoch 3,step 14000, training loss 0.0890737\n",
      "epoch 3,step 16000, training loss 0.0792876\n",
      "epoch 3,step 18000, training loss 0.0887901\n",
      "epoch 3,step 20000, training loss 0.0848054\n",
      "epoch 3,step 22000, training loss 0.0864537\n",
      "epoch 3,step 24000, training loss 0.120466\n",
      "epoch 3,step 26000, training loss 0.166844\n",
      "epoch 3,step 28000, training loss 0.103204\n",
      "epoch 3,step 30000, training loss 0.112298\n",
      "epoch 3,step 32000, training loss 0.0925205\n",
      "epoch 3,step 34000, training loss 0.0823055\n",
      "epoch 3,step 36000, training loss 0.0754776\n",
      "epoch 3,step 38000, training loss 0.0652113\n",
      "epoch 3,step 40000, training loss 0.104607\n",
      "epoch 3,step 42000, training loss 0.0883757\n",
      "epoch 3,step 44000, training loss 0.0922035\n",
      "epoch 3,step 46000, training loss 0.116224\n",
      "epoch 3,step 48000, training loss 0.116972\n",
      "epoch 3,step 50000, training loss 0.104285\n",
      "epoch 3,step 52000, training loss 0.0740238\n",
      "epoch 3,step 54000, training loss 0.146281\n",
      "epoch 3,step 56000, training loss 0.0941279\n",
      "epoch 3,step 58000, training loss 0.0998772\n",
      "epoch 3,step 60000, training loss 0.090708\n",
      "epoch 3,step 62000, training loss 0.142284\n",
      "epoch 3,step 64000, training loss 0.112932\n",
      "epoch 3,step 66000, training loss 0.0935758\n",
      "epoch 3,step 68000, training loss 0.100026\n",
      "epoch 3,step 70000, training loss 0.121412\n",
      "epoch 3,step 72000, training loss 0.0826147\n",
      "epoch 3,step 74000, training loss 0.105227\n",
      "epoch 3,step 76000, training loss 0.121169\n",
      "epoch 3,step 78000, training loss 0.0959013\n",
      "epoch 3,step 80000, training loss 0.0785759\n",
      "epoch 3,step 82000, training loss 0.090861\n",
      "epoch 3,step 84000, training loss 0.0942318\n",
      "epoch 3,step 86000, training loss 0.0939634\n",
      "epoch 3,step 88000, training loss 0.0890662\n",
      "epoch 3,step 90000, training loss 0.0864861\n",
      "epoch 3,step 92000, training loss 0.101709\n",
      "epoch 3,step 94000, training loss 0.0694954\n",
      "epoch 3,step 96000, training loss 0.0788115\n",
      "epoch 3,step 98000, training loss 0.0939368\n",
      "epoch 3,step 100000, training loss 0.0882519\n",
      "epoch 3,step 102000, training loss 0.0900291\n",
      "epoch 3,step 104000, training loss 0.129351\n",
      "epoch 3,step 106000, training loss 0.0778842\n",
      "epoch 3,step 108000, training loss 0.0951108\n",
      "epoch 3,step 110000, training loss 0.126426\n",
      "epoch 3,step 112000, training loss 0.0803891\n",
      "epoch 3,step 114000, training loss 0.0823112\n",
      "epoch 3,step 116000, training loss 0.0988257\n",
      "epoch 3,step 118000, training loss 0.101585\n",
      "epoch 3,step 120000, training loss 0.0890744\n",
      "epoch 3,step 122000, training loss 0.0915745\n",
      "epoch 3,step 124000, training loss 0.0874824\n",
      "epoch 3,step 126000, training loss 0.0767826\n",
      "epoch 3,step 128000, training loss 0.114772\n",
      "epoch 3,step 130000, training loss 0.118356\n",
      "epoch 3,step 132000, training loss 0.0889929\n",
      "epoch 3,step 134000, training loss 0.0815498\n",
      "epoch 3,step 136000, training loss 0.105997\n",
      "epoch 3,step 138000, training loss 0.0724526\n",
      "epoch 3,step 140000, training loss 0.0894538\n",
      "epoch 3,step 142000, training loss 0.107503\n",
      "epoch 3,step 144000, training loss 0.0728962\n",
      "epoch 3,step 146000, training loss 0.119552\n",
      "epoch 3,step 148000, training loss 0.126173\n",
      "epoch 3,step 150000, training loss 0.0989326\n",
      "epoch 3,step 152000, training loss 0.0966795\n",
      "epoch 3,step 154000, training loss 0.0919296\n",
      "epoch 3,step 156000, training loss 0.0885826\n",
      "epoch 3,step 158000, training loss 0.0957302\n",
      "epoch 3,step 160000, training loss 0.0743515\n",
      "epoch 3,step 162000, training loss 0.0762538\n",
      "epoch 3,step 164000, training loss 0.0921836\n",
      "epoch 3,step 166000, training loss 0.0848865\n",
      "epoch 3,step 168000, training loss 0.086122\n",
      "epoch 3,step 170000, training loss 0.119861\n",
      "epoch 3,step 172000, training loss 0.10022\n",
      "epoch 3,step 174000, training loss 0.106464\n",
      "epoch 3,step 176000, training loss 0.0968155\n",
      "epoch 3,step 178000, training loss 0.0641324\n",
      "epoch 3,step 180000, training loss 0.0776842\n",
      "epoch 3,step 182000, training loss 0.0823603\n",
      "epoch 3,step 184000, training loss 0.0903289\n",
      "epoch 3,step 186000, training loss 0.117768\n",
      "epoch 3,step 188000, training loss 0.102473\n",
      "epoch 3,step 190000, training loss 0.0690455\n",
      "epoch 3,step 192000, training loss 0.0857287\n",
      "epoch 3,step 194000, training loss 0.0927916\n",
      "epoch 3,step 196000, training loss 0.0986833\n",
      "epoch 3,step 198000, training loss 0.103065\n",
      "epoch 3,step 200000, training loss 0.0817236\n",
      "epoch 3,step 202000, training loss 0.0778546\n",
      "epoch 3,step 204000, training loss 0.069512\n",
      "epoch 3,step 206000, training loss 0.106803\n",
      "epoch 3,step 208000, training loss 0.0813428\n",
      "epoch 3,step 210000, training loss 0.0977543\n",
      "epoch 3,step 212000, training loss 0.106497\n",
      "epoch 3,step 214000, training loss 0.0972339\n",
      "epoch 3,step 216000, training loss 0.0896949\n",
      "epoch 3,step 218000, training loss 0.0982994\n",
      "epoch 3,step 220000, training loss 0.123529\n",
      "epoch 3,step 222000, training loss 0.0942499\n",
      "epoch 3,step 224000, training loss 0.120362\n",
      "epoch 3,step 226000, training loss 0.0891941\n",
      "epoch 3,step 228000, training loss 0.0748733\n",
      "epoch 3,step 230000, training loss 0.134645\n",
      "epoch 3,step 232000, training loss 0.0752594\n",
      "epoch 3,step 234000, training loss 0.0833865\n",
      "epoch 3,step 236000, training loss 0.0657329\n",
      "epoch 3,step 238000, training loss 0.0898384\n",
      "epoch 3,step 240000, training loss 0.106203\n",
      "epoch 3,step 242000, training loss 0.0783512\n",
      "epoch 3,step 244000, training loss 0.0751018\n",
      "epoch 3,step 246000, training loss 0.0808397\n",
      "epoch 3,step 248000, training loss 0.102751\n",
      "epoch 3,step 250000, training loss 0.119081\n",
      "epoch 3,step 252000, training loss 0.112518\n",
      "epoch 3,step 254000, training loss 0.0832397\n",
      "epoch 3,step 256000, training loss 0.0931683\n",
      "epoch 3,step 258000, training loss 0.106044\n",
      "epoch 3,step 260000, training loss 0.0927822\n",
      "epoch 3,step 262000, training loss 0.079673\n",
      "epoch 3,step 264000, training loss 0.0742206\n",
      "epoch 3,step 266000, training loss 0.0789951\n",
      "epoch 3,step 268000, training loss 0.103711\n",
      "epoch 3,step 270000, training loss 0.0723601\n",
      "epoch 3,step 272000, training loss 0.0948262\n",
      "epoch 3,step 274000, training loss 0.075924\n",
      "epoch 3,step 276000, training loss 0.0940252\n",
      "epoch 3,step 278000, training loss 0.0740239\n",
      "epoch 3,step 280000, training loss 0.0739075\n",
      "epoch 3,step 282000, training loss 0.0919656\n",
      "epoch 3,step 284000, training loss 0.0869832\n",
      "epoch 3,step 286000, training loss 0.0906756\n",
      "epoch 3,step 288000, training loss 0.0748976\n",
      "epoch 3,step 290000, training loss 0.0866643\n",
      "epoch 3,step 292000, training loss 0.0828004\n",
      "epoch 3,step 294000, training loss 0.0872136\n",
      "epoch 3,step 296000, training loss 0.121451\n",
      "epoch 3,step 298000, training loss 0.0712887\n",
      "epoch 3,step 300000, training loss 0.160537\n",
      "epoch 3,step 302000, training loss 0.0736379\n",
      "epoch 3,step 304000, training loss 0.0856568\n",
      "epoch 3,step 306000, training loss 0.120942\n",
      "epoch 3,step 308000, training loss 0.0647056\n",
      "epoch 3,step 310000, training loss 0.0959123\n",
      "epoch 3,step 312000, training loss 0.0720003\n",
      "epoch 3,step 314000, training loss 0.078375\n",
      "epoch 3,step 316000, training loss 0.0821567\n",
      "epoch 3,step 318000, training loss 0.0817717\n",
      "epoch 3,step 320000, training loss 0.0712653\n",
      "epoch 3,step 322000, training loss 0.0864167\n",
      "epoch 3,step 324000, training loss 0.0803933\n",
      "epoch 3,step 326000, training loss 0.107728\n",
      "epoch 3,step 328000, training loss 0.0846415\n",
      "epoch 3,step 330000, training loss 0.10151\n",
      "epoch 3,step 332000, training loss 0.0862524\n",
      "epoch 3,step 334000, training loss 0.0871363\n",
      "epoch 3,step 336000, training loss 0.0776476\n",
      "epoch 3,step 338000, training loss 0.0956822\n",
      "epoch 3,step 340000, training loss 0.0887202\n",
      "epoch 3,step 342000, training loss 0.106368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3,step 344000, training loss 0.101429\n",
      "epoch 3,step 346000, training loss 0.0693769\n",
      "epoch 3,step 348000, training loss 0.0969492\n",
      "epoch 3,step 350000, training loss 0.0783507\n",
      "epoch 3,step 352000, training loss 0.0824322\n",
      "epoch 3,step 354000, training loss 0.0742423\n",
      "epoch 3,step 356000, training loss 0.0659423\n",
      "epoch 3,step 358000, training loss 0.0950772\n",
      "epoch 3,step 360000, training loss 0.142044\n",
      "epoch 3,step 362000, training loss 0.0781179\n",
      "epoch 3,step 364000, training loss 0.0807323\n",
      "epoch 3,step 366000, training loss 0.0984496\n",
      "epoch 3,step 368000, training loss 0.0777527\n",
      "epoch 3,step 370000, training loss 0.0613568\n",
      "epoch 3,step 372000, training loss 0.0701322\n",
      "epoch 3,step 374000, training loss 0.102796\n",
      "epoch 3,step 376000, training loss 0.0731332\n",
      "epoch 3,step 378000, training loss 0.077013\n",
      "epoch 3,step 380000, training loss 0.0630085\n",
      "epoch 3,step 382000, training loss 0.068991\n",
      "epoch 3,step 384000, training loss 0.0915554\n",
      "epoch 3,step 386000, training loss 0.0719749\n",
      "epoch 3,step 388000, training loss 0.0813823\n",
      "epoch 3,step 390000, training loss 0.0946792\n",
      "epoch 3,step 392000, training loss 0.0691377\n",
      "epoch 3,step 394000, training loss 0.0643366\n",
      "epoch 3,step 396000, training loss 0.0604093\n",
      "epoch 3,step 398000, training loss 0.0630913\n",
      "epoch 3,training loss 0.114001 ,test loss 0.111516\n",
      "epoch 4,step 500, training loss 0.291352\n",
      "epoch 4,step 1000, training loss 0.157755\n",
      "epoch 4,step 1500, training loss 0.0832663\n",
      "epoch 4,step 2000, training loss 0.0810383\n",
      "epoch 4,step 2500, training loss 0.0966344\n",
      "epoch 4,step 3000, training loss 0.0767353\n",
      "epoch 4,step 3500, training loss 0.0691643\n",
      "epoch 4,step 4000, training loss 0.0714195\n",
      "epoch 4,step 4500, training loss 0.0886161\n",
      "epoch 4,step 5000, training loss 0.122443\n",
      "epoch 4,step 5500, training loss 0.312042\n",
      "epoch 4,step 6000, training loss 0.173945\n",
      "epoch 4,step 6500, training loss 0.121102\n",
      "epoch 4,step 7000, training loss 0.120835\n",
      "epoch 4,step 7500, training loss 0.112452\n",
      "epoch 4,step 8000, training loss 0.0821945\n",
      "epoch 4,step 8500, training loss 0.10041\n",
      "epoch 4,step 9000, training loss 0.0839764\n",
      "epoch 4,step 9500, training loss 0.130853\n",
      "epoch 4,step 10000, training loss 0.0628982\n",
      "epoch 4,step 10500, training loss 0.229567\n",
      "epoch 4,step 11000, training loss 0.126308\n",
      "epoch 4,step 11500, training loss 0.103151\n",
      "epoch 4,step 12000, training loss 0.103183\n",
      "epoch 4,step 12500, training loss 0.0617236\n",
      "epoch 4,step 13000, training loss 0.085295\n",
      "epoch 4,step 13500, training loss 0.0699553\n",
      "epoch 4,step 14000, training loss 0.0660785\n",
      "epoch 4,step 14500, training loss 0.083552\n",
      "epoch 4,step 15000, training loss 0.0776052\n",
      "epoch 4,step 15500, training loss 0.22513\n",
      "epoch 4,step 16000, training loss 0.12309\n",
      "epoch 4,step 16500, training loss 0.089806\n",
      "epoch 4,step 17000, training loss 0.0845753\n",
      "epoch 4,step 17500, training loss 0.0820733\n",
      "epoch 4,step 18000, training loss 0.11718\n",
      "epoch 4,step 18500, training loss 0.0814843\n",
      "epoch 4,step 19000, training loss 0.0700131\n",
      "epoch 4,step 19500, training loss 0.0678115\n",
      "epoch 4,step 20000, training loss 0.0740804\n",
      "epoch 4,step 20500, training loss 0.26386\n",
      "epoch 4,step 21000, training loss 0.114433\n",
      "epoch 4,step 21500, training loss 0.0767496\n",
      "epoch 4,step 22000, training loss 0.114985\n",
      "epoch 4,step 22500, training loss 0.0816353\n",
      "epoch 4,step 23000, training loss 0.0816717\n",
      "epoch 4,step 23500, training loss 0.102839\n",
      "epoch 4,step 24000, training loss 0.0753522\n",
      "epoch 4,step 24500, training loss 0.0845402\n",
      "epoch 4,step 25000, training loss 0.0771201\n",
      "epoch 4,step 25500, training loss 0.264314\n",
      "epoch 4,step 26000, training loss 0.16612\n",
      "epoch 4,step 26500, training loss 0.0887881\n",
      "epoch 4,step 27000, training loss 0.0693525\n",
      "epoch 4,step 27500, training loss 0.0756062\n",
      "epoch 4,step 28000, training loss 0.0698127\n",
      "epoch 4,step 28500, training loss 0.0693008\n",
      "epoch 4,step 29000, training loss 0.103459\n",
      "epoch 4,step 29500, training loss 0.0980454\n",
      "epoch 4,step 30000, training loss 0.106324\n",
      "epoch 4,step 30500, training loss 0.243145\n",
      "epoch 4,step 31000, training loss 0.150732\n",
      "epoch 4,step 31500, training loss 0.0938157\n",
      "epoch 4,step 32000, training loss 0.0879368\n",
      "epoch 4,step 32500, training loss 0.14158\n",
      "epoch 4,step 33000, training loss 0.1312\n",
      "epoch 4,step 33500, training loss 0.0976429\n",
      "epoch 4,step 34000, training loss 0.0882459\n",
      "epoch 4,step 34500, training loss 0.0803044\n",
      "epoch 4,step 35000, training loss 0.090022\n",
      "epoch 4,step 35500, training loss 0.260463\n",
      "epoch 4,step 36000, training loss 0.107073\n",
      "epoch 4,step 36500, training loss 0.0622233\n",
      "epoch 4,step 37000, training loss 0.0614688\n",
      "epoch 4,step 37500, training loss 0.0970841\n",
      "epoch 4,step 38000, training loss 0.0894478\n",
      "epoch 4,step 38500, training loss 0.0898589\n",
      "epoch 4,step 39000, training loss 0.0681134\n",
      "epoch 4,step 39500, training loss 0.0734163\n",
      "epoch 4,step 40000, training loss 0.0814418\n",
      "epoch 4,step 40500, training loss 0.29306\n",
      "epoch 4,step 41000, training loss 0.126975\n",
      "epoch 4,step 41500, training loss 0.173872\n",
      "epoch 4,step 42000, training loss 0.076425\n",
      "epoch 4,step 42500, training loss 0.0708127\n",
      "epoch 4,step 43000, training loss 0.074169\n",
      "epoch 4,step 43500, training loss 0.0752275\n",
      "epoch 4,step 44000, training loss 0.0803703\n",
      "epoch 4,step 44500, training loss 0.0636923\n",
      "epoch 4,step 45000, training loss 0.0677743\n",
      "epoch 4,step 45500, training loss 0.2412\n",
      "epoch 4,step 46000, training loss 0.15367\n",
      "epoch 4,step 46500, training loss 0.0830118\n",
      "epoch 4,step 47000, training loss 0.0766757\n",
      "epoch 4,step 47500, training loss 0.058451\n",
      "epoch 4,step 48000, training loss 0.0862274\n",
      "epoch 4,step 48500, training loss 0.0726573\n",
      "epoch 4,step 49000, training loss 0.0689599\n",
      "epoch 4,step 49500, training loss 0.0851049\n",
      "epoch 4,step 50000, training loss 0.0903861\n",
      "epoch 4,step 50500, training loss 0.263586\n",
      "epoch 4,step 51000, training loss 0.112425\n",
      "epoch 4,step 51500, training loss 0.0827952\n",
      "epoch 4,step 52000, training loss 0.058224\n",
      "epoch 4,step 52500, training loss 0.0781792\n",
      "epoch 4,step 53000, training loss 0.0797452\n",
      "epoch 4,step 53500, training loss 0.052664\n",
      "epoch 4,step 54000, training loss 0.0981647\n",
      "epoch 4,step 54500, training loss 0.0982567\n",
      "epoch 4,step 55000, training loss 0.078872\n",
      "epoch 4,step 55500, training loss 0.243383\n",
      "epoch 4,step 56000, training loss 0.157386\n",
      "epoch 4,step 56500, training loss 0.0757235\n",
      "epoch 4,step 57000, training loss 0.124062\n",
      "epoch 4,step 57500, training loss 0.0968852\n",
      "epoch 4,step 58000, training loss 0.12976\n",
      "epoch 4,step 58500, training loss 0.0842791\n",
      "epoch 4,step 59000, training loss 0.13592\n",
      "epoch 4,step 59500, training loss 0.1351\n",
      "epoch 4,step 60000, training loss 0.107272\n",
      "epoch 4,step 60500, training loss 0.24989\n",
      "epoch 4,step 61000, training loss 0.132418\n",
      "epoch 4,step 61500, training loss 0.090043\n",
      "epoch 4,step 62000, training loss 0.0840161\n",
      "epoch 4,step 62500, training loss 0.0904097\n",
      "epoch 4,step 63000, training loss 0.0579427\n",
      "epoch 4,step 63500, training loss 0.0790012\n",
      "epoch 4,step 64000, training loss 0.0895501\n",
      "epoch 4,step 64500, training loss 0.0687367\n",
      "epoch 4,step 65000, training loss 0.063008\n",
      "epoch 4,step 65500, training loss 0.238897\n",
      "epoch 4,step 66000, training loss 0.172491\n",
      "epoch 4,step 66500, training loss 0.0755654\n",
      "epoch 4,step 67000, training loss 0.0817946\n",
      "epoch 4,step 67500, training loss 0.131794\n",
      "epoch 4,step 68000, training loss 0.0717669\n",
      "epoch 4,step 68500, training loss 0.0966658\n",
      "epoch 4,step 69000, training loss 0.0642563\n",
      "epoch 4,step 69500, training loss 0.0969198\n",
      "epoch 4,step 70000, training loss 0.0823662\n",
      "epoch 4,step 70500, training loss 0.325386\n",
      "epoch 4,step 71000, training loss 0.191247\n",
      "epoch 4,step 71500, training loss 0.1206\n",
      "epoch 4,step 72000, training loss 0.0695822\n",
      "epoch 4,step 72500, training loss 0.0885924\n",
      "epoch 4,step 73000, training loss 0.0942368\n",
      "epoch 4,step 73500, training loss 0.101007\n",
      "epoch 4,step 74000, training loss 0.0731379\n",
      "epoch 4,step 74500, training loss 0.0780435\n",
      "epoch 4,step 75000, training loss 0.0740137\n",
      "epoch 4,step 75500, training loss 0.2182\n",
      "epoch 4,step 76000, training loss 0.161585\n",
      "epoch 4,step 76500, training loss 0.113467\n",
      "epoch 4,step 77000, training loss 0.088265\n",
      "epoch 4,step 77500, training loss 0.124107\n",
      "epoch 4,step 78000, training loss 0.112974\n",
      "epoch 4,step 78500, training loss 0.0910121\n",
      "epoch 4,step 79000, training loss 0.0670774\n",
      "epoch 4,step 79500, training loss 0.0854041\n",
      "epoch 4,step 80000, training loss 0.104038\n",
      "epoch 4,step 80500, training loss 0.231631\n",
      "epoch 4,step 81000, training loss 0.125361\n",
      "epoch 4,step 81500, training loss 0.0883645\n",
      "epoch 4,step 82000, training loss 0.0604699\n",
      "epoch 4,step 82500, training loss 0.0855291\n",
      "epoch 4,step 83000, training loss 0.0744417\n",
      "epoch 4,step 83500, training loss 0.0643225\n",
      "epoch 4,step 84000, training loss 0.0946714\n",
      "epoch 4,step 84500, training loss 0.066065\n",
      "epoch 4,step 85000, training loss 0.0879486\n",
      "epoch 4,step 85500, training loss 0.327557\n",
      "epoch 4,step 86000, training loss 0.123587\n",
      "epoch 4,step 86500, training loss 0.104827\n",
      "epoch 4,step 87000, training loss 0.118121\n",
      "epoch 4,step 87500, training loss 0.102066\n",
      "epoch 4,step 88000, training loss 0.0848429\n",
      "epoch 4,step 88500, training loss 0.11081\n",
      "epoch 4,step 89000, training loss 0.0823865\n",
      "epoch 4,step 89500, training loss 0.0847955\n",
      "epoch 4,step 90000, training loss 0.0713358\n",
      "epoch 4,step 90500, training loss 0.232998\n",
      "epoch 4,step 91000, training loss 0.138963\n",
      "epoch 4,step 91500, training loss 0.0997573\n",
      "epoch 4,step 92000, training loss 0.10722\n",
      "epoch 4,step 92500, training loss 0.0877263\n",
      "epoch 4,step 93000, training loss 0.154316\n",
      "epoch 4,step 93500, training loss 0.0997985\n",
      "epoch 4,step 94000, training loss 0.0816603\n",
      "epoch 4,step 94500, training loss 0.0836979\n",
      "epoch 4,step 95000, training loss 0.106932\n",
      "epoch 4,step 95500, training loss 0.232879\n",
      "epoch 4,step 96000, training loss 0.11611\n",
      "epoch 4,step 96500, training loss 0.0683137\n",
      "epoch 4,step 97000, training loss 0.0631187\n",
      "epoch 4,step 97500, training loss 0.0828258\n",
      "epoch 4,step 98000, training loss 0.102448\n",
      "epoch 4,step 98500, training loss 0.0630528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4,step 99000, training loss 0.0697096\n",
      "epoch 4,step 99500, training loss 0.0679751\n",
      "epoch 4,step 100000, training loss 0.0695239\n",
      "epoch 4,step 100500, training loss 0.244462\n",
      "epoch 4,step 101000, training loss 0.185945\n",
      "epoch 4,step 101500, training loss 0.091904\n",
      "epoch 4,step 102000, training loss 0.05364\n",
      "epoch 4,step 102500, training loss 0.0813156\n",
      "epoch 4,step 103000, training loss 0.102773\n",
      "epoch 4,step 103500, training loss 0.077803\n",
      "epoch 4,step 104000, training loss 0.0555122\n",
      "epoch 4,step 104500, training loss 0.0688142\n",
      "epoch 4,step 105000, training loss 0.0825056\n",
      "epoch 4,step 105500, training loss 0.248602\n",
      "epoch 4,step 106000, training loss 0.111418\n",
      "epoch 4,step 106500, training loss 0.095377\n",
      "epoch 4,step 107000, training loss 0.0847325\n",
      "epoch 4,step 107500, training loss 0.0804685\n",
      "epoch 4,step 108000, training loss 0.0802594\n",
      "epoch 4,step 108500, training loss 0.0860589\n",
      "epoch 4,step 109000, training loss 0.0975174\n",
      "epoch 4,step 109500, training loss 0.0865378\n",
      "epoch 4,step 110000, training loss 0.078288\n",
      "epoch 4,step 110500, training loss 0.28111\n",
      "epoch 4,step 111000, training loss 0.119461\n",
      "epoch 4,step 111500, training loss 0.0896857\n",
      "epoch 4,step 112000, training loss 0.0653963\n",
      "epoch 4,step 112500, training loss 0.0723939\n",
      "epoch 4,step 113000, training loss 0.0666439\n",
      "epoch 4,step 113500, training loss 0.0783678\n",
      "epoch 4,step 114000, training loss 0.0770537\n",
      "epoch 4,step 114500, training loss 0.0675485\n",
      "epoch 4,step 115000, training loss 0.0868819\n",
      "epoch 4,step 115500, training loss 0.251404\n",
      "epoch 4,step 116000, training loss 0.124189\n",
      "epoch 4,step 116500, training loss 0.0897065\n",
      "epoch 4,step 117000, training loss 0.0833302\n",
      "epoch 4,step 117500, training loss 0.0644592\n",
      "epoch 4,step 118000, training loss 0.0766585\n",
      "epoch 4,step 118500, training loss 0.0893202\n",
      "epoch 4,step 119000, training loss 0.0760409\n",
      "epoch 4,step 119500, training loss 0.0826801\n",
      "epoch 4,step 120000, training loss 0.0685638\n",
      "epoch 4,step 120500, training loss 0.238912\n",
      "epoch 4,step 121000, training loss 0.11572\n",
      "epoch 4,step 121500, training loss 0.096356\n",
      "epoch 4,step 122000, training loss 0.0762387\n",
      "epoch 4,step 122500, training loss 0.0849065\n",
      "epoch 4,step 123000, training loss 0.112235\n",
      "epoch 4,step 123500, training loss 0.080053\n",
      "epoch 4,step 124000, training loss 0.0700205\n",
      "epoch 4,step 124500, training loss 0.106223\n",
      "epoch 4,step 125000, training loss 0.0783549\n",
      "epoch 4,step 125500, training loss 0.332756\n",
      "epoch 4,step 126000, training loss 0.11359\n",
      "epoch 4,step 126500, training loss 0.0753384\n",
      "epoch 4,step 127000, training loss 0.082289\n",
      "epoch 4,step 127500, training loss 0.0769346\n",
      "epoch 4,step 128000, training loss 0.0672305\n",
      "epoch 4,step 128500, training loss 0.0990711\n",
      "epoch 4,step 129000, training loss 0.0860356\n",
      "epoch 4,step 129500, training loss 0.107258\n",
      "epoch 4,step 130000, training loss 0.107712\n",
      "epoch 4,step 130500, training loss 0.305427\n",
      "epoch 4,step 131000, training loss 0.166594\n",
      "epoch 4,step 131500, training loss 0.137116\n",
      "epoch 4,step 132000, training loss 0.095202\n",
      "epoch 4,step 132500, training loss 0.0668365\n",
      "epoch 4,step 133000, training loss 0.072733\n",
      "epoch 4,step 133500, training loss 0.0802237\n",
      "epoch 4,step 134000, training loss 0.0796703\n",
      "epoch 4,step 134500, training loss 0.0754358\n",
      "epoch 4,step 135000, training loss 0.0826989\n",
      "epoch 4,step 135500, training loss 0.264588\n",
      "epoch 4,step 136000, training loss 0.121643\n",
      "epoch 4,step 136500, training loss 0.097133\n",
      "epoch 4,step 137000, training loss 0.12111\n",
      "epoch 4,step 137500, training loss 0.115516\n",
      "epoch 4,step 138000, training loss 0.0901284\n",
      "epoch 4,step 138500, training loss 0.0674719\n",
      "epoch 4,step 139000, training loss 0.0792328\n",
      "epoch 4,step 139500, training loss 0.0873597\n",
      "epoch 4,step 140000, training loss 0.0709741\n",
      "epoch 4,step 140500, training loss 0.274758\n",
      "epoch 4,step 141000, training loss 0.124754\n",
      "epoch 4,step 141500, training loss 0.0647509\n",
      "epoch 4,step 142000, training loss 0.0794659\n",
      "epoch 4,step 142500, training loss 0.0824219\n",
      "epoch 4,step 143000, training loss 0.0780536\n",
      "epoch 4,step 143500, training loss 0.0887587\n",
      "epoch 4,step 144000, training loss 0.0612028\n",
      "epoch 4,step 144500, training loss 0.0817298\n",
      "epoch 4,step 145000, training loss 0.0840653\n",
      "epoch 4,step 145500, training loss 0.240497\n",
      "epoch 4,step 146000, training loss 0.0990252\n",
      "epoch 4,step 146500, training loss 0.0694539\n",
      "epoch 4,step 147000, training loss 0.112924\n",
      "epoch 4,step 147500, training loss 0.0893863\n",
      "epoch 4,step 148000, training loss 0.0862185\n",
      "epoch 4,step 148500, training loss 0.0772888\n",
      "epoch 4,step 149000, training loss 0.0859666\n",
      "epoch 4,step 149500, training loss 0.0673044\n",
      "epoch 4,step 150000, training loss 0.0704898\n",
      "epoch 4,step 150500, training loss 0.240305\n",
      "epoch 4,step 151000, training loss 0.124553\n",
      "epoch 4,step 151500, training loss 0.0915219\n",
      "epoch 4,step 152000, training loss 0.0806034\n",
      "epoch 4,step 152500, training loss 0.0843617\n",
      "epoch 4,step 153000, training loss 0.0829233\n",
      "epoch 4,step 153500, training loss 0.0660549\n",
      "epoch 4,step 154000, training loss 0.0986895\n",
      "epoch 4,step 154500, training loss 0.0669542\n",
      "epoch 4,step 155000, training loss 0.0732506\n",
      "epoch 4,step 155500, training loss 0.235925\n",
      "epoch 4,step 156000, training loss 0.154006\n",
      "epoch 4,step 156500, training loss 0.0612214\n",
      "epoch 4,step 157000, training loss 0.0935424\n",
      "epoch 4,step 157500, training loss 0.0708428\n",
      "epoch 4,step 158000, training loss 0.0958294\n",
      "epoch 4,step 158500, training loss 0.14384\n",
      "epoch 4,step 159000, training loss 0.0654343\n",
      "epoch 4,step 159500, training loss 0.131324\n",
      "epoch 4,step 160000, training loss 0.106848\n",
      "epoch 4,step 160500, training loss 0.262815\n",
      "epoch 4,step 161000, training loss 0.155249\n",
      "epoch 4,step 161500, training loss 0.095708\n",
      "epoch 4,step 162000, training loss 0.0897105\n",
      "epoch 4,step 162500, training loss 0.111933\n",
      "epoch 4,step 163000, training loss 0.0780641\n",
      "epoch 4,step 163500, training loss 0.0800315\n",
      "epoch 4,step 164000, training loss 0.0712731\n",
      "epoch 4,step 164500, training loss 0.0981313\n",
      "epoch 4,step 165000, training loss 0.076412\n",
      "epoch 4,step 165500, training loss 0.270689\n",
      "epoch 4,step 166000, training loss 0.175367\n",
      "epoch 4,step 166500, training loss 0.0806942\n",
      "epoch 4,step 167000, training loss 0.124771\n",
      "epoch 4,step 167500, training loss 0.0688128\n",
      "epoch 4,step 168000, training loss 0.0628815\n",
      "epoch 4,step 168500, training loss 0.0982218\n",
      "epoch 4,step 169000, training loss 0.0879511\n",
      "epoch 4,step 169500, training loss 0.0680005\n",
      "epoch 4,step 170000, training loss 0.0968508\n",
      "epoch 4,step 170500, training loss 0.256156\n",
      "epoch 4,step 171000, training loss 0.134399\n",
      "epoch 4,step 171500, training loss 0.119534\n",
      "epoch 4,step 172000, training loss 0.100144\n",
      "epoch 4,step 172500, training loss 0.0698093\n",
      "epoch 4,step 173000, training loss 0.0823594\n",
      "epoch 4,step 173500, training loss 0.0800228\n",
      "epoch 4,step 174000, training loss 0.0900455\n",
      "epoch 4,step 174500, training loss 0.0927513\n",
      "epoch 4,step 175000, training loss 0.0799881\n",
      "epoch 4,step 175500, training loss 0.227229\n",
      "epoch 4,step 176000, training loss 0.12126\n",
      "epoch 4,step 176500, training loss 0.104028\n",
      "epoch 4,step 177000, training loss 0.0806544\n",
      "epoch 4,step 177500, training loss 0.0956797\n",
      "epoch 4,step 178000, training loss 0.07717\n",
      "epoch 4,step 178500, training loss 0.0886809\n",
      "epoch 4,step 179000, training loss 0.0709225\n",
      "epoch 4,step 179500, training loss 0.0601469\n",
      "epoch 4,step 180000, training loss 0.0664414\n",
      "epoch 4,step 180500, training loss 0.252917\n",
      "epoch 4,step 181000, training loss 0.133526\n",
      "epoch 4,step 181500, training loss 0.0968471\n",
      "epoch 4,step 182000, training loss 0.0774494\n",
      "epoch 4,step 182500, training loss 0.103602\n",
      "epoch 4,step 183000, training loss 0.0999292\n",
      "epoch 4,step 183500, training loss 0.10589\n",
      "epoch 4,step 184000, training loss 0.0912501\n",
      "epoch 4,step 184500, training loss 0.0910559\n",
      "epoch 4,step 185000, training loss 0.113959\n",
      "epoch 4,step 185500, training loss 0.279197\n",
      "epoch 4,step 186000, training loss 0.137505\n",
      "epoch 4,step 186500, training loss 0.104016\n",
      "epoch 4,step 187000, training loss 0.119884\n",
      "epoch 4,step 187500, training loss 0.0872748\n",
      "epoch 4,step 188000, training loss 0.088232\n",
      "epoch 4,step 188500, training loss 0.0918796\n",
      "epoch 4,step 189000, training loss 0.101259\n",
      "epoch 4,step 189500, training loss 0.0902117\n",
      "epoch 4,step 190000, training loss 0.0965926\n",
      "epoch 4,step 190500, training loss 0.255182\n",
      "epoch 4,step 191000, training loss 0.221915\n",
      "epoch 4,step 191500, training loss 0.118025\n",
      "epoch 4,step 192000, training loss 0.0972569\n",
      "epoch 4,step 192500, training loss 0.0853729\n",
      "epoch 4,step 193000, training loss 0.108171\n",
      "epoch 4,step 193500, training loss 0.095958\n",
      "epoch 4,step 194000, training loss 0.0868777\n",
      "epoch 4,step 194500, training loss 0.108698\n",
      "epoch 4,step 195000, training loss 0.0818127\n",
      "epoch 4,step 195500, training loss 0.240787\n",
      "epoch 4,step 196000, training loss 0.133012\n",
      "epoch 4,step 196500, training loss 0.07569\n",
      "epoch 4,step 197000, training loss 0.0817247\n",
      "epoch 4,step 197500, training loss 0.0875366\n",
      "epoch 4,step 198000, training loss 0.0719276\n",
      "epoch 4,step 198500, training loss 0.0810589\n",
      "epoch 4,step 199000, training loss 0.109729\n",
      "epoch 4,step 199500, training loss 0.0700806\n",
      "epoch 4,step 200000, training loss 0.0663246\n",
      "epoch 4,step 200500, training loss 0.297354\n",
      "epoch 4,step 201000, training loss 0.209408\n",
      "epoch 4,step 201500, training loss 0.072591\n",
      "epoch 4,step 202000, training loss 0.0914537\n",
      "epoch 4,step 202500, training loss 0.0701372\n",
      "epoch 4,step 203000, training loss 0.0771048\n",
      "epoch 4,step 203500, training loss 0.0766087\n",
      "epoch 4,step 204000, training loss 0.0911222\n",
      "epoch 4,step 204500, training loss 0.0878679\n",
      "epoch 4,step 205000, training loss 0.0885795\n",
      "epoch 4,step 205500, training loss 0.238516\n",
      "epoch 4,step 206000, training loss 0.128146\n",
      "epoch 4,step 206500, training loss 0.0869926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4,step 207000, training loss 0.0911806\n",
      "epoch 4,step 207500, training loss 0.079505\n",
      "epoch 4,step 208000, training loss 0.0980342\n",
      "epoch 4,step 208500, training loss 0.0958656\n",
      "epoch 4,step 209000, training loss 0.0909675\n",
      "epoch 4,step 209500, training loss 0.0674962\n",
      "epoch 4,step 210000, training loss 0.0790835\n",
      "epoch 4,step 210500, training loss 0.262287\n",
      "epoch 4,step 211000, training loss 0.114961\n",
      "epoch 4,step 211500, training loss 0.0515051\n",
      "epoch 4,step 212000, training loss 0.0791433\n",
      "epoch 4,step 212500, training loss 0.107851\n",
      "epoch 4,step 213000, training loss 0.0976375\n",
      "epoch 4,step 213500, training loss 0.107181\n",
      "epoch 4,step 214000, training loss 0.104764\n",
      "epoch 4,step 214500, training loss 0.0675984\n",
      "epoch 4,step 215000, training loss 0.0835577\n",
      "epoch 4,step 215500, training loss 0.237435\n",
      "epoch 4,step 216000, training loss 0.164201\n",
      "epoch 4,step 216500, training loss 0.101999\n",
      "epoch 4,step 217000, training loss 0.0785978\n",
      "epoch 4,step 217500, training loss 0.0934426\n",
      "epoch 4,step 218000, training loss 0.0652794\n",
      "epoch 4,step 218500, training loss 0.0811607\n",
      "epoch 4,step 219000, training loss 0.0844612\n",
      "epoch 4,step 219500, training loss 0.0825987\n",
      "epoch 4,step 220000, training loss 0.0847169\n",
      "epoch 4,step 220500, training loss 0.244694\n",
      "epoch 4,step 221000, training loss 0.120809\n",
      "epoch 4,step 221500, training loss 0.0854083\n",
      "epoch 4,step 222000, training loss 0.0739412\n",
      "epoch 4,step 222500, training loss 0.0601876\n",
      "epoch 4,step 223000, training loss 0.0635372\n",
      "epoch 4,step 223500, training loss 0.0927836\n",
      "epoch 4,step 224000, training loss 0.0790085\n",
      "epoch 4,step 224500, training loss 0.0895784\n",
      "epoch 4,step 225000, training loss 0.075852\n",
      "epoch 4,step 225500, training loss 0.24984\n",
      "epoch 4,step 226000, training loss 0.121184\n",
      "epoch 4,step 226500, training loss 0.0922483\n",
      "epoch 4,step 227000, training loss 0.0826941\n",
      "epoch 4,step 227500, training loss 0.0698937\n",
      "epoch 4,step 228000, training loss 0.0592011\n",
      "epoch 4,step 228500, training loss 0.0757299\n",
      "epoch 4,step 229000, training loss 0.0868485\n",
      "epoch 4,step 229500, training loss 0.0659305\n",
      "epoch 4,step 230000, training loss 0.0808929\n",
      "epoch 4,step 230500, training loss 0.267524\n",
      "epoch 4,step 231000, training loss 0.148949\n",
      "epoch 4,step 231500, training loss 0.0952061\n",
      "epoch 4,step 232000, training loss 0.10578\n",
      "epoch 4,step 232500, training loss 0.104558\n",
      "epoch 4,step 233000, training loss 0.0816162\n",
      "epoch 4,step 233500, training loss 0.11131\n",
      "epoch 4,step 234000, training loss 0.0849048\n",
      "epoch 4,step 234500, training loss 0.0734373\n",
      "epoch 4,step 235000, training loss 0.0839476\n",
      "epoch 4,step 235500, training loss 0.246242\n",
      "epoch 4,step 236000, training loss 0.228121\n",
      "epoch 4,step 236500, training loss 0.0826535\n",
      "epoch 4,step 237000, training loss 0.0871857\n",
      "epoch 4,step 237500, training loss 0.0595593\n",
      "epoch 4,step 238000, training loss 0.123781\n",
      "epoch 4,step 238500, training loss 0.0803744\n",
      "epoch 4,step 239000, training loss 0.085536\n",
      "epoch 4,step 239500, training loss 0.0743791\n",
      "epoch 4,step 240000, training loss 0.0769667\n",
      "epoch 4,step 240500, training loss 0.199126\n",
      "epoch 4,step 241000, training loss 0.0973671\n",
      "epoch 4,step 241500, training loss 0.0873333\n",
      "epoch 4,step 242000, training loss 0.0861327\n",
      "epoch 4,step 242500, training loss 0.0826136\n",
      "epoch 4,step 243000, training loss 0.0866763\n",
      "epoch 4,step 243500, training loss 0.0650442\n",
      "epoch 4,step 244000, training loss 0.072036\n",
      "epoch 4,step 244500, training loss 0.0762076\n",
      "epoch 4,step 245000, training loss 0.0908278\n",
      "epoch 4,step 245500, training loss 0.255728\n",
      "epoch 4,step 246000, training loss 0.115695\n",
      "epoch 4,step 246500, training loss 0.060564\n",
      "epoch 4,step 247000, training loss 0.0993259\n",
      "epoch 4,step 247500, training loss 0.0931097\n",
      "epoch 4,step 248000, training loss 0.0803534\n",
      "epoch 4,step 248500, training loss 0.0802376\n",
      "epoch 4,step 249000, training loss 0.0798187\n",
      "epoch 4,step 249500, training loss 0.145247\n",
      "epoch 4,step 250000, training loss 0.0784101\n",
      "epoch 4,step 250500, training loss 0.26941\n",
      "epoch 4,step 251000, training loss 0.145548\n",
      "epoch 4,step 251500, training loss 0.0872044\n",
      "epoch 4,step 252000, training loss 0.0836718\n",
      "epoch 4,step 252500, training loss 0.0787543\n",
      "epoch 4,step 253000, training loss 0.0826836\n",
      "epoch 4,step 253500, training loss 0.066841\n",
      "epoch 4,step 254000, training loss 0.0884656\n",
      "epoch 4,step 254500, training loss 0.0970107\n",
      "epoch 4,step 255000, training loss 0.0647939\n",
      "epoch 4,step 255500, training loss 0.279702\n",
      "epoch 4,step 256000, training loss 0.209778\n",
      "epoch 4,step 256500, training loss 0.0752093\n",
      "epoch 4,step 257000, training loss 0.0722297\n",
      "epoch 4,step 257500, training loss 0.0942946\n",
      "epoch 4,step 258000, training loss 0.100303\n",
      "epoch 4,step 258500, training loss 0.0935835\n",
      "epoch 4,step 259000, training loss 0.0664983\n",
      "epoch 4,step 259500, training loss 0.0950316\n",
      "epoch 4,step 260000, training loss 0.0685947\n",
      "epoch 4,step 260500, training loss 0.2619\n",
      "epoch 4,step 261000, training loss 0.145467\n",
      "epoch 4,step 261500, training loss 0.0839051\n",
      "epoch 4,step 262000, training loss 0.0744923\n",
      "epoch 4,step 262500, training loss 0.0927927\n",
      "epoch 4,step 263000, training loss 0.0732201\n",
      "epoch 4,step 263500, training loss 0.0773402\n",
      "epoch 4,step 264000, training loss 0.0925385\n",
      "epoch 4,step 264500, training loss 0.128957\n",
      "epoch 4,step 265000, training loss 0.105614\n",
      "epoch 4,step 265500, training loss 0.309523\n",
      "epoch 4,step 266000, training loss 0.250466\n",
      "epoch 4,step 266500, training loss 0.109504\n",
      "epoch 4,step 267000, training loss 0.135475\n",
      "epoch 4,step 267500, training loss 0.0951729\n",
      "epoch 4,step 268000, training loss 0.0884504\n",
      "epoch 4,step 268500, training loss 0.0921441\n",
      "epoch 4,step 269000, training loss 0.0910646\n",
      "epoch 4,step 269500, training loss 0.0899468\n",
      "epoch 4,step 270000, training loss 0.0792626\n",
      "epoch 4,step 270500, training loss 0.228444\n",
      "epoch 4,step 271000, training loss 0.207381\n",
      "epoch 4,step 271500, training loss 0.123715\n",
      "epoch 4,step 272000, training loss 0.139063\n",
      "epoch 4,step 272500, training loss 0.0845715\n",
      "epoch 4,step 273000, training loss 0.0772143\n",
      "epoch 4,step 273500, training loss 0.0810581\n",
      "epoch 4,step 274000, training loss 0.102963\n",
      "epoch 4,step 274500, training loss 0.0830321\n",
      "epoch 4,step 275000, training loss 0.114915\n",
      "epoch 4,step 275500, training loss 0.22718\n",
      "epoch 4,step 276000, training loss 0.12941\n",
      "epoch 4,step 276500, training loss 0.0941001\n",
      "epoch 4,step 277000, training loss 0.0935708\n",
      "epoch 4,step 277500, training loss 0.105372\n",
      "epoch 4,step 278000, training loss 0.10908\n",
      "epoch 4,step 278500, training loss 0.0744531\n",
      "epoch 4,step 279000, training loss 0.111406\n",
      "epoch 4,step 279500, training loss 0.0850447\n",
      "epoch 4,step 280000, training loss 0.155054\n",
      "epoch 4,step 280500, training loss 0.235892\n",
      "epoch 4,step 281000, training loss 0.165404\n",
      "epoch 4,step 281500, training loss 0.121609\n",
      "epoch 4,step 282000, training loss 0.0897939\n",
      "epoch 4,step 282500, training loss 0.0835754\n",
      "epoch 4,step 283000, training loss 0.0846997\n",
      "epoch 4,step 283500, training loss 0.0828481\n",
      "epoch 4,step 284000, training loss 0.0801042\n",
      "epoch 4,step 284500, training loss 0.0866987\n",
      "epoch 4,step 285000, training loss 0.0669425\n",
      "epoch 4,step 285500, training loss 0.249297\n",
      "epoch 4,step 286000, training loss 0.147403\n",
      "epoch 4,step 286500, training loss 0.113177\n",
      "epoch 4,step 287000, training loss 0.105406\n",
      "epoch 4,step 287500, training loss 0.133057\n",
      "epoch 4,step 288000, training loss 0.103611\n",
      "epoch 4,step 288500, training loss 0.104502\n",
      "epoch 4,step 289000, training loss 0.122257\n",
      "epoch 4,step 289500, training loss 0.0769906\n",
      "epoch 4,step 290000, training loss 0.0754762\n",
      "epoch 4,step 290500, training loss 0.253456\n",
      "epoch 4,step 291000, training loss 0.136605\n",
      "epoch 4,step 291500, training loss 0.0873605\n",
      "epoch 4,step 292000, training loss 0.0834921\n",
      "epoch 4,step 292500, training loss 0.0810874\n",
      "epoch 4,step 293000, training loss 0.0923358\n",
      "epoch 4,step 293500, training loss 0.0791309\n",
      "epoch 4,step 294000, training loss 0.109277\n",
      "epoch 4,step 294500, training loss 0.0818987\n",
      "epoch 4,step 295000, training loss 0.0676444\n",
      "epoch 4,step 295500, training loss 0.24672\n",
      "epoch 4,step 296000, training loss 0.104666\n",
      "epoch 4,step 296500, training loss 0.0897844\n",
      "epoch 4,step 297000, training loss 0.100049\n",
      "epoch 4,step 297500, training loss 0.0878803\n",
      "epoch 4,step 298000, training loss 0.0828153\n",
      "epoch 4,step 298500, training loss 0.0610352\n",
      "epoch 4,step 299000, training loss 0.0740411\n",
      "epoch 4,step 299500, training loss 0.0871056\n",
      "epoch 4,step 300000, training loss 0.109805\n",
      "epoch 4,step 300500, training loss 0.289252\n",
      "epoch 4,step 301000, training loss 0.122852\n",
      "epoch 4,step 301500, training loss 0.105288\n",
      "epoch 4,step 302000, training loss 0.137422\n",
      "epoch 4,step 302500, training loss 0.070634\n",
      "epoch 4,step 303000, training loss 0.0936844\n",
      "epoch 4,step 303500, training loss 0.0810982\n",
      "epoch 4,step 304000, training loss 0.0611072\n",
      "epoch 4,step 304500, training loss 0.0790101\n",
      "epoch 4,step 305000, training loss 0.0639008\n",
      "epoch 4,step 305500, training loss 0.275973\n",
      "epoch 4,step 306000, training loss 0.158712\n",
      "epoch 4,step 306500, training loss 0.0843918\n",
      "epoch 4,step 307000, training loss 0.0639381\n",
      "epoch 4,step 307500, training loss 0.0745385\n",
      "epoch 4,step 308000, training loss 0.0832043\n",
      "epoch 4,step 308500, training loss 0.0692498\n",
      "epoch 4,step 309000, training loss 0.0674028\n",
      "epoch 4,step 309500, training loss 0.063613\n",
      "epoch 4,step 310000, training loss 0.096029\n",
      "epoch 4,step 310500, training loss 0.300779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4,step 311000, training loss 0.250559\n",
      "epoch 4,step 311500, training loss 0.105867\n",
      "epoch 4,step 312000, training loss 0.133403\n",
      "epoch 4,step 312500, training loss 0.107977\n",
      "epoch 4,step 313000, training loss 0.116467\n",
      "epoch 4,step 313500, training loss 0.063977\n",
      "epoch 4,step 314000, training loss 0.0838345\n",
      "epoch 4,step 314500, training loss 0.0659644\n",
      "epoch 4,step 315000, training loss 0.0929638\n",
      "epoch 4,step 315500, training loss 0.247179\n",
      "epoch 4,step 316000, training loss 0.137424\n",
      "epoch 4,step 316500, training loss 0.0772625\n",
      "epoch 4,step 317000, training loss 0.0911778\n",
      "epoch 4,step 317500, training loss 0.0735274\n",
      "epoch 4,step 318000, training loss 0.0767507\n",
      "epoch 4,step 318500, training loss 0.0790579\n",
      "epoch 4,step 319000, training loss 0.108951\n",
      "epoch 4,step 319500, training loss 0.0838299\n",
      "epoch 4,step 320000, training loss 0.0822514\n",
      "epoch 4,step 320500, training loss 0.333137\n",
      "epoch 4,step 321000, training loss 0.139649\n",
      "epoch 4,step 321500, training loss 0.0705349\n",
      "epoch 4,step 322000, training loss 0.0815566\n",
      "epoch 4,step 322500, training loss 0.0923422\n",
      "epoch 4,step 323000, training loss 0.0989279\n",
      "epoch 4,step 323500, training loss 0.175338\n",
      "epoch 4,step 324000, training loss 0.0657025\n",
      "epoch 4,step 324500, training loss 0.0651303\n",
      "epoch 4,step 325000, training loss 0.0848507\n",
      "epoch 4,step 325500, training loss 0.265762\n",
      "epoch 4,step 326000, training loss 0.107413\n",
      "epoch 4,step 326500, training loss 0.0592272\n",
      "epoch 4,step 327000, training loss 0.0756699\n",
      "epoch 4,step 327500, training loss 0.0757362\n",
      "epoch 4,step 328000, training loss 0.0769545\n",
      "epoch 4,step 328500, training loss 0.0623062\n",
      "epoch 4,step 329000, training loss 0.089208\n",
      "epoch 4,step 329500, training loss 0.118366\n",
      "epoch 4,step 330000, training loss 0.068141\n",
      "epoch 4,step 330500, training loss 0.278892\n",
      "epoch 4,step 331000, training loss 0.144789\n",
      "epoch 4,step 331500, training loss 0.0596702\n",
      "epoch 4,step 332000, training loss 0.0761739\n",
      "epoch 4,step 332500, training loss 0.0677168\n",
      "epoch 4,step 333000, training loss 0.0753103\n",
      "epoch 4,step 333500, training loss 0.0808066\n",
      "epoch 4,step 334000, training loss 0.0905112\n",
      "epoch 4,step 334500, training loss 0.0798581\n",
      "epoch 4,step 335000, training loss 0.0881121\n",
      "epoch 4,step 335500, training loss 0.233539\n",
      "epoch 4,step 336000, training loss 0.214408\n",
      "epoch 4,step 336500, training loss 0.0751507\n",
      "epoch 4,step 337000, training loss 0.111822\n",
      "epoch 4,step 337500, training loss 0.0618652\n",
      "epoch 4,step 338000, training loss 0.107799\n",
      "epoch 4,step 338500, training loss 0.0904516\n",
      "epoch 4,step 339000, training loss 0.0779322\n",
      "epoch 4,step 339500, training loss 0.0664711\n",
      "epoch 4,step 340000, training loss 0.0784809\n",
      "epoch 4,step 340500, training loss 0.259513\n",
      "epoch 4,step 341000, training loss 0.119873\n",
      "epoch 4,step 341500, training loss 0.0836753\n",
      "epoch 4,step 342000, training loss 0.0858041\n",
      "epoch 4,step 342500, training loss 0.06278\n",
      "epoch 4,step 343000, training loss 0.074907\n",
      "epoch 4,step 343500, training loss 0.0727768\n",
      "epoch 4,step 344000, training loss 0.08776\n",
      "epoch 4,step 344500, training loss 0.0714528\n",
      "epoch 4,step 345000, training loss 0.0847515\n",
      "epoch 4,step 345500, training loss 0.245327\n",
      "epoch 4,step 346000, training loss 0.127922\n",
      "epoch 4,step 346500, training loss 0.0629411\n",
      "epoch 4,step 347000, training loss 0.0666087\n",
      "epoch 4,step 347500, training loss 0.0659381\n",
      "epoch 4,step 348000, training loss 0.0689504\n",
      "epoch 4,step 348500, training loss 0.0707695\n",
      "epoch 4,step 349000, training loss 0.0916242\n",
      "epoch 4,step 349500, training loss 0.0709069\n",
      "epoch 4,step 350000, training loss 0.0637869\n",
      "epoch 4,step 350500, training loss 0.226403\n",
      "epoch 4,step 351000, training loss 0.153847\n",
      "epoch 4,step 351500, training loss 0.0858544\n",
      "epoch 4,step 352000, training loss 0.0683243\n",
      "epoch 4,step 352500, training loss 0.083548\n",
      "epoch 4,step 353000, training loss 0.0752182\n",
      "epoch 4,step 353500, training loss 0.0737635\n",
      "epoch 4,step 354000, training loss 0.0547425\n",
      "epoch 4,step 354500, training loss 0.0679014\n",
      "epoch 4,step 355000, training loss 0.0760818\n",
      "epoch 4,step 355500, training loss 0.243975\n",
      "epoch 4,step 356000, training loss 0.114935\n",
      "epoch 4,step 356500, training loss 0.0721089\n",
      "epoch 4,step 357000, training loss 0.0646327\n",
      "epoch 4,step 357500, training loss 0.0783955\n",
      "epoch 4,step 358000, training loss 0.0724628\n",
      "epoch 4,step 358500, training loss 0.0893738\n",
      "epoch 4,step 359000, training loss 0.0999523\n",
      "epoch 4,step 359500, training loss 0.0639448\n",
      "epoch 4,step 360000, training loss 0.0634216\n",
      "epoch 4,step 360500, training loss 0.217252\n",
      "epoch 4,step 361000, training loss 0.111502\n",
      "epoch 4,step 361500, training loss 0.0964897\n",
      "epoch 4,step 362000, training loss 0.0809517\n",
      "epoch 4,step 362500, training loss 0.085938\n",
      "epoch 4,step 363000, training loss 0.080096\n",
      "epoch 4,step 363500, training loss 0.108641\n",
      "epoch 4,step 364000, training loss 0.0785597\n",
      "epoch 4,step 364500, training loss 0.0654619\n",
      "epoch 4,step 365000, training loss 0.0805362\n",
      "epoch 4,step 365500, training loss 0.253316\n",
      "epoch 4,step 366000, training loss 0.12272\n",
      "epoch 4,step 366500, training loss 0.078972\n",
      "epoch 4,step 367000, training loss 0.102035\n",
      "epoch 4,step 367500, training loss 0.0956763\n",
      "epoch 4,step 368000, training loss 0.0867538\n",
      "epoch 4,step 368500, training loss 0.0709612\n",
      "epoch 4,step 369000, training loss 0.083497\n",
      "epoch 4,step 369500, training loss 0.125845\n",
      "epoch 4,step 370000, training loss 0.110023\n",
      "epoch 4,step 370500, training loss 0.209835\n",
      "epoch 4,step 371000, training loss 0.0961266\n",
      "epoch 4,step 371500, training loss 0.0650307\n",
      "epoch 4,step 372000, training loss 0.065481\n",
      "epoch 4,step 372500, training loss 0.0678471\n",
      "epoch 4,step 373000, training loss 0.083194\n",
      "epoch 4,step 373500, training loss 0.0704353\n",
      "epoch 4,step 374000, training loss 0.0602418\n",
      "epoch 4,step 374500, training loss 0.0672621\n",
      "epoch 4,step 375000, training loss 0.15779\n",
      "epoch 4,step 375500, training loss 0.25978\n",
      "epoch 4,step 376000, training loss 0.109707\n",
      "epoch 4,step 376500, training loss 0.0664617\n",
      "epoch 4,step 377000, training loss 0.0696434\n",
      "epoch 4,step 377500, training loss 0.0706051\n",
      "epoch 4,step 378000, training loss 0.0730935\n",
      "epoch 4,step 378500, training loss 0.0803537\n",
      "epoch 4,step 379000, training loss 0.0733794\n",
      "epoch 4,step 379500, training loss 0.0467862\n",
      "epoch 4,step 380000, training loss 0.0751763\n",
      "epoch 4,step 380500, training loss 0.226888\n",
      "epoch 4,step 381000, training loss 0.162662\n",
      "epoch 4,step 381500, training loss 0.0750845\n",
      "epoch 4,step 382000, training loss 0.0673334\n",
      "epoch 4,step 382500, training loss 0.11014\n",
      "epoch 4,step 383000, training loss 0.120696\n",
      "epoch 4,step 383500, training loss 0.0706469\n",
      "epoch 4,step 384000, training loss 0.0993802\n",
      "epoch 4,step 384500, training loss 0.0717174\n",
      "epoch 4,step 385000, training loss 0.0579683\n",
      "epoch 4,step 385500, training loss 0.295529\n",
      "epoch 4,step 386000, training loss 0.178178\n",
      "epoch 4,step 386500, training loss 0.0733212\n",
      "epoch 4,step 387000, training loss 0.0822156\n",
      "epoch 4,step 387500, training loss 0.0797774\n",
      "epoch 4,step 388000, training loss 0.063605\n",
      "epoch 4,step 388500, training loss 0.0664413\n",
      "epoch 4,step 389000, training loss 0.0800006\n",
      "epoch 4,step 389500, training loss 0.074619\n",
      "epoch 4,step 390000, training loss 0.0616552\n",
      "epoch 4,step 390500, training loss 0.198914\n",
      "epoch 4,step 391000, training loss 0.159605\n",
      "epoch 4,step 391500, training loss 0.0757792\n",
      "epoch 4,step 392000, training loss 0.0515638\n",
      "epoch 4,step 392500, training loss 0.0677308\n",
      "epoch 4,step 393000, training loss 0.0692691\n",
      "epoch 4,step 393500, training loss 0.120355\n",
      "epoch 4,step 394000, training loss 0.0634669\n",
      "epoch 4,step 394500, training loss 0.0604755\n",
      "epoch 4,step 395000, training loss 0.0701006\n",
      "epoch 4,step 395500, training loss 0.262299\n",
      "epoch 4,step 396000, training loss 0.13519\n",
      "epoch 4,step 396500, training loss 0.11955\n",
      "epoch 4,step 397000, training loss 0.0942716\n",
      "epoch 4,step 397500, training loss 0.0713095\n",
      "epoch 4,step 398000, training loss 0.0833768\n",
      "epoch 4,step 398500, training loss 0.057741\n",
      "epoch 4,step 399000, training loss 0.0708213\n",
      "epoch 4,step 399500, training loss 0.0604683\n",
      "epoch 4,step 400000, training loss 0.0678767\n",
      "epoch 4,step 400500, training loss 0.255998\n",
      "epoch 4,step 401000, training loss 0.103696\n",
      "epoch 4,step 401500, training loss 0.0796007\n",
      "epoch 4,step 402000, training loss 0.0854237\n",
      "epoch 4,step 402500, training loss 0.0730772\n",
      "epoch 4,step 403000, training loss 0.100538\n",
      "epoch 4,step 403500, training loss 0.0760269\n",
      "epoch 4,step 404000, training loss 0.0776554\n",
      "epoch 4,step 404500, training loss 0.0648768\n",
      "epoch 4,step 405000, training loss 0.066192\n",
      "epoch 4,step 405500, training loss 0.223171\n",
      "epoch 4,step 406000, training loss 0.168276\n",
      "epoch 4,step 406500, training loss 0.0729118\n",
      "epoch 4,step 407000, training loss 0.0734885\n",
      "epoch 4,step 407500, training loss 0.0866736\n",
      "epoch 4,step 408000, training loss 0.0711492\n",
      "epoch 4,step 408500, training loss 0.0743221\n",
      "epoch 4,step 409000, training loss 0.0857301\n",
      "epoch 4,step 409500, training loss 0.064637\n",
      "epoch 4,step 410000, training loss 0.0777803\n",
      "epoch 4,step 410500, training loss 0.24829\n",
      "epoch 4,step 411000, training loss 0.0908795\n",
      "epoch 4,step 411500, training loss 0.0615759\n",
      "epoch 4,step 412000, training loss 0.0758747\n",
      "epoch 4,step 412500, training loss 0.0900619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4,step 413000, training loss 0.0839752\n",
      "epoch 4,step 413500, training loss 0.0884221\n",
      "epoch 4,step 414000, training loss 0.116601\n",
      "epoch 4,step 414500, training loss 0.0823741\n",
      "epoch 4,step 415000, training loss 0.0750352\n",
      "epoch 4,step 415500, training loss 0.232379\n",
      "epoch 4,step 416000, training loss 0.146589\n",
      "epoch 4,step 416500, training loss 0.0768452\n",
      "epoch 4,step 417000, training loss 0.0747541\n",
      "epoch 4,step 417500, training loss 0.0736348\n",
      "epoch 4,step 418000, training loss 0.093455\n",
      "epoch 4,step 418500, training loss 0.0692364\n",
      "epoch 4,step 419000, training loss 0.0667018\n",
      "epoch 4,step 419500, training loss 0.0898247\n",
      "epoch 4,step 420000, training loss 0.0674349\n",
      "epoch 4,step 420500, training loss 0.224755\n",
      "epoch 4,step 421000, training loss 0.147762\n",
      "epoch 4,step 421500, training loss 0.0736708\n",
      "epoch 4,step 422000, training loss 0.0552275\n",
      "epoch 4,step 422500, training loss 0.0830048\n",
      "epoch 4,step 423000, training loss 0.0963117\n",
      "epoch 4,step 423500, training loss 0.103985\n",
      "epoch 4,step 424000, training loss 0.050612\n",
      "epoch 4,step 424500, training loss 0.0673451\n",
      "epoch 4,step 425000, training loss 0.0824206\n",
      "epoch 4,step 425500, training loss 0.224623\n",
      "epoch 4,step 426000, training loss 0.122374\n",
      "epoch 4,step 426500, training loss 0.0947662\n",
      "epoch 4,step 427000, training loss 0.095906\n",
      "epoch 4,step 427500, training loss 0.0958706\n",
      "epoch 4,step 428000, training loss 0.0843776\n",
      "epoch 4,step 428500, training loss 0.0733996\n",
      "epoch 4,step 429000, training loss 0.0759377\n",
      "epoch 4,step 429500, training loss 0.0918054\n",
      "epoch 4,step 430000, training loss 0.0931201\n",
      "epoch 4,step 430500, training loss 0.205834\n",
      "epoch 4,step 431000, training loss 0.143325\n",
      "epoch 4,step 431500, training loss 0.0622407\n",
      "epoch 4,step 432000, training loss 0.0736634\n",
      "epoch 4,step 432500, training loss 0.0650962\n",
      "epoch 4,step 433000, training loss 0.0597555\n",
      "epoch 4,step 433500, training loss 0.0790412\n",
      "epoch 4,step 434000, training loss 0.069493\n",
      "epoch 4,step 434500, training loss 0.0848902\n",
      "epoch 4,step 435000, training loss 0.0916958\n",
      "epoch 4,step 435500, training loss 0.2207\n",
      "epoch 4,step 436000, training loss 0.112274\n",
      "epoch 4,step 436500, training loss 0.0651308\n",
      "epoch 4,step 437000, training loss 0.0808412\n",
      "epoch 4,step 437500, training loss 0.0678424\n",
      "epoch 4,step 438000, training loss 0.0827301\n",
      "epoch 4,step 438500, training loss 0.0802797\n",
      "epoch 4,step 439000, training loss 0.0732421\n",
      "epoch 4,step 439500, training loss 0.0513801\n",
      "epoch 4,step 440000, training loss 0.074097\n",
      "epoch 4,step 440500, training loss 0.290776\n",
      "epoch 4,step 441000, training loss 0.169953\n",
      "epoch 4,step 441500, training loss 0.0912866\n",
      "epoch 4,step 442000, training loss 0.0525672\n",
      "epoch 4,step 442500, training loss 0.0731477\n",
      "epoch 4,step 443000, training loss 0.0682178\n",
      "epoch 4,step 443500, training loss 0.0660431\n",
      "epoch 4,step 444000, training loss 0.0631561\n",
      "epoch 4,step 444500, training loss 0.109708\n",
      "epoch 4,step 445000, training loss 0.0577176\n",
      "epoch 4,step 445500, training loss 0.26011\n",
      "epoch 4,step 446000, training loss 0.179934\n",
      "epoch 4,step 446500, training loss 0.086692\n",
      "epoch 4,step 447000, training loss 0.0793835\n",
      "epoch 4,step 447500, training loss 0.0824157\n",
      "epoch 4,step 448000, training loss 0.099663\n",
      "epoch 4,step 448500, training loss 0.0881306\n",
      "epoch 4,step 449000, training loss 0.0800517\n",
      "epoch 4,step 449500, training loss 0.0804118\n",
      "epoch 4,step 450000, training loss 0.127741\n",
      "epoch 4,step 450500, training loss 0.229838\n",
      "epoch 4,step 451000, training loss 0.0903423\n",
      "epoch 4,step 451500, training loss 0.0621677\n",
      "epoch 4,step 452000, training loss 0.11166\n",
      "epoch 4,step 452500, training loss 0.0710227\n",
      "epoch 4,step 453000, training loss 0.0704053\n",
      "epoch 4,step 453500, training loss 0.055279\n",
      "epoch 4,step 454000, training loss 0.0561377\n",
      "epoch 4,step 454500, training loss 0.0757535\n",
      "epoch 4,step 455000, training loss 0.0675202\n",
      "epoch 4,step 455500, training loss 0.210455\n",
      "epoch 4,step 456000, training loss 0.112205\n",
      "epoch 4,step 456500, training loss 0.0785306\n",
      "epoch 4,step 457000, training loss 0.0792163\n",
      "epoch 4,step 457500, training loss 0.0899922\n",
      "epoch 4,step 458000, training loss 0.0754362\n",
      "epoch 4,step 458500, training loss 0.0706123\n",
      "epoch 4,step 459000, training loss 0.0796396\n",
      "epoch 4,step 459500, training loss 0.08413\n",
      "epoch 4,step 460000, training loss 0.0707548\n",
      "epoch 4,step 460500, training loss 0.201455\n",
      "epoch 4,step 461000, training loss 0.144644\n",
      "epoch 4,step 461500, training loss 0.0697264\n",
      "epoch 4,step 462000, training loss 0.0676069\n",
      "epoch 4,step 462500, training loss 0.0562282\n",
      "epoch 4,step 463000, training loss 0.0796305\n",
      "epoch 4,step 463500, training loss 0.0693805\n",
      "epoch 4,step 464000, training loss 0.0558963\n",
      "epoch 4,step 464500, training loss 0.0743164\n",
      "epoch 4,step 465000, training loss 0.062085\n",
      "epoch 4,step 465500, training loss 0.210491\n",
      "epoch 4,step 466000, training loss 0.10484\n",
      "epoch 4,step 466500, training loss 0.0715904\n",
      "epoch 4,step 467000, training loss 0.0865861\n",
      "epoch 4,step 467500, training loss 0.0916048\n",
      "epoch 4,step 468000, training loss 0.0753199\n",
      "epoch 4,step 468500, training loss 0.0644983\n",
      "epoch 4,step 469000, training loss 0.0899681\n",
      "epoch 4,step 469500, training loss 0.0664401\n",
      "epoch 4,step 470000, training loss 0.059143\n",
      "epoch 4,step 470500, training loss 0.231142\n",
      "epoch 4,step 471000, training loss 0.142088\n",
      "epoch 4,step 471500, training loss 0.0609144\n",
      "epoch 4,step 472000, training loss 0.0812013\n",
      "epoch 4,step 472500, training loss 0.0723694\n",
      "epoch 4,step 473000, training loss 0.0862546\n",
      "epoch 4,step 473500, training loss 0.0927681\n",
      "epoch 4,step 474000, training loss 0.0614009\n",
      "epoch 4,step 474500, training loss 0.0795272\n",
      "epoch 4,step 475000, training loss 0.0536228\n",
      "epoch 4,step 475500, training loss 0.235063\n",
      "epoch 4,step 476000, training loss 0.179625\n",
      "epoch 4,step 476500, training loss 0.0632864\n",
      "epoch 4,step 477000, training loss 0.0567422\n",
      "epoch 4,step 477500, training loss 0.0623329\n",
      "epoch 4,step 478000, training loss 0.0878806\n",
      "epoch 4,step 478500, training loss 0.0818042\n",
      "epoch 4,step 479000, training loss 0.0959213\n",
      "epoch 4,step 479500, training loss 0.0715615\n",
      "epoch 4,step 480000, training loss 0.0878959\n",
      "epoch 4,step 480500, training loss 0.244727\n",
      "epoch 4,step 481000, training loss 0.123456\n",
      "epoch 4,step 481500, training loss 0.0506946\n",
      "epoch 4,step 482000, training loss 0.0620792\n",
      "epoch 4,step 482500, training loss 0.0643755\n",
      "epoch 4,step 483000, training loss 0.0676124\n",
      "epoch 4,step 483500, training loss 0.0628002\n",
      "epoch 4,step 484000, training loss 0.0679966\n",
      "epoch 4,step 484500, training loss 0.063766\n",
      "epoch 4,step 485000, training loss 0.0706914\n",
      "epoch 4,step 485500, training loss 0.226\n",
      "epoch 4,step 486000, training loss 0.122135\n",
      "epoch 4,step 486500, training loss 0.0633182\n",
      "epoch 4,step 487000, training loss 0.064302\n",
      "epoch 4,step 487500, training loss 0.0863073\n",
      "epoch 4,step 488000, training loss 0.0571189\n",
      "epoch 4,step 488500, training loss 0.06354\n",
      "epoch 4,step 489000, training loss 0.0695486\n",
      "epoch 4,step 489500, training loss 0.0671591\n",
      "epoch 4,step 490000, training loss 0.0649025\n",
      "epoch 4,step 490500, training loss 0.22974\n",
      "epoch 4,step 491000, training loss 0.123314\n",
      "epoch 4,step 491500, training loss 0.100699\n",
      "epoch 4,step 492000, training loss 0.0728224\n",
      "epoch 4,step 492500, training loss 0.0550453\n",
      "epoch 4,step 493000, training loss 0.123619\n",
      "epoch 4,step 493500, training loss 0.0729471\n",
      "epoch 4,step 494000, training loss 0.0774783\n",
      "epoch 4,step 494500, training loss 0.0595218\n",
      "epoch 4,step 495000, training loss 0.0618593\n",
      "epoch 4,step 495500, training loss 0.218994\n",
      "epoch 4,step 496000, training loss 0.103155\n",
      "epoch 4,step 496500, training loss 0.0730222\n",
      "epoch 4,step 497000, training loss 0.0520782\n",
      "epoch 4,step 497500, training loss 0.0544666\n",
      "epoch 4,step 498000, training loss 0.0657035\n",
      "epoch 4,step 498500, training loss 0.0455816\n",
      "epoch 4,step 499000, training loss 0.067961\n",
      "epoch 4,step 499500, training loss 0.099403\n",
      "epoch 4,training loss 0.099403 ,test loss 0.0957015\n",
      "epoch 5,step 3000, training loss 0.0916515\n",
      "epoch 5,step 6000, training loss 0.111066\n",
      "epoch 5,step 9000, training loss 0.111474\n",
      "epoch 5,step 12000, training loss 0.0583174\n",
      "epoch 5,step 15000, training loss 0.0585345\n",
      "epoch 5,step 18000, training loss 0.0741547\n",
      "epoch 5,step 21000, training loss 0.0707183\n",
      "epoch 5,step 24000, training loss 0.0665976\n",
      "epoch 5,step 27000, training loss 0.072028\n",
      "epoch 5,step 30000, training loss 0.068455\n",
      "epoch 5,step 33000, training loss 0.0670957\n",
      "epoch 5,step 36000, training loss 0.0867642\n",
      "epoch 5,step 39000, training loss 0.130337\n",
      "epoch 5,step 42000, training loss 0.0810931\n",
      "epoch 5,step 45000, training loss 0.0957909\n",
      "epoch 5,step 48000, training loss 0.0723146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5,step 51000, training loss 0.0647812\n",
      "epoch 5,step 54000, training loss 0.0636728\n",
      "epoch 5,step 57000, training loss 0.0482601\n",
      "epoch 5,step 60000, training loss 0.0751003\n",
      "epoch 5,step 63000, training loss 0.0682192\n",
      "epoch 5,step 66000, training loss 0.0727318\n",
      "epoch 5,step 69000, training loss 0.0846692\n",
      "epoch 5,step 72000, training loss 0.0834988\n",
      "epoch 5,step 75000, training loss 0.0784539\n",
      "epoch 5,step 78000, training loss 0.0554715\n",
      "epoch 5,step 81000, training loss 0.103179\n",
      "epoch 5,step 84000, training loss 0.0641639\n",
      "epoch 5,step 87000, training loss 0.0771049\n",
      "epoch 5,step 90000, training loss 0.0662212\n",
      "epoch 5,step 93000, training loss 0.10593\n",
      "epoch 5,step 96000, training loss 0.0903073\n",
      "epoch 5,step 99000, training loss 0.0748534\n",
      "epoch 5,step 102000, training loss 0.0753815\n",
      "epoch 5,step 105000, training loss 0.0998048\n",
      "epoch 5,step 108000, training loss 0.0627195\n",
      "epoch 5,step 111000, training loss 0.0877845\n",
      "epoch 5,step 114000, training loss 0.106056\n",
      "epoch 5,step 117000, training loss 0.079629\n",
      "epoch 5,step 120000, training loss 0.06611\n",
      "epoch 5,step 123000, training loss 0.0768289\n",
      "epoch 5,step 126000, training loss 0.0723082\n",
      "epoch 5,step 129000, training loss 0.0709407\n",
      "epoch 5,step 132000, training loss 0.0725458\n",
      "epoch 5,step 135000, training loss 0.064626\n",
      "epoch 5,step 138000, training loss 0.0765938\n",
      "epoch 5,step 141000, training loss 0.0516818\n",
      "epoch 5,step 144000, training loss 0.0600152\n",
      "epoch 5,step 147000, training loss 0.0682367\n",
      "epoch 5,step 150000, training loss 0.0651233\n",
      "epoch 5,step 153000, training loss 0.0646928\n",
      "epoch 5,step 156000, training loss 0.0855215\n",
      "epoch 5,step 159000, training loss 0.055627\n",
      "epoch 5,step 162000, training loss 0.0679368\n",
      "epoch 5,step 165000, training loss 0.101712\n",
      "epoch 5,step 168000, training loss 0.0627285\n",
      "epoch 5,step 171000, training loss 0.0647939\n",
      "epoch 5,step 174000, training loss 0.0674316\n",
      "epoch 5,step 177000, training loss 0.0780306\n",
      "epoch 5,step 180000, training loss 0.057029\n",
      "epoch 5,step 183000, training loss 0.0734296\n",
      "epoch 5,step 186000, training loss 0.0648606\n",
      "epoch 5,step 189000, training loss 0.0567768\n",
      "epoch 5,step 192000, training loss 0.0896558\n",
      "epoch 5,step 195000, training loss 0.0926292\n",
      "epoch 5,step 198000, training loss 0.0622069\n",
      "epoch 5,step 201000, training loss 0.0625397\n",
      "epoch 5,step 204000, training loss 0.0927174\n",
      "epoch 5,step 207000, training loss 0.0588821\n",
      "epoch 5,step 210000, training loss 0.0669561\n",
      "epoch 5,step 213000, training loss 0.0811529\n",
      "epoch 5,step 216000, training loss 0.0519428\n",
      "epoch 5,step 219000, training loss 0.095671\n",
      "epoch 5,step 222000, training loss 0.102598\n",
      "epoch 5,step 225000, training loss 0.0763004\n",
      "epoch 5,step 228000, training loss 0.081521\n",
      "epoch 5,step 231000, training loss 0.0685449\n",
      "epoch 5,step 234000, training loss 0.0672093\n",
      "epoch 5,step 237000, training loss 0.075389\n",
      "epoch 5,step 240000, training loss 0.0572972\n",
      "epoch 5,step 243000, training loss 0.0591634\n",
      "epoch 5,step 246000, training loss 0.0739004\n",
      "epoch 5,step 249000, training loss 0.0634371\n",
      "epoch 5,step 252000, training loss 0.0647356\n",
      "epoch 5,step 255000, training loss 0.0945019\n",
      "epoch 5,step 258000, training loss 0.0730244\n",
      "epoch 5,step 261000, training loss 0.0823876\n",
      "epoch 5,step 264000, training loss 0.0725787\n",
      "epoch 5,step 267000, training loss 0.0476262\n",
      "epoch 5,step 270000, training loss 0.0636526\n",
      "epoch 5,step 273000, training loss 0.0621917\n",
      "epoch 5,step 276000, training loss 0.0648565\n",
      "epoch 5,step 279000, training loss 0.085982\n",
      "epoch 5,step 282000, training loss 0.0701549\n",
      "epoch 5,step 285000, training loss 0.0473358\n",
      "epoch 5,step 288000, training loss 0.0679053\n",
      "epoch 5,step 291000, training loss 0.0658436\n",
      "epoch 5,step 294000, training loss 0.0819706\n",
      "epoch 5,step 297000, training loss 0.0761464\n",
      "epoch 5,step 300000, training loss 0.0622201\n",
      "epoch 5,step 303000, training loss 0.0625412\n",
      "epoch 5,step 306000, training loss 0.0545302\n",
      "epoch 5,step 309000, training loss 0.0737473\n",
      "epoch 5,step 312000, training loss 0.0563475\n",
      "epoch 5,step 315000, training loss 0.0753681\n",
      "epoch 5,step 318000, training loss 0.0952439\n",
      "epoch 5,step 321000, training loss 0.0776355\n",
      "epoch 5,step 324000, training loss 0.0664204\n",
      "epoch 5,step 327000, training loss 0.0783543\n",
      "epoch 5,step 330000, training loss 0.106843\n",
      "epoch 5,step 333000, training loss 0.0868863\n",
      "epoch 5,step 336000, training loss 0.120198\n",
      "epoch 5,step 339000, training loss 0.0749052\n",
      "epoch 5,step 342000, training loss 0.0588779\n",
      "epoch 5,step 345000, training loss 0.109331\n",
      "epoch 5,step 348000, training loss 0.0620905\n",
      "epoch 5,step 351000, training loss 0.0651778\n",
      "epoch 5,step 354000, training loss 0.0551796\n",
      "epoch 5,step 357000, training loss 0.0689771\n",
      "epoch 5,step 360000, training loss 0.0879922\n",
      "epoch 5,step 363000, training loss 0.0587301\n",
      "epoch 5,step 366000, training loss 0.0555298\n",
      "epoch 5,step 369000, training loss 0.0635775\n",
      "epoch 5,step 372000, training loss 0.0854119\n",
      "epoch 5,step 375000, training loss 0.0963955\n",
      "epoch 5,step 378000, training loss 0.0728132\n",
      "epoch 5,step 381000, training loss 0.0580309\n",
      "epoch 5,step 384000, training loss 0.073885\n",
      "epoch 5,step 387000, training loss 0.0898836\n",
      "epoch 5,step 390000, training loss 0.0796793\n",
      "epoch 5,step 393000, training loss 0.0638668\n",
      "epoch 5,step 396000, training loss 0.0603762\n",
      "epoch 5,step 399000, training loss 0.0604458\n",
      "epoch 5,step 402000, training loss 0.0782011\n",
      "epoch 5,step 405000, training loss 0.0511383\n",
      "epoch 5,step 408000, training loss 0.0673006\n",
      "epoch 5,step 411000, training loss 0.0556416\n",
      "epoch 5,step 414000, training loss 0.0755122\n",
      "epoch 5,step 417000, training loss 0.0583899\n",
      "epoch 5,step 420000, training loss 0.0539842\n",
      "epoch 5,step 423000, training loss 0.0705011\n",
      "epoch 5,step 426000, training loss 0.0627919\n",
      "epoch 5,step 429000, training loss 0.0711236\n",
      "epoch 5,step 432000, training loss 0.0577251\n",
      "epoch 5,step 435000, training loss 0.0753672\n",
      "epoch 5,step 438000, training loss 0.0763409\n",
      "epoch 5,step 441000, training loss 0.085205\n",
      "epoch 5,step 444000, training loss 0.106777\n",
      "epoch 5,step 447000, training loss 0.0614998\n",
      "epoch 5,step 450000, training loss 0.143701\n",
      "epoch 5,step 453000, training loss 0.0629374\n",
      "epoch 5,step 456000, training loss 0.0743448\n",
      "epoch 5,step 459000, training loss 0.097911\n",
      "epoch 5,step 462000, training loss 0.0477243\n",
      "epoch 5,step 465000, training loss 0.0740583\n",
      "epoch 5,step 468000, training loss 0.0542936\n",
      "epoch 5,step 471000, training loss 0.0604215\n",
      "epoch 5,step 474000, training loss 0.065467\n",
      "epoch 5,step 477000, training loss 0.0672817\n",
      "epoch 5,step 480000, training loss 0.0649104\n",
      "epoch 5,step 483000, training loss 0.0649734\n",
      "epoch 5,step 486000, training loss 0.0560838\n",
      "epoch 5,step 489000, training loss 0.0737208\n",
      "epoch 5,step 492000, training loss 0.0694533\n",
      "epoch 5,step 495000, training loss 0.0804765\n",
      "epoch 5,step 498000, training loss 0.0636709\n",
      "epoch 5,step 501000, training loss 0.0620957\n",
      "epoch 5,step 504000, training loss 0.0603842\n",
      "epoch 5,step 507000, training loss 0.0718224\n",
      "epoch 5,step 510000, training loss 0.0723011\n",
      "epoch 5,step 513000, training loss 0.0836605\n",
      "epoch 5,step 516000, training loss 0.079851\n",
      "epoch 5,step 519000, training loss 0.0574128\n",
      "epoch 5,step 522000, training loss 0.074864\n",
      "epoch 5,step 525000, training loss 0.059184\n",
      "epoch 5,step 528000, training loss 0.0673775\n",
      "epoch 5,step 531000, training loss 0.0597149\n",
      "epoch 5,step 534000, training loss 0.0517941\n",
      "epoch 5,step 537000, training loss 0.0753553\n",
      "epoch 5,step 540000, training loss 0.1139\n",
      "epoch 5,step 543000, training loss 0.0600545\n",
      "epoch 5,step 546000, training loss 0.0613846\n",
      "epoch 5,step 549000, training loss 0.0868598\n",
      "epoch 5,step 552000, training loss 0.064412\n",
      "epoch 5,step 555000, training loss 0.0503666\n",
      "epoch 5,step 558000, training loss 0.0578535\n",
      "epoch 5,step 561000, training loss 0.090389\n",
      "epoch 5,step 564000, training loss 0.0568238\n",
      "epoch 5,step 567000, training loss 0.0692422\n",
      "epoch 5,step 570000, training loss 0.0483844\n",
      "epoch 5,step 573000, training loss 0.0553717\n",
      "epoch 5,step 576000, training loss 0.0836061\n",
      "epoch 5,step 579000, training loss 0.0547309\n",
      "epoch 5,step 582000, training loss 0.0649512\n",
      "epoch 5,step 585000, training loss 0.077499\n",
      "epoch 5,step 588000, training loss 0.060466\n",
      "epoch 5,step 591000, training loss 0.0523446\n",
      "epoch 5,step 594000, training loss 0.0514316\n",
      "epoch 5,step 597000, training loss 0.0504759\n",
      "epoch 5,training loss 0.088743 ,test loss 0.0883919\n",
      "epoch 6,step 3500, training loss 0.0833728\n",
      "epoch 6,step 7000, training loss 0.0969783\n",
      "epoch 6,step 10500, training loss 0.10189\n",
      "epoch 6,step 14000, training loss 0.0525448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6,step 17500, training loss 0.0505308\n",
      "epoch 6,step 21000, training loss 0.0617704\n",
      "epoch 6,step 24500, training loss 0.060933\n",
      "epoch 6,step 28000, training loss 0.0569366\n",
      "epoch 6,step 31500, training loss 0.0673354\n",
      "epoch 6,step 35000, training loss 0.0614431\n",
      "epoch 6,step 38500, training loss 0.060764\n",
      "epoch 6,step 42000, training loss 0.0823051\n",
      "epoch 6,step 45500, training loss 0.11579\n",
      "epoch 6,step 49000, training loss 0.0652694\n",
      "epoch 6,step 52500, training loss 0.085005\n",
      "epoch 6,step 56000, training loss 0.0651484\n",
      "epoch 6,step 59500, training loss 0.061781\n",
      "epoch 6,step 63000, training loss 0.0577432\n",
      "epoch 6,step 66500, training loss 0.0435223\n",
      "epoch 6,step 70000, training loss 0.0697396\n",
      "epoch 6,step 73500, training loss 0.0564242\n",
      "epoch 6,step 77000, training loss 0.0686839\n",
      "epoch 6,step 80500, training loss 0.0804053\n",
      "epoch 6,step 84000, training loss 0.0774736\n",
      "epoch 6,step 87500, training loss 0.0728014\n",
      "epoch 6,step 91000, training loss 0.0429216\n",
      "epoch 6,step 94500, training loss 0.0977423\n",
      "epoch 6,step 98000, training loss 0.0620947\n",
      "epoch 6,step 101500, training loss 0.0851066\n",
      "epoch 6,step 105000, training loss 0.0602342\n",
      "epoch 6,step 108500, training loss 0.0952562\n",
      "epoch 6,step 112000, training loss 0.0769905\n",
      "epoch 6,step 115500, training loss 0.0657812\n",
      "epoch 6,step 119000, training loss 0.0680182\n",
      "epoch 6,step 122500, training loss 0.0845736\n",
      "epoch 6,step 126000, training loss 0.0553288\n",
      "epoch 6,step 129500, training loss 0.0696018\n",
      "epoch 6,step 133000, training loss 0.0789388\n",
      "epoch 6,step 136500, training loss 0.0608797\n",
      "epoch 6,step 140000, training loss 0.0584743\n",
      "epoch 6,step 143500, training loss 0.0720793\n",
      "epoch 6,step 147000, training loss 0.0666151\n",
      "epoch 6,step 150500, training loss 0.0641263\n",
      "epoch 6,step 154000, training loss 0.0649616\n",
      "epoch 6,step 157500, training loss 0.0553821\n",
      "epoch 6,step 161000, training loss 0.0714644\n",
      "epoch 6,step 164500, training loss 0.0455697\n",
      "epoch 6,step 168000, training loss 0.0518357\n",
      "epoch 6,step 171500, training loss 0.0609124\n",
      "epoch 6,step 175000, training loss 0.0606283\n",
      "epoch 6,step 178500, training loss 0.0620547\n",
      "epoch 6,step 182000, training loss 0.0814344\n",
      "epoch 6,step 185500, training loss 0.0503882\n",
      "epoch 6,step 189000, training loss 0.065631\n",
      "epoch 6,step 192500, training loss 0.101042\n",
      "epoch 6,step 196000, training loss 0.0571742\n",
      "epoch 6,step 199500, training loss 0.058271\n",
      "epoch 6,step 203000, training loss 0.0663045\n",
      "epoch 6,step 206500, training loss 0.0709393\n",
      "epoch 6,step 210000, training loss 0.0531163\n",
      "epoch 6,step 213500, training loss 0.0649944\n",
      "epoch 6,step 217000, training loss 0.0587417\n",
      "epoch 6,step 220500, training loss 0.0500817\n",
      "epoch 6,step 224000, training loss 0.0788387\n",
      "epoch 6,step 227500, training loss 0.0864857\n",
      "epoch 6,step 231000, training loss 0.0623529\n",
      "epoch 6,step 234500, training loss 0.0621279\n",
      "epoch 6,step 238000, training loss 0.080288\n",
      "epoch 6,step 241500, training loss 0.0554776\n",
      "epoch 6,step 245000, training loss 0.0564081\n",
      "epoch 6,step 248500, training loss 0.0765568\n",
      "epoch 6,step 252000, training loss 0.048559\n",
      "epoch 6,step 255500, training loss 0.0854094\n",
      "epoch 6,step 259000, training loss 0.0921984\n",
      "epoch 6,step 262500, training loss 0.0696049\n",
      "epoch 6,step 266000, training loss 0.0675846\n",
      "epoch 6,step 269500, training loss 0.0589307\n",
      "epoch 6,step 273000, training loss 0.0593098\n",
      "epoch 6,step 276500, training loss 0.0689204\n",
      "epoch 6,step 280000, training loss 0.0480174\n",
      "epoch 6,step 283500, training loss 0.0531849\n",
      "epoch 6,step 287000, training loss 0.0605188\n",
      "epoch 6,step 290500, training loss 0.0561509\n",
      "epoch 6,step 294000, training loss 0.0566632\n",
      "epoch 6,step 297500, training loss 0.0879327\n",
      "epoch 6,step 301000, training loss 0.065396\n",
      "epoch 6,step 304500, training loss 0.0781885\n",
      "epoch 6,step 308000, training loss 0.0628729\n",
      "epoch 6,step 311500, training loss 0.04056\n",
      "epoch 6,step 315000, training loss 0.057054\n",
      "epoch 6,step 318500, training loss 0.0532322\n",
      "epoch 6,step 322000, training loss 0.058834\n",
      "epoch 6,step 325500, training loss 0.07606\n",
      "epoch 6,step 329000, training loss 0.0673736\n",
      "epoch 6,step 332500, training loss 0.0437852\n",
      "epoch 6,step 336000, training loss 0.0669575\n",
      "epoch 6,step 339500, training loss 0.0625877\n",
      "epoch 6,step 343000, training loss 0.073282\n",
      "epoch 6,step 346500, training loss 0.0688608\n",
      "epoch 6,step 350000, training loss 0.0526389\n",
      "epoch 6,step 353500, training loss 0.0551505\n",
      "epoch 6,step 357000, training loss 0.05273\n",
      "epoch 6,step 360500, training loss 0.0660051\n",
      "epoch 6,step 364000, training loss 0.0515951\n",
      "epoch 6,step 367500, training loss 0.0636744\n",
      "epoch 6,step 371000, training loss 0.0779259\n",
      "epoch 6,step 374500, training loss 0.0710908\n",
      "epoch 6,step 378000, training loss 0.0632157\n",
      "epoch 6,step 381500, training loss 0.0703801\n",
      "epoch 6,step 385000, training loss 0.0903688\n",
      "epoch 6,step 388500, training loss 0.0886968\n",
      "epoch 6,step 392000, training loss 0.116251\n",
      "epoch 6,step 395500, training loss 0.0671135\n",
      "epoch 6,step 399000, training loss 0.0519995\n",
      "epoch 6,step 402500, training loss 0.106187\n",
      "epoch 6,step 406000, training loss 0.0596502\n",
      "epoch 6,step 409500, training loss 0.0586962\n",
      "epoch 6,step 413000, training loss 0.0496446\n",
      "epoch 6,step 416500, training loss 0.0631106\n",
      "epoch 6,step 420000, training loss 0.116918\n",
      "epoch 6,step 423500, training loss 0.0489159\n",
      "epoch 6,step 427000, training loss 0.0525161\n",
      "epoch 6,step 430500, training loss 0.0581377\n",
      "epoch 6,step 434000, training loss 0.0755365\n",
      "epoch 6,step 437500, training loss 0.0841053\n",
      "epoch 6,step 441000, training loss 0.0664104\n",
      "epoch 6,step 444500, training loss 0.0505234\n",
      "epoch 6,step 448000, training loss 0.0637404\n",
      "epoch 6,step 451500, training loss 0.0700063\n",
      "epoch 6,step 455000, training loss 0.0705777\n",
      "epoch 6,step 458500, training loss 0.0568712\n",
      "epoch 6,step 462000, training loss 0.0527598\n",
      "epoch 6,step 465500, training loss 0.0575174\n",
      "epoch 6,step 469000, training loss 0.0760048\n",
      "epoch 6,step 472500, training loss 0.0474314\n",
      "epoch 6,step 476000, training loss 0.0651311\n",
      "epoch 6,step 479500, training loss 0.0503774\n",
      "epoch 6,step 483000, training loss 0.0651887\n",
      "epoch 6,step 486500, training loss 0.0516321\n",
      "epoch 6,step 490000, training loss 0.0510661\n",
      "epoch 6,step 493500, training loss 0.0614772\n",
      "epoch 6,step 497000, training loss 0.0568543\n",
      "epoch 6,step 500500, training loss 0.0612438\n",
      "epoch 6,step 504000, training loss 0.0514464\n",
      "epoch 6,step 507500, training loss 0.0639804\n",
      "epoch 6,step 511000, training loss 0.0617731\n",
      "epoch 6,step 514500, training loss 0.070186\n",
      "epoch 6,step 518000, training loss 0.0836948\n",
      "epoch 6,step 521500, training loss 0.0506704\n",
      "epoch 6,step 525000, training loss 0.124368\n",
      "epoch 6,step 528500, training loss 0.0479904\n",
      "epoch 6,step 532000, training loss 0.0539781\n",
      "epoch 6,step 535500, training loss 0.0835845\n",
      "epoch 6,step 539000, training loss 0.0411758\n",
      "epoch 6,step 542500, training loss 0.0646304\n",
      "epoch 6,step 546000, training loss 0.0489127\n",
      "epoch 6,step 549500, training loss 0.0523609\n",
      "epoch 6,step 553000, training loss 0.0623303\n",
      "epoch 6,step 556500, training loss 0.0611211\n",
      "epoch 6,step 560000, training loss 0.0589515\n",
      "epoch 6,step 563500, training loss 0.0601105\n",
      "epoch 6,step 567000, training loss 0.0531856\n",
      "epoch 6,step 570500, training loss 0.0657943\n",
      "epoch 6,step 574000, training loss 0.0593727\n",
      "epoch 6,step 577500, training loss 0.0706347\n",
      "epoch 6,step 581000, training loss 0.0580927\n",
      "epoch 6,step 584500, training loss 0.0565947\n",
      "epoch 6,step 588000, training loss 0.0504516\n",
      "epoch 6,step 591500, training loss 0.0648398\n",
      "epoch 6,step 595000, training loss 0.0651578\n",
      "epoch 6,step 598500, training loss 0.0742044\n",
      "epoch 6,step 602000, training loss 0.074559\n",
      "epoch 6,step 605500, training loss 0.0492382\n",
      "epoch 6,step 609000, training loss 0.0725614\n",
      "epoch 6,step 612500, training loss 0.0536274\n",
      "epoch 6,step 616000, training loss 0.0561741\n",
      "epoch 6,step 619500, training loss 0.0581543\n",
      "epoch 6,step 623000, training loss 0.051133\n",
      "epoch 6,step 626500, training loss 0.0674418\n",
      "epoch 6,step 630000, training loss 0.11049\n",
      "epoch 6,step 633500, training loss 0.0551164\n",
      "epoch 6,step 637000, training loss 0.0526308\n",
      "epoch 6,step 640500, training loss 0.0740298\n",
      "epoch 6,step 644000, training loss 0.0587367\n",
      "epoch 6,step 647500, training loss 0.0479547\n",
      "epoch 6,step 651000, training loss 0.0527275\n",
      "epoch 6,step 654500, training loss 0.083124\n",
      "epoch 6,step 658000, training loss 0.0474969\n",
      "epoch 6,step 661500, training loss 0.0581851\n",
      "epoch 6,step 665000, training loss 0.0411263\n",
      "epoch 6,step 668500, training loss 0.0485988\n",
      "epoch 6,step 672000, training loss 0.0713935\n",
      "epoch 6,step 675500, training loss 0.0484575\n",
      "epoch 6,step 679000, training loss 0.0567596\n",
      "epoch 6,step 682500, training loss 0.0670718\n",
      "epoch 6,step 686000, training loss 0.0542081\n",
      "epoch 6,step 689500, training loss 0.0482737\n",
      "epoch 6,step 693000, training loss 0.0472265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6,step 696500, training loss 0.0416517\n",
      "epoch 6,training loss 0.078522 ,test loss 0.0767424\n",
      "epoch 7,step 4000, training loss 0.0713236\n",
      "epoch 7,step 8000, training loss 0.085202\n",
      "epoch 7,step 12000, training loss 0.0855602\n",
      "epoch 7,step 16000, training loss 0.0444065\n",
      "epoch 7,step 20000, training loss 0.0473168\n",
      "epoch 7,step 24000, training loss 0.0545937\n",
      "epoch 7,step 28000, training loss 0.0537029\n",
      "epoch 7,step 32000, training loss 0.0505233\n",
      "epoch 7,step 36000, training loss 0.0577815\n",
      "epoch 7,step 40000, training loss 0.0514408\n",
      "epoch 7,step 44000, training loss 0.0533305\n",
      "epoch 7,step 48000, training loss 0.073016\n",
      "epoch 7,step 52000, training loss 0.105689\n",
      "epoch 7,step 56000, training loss 0.0576356\n",
      "epoch 7,step 60000, training loss 0.0772458\n",
      "epoch 7,step 64000, training loss 0.0547852\n",
      "epoch 7,step 68000, training loss 0.0528335\n",
      "epoch 7,step 72000, training loss 0.0492812\n",
      "epoch 7,step 76000, training loss 0.0391823\n",
      "epoch 7,step 80000, training loss 0.0621229\n",
      "epoch 7,step 84000, training loss 0.0450779\n",
      "epoch 7,step 88000, training loss 0.0600197\n",
      "epoch 7,step 92000, training loss 0.0725471\n",
      "epoch 7,step 96000, training loss 0.065037\n",
      "epoch 7,step 100000, training loss 0.0648486\n",
      "epoch 7,step 104000, training loss 0.0398637\n",
      "epoch 7,step 108000, training loss 0.0859053\n",
      "epoch 7,step 112000, training loss 0.0547023\n",
      "epoch 7,step 116000, training loss 0.0722838\n",
      "epoch 7,step 120000, training loss 0.0553013\n",
      "epoch 7,step 124000, training loss 0.0834235\n",
      "epoch 7,step 128000, training loss 0.0696866\n",
      "epoch 7,step 132000, training loss 0.0601939\n",
      "epoch 7,step 136000, training loss 0.0573674\n",
      "epoch 7,step 140000, training loss 0.0762673\n",
      "epoch 7,step 144000, training loss 0.0480137\n",
      "epoch 7,step 148000, training loss 0.0674391\n",
      "epoch 7,step 152000, training loss 0.0695799\n",
      "epoch 7,step 156000, training loss 0.0596926\n",
      "epoch 7,step 160000, training loss 0.0510862\n",
      "epoch 7,step 164000, training loss 0.0722622\n",
      "epoch 7,step 168000, training loss 0.0567045\n",
      "epoch 7,step 172000, training loss 0.0610789\n",
      "epoch 7,step 176000, training loss 0.0603496\n",
      "epoch 7,step 180000, training loss 0.0550421\n",
      "epoch 7,step 184000, training loss 0.0646757\n",
      "epoch 7,step 188000, training loss 0.0423381\n",
      "epoch 7,step 192000, training loss 0.0533155\n",
      "epoch 7,step 196000, training loss 0.0580874\n",
      "epoch 7,step 200000, training loss 0.0524227\n",
      "epoch 7,step 204000, training loss 0.0527827\n",
      "epoch 7,step 208000, training loss 0.0691044\n",
      "epoch 7,step 212000, training loss 0.0433306\n",
      "epoch 7,step 216000, training loss 0.0547828\n",
      "epoch 7,step 220000, training loss 0.0903191\n",
      "epoch 7,step 224000, training loss 0.0525703\n",
      "epoch 7,step 228000, training loss 0.0532042\n",
      "epoch 7,step 232000, training loss 0.0630728\n",
      "epoch 7,step 236000, training loss 0.068535\n",
      "epoch 7,step 240000, training loss 0.0508989\n",
      "epoch 7,step 244000, training loss 0.0570234\n",
      "epoch 7,step 248000, training loss 0.0498641\n",
      "epoch 7,step 252000, training loss 0.0515745\n",
      "epoch 7,step 256000, training loss 0.0773602\n",
      "epoch 7,step 260000, training loss 0.0805808\n",
      "epoch 7,step 264000, training loss 0.0567173\n",
      "epoch 7,step 268000, training loss 0.0506874\n",
      "epoch 7,step 272000, training loss 0.0678639\n",
      "epoch 7,step 276000, training loss 0.0493271\n",
      "epoch 7,step 280000, training loss 0.0499064\n",
      "epoch 7,step 284000, training loss 0.0708375\n",
      "epoch 7,step 288000, training loss 0.0426987\n",
      "epoch 7,step 292000, training loss 0.0761295\n",
      "epoch 7,step 296000, training loss 0.0790347\n",
      "epoch 7,step 300000, training loss 0.0592252\n",
      "epoch 7,step 304000, training loss 0.0597876\n",
      "epoch 7,step 308000, training loss 0.0489963\n",
      "epoch 7,step 312000, training loss 0.0537351\n",
      "epoch 7,step 316000, training loss 0.0605818\n",
      "epoch 7,step 320000, training loss 0.0416809\n",
      "epoch 7,step 324000, training loss 0.0482632\n",
      "epoch 7,step 328000, training loss 0.0526007\n",
      "epoch 7,step 332000, training loss 0.0486491\n",
      "epoch 7,step 336000, training loss 0.0499729\n",
      "epoch 7,step 340000, training loss 0.0743871\n",
      "epoch 7,step 344000, training loss 0.058927\n",
      "epoch 7,step 348000, training loss 0.0694922\n",
      "epoch 7,step 352000, training loss 0.0602519\n",
      "epoch 7,step 356000, training loss 0.0383432\n",
      "epoch 7,step 360000, training loss 0.0521211\n",
      "epoch 7,step 364000, training loss 0.0520837\n",
      "epoch 7,step 368000, training loss 0.0531293\n",
      "epoch 7,step 372000, training loss 0.0739357\n",
      "epoch 7,step 376000, training loss 0.0635265\n",
      "epoch 7,step 380000, training loss 0.0431278\n",
      "epoch 7,step 384000, training loss 0.0597976\n",
      "epoch 7,step 388000, training loss 0.060235\n",
      "epoch 7,step 392000, training loss 0.0694239\n",
      "epoch 7,step 396000, training loss 0.068263\n",
      "epoch 7,step 400000, training loss 0.0531183\n",
      "epoch 7,step 404000, training loss 0.0550181\n",
      "epoch 7,step 408000, training loss 0.0526645\n",
      "epoch 7,step 412000, training loss 0.0627612\n",
      "epoch 7,step 416000, training loss 0.0494032\n",
      "epoch 7,step 420000, training loss 0.0586332\n",
      "epoch 7,step 424000, training loss 0.0800554\n",
      "epoch 7,step 428000, training loss 0.0796871\n",
      "epoch 7,step 432000, training loss 0.067389\n",
      "epoch 7,step 436000, training loss 0.0748387\n",
      "epoch 7,step 440000, training loss 0.0877645\n",
      "epoch 7,step 444000, training loss 0.0637386\n",
      "epoch 7,step 448000, training loss 0.0699599\n",
      "epoch 7,step 452000, training loss 0.0545582\n",
      "epoch 7,step 456000, training loss 0.04744\n",
      "epoch 7,step 460000, training loss 0.0999998\n",
      "epoch 7,step 464000, training loss 0.0473235\n",
      "epoch 7,step 468000, training loss 0.0547662\n",
      "epoch 7,step 472000, training loss 0.0469911\n",
      "epoch 7,step 476000, training loss 0.056306\n",
      "epoch 7,step 480000, training loss 0.0814263\n",
      "epoch 7,step 484000, training loss 0.0418255\n",
      "epoch 7,step 488000, training loss 0.0469928\n",
      "epoch 7,step 492000, training loss 0.0504244\n",
      "epoch 7,step 496000, training loss 0.0683095\n",
      "epoch 7,step 500000, training loss 0.0695243\n",
      "epoch 7,step 504000, training loss 0.0635606\n",
      "epoch 7,step 508000, training loss 0.0459107\n",
      "epoch 7,step 512000, training loss 0.0599372\n",
      "epoch 7,step 516000, training loss 0.067877\n",
      "epoch 7,step 520000, training loss 0.0647099\n",
      "epoch 7,step 524000, training loss 0.0508057\n",
      "epoch 7,step 528000, training loss 0.0486287\n",
      "epoch 7,step 532000, training loss 0.0490324\n",
      "epoch 7,step 536000, training loss 0.065677\n",
      "epoch 7,step 540000, training loss 0.0459037\n",
      "epoch 7,step 544000, training loss 0.0596912\n",
      "epoch 7,step 548000, training loss 0.0476626\n",
      "epoch 7,step 552000, training loss 0.0606787\n",
      "epoch 7,step 556000, training loss 0.046984\n",
      "epoch 7,step 560000, training loss 0.0466234\n",
      "epoch 7,step 564000, training loss 0.054682\n",
      "epoch 7,step 568000, training loss 0.0517513\n",
      "epoch 7,step 572000, training loss 0.0566323\n",
      "epoch 7,step 576000, training loss 0.0466281\n",
      "epoch 7,step 580000, training loss 0.0635205\n",
      "epoch 7,step 584000, training loss 0.0543935\n",
      "epoch 7,step 588000, training loss 0.0661112\n",
      "epoch 7,step 592000, training loss 0.0813603\n",
      "epoch 7,step 596000, training loss 0.0475649\n",
      "epoch 7,step 600000, training loss 0.115796\n",
      "epoch 7,step 604000, training loss 0.0489927\n",
      "epoch 7,step 608000, training loss 0.0488914\n",
      "epoch 7,step 612000, training loss 0.0786359\n",
      "epoch 7,step 616000, training loss 0.036581\n",
      "epoch 7,step 620000, training loss 0.0589299\n",
      "epoch 7,step 624000, training loss 0.0440806\n",
      "epoch 7,step 628000, training loss 0.0491591\n",
      "epoch 7,step 632000, training loss 0.0576605\n",
      "epoch 7,step 636000, training loss 0.0542993\n",
      "epoch 7,step 640000, training loss 0.051584\n",
      "epoch 7,step 644000, training loss 0.0497332\n",
      "epoch 7,step 648000, training loss 0.0452602\n",
      "epoch 7,step 652000, training loss 0.0620753\n",
      "epoch 7,step 656000, training loss 0.054429\n",
      "epoch 7,step 660000, training loss 0.0671836\n",
      "epoch 7,step 664000, training loss 0.0593855\n",
      "epoch 7,step 668000, training loss 0.0573674\n",
      "epoch 7,step 672000, training loss 0.0499103\n",
      "epoch 7,step 676000, training loss 0.0620806\n",
      "epoch 7,step 680000, training loss 0.0606823\n",
      "epoch 7,step 684000, training loss 0.0688779\n",
      "epoch 7,step 688000, training loss 0.0689967\n",
      "epoch 7,step 692000, training loss 0.0469159\n",
      "epoch 7,step 696000, training loss 0.065707\n",
      "epoch 7,step 700000, training loss 0.0479477\n",
      "epoch 7,step 704000, training loss 0.0510373\n",
      "epoch 7,step 708000, training loss 0.0526928\n",
      "epoch 7,step 712000, training loss 0.046253\n",
      "epoch 7,step 716000, training loss 0.0666334\n",
      "epoch 7,step 720000, training loss 0.106905\n",
      "epoch 7,step 724000, training loss 0.0518447\n",
      "epoch 7,step 728000, training loss 0.0473291\n",
      "epoch 7,step 732000, training loss 0.0663874\n",
      "epoch 7,step 736000, training loss 0.0524675\n",
      "epoch 7,step 740000, training loss 0.042634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7,step 744000, training loss 0.0468207\n",
      "epoch 7,step 748000, training loss 0.0767568\n",
      "epoch 7,step 752000, training loss 0.0452489\n",
      "epoch 7,step 756000, training loss 0.0552735\n",
      "epoch 7,step 760000, training loss 0.0374519\n",
      "epoch 7,step 764000, training loss 0.045368\n",
      "epoch 7,step 768000, training loss 0.0668898\n",
      "epoch 7,step 772000, training loss 0.043721\n",
      "epoch 7,step 776000, training loss 0.0553225\n",
      "epoch 7,step 780000, training loss 0.064592\n",
      "epoch 7,step 784000, training loss 0.0495514\n",
      "epoch 7,step 788000, training loss 0.0434177\n",
      "epoch 7,step 792000, training loss 0.0414823\n",
      "epoch 7,step 796000, training loss 0.0394902\n",
      "epoch 7,training loss 0.0691881 ,test loss 0.0705589\n",
      "epoch 8,step 4500, training loss 0.0666826\n",
      "epoch 8,step 9000, training loss 0.0819294\n",
      "epoch 8,step 13500, training loss 0.084871\n",
      "epoch 8,step 18000, training loss 0.0427777\n",
      "epoch 8,step 22500, training loss 0.0418254\n",
      "epoch 8,step 27000, training loss 0.0504773\n",
      "epoch 8,step 31500, training loss 0.0517447\n",
      "epoch 8,step 36000, training loss 0.0499792\n",
      "epoch 8,step 40500, training loss 0.055618\n",
      "epoch 8,step 45000, training loss 0.0479134\n",
      "epoch 8,step 49500, training loss 0.0482561\n",
      "epoch 8,step 54000, training loss 0.0682559\n",
      "epoch 8,step 58500, training loss 0.098718\n",
      "epoch 8,step 63000, training loss 0.0520227\n",
      "epoch 8,step 67500, training loss 0.0744674\n",
      "epoch 8,step 72000, training loss 0.0527463\n",
      "epoch 8,step 76500, training loss 0.0481903\n",
      "epoch 8,step 81000, training loss 0.0444445\n",
      "epoch 8,step 85500, training loss 0.0342517\n",
      "epoch 8,step 90000, training loss 0.0577718\n",
      "epoch 8,step 94500, training loss 0.0467852\n",
      "epoch 8,step 99000, training loss 0.0549388\n",
      "epoch 8,step 103500, training loss 0.0672094\n",
      "epoch 8,step 108000, training loss 0.0610997\n",
      "epoch 8,step 112500, training loss 0.0589897\n",
      "epoch 8,step 117000, training loss 0.0372143\n",
      "epoch 8,step 121500, training loss 0.0851202\n",
      "epoch 8,step 126000, training loss 0.0504213\n",
      "epoch 8,step 130500, training loss 0.0582964\n",
      "epoch 8,step 135000, training loss 0.0479714\n",
      "epoch 8,step 139500, training loss 0.0756766\n",
      "epoch 8,step 144000, training loss 0.0629935\n",
      "epoch 8,step 148500, training loss 0.0538009\n",
      "epoch 8,step 153000, training loss 0.0511479\n",
      "epoch 8,step 157500, training loss 0.0700442\n",
      "epoch 8,step 162000, training loss 0.0500997\n",
      "epoch 8,step 166500, training loss 0.0668473\n",
      "epoch 8,step 171000, training loss 0.0668638\n",
      "epoch 8,step 175500, training loss 0.0562045\n",
      "epoch 8,step 180000, training loss 0.0446279\n",
      "epoch 8,step 184500, training loss 0.0632681\n",
      "epoch 8,step 189000, training loss 0.0494307\n",
      "epoch 8,step 193500, training loss 0.052954\n",
      "epoch 8,step 198000, training loss 0.0541329\n",
      "epoch 8,step 202500, training loss 0.0481432\n",
      "epoch 8,step 207000, training loss 0.0587151\n",
      "epoch 8,step 211500, training loss 0.0373627\n",
      "epoch 8,step 216000, training loss 0.0457815\n",
      "epoch 8,step 220500, training loss 0.0532521\n",
      "epoch 8,step 225000, training loss 0.0486831\n",
      "epoch 8,step 229500, training loss 0.0473224\n",
      "epoch 8,step 234000, training loss 0.06378\n",
      "epoch 8,step 238500, training loss 0.0398825\n",
      "epoch 8,step 243000, training loss 0.0514741\n",
      "epoch 8,step 247500, training loss 0.0807329\n",
      "epoch 8,step 252000, training loss 0.0487212\n",
      "epoch 8,step 256500, training loss 0.0480443\n",
      "epoch 8,step 261000, training loss 0.0604334\n",
      "epoch 8,step 265500, training loss 0.0621963\n",
      "epoch 8,step 270000, training loss 0.0457283\n",
      "epoch 8,step 274500, training loss 0.0507493\n",
      "epoch 8,step 279000, training loss 0.0474697\n",
      "epoch 8,step 283500, training loss 0.0483198\n",
      "epoch 8,step 288000, training loss 0.0675633\n",
      "epoch 8,step 292500, training loss 0.073166\n",
      "epoch 8,step 297000, training loss 0.0522894\n",
      "epoch 8,step 301500, training loss 0.0451555\n",
      "epoch 8,step 306000, training loss 0.0642157\n",
      "epoch 8,step 310500, training loss 0.0444987\n",
      "epoch 8,step 315000, training loss 0.0470182\n",
      "epoch 8,step 319500, training loss 0.062516\n",
      "epoch 8,step 324000, training loss 0.0398964\n",
      "epoch 8,step 328500, training loss 0.0764739\n",
      "epoch 8,step 333000, training loss 0.0777249\n",
      "epoch 8,step 337500, training loss 0.0553779\n",
      "epoch 8,step 342000, training loss 0.0561164\n",
      "epoch 8,step 346500, training loss 0.0480619\n",
      "epoch 8,step 351000, training loss 0.047937\n",
      "epoch 8,step 355500, training loss 0.0561306\n",
      "epoch 8,step 360000, training loss 0.03637\n",
      "epoch 8,step 364500, training loss 0.0438178\n",
      "epoch 8,step 369000, training loss 0.0480974\n",
      "epoch 8,step 373500, training loss 0.0467616\n",
      "epoch 8,step 378000, training loss 0.0475221\n",
      "epoch 8,step 382500, training loss 0.0682009\n",
      "epoch 8,step 387000, training loss 0.0535937\n",
      "epoch 8,step 391500, training loss 0.0650865\n",
      "epoch 8,step 396000, training loss 0.0537096\n",
      "epoch 8,step 400500, training loss 0.0340387\n",
      "epoch 8,step 405000, training loss 0.048335\n",
      "epoch 8,step 409500, training loss 0.0456666\n",
      "epoch 8,step 414000, training loss 0.0471389\n",
      "epoch 8,step 418500, training loss 0.0701149\n",
      "epoch 8,step 423000, training loss 0.0578678\n",
      "epoch 8,step 427500, training loss 0.0419466\n",
      "epoch 8,step 432000, training loss 0.0560966\n",
      "epoch 8,step 436500, training loss 0.05216\n",
      "epoch 8,step 441000, training loss 0.0626216\n",
      "epoch 8,step 445500, training loss 0.0617796\n",
      "epoch 8,step 450000, training loss 0.0485332\n",
      "epoch 8,step 454500, training loss 0.0460733\n",
      "epoch 8,step 459000, training loss 0.0434432\n",
      "epoch 8,step 463500, training loss 0.0559728\n",
      "epoch 8,step 468000, training loss 0.04169\n",
      "epoch 8,step 472500, training loss 0.0524638\n",
      "epoch 8,step 477000, training loss 0.0767009\n",
      "epoch 8,step 481500, training loss 0.067575\n",
      "epoch 8,step 486000, training loss 0.0598042\n",
      "epoch 8,step 490500, training loss 0.0617262\n",
      "epoch 8,step 495000, training loss 0.0777306\n",
      "epoch 8,step 499500, training loss 0.0669745\n",
      "epoch 8,step 504000, training loss 0.0764677\n",
      "epoch 8,step 508500, training loss 0.0555613\n",
      "epoch 8,step 513000, training loss 0.0458567\n",
      "epoch 8,step 517500, training loss 0.0942393\n",
      "epoch 8,step 522000, training loss 0.0473613\n",
      "epoch 8,step 526500, training loss 0.0538119\n",
      "epoch 8,step 531000, training loss 0.041647\n",
      "epoch 8,step 535500, training loss 0.0553246\n",
      "epoch 8,step 540000, training loss 0.0609429\n",
      "epoch 8,step 544500, training loss 0.0430862\n",
      "epoch 8,step 549000, training loss 0.0449946\n",
      "epoch 8,step 553500, training loss 0.0462951\n",
      "epoch 8,step 558000, training loss 0.064131\n",
      "epoch 8,step 562500, training loss 0.0639066\n",
      "epoch 8,step 567000, training loss 0.0565525\n",
      "epoch 8,step 571500, training loss 0.042501\n",
      "epoch 8,step 576000, training loss 0.0554374\n",
      "epoch 8,step 580500, training loss 0.0603193\n",
      "epoch 8,step 585000, training loss 0.0588307\n",
      "epoch 8,step 589500, training loss 0.0479486\n",
      "epoch 8,step 594000, training loss 0.046655\n",
      "epoch 8,step 598500, training loss 0.0462732\n",
      "epoch 8,step 603000, training loss 0.0584625\n",
      "epoch 8,step 607500, training loss 0.0443631\n",
      "epoch 8,step 612000, training loss 0.0548316\n",
      "epoch 8,step 616500, training loss 0.0461488\n",
      "epoch 8,step 621000, training loss 0.0541219\n",
      "epoch 8,step 625500, training loss 0.0457201\n",
      "epoch 8,step 630000, training loss 0.0426345\n",
      "epoch 8,step 634500, training loss 0.0523199\n",
      "epoch 8,step 639000, training loss 0.0492665\n",
      "epoch 8,step 643500, training loss 0.0582194\n",
      "epoch 8,step 648000, training loss 0.0455726\n",
      "epoch 8,step 652500, training loss 0.0609143\n",
      "epoch 8,step 657000, training loss 0.0522107\n",
      "epoch 8,step 661500, training loss 0.0640135\n",
      "epoch 8,step 666000, training loss 0.0764763\n",
      "epoch 8,step 670500, training loss 0.0477731\n",
      "epoch 8,step 675000, training loss 0.108098\n",
      "epoch 8,step 679500, training loss 0.0503869\n",
      "epoch 8,step 684000, training loss 0.0469563\n",
      "epoch 8,step 688500, training loss 0.0742387\n",
      "epoch 8,step 693000, training loss 0.0372748\n",
      "epoch 8,step 697500, training loss 0.0567288\n",
      "epoch 8,step 702000, training loss 0.0410471\n",
      "epoch 8,step 706500, training loss 0.0460811\n",
      "epoch 8,step 711000, training loss 0.0558037\n",
      "epoch 8,step 715500, training loss 0.0474391\n",
      "epoch 8,step 720000, training loss 0.0467296\n",
      "epoch 8,step 724500, training loss 0.0487955\n",
      "epoch 8,step 729000, training loss 0.0425057\n",
      "epoch 8,step 733500, training loss 0.0587115\n",
      "epoch 8,step 738000, training loss 0.0497841\n",
      "epoch 8,step 742500, training loss 0.0611068\n",
      "epoch 8,step 747000, training loss 0.0525213\n",
      "epoch 8,step 751500, training loss 0.0530688\n",
      "epoch 8,step 756000, training loss 0.0470837\n",
      "epoch 8,step 760500, training loss 0.0576799\n",
      "epoch 8,step 765000, training loss 0.0547619\n",
      "epoch 8,step 769500, training loss 0.0655469\n",
      "epoch 8,step 774000, training loss 0.0651316\n",
      "epoch 8,step 778500, training loss 0.0420501\n",
      "epoch 8,step 783000, training loss 0.0594342\n",
      "epoch 8,step 787500, training loss 0.0455909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8,step 792000, training loss 0.049217\n",
      "epoch 8,step 796500, training loss 0.0484921\n",
      "epoch 8,step 801000, training loss 0.0428839\n",
      "epoch 8,step 805500, training loss 0.0639986\n",
      "epoch 8,step 810000, training loss 0.10171\n",
      "epoch 8,step 814500, training loss 0.0483229\n",
      "epoch 8,step 819000, training loss 0.0448065\n",
      "epoch 8,step 823500, training loss 0.0615884\n",
      "epoch 8,step 828000, training loss 0.0507907\n",
      "epoch 8,step 832500, training loss 0.0382252\n",
      "epoch 8,step 837000, training loss 0.0416098\n",
      "epoch 8,step 841500, training loss 0.0710373\n",
      "epoch 8,step 846000, training loss 0.0398485\n",
      "epoch 8,step 850500, training loss 0.0515763\n",
      "epoch 8,step 855000, training loss 0.0346541\n",
      "epoch 8,step 859500, training loss 0.0397981\n",
      "epoch 8,step 864000, training loss 0.0559604\n",
      "epoch 8,step 868500, training loss 0.0409851\n",
      "epoch 8,step 873000, training loss 0.0505247\n",
      "epoch 8,step 877500, training loss 0.0594202\n",
      "epoch 8,step 882000, training loss 0.0476598\n",
      "epoch 8,step 886500, training loss 0.04251\n",
      "epoch 8,step 891000, training loss 0.0389801\n",
      "epoch 8,step 895500, training loss 0.0356105\n",
      "epoch 8,training loss 0.0615171 ,test loss 0.0665956\n",
      "epoch 9,step 1000, training loss 0.178762\n",
      "epoch 9,step 2000, training loss 0.107034\n",
      "epoch 9,step 3000, training loss 0.0554197\n",
      "epoch 9,step 4000, training loss 0.0531625\n",
      "epoch 9,step 5000, training loss 0.0669064\n",
      "epoch 9,step 6000, training loss 0.041552\n",
      "epoch 9,step 7000, training loss 0.0400163\n",
      "epoch 9,step 8000, training loss 0.041981\n",
      "epoch 9,step 9000, training loss 0.0580741\n",
      "epoch 9,step 10000, training loss 0.0775271\n",
      "epoch 9,step 11000, training loss 0.177098\n",
      "epoch 9,step 12000, training loss 0.114099\n",
      "epoch 9,step 13000, training loss 0.0729774\n",
      "epoch 9,step 14000, training loss 0.0720089\n",
      "epoch 9,step 15000, training loss 0.0669494\n",
      "epoch 9,step 16000, training loss 0.0442838\n",
      "epoch 9,step 17000, training loss 0.0606922\n",
      "epoch 9,step 18000, training loss 0.0468988\n",
      "epoch 9,step 19000, training loss 0.0836475\n",
      "epoch 9,step 20000, training loss 0.0361893\n",
      "epoch 9,step 21000, training loss 0.139172\n",
      "epoch 9,step 22000, training loss 0.0816341\n",
      "epoch 9,step 23000, training loss 0.0657768\n",
      "epoch 9,step 24000, training loss 0.0654812\n",
      "epoch 9,step 25000, training loss 0.0360299\n",
      "epoch 9,step 26000, training loss 0.0503843\n",
      "epoch 9,step 27000, training loss 0.0421264\n",
      "epoch 9,step 28000, training loss 0.0394278\n",
      "epoch 9,step 29000, training loss 0.0517555\n",
      "epoch 9,step 30000, training loss 0.0428439\n",
      "epoch 9,step 31000, training loss 0.134913\n",
      "epoch 9,step 32000, training loss 0.0694997\n",
      "epoch 9,step 33000, training loss 0.0510836\n",
      "epoch 9,step 34000, training loss 0.0474668\n",
      "epoch 9,step 35000, training loss 0.0471779\n",
      "epoch 9,step 36000, training loss 0.0743482\n",
      "epoch 9,step 37000, training loss 0.0465536\n",
      "epoch 9,step 38000, training loss 0.041953\n",
      "epoch 9,step 39000, training loss 0.0385287\n",
      "epoch 9,step 40000, training loss 0.0436821\n",
      "epoch 9,step 41000, training loss 0.136057\n",
      "epoch 9,step 42000, training loss 0.0664264\n",
      "epoch 9,step 43000, training loss 0.0433327\n",
      "epoch 9,step 44000, training loss 0.068249\n",
      "epoch 9,step 45000, training loss 0.0524484\n",
      "epoch 9,step 46000, training loss 0.0452301\n",
      "epoch 9,step 47000, training loss 0.064622\n",
      "epoch 9,step 48000, training loss 0.0430755\n",
      "epoch 9,step 49000, training loss 0.0474246\n",
      "epoch 9,step 50000, training loss 0.0448471\n",
      "epoch 9,step 51000, training loss 0.141214\n",
      "epoch 9,step 52000, training loss 0.107268\n",
      "epoch 9,step 53000, training loss 0.0566523\n",
      "epoch 9,step 54000, training loss 0.0449816\n",
      "epoch 9,step 55000, training loss 0.044785\n",
      "epoch 9,step 56000, training loss 0.0420555\n",
      "epoch 9,step 57000, training loss 0.0385515\n",
      "epoch 9,step 58000, training loss 0.0657626\n",
      "epoch 9,step 59000, training loss 0.0696495\n",
      "epoch 9,step 60000, training loss 0.0636443\n",
      "epoch 9,step 61000, training loss 0.132788\n",
      "epoch 9,step 62000, training loss 0.0944683\n",
      "epoch 9,step 63000, training loss 0.0524935\n",
      "epoch 9,step 64000, training loss 0.0440284\n",
      "epoch 9,step 65000, training loss 0.0944442\n",
      "epoch 9,step 66000, training loss 0.0801647\n",
      "epoch 9,step 67000, training loss 0.0604444\n",
      "epoch 9,step 68000, training loss 0.0474722\n",
      "epoch 9,step 69000, training loss 0.0467961\n",
      "epoch 9,step 70000, training loss 0.0473508\n",
      "epoch 9,step 71000, training loss 0.131495\n",
      "epoch 9,step 72000, training loss 0.0627906\n",
      "epoch 9,step 73000, training loss 0.043079\n",
      "epoch 9,step 74000, training loss 0.037168\n",
      "epoch 9,step 75000, training loss 0.0649139\n",
      "epoch 9,step 76000, training loss 0.0527917\n",
      "epoch 9,step 77000, training loss 0.0503718\n",
      "epoch 9,step 78000, training loss 0.0385541\n",
      "epoch 9,step 79000, training loss 0.0459005\n",
      "epoch 9,step 80000, training loss 0.0479479\n",
      "epoch 9,step 81000, training loss 0.171621\n",
      "epoch 9,step 82000, training loss 0.0736836\n",
      "epoch 9,step 83000, training loss 0.111784\n",
      "epoch 9,step 84000, training loss 0.0480082\n",
      "epoch 9,step 85000, training loss 0.0415144\n",
      "epoch 9,step 86000, training loss 0.0440952\n",
      "epoch 9,step 87000, training loss 0.0444164\n",
      "epoch 9,step 88000, training loss 0.0434473\n",
      "epoch 9,step 89000, training loss 0.0347214\n",
      "epoch 9,step 90000, training loss 0.040426\n",
      "epoch 9,step 91000, training loss 0.128505\n",
      "epoch 9,step 92000, training loss 0.102513\n",
      "epoch 9,step 93000, training loss 0.049724\n",
      "epoch 9,step 94000, training loss 0.0517896\n",
      "epoch 9,step 95000, training loss 0.0315591\n",
      "epoch 9,step 96000, training loss 0.04498\n",
      "epoch 9,step 97000, training loss 0.0424594\n",
      "epoch 9,step 98000, training loss 0.0462325\n",
      "epoch 9,step 99000, training loss 0.0472115\n",
      "epoch 9,step 100000, training loss 0.0540332\n",
      "epoch 9,step 101000, training loss 0.136063\n",
      "epoch 9,step 102000, training loss 0.0715688\n",
      "epoch 9,step 103000, training loss 0.0507419\n",
      "epoch 9,step 104000, training loss 0.0364143\n",
      "epoch 9,step 105000, training loss 0.0449837\n",
      "epoch 9,step 106000, training loss 0.047043\n",
      "epoch 9,step 107000, training loss 0.0332028\n",
      "epoch 9,step 108000, training loss 0.0621459\n",
      "epoch 9,step 109000, training loss 0.0633349\n",
      "epoch 9,step 110000, training loss 0.0503489\n",
      "epoch 9,step 111000, training loss 0.137475\n",
      "epoch 9,step 112000, training loss 0.102262\n",
      "epoch 9,step 113000, training loss 0.0508751\n",
      "epoch 9,step 114000, training loss 0.0848574\n",
      "epoch 9,step 115000, training loss 0.0640939\n",
      "epoch 9,step 116000, training loss 0.0814663\n",
      "epoch 9,step 117000, training loss 0.0537303\n",
      "epoch 9,step 118000, training loss 0.0893822\n",
      "epoch 9,step 119000, training loss 0.0974828\n",
      "epoch 9,step 120000, training loss 0.0589459\n",
      "epoch 9,step 121000, training loss 0.136808\n",
      "epoch 9,step 122000, training loss 0.0829743\n",
      "epoch 9,step 123000, training loss 0.0656979\n",
      "epoch 9,step 124000, training loss 0.0532282\n",
      "epoch 9,step 125000, training loss 0.0578674\n",
      "epoch 9,step 126000, training loss 0.0343419\n",
      "epoch 9,step 127000, training loss 0.0515999\n",
      "epoch 9,step 128000, training loss 0.0496523\n",
      "epoch 9,step 129000, training loss 0.0438494\n",
      "epoch 9,step 130000, training loss 0.0335219\n",
      "epoch 9,step 131000, training loss 0.115035\n",
      "epoch 9,step 132000, training loss 0.119533\n",
      "epoch 9,step 133000, training loss 0.0515165\n",
      "epoch 9,step 134000, training loss 0.057167\n",
      "epoch 9,step 135000, training loss 0.0812514\n",
      "epoch 9,step 136000, training loss 0.0370483\n",
      "epoch 9,step 137000, training loss 0.0769289\n",
      "epoch 9,step 138000, training loss 0.040106\n",
      "epoch 9,step 139000, training loss 0.0634138\n",
      "epoch 9,step 140000, training loss 0.0531819\n",
      "epoch 9,step 141000, training loss 0.198713\n",
      "epoch 9,step 142000, training loss 0.128092\n",
      "epoch 9,step 143000, training loss 0.069485\n",
      "epoch 9,step 144000, training loss 0.0420156\n",
      "epoch 9,step 145000, training loss 0.0587965\n",
      "epoch 9,step 146000, training loss 0.0512879\n",
      "epoch 9,step 147000, training loss 0.0616576\n",
      "epoch 9,step 148000, training loss 0.0468869\n",
      "epoch 9,step 149000, training loss 0.0427555\n",
      "epoch 9,step 150000, training loss 0.0473343\n",
      "epoch 9,step 151000, training loss 0.11595\n",
      "epoch 9,step 152000, training loss 0.107193\n",
      "epoch 9,step 153000, training loss 0.0705548\n",
      "epoch 9,step 154000, training loss 0.0623995\n",
      "epoch 9,step 155000, training loss 0.0725134\n",
      "epoch 9,step 156000, training loss 0.0797647\n",
      "epoch 9,step 157000, training loss 0.0553545\n",
      "epoch 9,step 158000, training loss 0.0414083\n",
      "epoch 9,step 159000, training loss 0.0569143\n",
      "epoch 9,step 160000, training loss 0.0576811\n",
      "epoch 9,step 161000, training loss 0.126372\n",
      "epoch 9,step 162000, training loss 0.085762\n",
      "epoch 9,step 163000, training loss 0.0498859\n",
      "epoch 9,step 164000, training loss 0.0417459\n",
      "epoch 9,step 165000, training loss 0.0484465\n",
      "epoch 9,step 166000, training loss 0.0429834\n",
      "epoch 9,step 167000, training loss 0.0422285\n",
      "epoch 9,step 168000, training loss 0.0556146\n",
      "epoch 9,step 169000, training loss 0.0407193\n",
      "epoch 9,step 170000, training loss 0.0460589\n",
      "epoch 9,step 171000, training loss 0.182931\n",
      "epoch 9,step 172000, training loss 0.080911\n",
      "epoch 9,step 173000, training loss 0.0559498\n",
      "epoch 9,step 174000, training loss 0.0832562\n",
      "epoch 9,step 175000, training loss 0.0626348\n",
      "epoch 9,step 176000, training loss 0.0542663\n",
      "epoch 9,step 177000, training loss 0.075697\n",
      "epoch 9,step 178000, training loss 0.0521452\n",
      "epoch 9,step 179000, training loss 0.0521548\n",
      "epoch 9,step 180000, training loss 0.0402589\n",
      "epoch 9,step 181000, training loss 0.124694\n",
      "epoch 9,step 182000, training loss 0.0842839\n",
      "epoch 9,step 183000, training loss 0.0611835\n",
      "epoch 9,step 184000, training loss 0.0700084\n",
      "epoch 9,step 185000, training loss 0.0542466\n",
      "epoch 9,step 186000, training loss 0.103682\n",
      "epoch 9,step 187000, training loss 0.0616891\n",
      "epoch 9,step 188000, training loss 0.0566354\n",
      "epoch 9,step 189000, training loss 0.0520249\n",
      "epoch 9,step 190000, training loss 0.059183\n",
      "epoch 9,step 191000, training loss 0.124959\n",
      "epoch 9,step 192000, training loss 0.0646375\n",
      "epoch 9,step 193000, training loss 0.0372756\n",
      "epoch 9,step 194000, training loss 0.0375026\n",
      "epoch 9,step 195000, training loss 0.0509731\n",
      "epoch 9,step 196000, training loss 0.0592605\n",
      "epoch 9,step 197000, training loss 0.039476\n",
      "epoch 9,step 198000, training loss 0.0412109\n",
      "epoch 9,step 199000, training loss 0.0394069\n",
      "epoch 9,step 200000, training loss 0.0414107\n",
      "epoch 9,step 201000, training loss 0.135088\n",
      "epoch 9,step 202000, training loss 0.127788\n",
      "epoch 9,step 203000, training loss 0.0578268\n",
      "epoch 9,step 204000, training loss 0.0447162\n",
      "epoch 9,step 205000, training loss 0.0584198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9,step 206000, training loss 0.0672104\n",
      "epoch 9,step 207000, training loss 0.0468422\n",
      "epoch 9,step 208000, training loss 0.0386223\n",
      "epoch 9,step 209000, training loss 0.0467537\n",
      "epoch 9,step 210000, training loss 0.0480256\n",
      "epoch 9,step 211000, training loss 0.147589\n",
      "epoch 9,step 212000, training loss 0.0724549\n",
      "epoch 9,step 213000, training loss 0.055083\n",
      "epoch 9,step 214000, training loss 0.0537188\n",
      "epoch 9,step 215000, training loss 0.0481643\n",
      "epoch 9,step 216000, training loss 0.0499055\n",
      "epoch 9,step 217000, training loss 0.0516799\n",
      "epoch 9,step 218000, training loss 0.0589151\n",
      "epoch 9,step 219000, training loss 0.0530112\n",
      "epoch 9,step 220000, training loss 0.0506972\n",
      "epoch 9,step 221000, training loss 0.163452\n",
      "epoch 9,step 222000, training loss 0.075004\n",
      "epoch 9,step 223000, training loss 0.0608526\n",
      "epoch 9,step 224000, training loss 0.0449866\n",
      "epoch 9,step 225000, training loss 0.0452821\n",
      "epoch 9,step 226000, training loss 0.0407941\n",
      "epoch 9,step 227000, training loss 0.0458891\n",
      "epoch 9,step 228000, training loss 0.0469055\n",
      "epoch 9,step 229000, training loss 0.0445329\n",
      "epoch 9,step 230000, training loss 0.0567901\n",
      "epoch 9,step 231000, training loss 0.127684\n",
      "epoch 9,step 232000, training loss 0.0810655\n",
      "epoch 9,step 233000, training loss 0.0385872\n",
      "epoch 9,step 234000, training loss 0.0483977\n",
      "epoch 9,step 235000, training loss 0.0364453\n",
      "epoch 9,step 236000, training loss 0.0356561\n",
      "epoch 9,step 237000, training loss 0.0528323\n",
      "epoch 9,step 238000, training loss 0.0441351\n",
      "epoch 9,step 239000, training loss 0.0437079\n",
      "epoch 9,step 240000, training loss 0.0431611\n",
      "epoch 9,step 241000, training loss 0.140362\n",
      "epoch 9,step 242000, training loss 0.071768\n",
      "epoch 9,step 243000, training loss 0.058765\n",
      "epoch 9,step 244000, training loss 0.0490811\n",
      "epoch 9,step 245000, training loss 0.0515647\n",
      "epoch 9,step 246000, training loss 0.0700008\n",
      "epoch 9,step 247000, training loss 0.0465366\n",
      "epoch 9,step 248000, training loss 0.0433963\n",
      "epoch 9,step 249000, training loss 0.0582253\n",
      "epoch 9,step 250000, training loss 0.0435814\n",
      "epoch 9,step 251000, training loss 0.207065\n",
      "epoch 9,step 252000, training loss 0.0687109\n",
      "epoch 9,step 253000, training loss 0.0486438\n",
      "epoch 9,step 254000, training loss 0.046837\n",
      "epoch 9,step 255000, training loss 0.0457447\n",
      "epoch 9,step 256000, training loss 0.0328979\n",
      "epoch 9,step 257000, training loss 0.056299\n",
      "epoch 9,step 258000, training loss 0.0463377\n",
      "epoch 9,step 259000, training loss 0.0707046\n",
      "epoch 9,step 260000, training loss 0.056598\n",
      "epoch 9,step 261000, training loss 0.197726\n",
      "epoch 9,step 262000, training loss 0.101579\n",
      "epoch 9,step 263000, training loss 0.0953692\n",
      "epoch 9,step 264000, training loss 0.055503\n",
      "epoch 9,step 265000, training loss 0.036489\n",
      "epoch 9,step 266000, training loss 0.0387437\n",
      "epoch 9,step 267000, training loss 0.0473058\n",
      "epoch 9,step 268000, training loss 0.0476714\n",
      "epoch 9,step 269000, training loss 0.0444526\n",
      "epoch 9,step 270000, training loss 0.0471641\n",
      "epoch 9,step 271000, training loss 0.139338\n",
      "epoch 9,step 272000, training loss 0.070946\n",
      "epoch 9,step 273000, training loss 0.0625294\n",
      "epoch 9,step 274000, training loss 0.0755139\n",
      "epoch 9,step 275000, training loss 0.0772724\n",
      "epoch 9,step 276000, training loss 0.0549492\n",
      "epoch 9,step 277000, training loss 0.0411059\n",
      "epoch 9,step 278000, training loss 0.0460505\n",
      "epoch 9,step 279000, training loss 0.0503724\n",
      "epoch 9,step 280000, training loss 0.0434378\n",
      "epoch 9,step 281000, training loss 0.179136\n",
      "epoch 9,step 282000, training loss 0.0835705\n",
      "epoch 9,step 283000, training loss 0.0389657\n",
      "epoch 9,step 284000, training loss 0.0538359\n",
      "epoch 9,step 285000, training loss 0.0442407\n",
      "epoch 9,step 286000, training loss 0.0414371\n",
      "epoch 9,step 287000, training loss 0.0531173\n",
      "epoch 9,step 288000, training loss 0.0396936\n",
      "epoch 9,step 289000, training loss 0.049534\n",
      "epoch 9,step 290000, training loss 0.0542597\n",
      "epoch 9,step 291000, training loss 0.120036\n",
      "epoch 9,step 292000, training loss 0.0695351\n",
      "epoch 9,step 293000, training loss 0.0410967\n",
      "epoch 9,step 294000, training loss 0.0631645\n",
      "epoch 9,step 295000, training loss 0.0564492\n",
      "epoch 9,step 296000, training loss 0.056214\n",
      "epoch 9,step 297000, training loss 0.0442087\n",
      "epoch 9,step 298000, training loss 0.0480733\n",
      "epoch 9,step 299000, training loss 0.0387248\n",
      "epoch 9,step 300000, training loss 0.0432164\n",
      "epoch 9,step 301000, training loss 0.134123\n",
      "epoch 9,step 302000, training loss 0.0869886\n",
      "epoch 9,step 303000, training loss 0.0542242\n",
      "epoch 9,step 304000, training loss 0.0528848\n",
      "epoch 9,step 305000, training loss 0.0463892\n",
      "epoch 9,step 306000, training loss 0.054371\n",
      "epoch 9,step 307000, training loss 0.0432477\n",
      "epoch 9,step 308000, training loss 0.0679348\n",
      "epoch 9,step 309000, training loss 0.0370154\n",
      "epoch 9,step 310000, training loss 0.0449677\n",
      "epoch 9,step 311000, training loss 0.152917\n",
      "epoch 9,step 312000, training loss 0.10207\n",
      "epoch 9,step 313000, training loss 0.0401021\n",
      "epoch 9,step 314000, training loss 0.0648297\n",
      "epoch 9,step 315000, training loss 0.0472071\n",
      "epoch 9,step 316000, training loss 0.0602224\n",
      "epoch 9,step 317000, training loss 0.0903459\n",
      "epoch 9,step 318000, training loss 0.0429678\n",
      "epoch 9,step 319000, training loss 0.082064\n",
      "epoch 9,step 320000, training loss 0.0661904\n",
      "epoch 9,step 321000, training loss 0.139942\n",
      "epoch 9,step 322000, training loss 0.113398\n",
      "epoch 9,step 323000, training loss 0.0519372\n",
      "epoch 9,step 324000, training loss 0.0572269\n",
      "epoch 9,step 325000, training loss 0.0659385\n",
      "epoch 9,step 326000, training loss 0.0424392\n",
      "epoch 9,step 327000, training loss 0.0469507\n",
      "epoch 9,step 328000, training loss 0.0401416\n",
      "epoch 9,step 329000, training loss 0.0614817\n",
      "epoch 9,step 330000, training loss 0.0480165\n",
      "epoch 9,step 331000, training loss 0.138122\n",
      "epoch 9,step 332000, training loss 0.122464\n",
      "epoch 9,step 333000, training loss 0.0466839\n",
      "epoch 9,step 334000, training loss 0.0665357\n",
      "epoch 9,step 335000, training loss 0.0442373\n",
      "epoch 9,step 336000, training loss 0.0395343\n",
      "epoch 9,step 337000, training loss 0.0503557\n",
      "epoch 9,step 338000, training loss 0.041485\n",
      "epoch 9,step 339000, training loss 0.0398604\n",
      "epoch 9,step 340000, training loss 0.0612704\n",
      "epoch 9,step 341000, training loss 0.152755\n",
      "epoch 9,step 342000, training loss 0.0928102\n",
      "epoch 9,step 343000, training loss 0.0642257\n",
      "epoch 9,step 344000, training loss 0.0633461\n",
      "epoch 9,step 345000, training loss 0.041491\n",
      "epoch 9,step 346000, training loss 0.0420366\n",
      "epoch 9,step 347000, training loss 0.0470451\n",
      "epoch 9,step 348000, training loss 0.0551707\n",
      "epoch 9,step 349000, training loss 0.0561933\n",
      "epoch 9,step 350000, training loss 0.0441715\n",
      "epoch 9,step 351000, training loss 0.131829\n",
      "epoch 9,step 352000, training loss 0.0758343\n",
      "epoch 9,step 353000, training loss 0.0628535\n",
      "epoch 9,step 354000, training loss 0.0451344\n",
      "epoch 9,step 355000, training loss 0.0584322\n",
      "epoch 9,step 356000, training loss 0.0453693\n",
      "epoch 9,step 357000, training loss 0.0441144\n",
      "epoch 9,step 358000, training loss 0.0359403\n",
      "epoch 9,step 359000, training loss 0.0394927\n",
      "epoch 9,step 360000, training loss 0.0370463\n",
      "epoch 9,step 361000, training loss 0.151505\n",
      "epoch 9,step 362000, training loss 0.0871184\n",
      "epoch 9,step 363000, training loss 0.056725\n",
      "epoch 9,step 364000, training loss 0.0415606\n",
      "epoch 9,step 365000, training loss 0.0710986\n",
      "epoch 9,step 366000, training loss 0.0604033\n",
      "epoch 9,step 367000, training loss 0.0575848\n",
      "epoch 9,step 368000, training loss 0.0566301\n",
      "epoch 9,step 369000, training loss 0.0527246\n",
      "epoch 9,step 370000, training loss 0.0694418\n",
      "epoch 9,step 371000, training loss 0.146805\n",
      "epoch 9,step 372000, training loss 0.084632\n",
      "epoch 9,step 373000, training loss 0.0418298\n",
      "epoch 9,step 374000, training loss 0.0716325\n",
      "epoch 9,step 375000, training loss 0.0499732\n",
      "epoch 9,step 376000, training loss 0.0495604\n",
      "epoch 9,step 377000, training loss 0.0480326\n",
      "epoch 9,step 378000, training loss 0.0655443\n",
      "epoch 9,step 379000, training loss 0.0409932\n",
      "epoch 9,step 380000, training loss 0.0512157\n",
      "epoch 9,step 381000, training loss 0.146565\n",
      "epoch 9,step 382000, training loss 0.156059\n",
      "epoch 9,step 383000, training loss 0.0741929\n",
      "epoch 9,step 384000, training loss 0.0526\n",
      "epoch 9,step 385000, training loss 0.0422708\n",
      "epoch 9,step 386000, training loss 0.0577404\n",
      "epoch 9,step 387000, training loss 0.0543631\n",
      "epoch 9,step 388000, training loss 0.0444414\n",
      "epoch 9,step 389000, training loss 0.0528073\n",
      "epoch 9,step 390000, training loss 0.0440088\n",
      "epoch 9,step 391000, training loss 0.145307\n",
      "epoch 9,step 392000, training loss 0.0887171\n",
      "epoch 9,step 393000, training loss 0.0424176\n",
      "epoch 9,step 394000, training loss 0.0441814\n",
      "epoch 9,step 395000, training loss 0.0520565\n",
      "epoch 9,step 396000, training loss 0.0413776\n",
      "epoch 9,step 397000, training loss 0.043874\n",
      "epoch 9,step 398000, training loss 0.0609148\n",
      "epoch 9,step 399000, training loss 0.0411248\n",
      "epoch 9,step 400000, training loss 0.0346476\n",
      "epoch 9,step 401000, training loss 0.16218\n",
      "epoch 9,step 402000, training loss 0.152935\n",
      "epoch 9,step 403000, training loss 0.0443428\n",
      "epoch 9,step 404000, training loss 0.0474438\n",
      "epoch 9,step 405000, training loss 0.0405827\n",
      "epoch 9,step 406000, training loss 0.0477726\n",
      "epoch 9,step 407000, training loss 0.0394578\n",
      "epoch 9,step 408000, training loss 0.0544334\n",
      "epoch 9,step 409000, training loss 0.0556762\n",
      "epoch 9,step 410000, training loss 0.0448584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9,step 411000, training loss 0.136197\n",
      "epoch 9,step 412000, training loss 0.0782569\n",
      "epoch 9,step 413000, training loss 0.0537093\n",
      "epoch 9,step 414000, training loss 0.0580408\n",
      "epoch 9,step 415000, training loss 0.0437371\n",
      "epoch 9,step 416000, training loss 0.0576298\n",
      "epoch 9,step 417000, training loss 0.056652\n",
      "epoch 9,step 418000, training loss 0.051602\n",
      "epoch 9,step 419000, training loss 0.0386088\n",
      "epoch 9,step 420000, training loss 0.0435934\n",
      "epoch 9,step 421000, training loss 0.138729\n",
      "epoch 9,step 422000, training loss 0.0768952\n",
      "epoch 9,step 423000, training loss 0.0279774\n",
      "epoch 9,step 424000, training loss 0.0432997\n",
      "epoch 9,step 425000, training loss 0.0634926\n",
      "epoch 9,step 426000, training loss 0.0575729\n",
      "epoch 9,step 427000, training loss 0.057381\n",
      "epoch 9,step 428000, training loss 0.0625846\n",
      "epoch 9,step 429000, training loss 0.0401543\n",
      "epoch 9,step 430000, training loss 0.047706\n",
      "epoch 9,step 431000, training loss 0.131216\n",
      "epoch 9,step 432000, training loss 0.106941\n",
      "epoch 9,step 433000, training loss 0.0619077\n",
      "epoch 9,step 434000, training loss 0.0402823\n",
      "epoch 9,step 435000, training loss 0.0616008\n",
      "epoch 9,step 436000, training loss 0.0427017\n",
      "epoch 9,step 437000, training loss 0.0503146\n",
      "epoch 9,step 438000, training loss 0.0504468\n",
      "epoch 9,step 439000, training loss 0.0491161\n",
      "epoch 9,step 440000, training loss 0.0525133\n",
      "epoch 9,step 441000, training loss 0.153107\n",
      "epoch 9,step 442000, training loss 0.0708314\n",
      "epoch 9,step 443000, training loss 0.0541016\n",
      "epoch 9,step 444000, training loss 0.0382733\n",
      "epoch 9,step 445000, training loss 0.0339308\n",
      "epoch 9,step 446000, training loss 0.0378765\n",
      "epoch 9,step 447000, training loss 0.0520962\n",
      "epoch 9,step 448000, training loss 0.0422349\n",
      "epoch 9,step 449000, training loss 0.0508653\n",
      "epoch 9,step 450000, training loss 0.0489175\n",
      "epoch 9,step 451000, training loss 0.138475\n",
      "epoch 9,step 452000, training loss 0.0720522\n",
      "epoch 9,step 453000, training loss 0.0550018\n",
      "epoch 9,step 454000, training loss 0.0553742\n",
      "epoch 9,step 455000, training loss 0.043111\n",
      "epoch 9,step 456000, training loss 0.0382255\n",
      "epoch 9,step 457000, training loss 0.0438454\n",
      "epoch 9,step 458000, training loss 0.048941\n",
      "epoch 9,step 459000, training loss 0.034203\n",
      "epoch 9,step 460000, training loss 0.047867\n",
      "epoch 9,step 461000, training loss 0.165421\n",
      "epoch 9,step 462000, training loss 0.0908618\n",
      "epoch 9,step 463000, training loss 0.0580674\n",
      "epoch 9,step 464000, training loss 0.0608547\n",
      "epoch 9,step 465000, training loss 0.0639272\n",
      "epoch 9,step 466000, training loss 0.0498431\n",
      "epoch 9,step 467000, training loss 0.0771912\n",
      "epoch 9,step 468000, training loss 0.0490991\n",
      "epoch 9,step 469000, training loss 0.0454874\n",
      "epoch 9,step 470000, training loss 0.0545383\n",
      "epoch 9,step 471000, training loss 0.144843\n",
      "epoch 9,step 472000, training loss 0.152413\n",
      "epoch 9,step 473000, training loss 0.0567992\n",
      "epoch 9,step 474000, training loss 0.0525752\n",
      "epoch 9,step 475000, training loss 0.038256\n",
      "epoch 9,step 476000, training loss 0.0783833\n",
      "epoch 9,step 477000, training loss 0.0508375\n",
      "epoch 9,step 478000, training loss 0.0557162\n",
      "epoch 9,step 479000, training loss 0.0436935\n",
      "epoch 9,step 480000, training loss 0.0504695\n",
      "epoch 9,step 481000, training loss 0.115474\n",
      "epoch 9,step 482000, training loss 0.0655854\n",
      "epoch 9,step 483000, training loss 0.0526637\n",
      "epoch 9,step 484000, training loss 0.0511817\n",
      "epoch 9,step 485000, training loss 0.0490993\n",
      "epoch 9,step 486000, training loss 0.0527916\n",
      "epoch 9,step 487000, training loss 0.0413182\n",
      "epoch 9,step 488000, training loss 0.0432379\n",
      "epoch 9,step 489000, training loss 0.0494386\n",
      "epoch 9,step 490000, training loss 0.0563432\n",
      "epoch 9,step 491000, training loss 0.132539\n",
      "epoch 9,step 492000, training loss 0.0725643\n",
      "epoch 9,step 493000, training loss 0.0375219\n",
      "epoch 9,step 494000, training loss 0.0664025\n",
      "epoch 9,step 495000, training loss 0.0526544\n",
      "epoch 9,step 496000, training loss 0.0444315\n",
      "epoch 9,step 497000, training loss 0.0422627\n",
      "epoch 9,step 498000, training loss 0.0473169\n",
      "epoch 9,step 499000, training loss 0.0828159\n",
      "epoch 9,step 500000, training loss 0.044134\n",
      "epoch 9,step 501000, training loss 0.163513\n",
      "epoch 9,step 502000, training loss 0.0834325\n",
      "epoch 9,step 503000, training loss 0.0542573\n",
      "epoch 9,step 504000, training loss 0.0529683\n",
      "epoch 9,step 505000, training loss 0.0504551\n",
      "epoch 9,step 506000, training loss 0.0587134\n",
      "epoch 9,step 507000, training loss 0.0374451\n",
      "epoch 9,step 508000, training loss 0.046208\n",
      "epoch 9,step 509000, training loss 0.0530031\n",
      "epoch 9,step 510000, training loss 0.0457782\n",
      "epoch 9,step 511000, training loss 0.162287\n",
      "epoch 9,step 512000, training loss 0.152265\n",
      "epoch 9,step 513000, training loss 0.0508806\n",
      "epoch 9,step 514000, training loss 0.0514984\n",
      "epoch 9,step 515000, training loss 0.0520508\n",
      "epoch 9,step 516000, training loss 0.0605845\n",
      "epoch 9,step 517000, training loss 0.0600344\n",
      "epoch 9,step 518000, training loss 0.0496753\n",
      "epoch 9,step 519000, training loss 0.0628273\n",
      "epoch 9,step 520000, training loss 0.0410916\n",
      "epoch 9,step 521000, training loss 0.150058\n",
      "epoch 9,step 522000, training loss 0.108222\n",
      "epoch 9,step 523000, training loss 0.0505164\n",
      "epoch 9,step 524000, training loss 0.0453555\n",
      "epoch 9,step 525000, training loss 0.0538121\n",
      "epoch 9,step 526000, training loss 0.046238\n",
      "epoch 9,step 527000, training loss 0.054023\n",
      "epoch 9,step 528000, training loss 0.0602638\n",
      "epoch 9,step 529000, training loss 0.0856469\n",
      "epoch 9,step 530000, training loss 0.0736785\n",
      "epoch 9,step 531000, training loss 0.186583\n",
      "epoch 9,step 532000, training loss 0.166373\n",
      "epoch 9,step 533000, training loss 0.0724991\n",
      "epoch 9,step 534000, training loss 0.0834015\n",
      "epoch 9,step 535000, training loss 0.0609563\n",
      "epoch 9,step 536000, training loss 0.053303\n",
      "epoch 9,step 537000, training loss 0.0542228\n",
      "epoch 9,step 538000, training loss 0.0521711\n",
      "epoch 9,step 539000, training loss 0.0586417\n",
      "epoch 9,step 540000, training loss 0.0545442\n",
      "epoch 9,step 541000, training loss 0.117695\n",
      "epoch 9,step 542000, training loss 0.134883\n",
      "epoch 9,step 543000, training loss 0.0844021\n",
      "epoch 9,step 544000, training loss 0.0808948\n",
      "epoch 9,step 545000, training loss 0.0550713\n",
      "epoch 9,step 546000, training loss 0.0491192\n",
      "epoch 9,step 547000, training loss 0.0450369\n",
      "epoch 9,step 548000, training loss 0.0586812\n",
      "epoch 9,step 549000, training loss 0.0413208\n",
      "epoch 9,step 550000, training loss 0.0696134\n",
      "epoch 9,step 551000, training loss 0.134723\n",
      "epoch 9,step 552000, training loss 0.083211\n",
      "epoch 9,step 553000, training loss 0.0614851\n",
      "epoch 9,step 554000, training loss 0.0667421\n",
      "epoch 9,step 555000, training loss 0.0626465\n",
      "epoch 9,step 556000, training loss 0.0621428\n",
      "epoch 9,step 557000, training loss 0.0418909\n",
      "epoch 9,step 558000, training loss 0.0686842\n",
      "epoch 9,step 559000, training loss 0.0533033\n",
      "epoch 9,step 560000, training loss 0.0724796\n",
      "epoch 9,step 561000, training loss 0.140568\n",
      "epoch 9,step 562000, training loss 0.117567\n",
      "epoch 9,step 563000, training loss 0.0701949\n",
      "epoch 9,step 564000, training loss 0.0571414\n",
      "epoch 9,step 565000, training loss 0.0539562\n",
      "epoch 9,step 566000, training loss 0.0541121\n",
      "epoch 9,step 567000, training loss 0.0472599\n",
      "epoch 9,step 568000, training loss 0.0525488\n",
      "epoch 9,step 569000, training loss 0.0522368\n",
      "epoch 9,step 570000, training loss 0.0434856\n",
      "epoch 9,step 571000, training loss 0.152926\n",
      "epoch 9,step 572000, training loss 0.104021\n",
      "epoch 9,step 573000, training loss 0.0734233\n",
      "epoch 9,step 574000, training loss 0.0617768\n",
      "epoch 9,step 575000, training loss 0.087327\n",
      "epoch 9,step 576000, training loss 0.0667008\n",
      "epoch 9,step 577000, training loss 0.0641726\n",
      "epoch 9,step 578000, training loss 0.0733695\n",
      "epoch 9,step 579000, training loss 0.0456838\n",
      "epoch 9,step 580000, training loss 0.0463538\n",
      "epoch 9,step 581000, training loss 0.156013\n",
      "epoch 9,step 582000, training loss 0.0883983\n",
      "epoch 9,step 583000, training loss 0.056071\n",
      "epoch 9,step 584000, training loss 0.0587196\n",
      "epoch 9,step 585000, training loss 0.0518845\n",
      "epoch 9,step 586000, training loss 0.0521277\n",
      "epoch 9,step 587000, training loss 0.0506147\n",
      "epoch 9,step 588000, training loss 0.0655953\n",
      "epoch 9,step 589000, training loss 0.0558701\n",
      "epoch 9,step 590000, training loss 0.0417788\n",
      "epoch 9,step 591000, training loss 0.138745\n",
      "epoch 9,step 592000, training loss 0.0753733\n",
      "epoch 9,step 593000, training loss 0.0584867\n",
      "epoch 9,step 594000, training loss 0.0611108\n",
      "epoch 9,step 595000, training loss 0.0527691\n",
      "epoch 9,step 596000, training loss 0.0483424\n",
      "epoch 9,step 597000, training loss 0.0380427\n",
      "epoch 9,step 598000, training loss 0.0458573\n",
      "epoch 9,step 599000, training loss 0.0520352\n",
      "epoch 9,step 600000, training loss 0.0577499\n",
      "epoch 9,step 601000, training loss 0.192981\n",
      "epoch 9,step 602000, training loss 0.077876\n",
      "epoch 9,step 603000, training loss 0.0692022\n",
      "epoch 9,step 604000, training loss 0.0849102\n",
      "epoch 9,step 605000, training loss 0.0397034\n",
      "epoch 9,step 606000, training loss 0.054781\n",
      "epoch 9,step 607000, training loss 0.0460891\n",
      "epoch 9,step 608000, training loss 0.0411742\n",
      "epoch 9,step 609000, training loss 0.0490732\n",
      "epoch 9,step 610000, training loss 0.0416307\n",
      "epoch 9,step 611000, training loss 0.17035\n",
      "epoch 9,step 612000, training loss 0.0992013\n",
      "epoch 9,step 613000, training loss 0.0527643\n",
      "epoch 9,step 614000, training loss 0.0380003\n",
      "epoch 9,step 615000, training loss 0.0437735\n",
      "epoch 9,step 616000, training loss 0.0561388\n",
      "epoch 9,step 617000, training loss 0.0438408\n",
      "epoch 9,step 618000, training loss 0.0402648\n",
      "epoch 9,step 619000, training loss 0.0393796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9,step 620000, training loss 0.0599523\n",
      "epoch 9,step 621000, training loss 0.18898\n",
      "epoch 9,step 622000, training loss 0.181476\n",
      "epoch 9,step 623000, training loss 0.0735122\n",
      "epoch 9,step 624000, training loss 0.0790124\n",
      "epoch 9,step 625000, training loss 0.0603432\n",
      "epoch 9,step 626000, training loss 0.0681033\n",
      "epoch 9,step 627000, training loss 0.0412289\n",
      "epoch 9,step 628000, training loss 0.0512525\n",
      "epoch 9,step 629000, training loss 0.0436944\n",
      "epoch 9,step 630000, training loss 0.0552167\n",
      "epoch 9,step 631000, training loss 0.152409\n",
      "epoch 9,step 632000, training loss 0.0860176\n",
      "epoch 9,step 633000, training loss 0.0467384\n",
      "epoch 9,step 634000, training loss 0.0591286\n",
      "epoch 9,step 635000, training loss 0.0414378\n",
      "epoch 9,step 636000, training loss 0.0386397\n",
      "epoch 9,step 637000, training loss 0.0412268\n",
      "epoch 9,step 638000, training loss 0.0652492\n",
      "epoch 9,step 639000, training loss 0.0523521\n",
      "epoch 9,step 640000, training loss 0.0510654\n",
      "epoch 9,step 641000, training loss 0.195381\n",
      "epoch 9,step 642000, training loss 0.101407\n",
      "epoch 9,step 643000, training loss 0.044726\n",
      "epoch 9,step 644000, training loss 0.0537363\n",
      "epoch 9,step 645000, training loss 0.0579626\n",
      "epoch 9,step 646000, training loss 0.0576939\n",
      "epoch 9,step 647000, training loss 0.126007\n",
      "epoch 9,step 648000, training loss 0.0461092\n",
      "epoch 9,step 649000, training loss 0.0455518\n",
      "epoch 9,step 650000, training loss 0.0535092\n",
      "epoch 9,step 651000, training loss 0.148382\n",
      "epoch 9,step 652000, training loss 0.0776641\n",
      "epoch 9,step 653000, training loss 0.0428343\n",
      "epoch 9,step 654000, training loss 0.0475866\n",
      "epoch 9,step 655000, training loss 0.0457865\n",
      "epoch 9,step 656000, training loss 0.0478541\n",
      "epoch 9,step 657000, training loss 0.0433848\n",
      "epoch 9,step 658000, training loss 0.0626525\n",
      "epoch 9,step 659000, training loss 0.0686276\n",
      "epoch 9,step 660000, training loss 0.0431563\n",
      "epoch 9,step 661000, training loss 0.164925\n",
      "epoch 9,step 662000, training loss 0.0974709\n",
      "epoch 9,step 663000, training loss 0.0495982\n",
      "epoch 9,step 664000, training loss 0.0519602\n",
      "epoch 9,step 665000, training loss 0.0396466\n",
      "epoch 9,step 666000, training loss 0.0427303\n",
      "epoch 9,step 667000, training loss 0.051872\n",
      "epoch 9,step 668000, training loss 0.0632768\n",
      "epoch 9,step 669000, training loss 0.0517102\n",
      "epoch 9,step 670000, training loss 0.0539808\n",
      "epoch 9,step 671000, training loss 0.143139\n",
      "epoch 9,step 672000, training loss 0.155208\n",
      "epoch 9,step 673000, training loss 0.0549501\n",
      "epoch 9,step 674000, training loss 0.0691669\n",
      "epoch 9,step 675000, training loss 0.0436292\n",
      "epoch 9,step 676000, training loss 0.062846\n",
      "epoch 9,step 677000, training loss 0.0622783\n",
      "epoch 9,step 678000, training loss 0.0564086\n",
      "epoch 9,step 679000, training loss 0.0445054\n",
      "epoch 9,step 680000, training loss 0.0528237\n",
      "epoch 9,step 681000, training loss 0.156772\n",
      "epoch 9,step 682000, training loss 0.0893811\n",
      "epoch 9,step 683000, training loss 0.0654479\n",
      "epoch 9,step 684000, training loss 0.0659658\n",
      "epoch 9,step 685000, training loss 0.0443709\n",
      "epoch 9,step 686000, training loss 0.0519382\n",
      "epoch 9,step 687000, training loss 0.05108\n",
      "epoch 9,step 688000, training loss 0.0674177\n",
      "epoch 9,step 689000, training loss 0.053901\n",
      "epoch 9,step 690000, training loss 0.0529077\n",
      "epoch 9,step 691000, training loss 0.150034\n",
      "epoch 9,step 692000, training loss 0.0917112\n",
      "epoch 9,step 693000, training loss 0.0414631\n",
      "epoch 9,step 694000, training loss 0.0454143\n",
      "epoch 9,step 695000, training loss 0.0454717\n",
      "epoch 9,step 696000, training loss 0.0446888\n",
      "epoch 9,step 697000, training loss 0.0496903\n",
      "epoch 9,step 698000, training loss 0.0605192\n",
      "epoch 9,step 699000, training loss 0.0508986\n",
      "epoch 9,step 700000, training loss 0.0396702\n",
      "epoch 9,step 701000, training loss 0.142267\n",
      "epoch 9,step 702000, training loss 0.116666\n",
      "epoch 9,step 703000, training loss 0.0637917\n",
      "epoch 9,step 704000, training loss 0.0449918\n",
      "epoch 9,step 705000, training loss 0.0473969\n",
      "epoch 9,step 706000, training loss 0.0499655\n",
      "epoch 9,step 707000, training loss 0.0512257\n",
      "epoch 9,step 708000, training loss 0.0393111\n",
      "epoch 9,step 709000, training loss 0.04396\n",
      "epoch 9,step 710000, training loss 0.0475053\n",
      "epoch 9,step 711000, training loss 0.155632\n",
      "epoch 9,step 712000, training loss 0.0884767\n",
      "epoch 9,step 713000, training loss 0.0487707\n",
      "epoch 9,step 714000, training loss 0.0467989\n",
      "epoch 9,step 715000, training loss 0.0500293\n",
      "epoch 9,step 716000, training loss 0.0478202\n",
      "epoch 9,step 717000, training loss 0.0647703\n",
      "epoch 9,step 718000, training loss 0.0689298\n",
      "epoch 9,step 719000, training loss 0.0459666\n",
      "epoch 9,step 720000, training loss 0.0430958\n",
      "epoch 9,step 721000, training loss 0.12049\n",
      "epoch 9,step 722000, training loss 0.0677271\n",
      "epoch 9,step 723000, training loss 0.0724799\n",
      "epoch 9,step 724000, training loss 0.0567095\n",
      "epoch 9,step 725000, training loss 0.0575815\n",
      "epoch 9,step 726000, training loss 0.0438138\n",
      "epoch 9,step 727000, training loss 0.0663741\n",
      "epoch 9,step 728000, training loss 0.0507031\n",
      "epoch 9,step 729000, training loss 0.0441529\n",
      "epoch 9,step 730000, training loss 0.0488106\n",
      "epoch 9,step 731000, training loss 0.152879\n",
      "epoch 9,step 732000, training loss 0.0821948\n",
      "epoch 9,step 733000, training loss 0.0476357\n",
      "epoch 9,step 734000, training loss 0.0643806\n",
      "epoch 9,step 735000, training loss 0.0559319\n",
      "epoch 9,step 736000, training loss 0.0551959\n",
      "epoch 9,step 737000, training loss 0.045638\n",
      "epoch 9,step 738000, training loss 0.0596249\n",
      "epoch 9,step 739000, training loss 0.0886915\n",
      "epoch 9,step 740000, training loss 0.0672215\n",
      "epoch 9,step 741000, training loss 0.114611\n",
      "epoch 9,step 742000, training loss 0.0646425\n",
      "epoch 9,step 743000, training loss 0.0457313\n",
      "epoch 9,step 744000, training loss 0.0367188\n",
      "epoch 9,step 745000, training loss 0.0402273\n",
      "epoch 9,step 746000, training loss 0.0446177\n",
      "epoch 9,step 747000, training loss 0.0451931\n",
      "epoch 9,step 748000, training loss 0.0376996\n",
      "epoch 9,step 749000, training loss 0.0411271\n",
      "epoch 9,step 750000, training loss 0.103542\n",
      "epoch 9,step 751000, training loss 0.145589\n",
      "epoch 9,step 752000, training loss 0.0734635\n",
      "epoch 9,step 753000, training loss 0.0431557\n",
      "epoch 9,step 754000, training loss 0.0487458\n",
      "epoch 9,step 755000, training loss 0.0482689\n",
      "epoch 9,step 756000, training loss 0.0477636\n",
      "epoch 9,step 757000, training loss 0.0499818\n",
      "epoch 9,step 758000, training loss 0.0472283\n",
      "epoch 9,step 759000, training loss 0.0298756\n",
      "epoch 9,step 760000, training loss 0.042716\n",
      "epoch 9,step 761000, training loss 0.1332\n",
      "epoch 9,step 762000, training loss 0.121938\n",
      "epoch 9,step 763000, training loss 0.0488993\n",
      "epoch 9,step 764000, training loss 0.0407837\n",
      "epoch 9,step 765000, training loss 0.0684518\n",
      "epoch 9,step 766000, training loss 0.074294\n",
      "epoch 9,step 767000, training loss 0.0414812\n",
      "epoch 9,step 768000, training loss 0.0632988\n",
      "epoch 9,step 769000, training loss 0.0452717\n",
      "epoch 9,step 770000, training loss 0.0352966\n",
      "epoch 9,step 771000, training loss 0.168461\n",
      "epoch 9,step 772000, training loss 0.121578\n",
      "epoch 9,step 773000, training loss 0.0573633\n",
      "epoch 9,step 774000, training loss 0.0595264\n",
      "epoch 9,step 775000, training loss 0.0509184\n",
      "epoch 9,step 776000, training loss 0.0419643\n",
      "epoch 9,step 777000, training loss 0.0415822\n",
      "epoch 9,step 778000, training loss 0.054198\n",
      "epoch 9,step 779000, training loss 0.0524204\n",
      "epoch 9,step 780000, training loss 0.0401172\n",
      "epoch 9,step 781000, training loss 0.130689\n",
      "epoch 9,step 782000, training loss 0.111525\n",
      "epoch 9,step 783000, training loss 0.0530042\n",
      "epoch 9,step 784000, training loss 0.0396052\n",
      "epoch 9,step 785000, training loss 0.0478914\n",
      "epoch 9,step 786000, training loss 0.0422268\n",
      "epoch 9,step 787000, training loss 0.0858441\n",
      "epoch 9,step 788000, training loss 0.0481511\n",
      "epoch 9,step 789000, training loss 0.0424461\n",
      "epoch 9,step 790000, training loss 0.0528208\n",
      "epoch 9,step 791000, training loss 0.177977\n",
      "epoch 9,step 792000, training loss 0.091582\n",
      "epoch 9,step 793000, training loss 0.072997\n",
      "epoch 9,step 794000, training loss 0.0665728\n",
      "epoch 9,step 795000, training loss 0.0445919\n",
      "epoch 9,step 796000, training loss 0.051806\n",
      "epoch 9,step 797000, training loss 0.0382642\n",
      "epoch 9,step 798000, training loss 0.0477169\n",
      "epoch 9,step 799000, training loss 0.0420497\n",
      "epoch 9,step 800000, training loss 0.0454933\n",
      "epoch 9,step 801000, training loss 0.150602\n",
      "epoch 9,step 802000, training loss 0.0700448\n",
      "epoch 9,step 803000, training loss 0.0548779\n",
      "epoch 9,step 804000, training loss 0.0616261\n",
      "epoch 9,step 805000, training loss 0.0477828\n",
      "epoch 9,step 806000, training loss 0.0661988\n",
      "epoch 9,step 807000, training loss 0.046453\n",
      "epoch 9,step 808000, training loss 0.0545161\n",
      "epoch 9,step 809000, training loss 0.0479726\n",
      "epoch 9,step 810000, training loss 0.0422338\n",
      "epoch 9,step 811000, training loss 0.128698\n",
      "epoch 9,step 812000, training loss 0.111852\n",
      "epoch 9,step 813000, training loss 0.0493688\n",
      "epoch 9,step 814000, training loss 0.047939\n",
      "epoch 9,step 815000, training loss 0.0549795\n",
      "epoch 9,step 816000, training loss 0.0456897\n",
      "epoch 9,step 817000, training loss 0.0496191\n",
      "epoch 9,step 818000, training loss 0.0557672\n",
      "epoch 9,step 819000, training loss 0.044773\n",
      "epoch 9,step 820000, training loss 0.0497651\n",
      "epoch 9,step 821000, training loss 0.152538\n",
      "epoch 9,step 822000, training loss 0.0649792\n",
      "epoch 9,step 823000, training loss 0.0436537\n",
      "epoch 9,step 824000, training loss 0.0574942\n",
      "epoch 9,step 825000, training loss 0.0612934\n",
      "epoch 9,step 826000, training loss 0.0559381\n",
      "epoch 9,step 827000, training loss 0.0504933\n",
      "epoch 9,step 828000, training loss 0.0796863\n",
      "epoch 9,step 829000, training loss 0.0592936\n",
      "epoch 9,step 830000, training loss 0.0509968\n",
      "epoch 9,step 831000, training loss 0.144983\n",
      "epoch 9,step 832000, training loss 0.0978132\n",
      "epoch 9,step 833000, training loss 0.0461334\n",
      "epoch 9,step 834000, training loss 0.0528358\n",
      "epoch 9,step 835000, training loss 0.0508108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9,step 836000, training loss 0.0669801\n",
      "epoch 9,step 837000, training loss 0.0435916\n",
      "epoch 9,step 838000, training loss 0.0459953\n",
      "epoch 9,step 839000, training loss 0.0622878\n",
      "epoch 9,step 840000, training loss 0.0436946\n",
      "epoch 9,step 841000, training loss 0.148893\n",
      "epoch 9,step 842000, training loss 0.109917\n",
      "epoch 9,step 843000, training loss 0.0580276\n",
      "epoch 9,step 844000, training loss 0.0426184\n",
      "epoch 9,step 845000, training loss 0.0517614\n",
      "epoch 9,step 846000, training loss 0.0537494\n",
      "epoch 9,step 847000, training loss 0.0705906\n",
      "epoch 9,step 848000, training loss 0.0372287\n",
      "epoch 9,step 849000, training loss 0.0460541\n",
      "epoch 9,step 850000, training loss 0.0517067\n",
      "epoch 9,step 851000, training loss 0.132239\n",
      "epoch 9,step 852000, training loss 0.0809353\n",
      "epoch 9,step 853000, training loss 0.0549686\n",
      "epoch 9,step 854000, training loss 0.064352\n",
      "epoch 9,step 855000, training loss 0.0580741\n",
      "epoch 9,step 856000, training loss 0.0534832\n",
      "epoch 9,step 857000, training loss 0.0483069\n",
      "epoch 9,step 858000, training loss 0.0509886\n",
      "epoch 9,step 859000, training loss 0.0480896\n",
      "epoch 9,step 860000, training loss 0.0614454\n",
      "epoch 9,step 861000, training loss 0.129381\n",
      "epoch 9,step 862000, training loss 0.0961031\n",
      "epoch 9,step 863000, training loss 0.0433172\n",
      "epoch 9,step 864000, training loss 0.0444962\n",
      "epoch 9,step 865000, training loss 0.0375893\n",
      "epoch 9,step 866000, training loss 0.0373414\n",
      "epoch 9,step 867000, training loss 0.0493476\n",
      "epoch 9,step 868000, training loss 0.0442291\n",
      "epoch 9,step 869000, training loss 0.0559993\n",
      "epoch 9,step 870000, training loss 0.0552173\n",
      "epoch 9,step 871000, training loss 0.13703\n",
      "epoch 9,step 872000, training loss 0.0772962\n",
      "epoch 9,step 873000, training loss 0.0440046\n",
      "epoch 9,step 874000, training loss 0.0446701\n",
      "epoch 9,step 875000, training loss 0.04312\n",
      "epoch 9,step 876000, training loss 0.0598978\n",
      "epoch 9,step 877000, training loss 0.0603689\n",
      "epoch 9,step 878000, training loss 0.0385539\n",
      "epoch 9,step 879000, training loss 0.0311323\n",
      "epoch 9,step 880000, training loss 0.0455745\n",
      "epoch 9,step 881000, training loss 0.187309\n",
      "epoch 9,step 882000, training loss 0.118923\n",
      "epoch 9,step 883000, training loss 0.076094\n",
      "epoch 9,step 884000, training loss 0.040574\n",
      "epoch 9,step 885000, training loss 0.0461657\n",
      "epoch 9,step 886000, training loss 0.0402878\n",
      "epoch 9,step 887000, training loss 0.0423215\n",
      "epoch 9,step 888000, training loss 0.0396334\n",
      "epoch 9,step 889000, training loss 0.0795456\n",
      "epoch 9,step 890000, training loss 0.0385767\n",
      "epoch 9,step 891000, training loss 0.155089\n",
      "epoch 9,step 892000, training loss 0.128159\n",
      "epoch 9,step 893000, training loss 0.0557337\n",
      "epoch 9,step 894000, training loss 0.0527993\n",
      "epoch 9,step 895000, training loss 0.0631948\n",
      "epoch 9,step 896000, training loss 0.0675667\n",
      "epoch 9,step 897000, training loss 0.0558095\n",
      "epoch 9,step 898000, training loss 0.0553359\n",
      "epoch 9,step 899000, training loss 0.0566543\n",
      "epoch 9,step 900000, training loss 0.0969647\n",
      "epoch 9,step 901000, training loss 0.131843\n",
      "epoch 9,step 902000, training loss 0.0620148\n",
      "epoch 9,step 903000, training loss 0.0428117\n",
      "epoch 9,step 904000, training loss 0.0703879\n",
      "epoch 9,step 905000, training loss 0.0461414\n",
      "epoch 9,step 906000, training loss 0.0455477\n",
      "epoch 9,step 907000, training loss 0.0381277\n",
      "epoch 9,step 908000, training loss 0.0402636\n",
      "epoch 9,step 909000, training loss 0.0464003\n",
      "epoch 9,step 910000, training loss 0.042345\n",
      "epoch 9,step 911000, training loss 0.128839\n",
      "epoch 9,step 912000, training loss 0.0757434\n",
      "epoch 9,step 913000, training loss 0.0495974\n",
      "epoch 9,step 914000, training loss 0.0590608\n",
      "epoch 9,step 915000, training loss 0.0582319\n",
      "epoch 9,step 916000, training loss 0.0548522\n",
      "epoch 9,step 917000, training loss 0.0440793\n",
      "epoch 9,step 918000, training loss 0.0539081\n",
      "epoch 9,step 919000, training loss 0.0567649\n",
      "epoch 9,step 920000, training loss 0.0489223\n",
      "epoch 9,step 921000, training loss 0.117161\n",
      "epoch 9,step 922000, training loss 0.109512\n",
      "epoch 9,step 923000, training loss 0.0488255\n",
      "epoch 9,step 924000, training loss 0.0432019\n",
      "epoch 9,step 925000, training loss 0.0368027\n",
      "epoch 9,step 926000, training loss 0.051805\n",
      "epoch 9,step 927000, training loss 0.0438467\n",
      "epoch 9,step 928000, training loss 0.0365704\n",
      "epoch 9,step 929000, training loss 0.0465431\n",
      "epoch 9,step 930000, training loss 0.0397389\n",
      "epoch 9,step 931000, training loss 0.142607\n",
      "epoch 9,step 932000, training loss 0.0679684\n",
      "epoch 9,step 933000, training loss 0.0471458\n",
      "epoch 9,step 934000, training loss 0.0564992\n",
      "epoch 9,step 935000, training loss 0.0688999\n",
      "epoch 9,step 936000, training loss 0.0469277\n",
      "epoch 9,step 937000, training loss 0.041215\n",
      "epoch 9,step 938000, training loss 0.0583845\n",
      "epoch 9,step 939000, training loss 0.0494092\n",
      "epoch 9,step 940000, training loss 0.0375243\n",
      "epoch 9,step 941000, training loss 0.157468\n",
      "epoch 9,step 942000, training loss 0.0948915\n",
      "epoch 9,step 943000, training loss 0.0427389\n",
      "epoch 9,step 944000, training loss 0.058051\n",
      "epoch 9,step 945000, training loss 0.0488096\n",
      "epoch 9,step 946000, training loss 0.0520017\n",
      "epoch 9,step 947000, training loss 0.0568531\n",
      "epoch 9,step 948000, training loss 0.037947\n",
      "epoch 9,step 949000, training loss 0.0547597\n",
      "epoch 9,step 950000, training loss 0.0340809\n",
      "epoch 9,step 951000, training loss 0.140631\n",
      "epoch 9,step 952000, training loss 0.123852\n",
      "epoch 9,step 953000, training loss 0.0433232\n",
      "epoch 9,step 954000, training loss 0.0359673\n",
      "epoch 9,step 955000, training loss 0.0391329\n",
      "epoch 9,step 956000, training loss 0.055783\n",
      "epoch 9,step 957000, training loss 0.0568456\n",
      "epoch 9,step 958000, training loss 0.0640354\n",
      "epoch 9,step 959000, training loss 0.0453849\n",
      "epoch 9,step 960000, training loss 0.0528693\n",
      "epoch 9,step 961000, training loss 0.143261\n",
      "epoch 9,step 962000, training loss 0.0810769\n",
      "epoch 9,step 963000, training loss 0.035768\n",
      "epoch 9,step 964000, training loss 0.0381076\n",
      "epoch 9,step 965000, training loss 0.0382999\n",
      "epoch 9,step 966000, training loss 0.0446228\n",
      "epoch 9,step 967000, training loss 0.0433316\n",
      "epoch 9,step 968000, training loss 0.0436454\n",
      "epoch 9,step 969000, training loss 0.0420372\n",
      "epoch 9,step 970000, training loss 0.046708\n",
      "epoch 9,step 971000, training loss 0.149142\n",
      "epoch 9,step 972000, training loss 0.0784006\n",
      "epoch 9,step 973000, training loss 0.0426653\n",
      "epoch 9,step 974000, training loss 0.0429269\n",
      "epoch 9,step 975000, training loss 0.0575865\n",
      "epoch 9,step 976000, training loss 0.0378796\n",
      "epoch 9,step 977000, training loss 0.0425869\n",
      "epoch 9,step 978000, training loss 0.0433097\n",
      "epoch 9,step 979000, training loss 0.0414371\n",
      "epoch 9,step 980000, training loss 0.0456848\n",
      "epoch 9,step 981000, training loss 0.143117\n",
      "epoch 9,step 982000, training loss 0.0854388\n",
      "epoch 9,step 983000, training loss 0.060899\n",
      "epoch 9,step 984000, training loss 0.0468412\n",
      "epoch 9,step 985000, training loss 0.0390724\n",
      "epoch 9,step 986000, training loss 0.0852291\n",
      "epoch 9,step 987000, training loss 0.0459416\n",
      "epoch 9,step 988000, training loss 0.0491325\n",
      "epoch 9,step 989000, training loss 0.0398112\n",
      "epoch 9,step 990000, training loss 0.0355161\n",
      "epoch 9,step 991000, training loss 0.135249\n",
      "epoch 9,step 992000, training loss 0.0686044\n",
      "epoch 9,step 993000, training loss 0.0464484\n",
      "epoch 9,step 994000, training loss 0.0310893\n",
      "epoch 9,step 995000, training loss 0.0332595\n",
      "epoch 9,step 996000, training loss 0.0418578\n",
      "epoch 9,step 997000, training loss 0.031452\n",
      "epoch 9,step 998000, training loss 0.0416048\n",
      "epoch 9,step 999000, training loss 0.0578046\n",
      "epoch 9,training loss 0.0578046 ,test loss 0.0635805\n",
      "epoch 10,step 5500, training loss 0.0602911\n",
      "epoch 10,step 11000, training loss 0.0709304\n",
      "epoch 10,step 16500, training loss 0.0642689\n",
      "epoch 10,step 22000, training loss 0.0362767\n",
      "epoch 10,step 27500, training loss 0.0365998\n",
      "epoch 10,step 33000, training loss 0.039952\n",
      "epoch 10,step 38500, training loss 0.0422251\n",
      "epoch 10,step 44000, training loss 0.0405407\n",
      "epoch 10,step 49500, training loss 0.0471097\n",
      "epoch 10,step 55000, training loss 0.0433652\n",
      "epoch 10,step 60500, training loss 0.0426764\n",
      "epoch 10,step 66000, training loss 0.0572112\n",
      "epoch 10,step 71500, training loss 0.0871145\n",
      "epoch 10,step 77000, training loss 0.0460705\n",
      "epoch 10,step 82500, training loss 0.0656118\n",
      "epoch 10,step 88000, training loss 0.0470851\n",
      "epoch 10,step 93500, training loss 0.0395928\n",
      "epoch 10,step 99000, training loss 0.0388728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10,step 104500, training loss 0.0293022\n",
      "epoch 10,step 110000, training loss 0.0522699\n",
      "epoch 10,step 115500, training loss 0.0449173\n",
      "epoch 10,step 121000, training loss 0.0512913\n",
      "epoch 10,step 126500, training loss 0.0662864\n",
      "epoch 10,step 132000, training loss 0.0592534\n",
      "epoch 10,step 137500, training loss 0.057327\n",
      "epoch 10,step 143000, training loss 0.0309271\n",
      "epoch 10,step 148500, training loss 0.0739053\n",
      "epoch 10,step 154000, training loss 0.0474702\n",
      "epoch 10,step 159500, training loss 0.0513509\n",
      "epoch 10,step 165000, training loss 0.0445857\n",
      "epoch 10,step 170500, training loss 0.0677935\n",
      "epoch 10,step 176000, training loss 0.0510471\n",
      "epoch 10,step 181500, training loss 0.0470088\n",
      "epoch 10,step 187000, training loss 0.0446538\n",
      "epoch 10,step 192500, training loss 0.0636052\n",
      "epoch 10,step 198000, training loss 0.0402577\n",
      "epoch 10,step 203500, training loss 0.0482065\n",
      "epoch 10,step 209000, training loss 0.0538791\n",
      "epoch 10,step 214500, training loss 0.0476934\n",
      "epoch 10,step 220000, training loss 0.0403769\n",
      "epoch 10,step 225500, training loss 0.0579424\n",
      "epoch 10,step 231000, training loss 0.0458893\n",
      "epoch 10,step 236500, training loss 0.047646\n",
      "epoch 10,step 242000, training loss 0.0489287\n",
      "epoch 10,step 247500, training loss 0.0446869\n",
      "epoch 10,step 253000, training loss 0.052574\n",
      "epoch 10,step 258500, training loss 0.0335673\n",
      "epoch 10,step 264000, training loss 0.039742\n",
      "epoch 10,step 269500, training loss 0.048412\n",
      "epoch 10,step 275000, training loss 0.0422509\n",
      "epoch 10,step 280500, training loss 0.0446826\n",
      "epoch 10,step 286000, training loss 0.0536258\n",
      "epoch 10,step 291500, training loss 0.0348176\n",
      "epoch 10,step 297000, training loss 0.0456838\n",
      "epoch 10,step 302500, training loss 0.0727048\n",
      "epoch 10,step 308000, training loss 0.042314\n",
      "epoch 10,step 313500, training loss 0.0427672\n",
      "epoch 10,step 319000, training loss 0.0502186\n",
      "epoch 10,step 324500, training loss 0.055385\n",
      "epoch 10,step 330000, training loss 0.0408511\n",
      "epoch 10,step 335500, training loss 0.0441462\n",
      "epoch 10,step 341000, training loss 0.0429957\n",
      "epoch 10,step 346500, training loss 0.0423232\n",
      "epoch 10,step 352000, training loss 0.0636799\n",
      "epoch 10,step 357500, training loss 0.0638553\n",
      "epoch 10,step 363000, training loss 0.0471524\n",
      "epoch 10,step 368500, training loss 0.0398178\n",
      "epoch 10,step 374000, training loss 0.0577147\n",
      "epoch 10,step 379500, training loss 0.0374811\n",
      "epoch 10,step 385000, training loss 0.0419497\n",
      "epoch 10,step 390500, training loss 0.0539127\n",
      "epoch 10,step 396000, training loss 0.0372546\n",
      "epoch 10,step 401500, training loss 0.0702498\n",
      "epoch 10,step 407000, training loss 0.0632931\n",
      "epoch 10,step 412500, training loss 0.0477889\n",
      "epoch 10,step 418000, training loss 0.0472738\n",
      "epoch 10,step 423500, training loss 0.0403255\n",
      "epoch 10,step 429000, training loss 0.044243\n",
      "epoch 10,step 434500, training loss 0.0527072\n",
      "epoch 10,step 440000, training loss 0.033917\n",
      "epoch 10,step 445500, training loss 0.0379622\n",
      "epoch 10,step 451000, training loss 0.0430787\n",
      "epoch 10,step 456500, training loss 0.0423758\n",
      "epoch 10,step 462000, training loss 0.0433663\n",
      "epoch 10,step 467500, training loss 0.0602832\n",
      "epoch 10,step 473000, training loss 0.0458909\n",
      "epoch 10,step 478500, training loss 0.0600811\n",
      "epoch 10,step 484000, training loss 0.0485147\n",
      "epoch 10,step 489500, training loss 0.0329739\n",
      "epoch 10,step 495000, training loss 0.0446174\n",
      "epoch 10,step 500500, training loss 0.0412894\n",
      "epoch 10,step 506000, training loss 0.0451545\n",
      "epoch 10,step 511500, training loss 0.0595429\n",
      "epoch 10,step 517000, training loss 0.0505669\n",
      "epoch 10,step 522500, training loss 0.03724\n",
      "epoch 10,step 528000, training loss 0.0491644\n",
      "epoch 10,step 533500, training loss 0.0443166\n",
      "epoch 10,step 539000, training loss 0.0507163\n",
      "epoch 10,step 544500, training loss 0.0501964\n",
      "epoch 10,step 550000, training loss 0.0423406\n",
      "epoch 10,step 555500, training loss 0.044011\n",
      "epoch 10,step 561000, training loss 0.040163\n",
      "epoch 10,step 566500, training loss 0.0481983\n",
      "epoch 10,step 572000, training loss 0.0373326\n",
      "epoch 10,step 577500, training loss 0.0477802\n",
      "epoch 10,step 583000, training loss 0.0686944\n",
      "epoch 10,step 588500, training loss 0.0571551\n",
      "epoch 10,step 594000, training loss 0.0476581\n",
      "epoch 10,step 599500, training loss 0.0494742\n",
      "epoch 10,step 605000, training loss 0.0656479\n",
      "epoch 10,step 610500, training loss 0.0577299\n",
      "epoch 10,step 616000, training loss 0.0634122\n",
      "epoch 10,step 621500, training loss 0.0497127\n",
      "epoch 10,step 627000, training loss 0.0407808\n",
      "epoch 10,step 632500, training loss 0.080581\n",
      "epoch 10,step 638000, training loss 0.0390099\n",
      "epoch 10,step 643500, training loss 0.0437168\n",
      "epoch 10,step 649000, training loss 0.0351668\n",
      "epoch 10,step 654500, training loss 0.0457025\n",
      "epoch 10,step 660000, training loss 0.0641518\n",
      "epoch 10,step 665500, training loss 0.0355274\n",
      "epoch 10,step 671000, training loss 0.0400245\n",
      "epoch 10,step 676500, training loss 0.0383427\n",
      "epoch 10,step 682000, training loss 0.0554856\n",
      "epoch 10,step 687500, training loss 0.0570736\n",
      "epoch 10,step 693000, training loss 0.0547741\n",
      "epoch 10,step 698500, training loss 0.0389864\n",
      "epoch 10,step 704000, training loss 0.0489366\n",
      "epoch 10,step 709500, training loss 0.0545385\n",
      "epoch 10,step 715000, training loss 0.0497617\n",
      "epoch 10,step 720500, training loss 0.0426302\n",
      "epoch 10,step 726000, training loss 0.0403695\n",
      "epoch 10,step 731500, training loss 0.0368899\n",
      "epoch 10,step 737000, training loss 0.0471835\n",
      "epoch 10,step 742500, training loss 0.0414024\n",
      "epoch 10,step 748000, training loss 0.0490253\n",
      "epoch 10,step 753500, training loss 0.0447485\n",
      "epoch 10,step 759000, training loss 0.0488451\n",
      "epoch 10,step 764500, training loss 0.0448905\n",
      "epoch 10,step 770000, training loss 0.0367299\n",
      "epoch 10,step 775500, training loss 0.0516469\n",
      "epoch 10,step 781000, training loss 0.0487109\n",
      "epoch 10,step 786500, training loss 0.0499871\n",
      "epoch 10,step 792000, training loss 0.0425919\n",
      "epoch 10,step 797500, training loss 0.0570482\n",
      "epoch 10,step 803000, training loss 0.0455565\n",
      "epoch 10,step 808500, training loss 0.0523647\n",
      "epoch 10,step 814000, training loss 0.0624555\n",
      "epoch 10,step 819500, training loss 0.0386958\n",
      "epoch 10,step 825000, training loss 0.0947789\n",
      "epoch 10,step 830500, training loss 0.0407015\n",
      "epoch 10,step 836000, training loss 0.0380698\n",
      "epoch 10,step 841500, training loss 0.0623172\n",
      "epoch 10,step 847000, training loss 0.0329077\n",
      "epoch 10,step 852500, training loss 0.0451283\n",
      "epoch 10,step 858000, training loss 0.036723\n",
      "epoch 10,step 863500, training loss 0.0460682\n",
      "epoch 10,step 869000, training loss 0.0491473\n",
      "epoch 10,step 874500, training loss 0.041029\n",
      "epoch 10,step 880000, training loss 0.040191\n",
      "epoch 10,step 885500, training loss 0.0444598\n",
      "epoch 10,step 891000, training loss 0.0378758\n",
      "epoch 10,step 896500, training loss 0.0501723\n",
      "epoch 10,step 902000, training loss 0.0451475\n",
      "epoch 10,step 907500, training loss 0.0588821\n",
      "epoch 10,step 913000, training loss 0.0494731\n",
      "epoch 10,step 918500, training loss 0.0480925\n",
      "epoch 10,step 924000, training loss 0.0409975\n",
      "epoch 10,step 929500, training loss 0.0485943\n",
      "epoch 10,step 935000, training loss 0.0461814\n",
      "epoch 10,step 940500, training loss 0.0531235\n",
      "epoch 10,step 946000, training loss 0.059626\n",
      "epoch 10,step 951500, training loss 0.03685\n",
      "epoch 10,step 957000, training loss 0.0533373\n",
      "epoch 10,step 962500, training loss 0.0425514\n",
      "epoch 10,step 968000, training loss 0.043578\n",
      "epoch 10,step 973500, training loss 0.0413535\n",
      "epoch 10,step 979000, training loss 0.0349283\n",
      "epoch 10,step 984500, training loss 0.0595746\n",
      "epoch 10,step 990000, training loss 0.0926357\n",
      "epoch 10,step 995500, training loss 0.0441058\n",
      "epoch 10,step 1001000, training loss 0.0413182\n",
      "epoch 10,step 1006500, training loss 0.0542018\n",
      "epoch 10,step 1012000, training loss 0.0450621\n",
      "epoch 10,step 1017500, training loss 0.0356569\n",
      "epoch 10,step 1023000, training loss 0.0364628\n",
      "epoch 10,step 1028500, training loss 0.0617516\n",
      "epoch 10,step 1034000, training loss 0.0339733\n",
      "epoch 10,step 1039500, training loss 0.0465453\n",
      "epoch 10,step 1045000, training loss 0.0304986\n",
      "epoch 10,step 1050500, training loss 0.035858\n",
      "epoch 10,step 1056000, training loss 0.0495237\n",
      "epoch 10,step 1061500, training loss 0.0369425\n",
      "epoch 10,step 1067000, training loss 0.0445115\n",
      "epoch 10,step 1072500, training loss 0.0557653\n",
      "epoch 10,step 1078000, training loss 0.0452494\n",
      "epoch 10,step 1083500, training loss 0.038621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10,step 1089000, training loss 0.0344079\n",
      "epoch 10,step 1094500, training loss 0.0333705\n",
      "epoch 10,training loss 0.0555302 ,test loss 0.0602001\n",
      "epoch 11,step 6000, training loss 0.056257\n",
      "epoch 11,step 12000, training loss 0.0689045\n",
      "epoch 11,step 18000, training loss 0.0645031\n",
      "epoch 11,step 24000, training loss 0.0343632\n",
      "epoch 11,step 30000, training loss 0.0325503\n",
      "epoch 11,step 36000, training loss 0.0365988\n",
      "epoch 11,step 42000, training loss 0.0413062\n",
      "epoch 11,step 48000, training loss 0.0379519\n",
      "epoch 11,step 54000, training loss 0.0443876\n",
      "epoch 11,step 60000, training loss 0.0402934\n",
      "epoch 11,step 66000, training loss 0.0385616\n",
      "epoch 11,step 72000, training loss 0.051707\n",
      "epoch 11,step 78000, training loss 0.0811609\n",
      "epoch 11,step 84000, training loss 0.0439545\n",
      "epoch 11,step 90000, training loss 0.0587828\n",
      "epoch 11,step 96000, training loss 0.0438129\n",
      "epoch 11,step 102000, training loss 0.0362617\n",
      "epoch 11,step 108000, training loss 0.0348271\n",
      "epoch 11,step 114000, training loss 0.0284687\n",
      "epoch 11,step 120000, training loss 0.0481386\n",
      "epoch 11,step 126000, training loss 0.0403613\n",
      "epoch 11,step 132000, training loss 0.0467008\n",
      "epoch 11,step 138000, training loss 0.0622127\n",
      "epoch 11,step 144000, training loss 0.0570607\n",
      "epoch 11,step 150000, training loss 0.0557114\n",
      "epoch 11,step 156000, training loss 0.0289975\n",
      "epoch 11,step 162000, training loss 0.0724784\n",
      "epoch 11,step 168000, training loss 0.0442481\n",
      "epoch 11,step 174000, training loss 0.0485472\n",
      "epoch 11,step 180000, training loss 0.0414489\n",
      "epoch 11,step 186000, training loss 0.0643812\n",
      "epoch 11,step 192000, training loss 0.043653\n",
      "epoch 11,step 198000, training loss 0.0443934\n",
      "epoch 11,step 204000, training loss 0.0437713\n",
      "epoch 11,step 210000, training loss 0.0609317\n",
      "epoch 11,step 216000, training loss 0.0387064\n",
      "epoch 11,step 222000, training loss 0.0474908\n",
      "epoch 11,step 228000, training loss 0.05279\n",
      "epoch 11,step 234000, training loss 0.0483764\n",
      "epoch 11,step 240000, training loss 0.0387944\n",
      "epoch 11,step 246000, training loss 0.0543274\n",
      "epoch 11,step 252000, training loss 0.0418986\n",
      "epoch 11,step 258000, training loss 0.0422151\n",
      "epoch 11,step 264000, training loss 0.0447839\n",
      "epoch 11,step 270000, training loss 0.0405082\n",
      "epoch 11,step 276000, training loss 0.0483308\n",
      "epoch 11,step 282000, training loss 0.03158\n",
      "epoch 11,step 288000, training loss 0.0380024\n",
      "epoch 11,step 294000, training loss 0.0447361\n",
      "epoch 11,step 300000, training loss 0.0395059\n",
      "epoch 11,step 306000, training loss 0.0426085\n",
      "epoch 11,step 312000, training loss 0.0516329\n",
      "epoch 11,step 318000, training loss 0.031849\n",
      "epoch 11,step 324000, training loss 0.0436833\n",
      "epoch 11,step 330000, training loss 0.068509\n",
      "epoch 11,step 336000, training loss 0.0384003\n",
      "epoch 11,step 342000, training loss 0.0371981\n",
      "epoch 11,step 348000, training loss 0.0483953\n",
      "epoch 11,step 354000, training loss 0.0522054\n",
      "epoch 11,step 360000, training loss 0.0389275\n",
      "epoch 11,step 366000, training loss 0.039958\n",
      "epoch 11,step 372000, training loss 0.0398407\n",
      "epoch 11,step 378000, training loss 0.040699\n",
      "epoch 11,step 384000, training loss 0.0588118\n",
      "epoch 11,step 390000, training loss 0.0599102\n",
      "epoch 11,step 396000, training loss 0.0440562\n",
      "epoch 11,step 402000, training loss 0.0393994\n",
      "epoch 11,step 408000, training loss 0.0569634\n",
      "epoch 11,step 414000, training loss 0.0344206\n",
      "epoch 11,step 420000, training loss 0.0409501\n",
      "epoch 11,step 426000, training loss 0.0520388\n",
      "epoch 11,step 432000, training loss 0.0357267\n",
      "epoch 11,step 438000, training loss 0.0686845\n",
      "epoch 11,step 444000, training loss 0.0591425\n",
      "epoch 11,step 450000, training loss 0.0461689\n",
      "epoch 11,step 456000, training loss 0.0453346\n",
      "epoch 11,step 462000, training loss 0.0408984\n",
      "epoch 11,step 468000, training loss 0.0422754\n",
      "epoch 11,step 474000, training loss 0.048513\n",
      "epoch 11,step 480000, training loss 0.0311786\n",
      "epoch 11,step 486000, training loss 0.0354598\n",
      "epoch 11,step 492000, training loss 0.0383479\n",
      "epoch 11,step 498000, training loss 0.0390389\n",
      "epoch 11,step 504000, training loss 0.0409293\n",
      "epoch 11,step 510000, training loss 0.0564419\n",
      "epoch 11,step 516000, training loss 0.042951\n",
      "epoch 11,step 522000, training loss 0.0546376\n",
      "epoch 11,step 528000, training loss 0.0443433\n",
      "epoch 11,step 534000, training loss 0.0276546\n",
      "epoch 11,step 540000, training loss 0.0395218\n",
      "epoch 11,step 546000, training loss 0.0367433\n",
      "epoch 11,step 552000, training loss 0.0441947\n",
      "epoch 11,step 558000, training loss 0.055028\n",
      "epoch 11,step 564000, training loss 0.044924\n",
      "epoch 11,step 570000, training loss 0.0327344\n",
      "epoch 11,step 576000, training loss 0.0453937\n",
      "epoch 11,step 582000, training loss 0.0415715\n",
      "epoch 11,step 588000, training loss 0.0480388\n",
      "epoch 11,step 594000, training loss 0.047108\n",
      "epoch 11,step 600000, training loss 0.0389135\n",
      "epoch 11,step 606000, training loss 0.0401089\n",
      "epoch 11,step 612000, training loss 0.0364892\n",
      "epoch 11,step 618000, training loss 0.045674\n",
      "epoch 11,step 624000, training loss 0.0367749\n",
      "epoch 11,step 630000, training loss 0.0447626\n",
      "epoch 11,step 636000, training loss 0.0625024\n",
      "epoch 11,step 642000, training loss 0.0549272\n",
      "epoch 11,step 648000, training loss 0.0461264\n",
      "epoch 11,step 654000, training loss 0.0493139\n",
      "epoch 11,step 660000, training loss 0.0609409\n",
      "epoch 11,step 666000, training loss 0.0517097\n",
      "epoch 11,step 672000, training loss 0.0562949\n",
      "epoch 11,step 678000, training loss 0.0460194\n",
      "epoch 11,step 684000, training loss 0.0374234\n",
      "epoch 11,step 690000, training loss 0.0766927\n",
      "epoch 11,step 696000, training loss 0.036517\n",
      "epoch 11,step 702000, training loss 0.0418373\n",
      "epoch 11,step 708000, training loss 0.0340382\n",
      "epoch 11,step 714000, training loss 0.0410294\n",
      "epoch 11,step 720000, training loss 0.0590467\n",
      "epoch 11,step 726000, training loss 0.0326029\n",
      "epoch 11,step 732000, training loss 0.0376748\n",
      "epoch 11,step 738000, training loss 0.0388553\n",
      "epoch 11,step 744000, training loss 0.0534678\n",
      "epoch 11,step 750000, training loss 0.0534748\n",
      "epoch 11,step 756000, training loss 0.0560357\n",
      "epoch 11,step 762000, training loss 0.036165\n",
      "epoch 11,step 768000, training loss 0.0473657\n",
      "epoch 11,step 774000, training loss 0.0500594\n",
      "epoch 11,step 780000, training loss 0.0469414\n",
      "epoch 11,step 786000, training loss 0.0411127\n",
      "epoch 11,step 792000, training loss 0.0395292\n",
      "epoch 11,step 798000, training loss 0.0334375\n",
      "epoch 11,step 804000, training loss 0.044676\n",
      "epoch 11,step 810000, training loss 0.03946\n",
      "epoch 11,step 816000, training loss 0.0449797\n",
      "epoch 11,step 822000, training loss 0.044408\n",
      "epoch 11,step 828000, training loss 0.0444813\n",
      "epoch 11,step 834000, training loss 0.0443105\n",
      "epoch 11,step 840000, training loss 0.0353745\n",
      "epoch 11,step 846000, training loss 0.0503361\n",
      "epoch 11,step 852000, training loss 0.046923\n",
      "epoch 11,step 858000, training loss 0.0464692\n",
      "epoch 11,step 864000, training loss 0.0437829\n",
      "epoch 11,step 870000, training loss 0.0503198\n",
      "epoch 11,step 876000, training loss 0.0437752\n",
      "epoch 11,step 882000, training loss 0.0410612\n",
      "epoch 11,step 888000, training loss 0.0577974\n",
      "epoch 11,step 894000, training loss 0.034038\n",
      "epoch 11,step 900000, training loss 0.0883862\n",
      "epoch 11,step 906000, training loss 0.0399604\n",
      "epoch 11,step 912000, training loss 0.0347657\n",
      "epoch 11,step 918000, training loss 0.0578976\n",
      "epoch 11,step 924000, training loss 0.0305761\n",
      "epoch 11,step 930000, training loss 0.0434073\n",
      "epoch 11,step 936000, training loss 0.0341636\n",
      "epoch 11,step 942000, training loss 0.0429008\n",
      "epoch 11,step 948000, training loss 0.0451106\n",
      "epoch 11,step 954000, training loss 0.0371445\n",
      "epoch 11,step 960000, training loss 0.0372519\n",
      "epoch 11,step 966000, training loss 0.042083\n",
      "epoch 11,step 972000, training loss 0.0353352\n",
      "epoch 11,step 978000, training loss 0.0457354\n",
      "epoch 11,step 984000, training loss 0.0422652\n",
      "epoch 11,step 990000, training loss 0.0535654\n",
      "epoch 11,step 996000, training loss 0.0436783\n",
      "epoch 11,step 1002000, training loss 0.0442943\n",
      "epoch 11,step 1008000, training loss 0.0377937\n",
      "epoch 11,step 1014000, training loss 0.0476325\n",
      "epoch 11,step 1020000, training loss 0.0421971\n",
      "epoch 11,step 1026000, training loss 0.0500084\n",
      "epoch 11,step 1032000, training loss 0.0579216\n",
      "epoch 11,step 1038000, training loss 0.0359153\n",
      "epoch 11,step 1044000, training loss 0.0501184\n",
      "epoch 11,step 1050000, training loss 0.0412095\n",
      "epoch 11,step 1056000, training loss 0.0427373\n",
      "epoch 11,step 1062000, training loss 0.0395977\n",
      "epoch 11,step 1068000, training loss 0.0332363\n",
      "epoch 11,step 1074000, training loss 0.057872\n",
      "epoch 11,step 1080000, training loss 0.0902253\n",
      "epoch 11,step 1086000, training loss 0.0407881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11,step 1092000, training loss 0.0371852\n",
      "epoch 11,step 1098000, training loss 0.0516663\n",
      "epoch 11,step 1104000, training loss 0.0429933\n",
      "epoch 11,step 1110000, training loss 0.0341204\n",
      "epoch 11,step 1116000, training loss 0.0347219\n",
      "epoch 11,step 1122000, training loss 0.0598216\n",
      "epoch 11,step 1128000, training loss 0.0339443\n",
      "epoch 11,step 1134000, training loss 0.0445575\n",
      "epoch 11,step 1140000, training loss 0.0307502\n",
      "epoch 11,step 1146000, training loss 0.0338777\n",
      "epoch 11,step 1152000, training loss 0.0460022\n",
      "epoch 11,step 1158000, training loss 0.035221\n",
      "epoch 11,step 1164000, training loss 0.0441107\n",
      "epoch 11,step 1170000, training loss 0.0523386\n",
      "epoch 11,step 1176000, training loss 0.0398661\n",
      "epoch 11,step 1182000, training loss 0.0377139\n",
      "epoch 11,step 1188000, training loss 0.0330103\n",
      "epoch 11,step 1194000, training loss 0.0307939\n",
      "epoch 11,training loss 0.0535367 ,test loss 0.0576925\n",
      "epoch 12,step 6500, training loss 0.0532976\n",
      "epoch 12,step 13000, training loss 0.0635738\n",
      "epoch 12,step 19500, training loss 0.0590936\n",
      "epoch 12,step 26000, training loss 0.0326001\n",
      "epoch 12,step 32500, training loss 0.0322775\n",
      "epoch 12,step 39000, training loss 0.0344358\n",
      "epoch 12,step 45500, training loss 0.0380458\n",
      "epoch 12,step 52000, training loss 0.03648\n",
      "epoch 12,step 58500, training loss 0.043244\n",
      "epoch 12,step 65000, training loss 0.0385986\n",
      "epoch 12,step 71500, training loss 0.036446\n",
      "epoch 12,step 78000, training loss 0.0480686\n",
      "epoch 12,step 84500, training loss 0.0765981\n",
      "epoch 12,step 91000, training loss 0.0422046\n",
      "epoch 12,step 97500, training loss 0.0575891\n",
      "epoch 12,step 104000, training loss 0.0419224\n",
      "epoch 12,step 110500, training loss 0.0360738\n",
      "epoch 12,step 117000, training loss 0.0331378\n",
      "epoch 12,step 123500, training loss 0.030906\n",
      "epoch 12,step 130000, training loss 0.0477225\n",
      "epoch 12,step 136500, training loss 0.0390741\n",
      "epoch 12,step 143000, training loss 0.0437029\n",
      "epoch 12,step 149500, training loss 0.0613235\n",
      "epoch 12,step 156000, training loss 0.0547672\n",
      "epoch 12,step 162500, training loss 0.052711\n",
      "epoch 12,step 169000, training loss 0.0289148\n",
      "epoch 12,step 175500, training loss 0.0691485\n",
      "epoch 12,step 182000, training loss 0.0445618\n",
      "epoch 12,step 188500, training loss 0.0493805\n",
      "epoch 12,step 195000, training loss 0.0388631\n",
      "epoch 12,step 201500, training loss 0.0585414\n",
      "epoch 12,step 208000, training loss 0.0416144\n",
      "epoch 12,step 214500, training loss 0.0438984\n",
      "epoch 12,step 221000, training loss 0.0429705\n",
      "epoch 12,step 227500, training loss 0.0576749\n",
      "epoch 12,step 234000, training loss 0.0348038\n",
      "epoch 12,step 240500, training loss 0.0463852\n",
      "epoch 12,step 247000, training loss 0.0507236\n",
      "epoch 12,step 253500, training loss 0.046104\n",
      "epoch 12,step 260000, training loss 0.0373771\n",
      "epoch 12,step 266500, training loss 0.0502693\n",
      "epoch 12,step 273000, training loss 0.0409653\n",
      "epoch 12,step 279500, training loss 0.040703\n",
      "epoch 12,step 286000, training loss 0.0425426\n",
      "epoch 12,step 292500, training loss 0.0387608\n",
      "epoch 12,step 299000, training loss 0.0466329\n",
      "epoch 12,step 305500, training loss 0.0295488\n",
      "epoch 12,step 312000, training loss 0.0375215\n",
      "epoch 12,step 318500, training loss 0.0425079\n",
      "epoch 12,step 325000, training loss 0.0384495\n",
      "epoch 12,step 331500, training loss 0.0387727\n",
      "epoch 12,step 338000, training loss 0.0476705\n",
      "epoch 12,step 344500, training loss 0.0285226\n",
      "epoch 12,step 351000, training loss 0.0397855\n",
      "epoch 12,step 357500, training loss 0.0669178\n",
      "epoch 12,step 364000, training loss 0.0377122\n",
      "epoch 12,step 370500, training loss 0.0361697\n",
      "epoch 12,step 377000, training loss 0.0459907\n",
      "epoch 12,step 383500, training loss 0.0492146\n",
      "epoch 12,step 390000, training loss 0.0371387\n",
      "epoch 12,step 396500, training loss 0.0394325\n",
      "epoch 12,step 403000, training loss 0.0376841\n",
      "epoch 12,step 409500, training loss 0.0396794\n",
      "epoch 12,step 416000, training loss 0.0552329\n",
      "epoch 12,step 422500, training loss 0.0575972\n",
      "epoch 12,step 429000, training loss 0.0407668\n",
      "epoch 12,step 435500, training loss 0.0377202\n",
      "epoch 12,step 442000, training loss 0.0564875\n",
      "epoch 12,step 448500, training loss 0.03349\n",
      "epoch 12,step 455000, training loss 0.0403115\n",
      "epoch 12,step 461500, training loss 0.0528453\n",
      "epoch 12,step 468000, training loss 0.035667\n",
      "epoch 12,step 474500, training loss 0.0673142\n",
      "epoch 12,step 481000, training loss 0.0581441\n",
      "epoch 12,step 487500, training loss 0.0467766\n",
      "epoch 12,step 494000, training loss 0.0445935\n",
      "epoch 12,step 500500, training loss 0.0387456\n",
      "epoch 12,step 507000, training loss 0.0386524\n",
      "epoch 12,step 513500, training loss 0.0444019\n",
      "epoch 12,step 520000, training loss 0.0294541\n",
      "epoch 12,step 526500, training loss 0.0358164\n",
      "epoch 12,step 533000, training loss 0.0378985\n",
      "epoch 12,step 539500, training loss 0.0370713\n",
      "epoch 12,step 546000, training loss 0.0371305\n",
      "epoch 12,step 552500, training loss 0.0550477\n",
      "epoch 12,step 559000, training loss 0.0410542\n",
      "epoch 12,step 565500, training loss 0.0508574\n",
      "epoch 12,step 572000, training loss 0.04225\n",
      "epoch 12,step 578500, training loss 0.02779\n",
      "epoch 12,step 585000, training loss 0.0366586\n",
      "epoch 12,step 591500, training loss 0.0353033\n",
      "epoch 12,step 598000, training loss 0.041418\n",
      "epoch 12,step 604500, training loss 0.0541128\n",
      "epoch 12,step 611000, training loss 0.0424165\n",
      "epoch 12,step 617500, training loss 0.0355679\n",
      "epoch 12,step 624000, training loss 0.045084\n",
      "epoch 12,step 630500, training loss 0.0401225\n",
      "epoch 12,step 637000, training loss 0.0461533\n",
      "epoch 12,step 643500, training loss 0.0451374\n",
      "epoch 12,step 650000, training loss 0.0384962\n",
      "epoch 12,step 656500, training loss 0.0422192\n",
      "epoch 12,step 663000, training loss 0.0366302\n",
      "epoch 12,step 669500, training loss 0.0463231\n",
      "epoch 12,step 676000, training loss 0.0350362\n",
      "epoch 12,step 682500, training loss 0.0457672\n",
      "epoch 12,step 689000, training loss 0.0590714\n",
      "epoch 12,step 695500, training loss 0.053777\n",
      "epoch 12,step 702000, training loss 0.0439718\n",
      "epoch 12,step 708500, training loss 0.0473466\n",
      "epoch 12,step 715000, training loss 0.0585759\n",
      "epoch 12,step 721500, training loss 0.0502389\n",
      "epoch 12,step 728000, training loss 0.0576667\n",
      "epoch 12,step 734500, training loss 0.044227\n",
      "epoch 12,step 741000, training loss 0.0377305\n",
      "epoch 12,step 747500, training loss 0.0717555\n",
      "epoch 12,step 754000, training loss 0.0356008\n",
      "epoch 12,step 760500, training loss 0.0398385\n",
      "epoch 12,step 767000, training loss 0.0325669\n",
      "epoch 12,step 773500, training loss 0.0370227\n",
      "epoch 12,step 780000, training loss 0.0620463\n",
      "epoch 12,step 786500, training loss 0.0343396\n",
      "epoch 12,step 793000, training loss 0.0350002\n",
      "epoch 12,step 799500, training loss 0.0355201\n",
      "epoch 12,step 806000, training loss 0.0504572\n",
      "epoch 12,step 812500, training loss 0.0490091\n",
      "epoch 12,step 819000, training loss 0.0473372\n",
      "epoch 12,step 825500, training loss 0.0358127\n",
      "epoch 12,step 832000, training loss 0.0454956\n",
      "epoch 12,step 838500, training loss 0.0459343\n",
      "epoch 12,step 845000, training loss 0.0440971\n",
      "epoch 12,step 851500, training loss 0.0384556\n",
      "epoch 12,step 858000, training loss 0.0374314\n",
      "epoch 12,step 864500, training loss 0.031993\n",
      "epoch 12,step 871000, training loss 0.042066\n",
      "epoch 12,step 877500, training loss 0.036016\n",
      "epoch 12,step 884000, training loss 0.0422691\n",
      "epoch 12,step 890500, training loss 0.0414102\n",
      "epoch 12,step 897000, training loss 0.0436211\n",
      "epoch 12,step 903500, training loss 0.0402118\n",
      "epoch 12,step 910000, training loss 0.0335901\n",
      "epoch 12,step 916500, training loss 0.0447366\n",
      "epoch 12,step 923000, training loss 0.0436539\n",
      "epoch 12,step 929500, training loss 0.0464769\n",
      "epoch 12,step 936000, training loss 0.0413054\n",
      "epoch 12,step 942500, training loss 0.0472384\n",
      "epoch 12,step 949000, training loss 0.04105\n",
      "epoch 12,step 955500, training loss 0.0423214\n",
      "epoch 12,step 962000, training loss 0.0554419\n",
      "epoch 12,step 968500, training loss 0.0334151\n",
      "epoch 12,step 975000, training loss 0.0847613\n",
      "epoch 12,step 981500, training loss 0.0356828\n",
      "epoch 12,step 988000, training loss 0.0342627\n",
      "epoch 12,step 994500, training loss 0.0543759\n",
      "epoch 12,step 1001000, training loss 0.0306442\n",
      "epoch 12,step 1007500, training loss 0.0428828\n",
      "epoch 12,step 1014000, training loss 0.0318808\n",
      "epoch 12,step 1020500, training loss 0.041438\n",
      "epoch 12,step 1027000, training loss 0.041029\n",
      "epoch 12,step 1033500, training loss 0.0371382\n",
      "epoch 12,step 1040000, training loss 0.0360676\n",
      "epoch 12,step 1046500, training loss 0.0439936\n",
      "epoch 12,step 1053000, training loss 0.0347189\n",
      "epoch 12,step 1059500, training loss 0.0449148\n",
      "epoch 12,step 1066000, training loss 0.043637\n",
      "epoch 12,step 1072500, training loss 0.0554277\n",
      "epoch 12,step 1079000, training loss 0.0442817\n",
      "epoch 12,step 1085500, training loss 0.0405912\n",
      "epoch 12,step 1092000, training loss 0.037315\n",
      "epoch 12,step 1098500, training loss 0.0487182\n",
      "epoch 12,step 1105000, training loss 0.0417398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12,step 1111500, training loss 0.048737\n",
      "epoch 12,step 1118000, training loss 0.0561142\n",
      "epoch 12,step 1124500, training loss 0.0349757\n",
      "epoch 12,step 1131000, training loss 0.0473476\n",
      "epoch 12,step 1137500, training loss 0.0390591\n",
      "epoch 12,step 1144000, training loss 0.0417407\n",
      "epoch 12,step 1150500, training loss 0.0394218\n",
      "epoch 12,step 1157000, training loss 0.031938\n",
      "epoch 12,step 1163500, training loss 0.055023\n",
      "epoch 12,step 1170000, training loss 0.086333\n",
      "epoch 12,step 1176500, training loss 0.0395875\n",
      "epoch 12,step 1183000, training loss 0.0351819\n",
      "epoch 12,step 1189500, training loss 0.0500259\n",
      "epoch 12,step 1196000, training loss 0.0405433\n",
      "epoch 12,step 1202500, training loss 0.0316087\n",
      "epoch 12,step 1209000, training loss 0.0343931\n",
      "epoch 12,step 1215500, training loss 0.0576024\n",
      "epoch 12,step 1222000, training loss 0.0335561\n",
      "epoch 12,step 1228500, training loss 0.0441668\n",
      "epoch 12,step 1235000, training loss 0.0291441\n",
      "epoch 12,step 1241500, training loss 0.0321155\n",
      "epoch 12,step 1248000, training loss 0.0439964\n",
      "epoch 12,step 1254500, training loss 0.0343061\n",
      "epoch 12,step 1261000, training loss 0.0432226\n",
      "epoch 12,step 1267500, training loss 0.0524812\n",
      "epoch 12,step 1274000, training loss 0.0388078\n",
      "epoch 12,step 1280500, training loss 0.0368857\n",
      "epoch 12,step 1287000, training loss 0.0326454\n",
      "epoch 12,step 1293500, training loss 0.030422\n",
      "epoch 12,training loss 0.0526695 ,test loss 0.0549914\n",
      "epoch 13,step 7000, training loss 0.0508252\n",
      "epoch 13,step 14000, training loss 0.0629733\n",
      "epoch 13,step 21000, training loss 0.0570712\n",
      "epoch 13,step 28000, training loss 0.0326165\n",
      "epoch 13,step 35000, training loss 0.0295048\n",
      "epoch 13,step 42000, training loss 0.0331732\n",
      "epoch 13,step 49000, training loss 0.0382406\n",
      "epoch 13,step 56000, training loss 0.032851\n",
      "epoch 13,step 63000, training loss 0.0416925\n",
      "epoch 13,step 70000, training loss 0.0362851\n",
      "epoch 13,step 77000, training loss 0.0365263\n",
      "epoch 13,step 84000, training loss 0.0449327\n",
      "epoch 13,step 91000, training loss 0.0749661\n",
      "epoch 13,step 98000, training loss 0.0381156\n",
      "epoch 13,step 105000, training loss 0.0552792\n",
      "epoch 13,step 112000, training loss 0.0397965\n",
      "epoch 13,step 119000, training loss 0.0351677\n",
      "epoch 13,step 126000, training loss 0.0312443\n",
      "epoch 13,step 133000, training loss 0.0279446\n",
      "epoch 13,step 140000, training loss 0.0438295\n",
      "epoch 13,step 147000, training loss 0.0362035\n",
      "epoch 13,step 154000, training loss 0.0415296\n",
      "epoch 13,step 161000, training loss 0.0578762\n",
      "epoch 13,step 168000, training loss 0.0507302\n",
      "epoch 13,step 175000, training loss 0.0495946\n",
      "epoch 13,step 182000, training loss 0.0282356\n",
      "epoch 13,step 189000, training loss 0.0660577\n",
      "epoch 13,step 196000, training loss 0.0439676\n",
      "epoch 13,step 203000, training loss 0.0493325\n",
      "epoch 13,step 210000, training loss 0.0392026\n",
      "epoch 13,step 217000, training loss 0.0582448\n",
      "epoch 13,step 224000, training loss 0.0399123\n",
      "epoch 13,step 231000, training loss 0.0430876\n",
      "epoch 13,step 238000, training loss 0.0416423\n",
      "epoch 13,step 245000, training loss 0.056398\n",
      "epoch 13,step 252000, training loss 0.0361698\n",
      "epoch 13,step 259000, training loss 0.046399\n",
      "epoch 13,step 266000, training loss 0.051034\n",
      "epoch 13,step 273000, training loss 0.050282\n",
      "epoch 13,step 280000, training loss 0.0388751\n",
      "epoch 13,step 287000, training loss 0.0496412\n",
      "epoch 13,step 294000, training loss 0.0428554\n",
      "epoch 13,step 301000, training loss 0.0391124\n",
      "epoch 13,step 308000, training loss 0.0405891\n",
      "epoch 13,step 315000, training loss 0.0381518\n",
      "epoch 13,step 322000, training loss 0.0452367\n",
      "epoch 13,step 329000, training loss 0.0279239\n",
      "epoch 13,step 336000, training loss 0.036235\n",
      "epoch 13,step 343000, training loss 0.0409785\n",
      "epoch 13,step 350000, training loss 0.0368768\n",
      "epoch 13,step 357000, training loss 0.0381394\n",
      "epoch 13,step 364000, training loss 0.0474476\n",
      "epoch 13,step 371000, training loss 0.0295886\n",
      "epoch 13,step 378000, training loss 0.0391237\n",
      "epoch 13,step 385000, training loss 0.0628662\n",
      "epoch 13,step 392000, training loss 0.0345358\n",
      "epoch 13,step 399000, training loss 0.0330877\n",
      "epoch 13,step 406000, training loss 0.0448169\n",
      "epoch 13,step 413000, training loss 0.0477415\n",
      "epoch 13,step 420000, training loss 0.0362409\n",
      "epoch 13,step 427000, training loss 0.0376411\n",
      "epoch 13,step 434000, training loss 0.0380063\n",
      "epoch 13,step 441000, training loss 0.0371112\n",
      "epoch 13,step 448000, training loss 0.0511054\n",
      "epoch 13,step 455000, training loss 0.055173\n",
      "epoch 13,step 462000, training loss 0.0399997\n",
      "epoch 13,step 469000, training loss 0.0381806\n",
      "epoch 13,step 476000, training loss 0.0524928\n",
      "epoch 13,step 483000, training loss 0.0300312\n",
      "epoch 13,step 490000, training loss 0.0383838\n",
      "epoch 13,step 497000, training loss 0.0492953\n",
      "epoch 13,step 504000, training loss 0.0330232\n",
      "epoch 13,step 511000, training loss 0.0635071\n",
      "epoch 13,step 518000, training loss 0.0511806\n",
      "epoch 13,step 525000, training loss 0.0431157\n",
      "epoch 13,step 532000, training loss 0.0427433\n",
      "epoch 13,step 539000, training loss 0.0383647\n",
      "epoch 13,step 546000, training loss 0.0382029\n",
      "epoch 13,step 553000, training loss 0.044393\n",
      "epoch 13,step 560000, training loss 0.0290989\n",
      "epoch 13,step 567000, training loss 0.0342111\n",
      "epoch 13,step 574000, training loss 0.0364257\n",
      "epoch 13,step 581000, training loss 0.0356096\n",
      "epoch 13,step 588000, training loss 0.0361034\n",
      "epoch 13,step 595000, training loss 0.0508455\n",
      "epoch 13,step 602000, training loss 0.0390076\n",
      "epoch 13,step 609000, training loss 0.0491933\n",
      "epoch 13,step 616000, training loss 0.0412283\n",
      "epoch 13,step 623000, training loss 0.0258537\n",
      "epoch 13,step 630000, training loss 0.0356\n",
      "epoch 13,step 637000, training loss 0.0336654\n",
      "epoch 13,step 644000, training loss 0.0392842\n",
      "epoch 13,step 651000, training loss 0.0524845\n",
      "epoch 13,step 658000, training loss 0.0401778\n",
      "epoch 13,step 665000, training loss 0.0328804\n",
      "epoch 13,step 672000, training loss 0.0421885\n",
      "epoch 13,step 679000, training loss 0.0372957\n",
      "epoch 13,step 686000, training loss 0.0441761\n",
      "epoch 13,step 693000, training loss 0.0428445\n",
      "epoch 13,step 700000, training loss 0.036778\n",
      "epoch 13,step 707000, training loss 0.0399031\n",
      "epoch 13,step 714000, training loss 0.0359935\n",
      "epoch 13,step 721000, training loss 0.044703\n",
      "epoch 13,step 728000, training loss 0.0329851\n",
      "epoch 13,step 735000, training loss 0.0444743\n",
      "epoch 13,step 742000, training loss 0.0571105\n",
      "epoch 13,step 749000, training loss 0.0517277\n",
      "epoch 13,step 756000, training loss 0.0439418\n",
      "epoch 13,step 763000, training loss 0.0448199\n",
      "epoch 13,step 770000, training loss 0.0576763\n",
      "epoch 13,step 777000, training loss 0.0491181\n",
      "epoch 13,step 784000, training loss 0.0516746\n",
      "epoch 13,step 791000, training loss 0.0406797\n",
      "epoch 13,step 798000, training loss 0.0346831\n",
      "epoch 13,step 805000, training loss 0.0681169\n",
      "epoch 13,step 812000, training loss 0.0339306\n",
      "epoch 13,step 819000, training loss 0.0375199\n",
      "epoch 13,step 826000, training loss 0.0304606\n",
      "epoch 13,step 833000, training loss 0.0366273\n",
      "epoch 13,step 840000, training loss 0.0626521\n",
      "epoch 13,step 847000, training loss 0.0331206\n",
      "epoch 13,step 854000, training loss 0.0330575\n",
      "epoch 13,step 861000, training loss 0.03659\n",
      "epoch 13,step 868000, training loss 0.0513087\n",
      "epoch 13,step 875000, training loss 0.0495068\n",
      "epoch 13,step 882000, training loss 0.0489479\n",
      "epoch 13,step 889000, training loss 0.0330617\n",
      "epoch 13,step 896000, training loss 0.0456675\n",
      "epoch 13,step 903000, training loss 0.0468503\n",
      "epoch 13,step 910000, training loss 0.044176\n",
      "epoch 13,step 917000, training loss 0.0376593\n",
      "epoch 13,step 924000, training loss 0.0366872\n",
      "epoch 13,step 931000, training loss 0.0322352\n",
      "epoch 13,step 938000, training loss 0.0426465\n",
      "epoch 13,step 945000, training loss 0.0350808\n",
      "epoch 13,step 952000, training loss 0.0427188\n",
      "epoch 13,step 959000, training loss 0.0399126\n",
      "epoch 13,step 966000, training loss 0.0403719\n",
      "epoch 13,step 973000, training loss 0.0419823\n",
      "epoch 13,step 980000, training loss 0.0344041\n",
      "epoch 13,step 987000, training loss 0.0455897\n",
      "epoch 13,step 994000, training loss 0.0420402\n",
      "epoch 13,step 1001000, training loss 0.042452\n",
      "epoch 13,step 1008000, training loss 0.0409974\n",
      "epoch 13,step 1015000, training loss 0.0463406\n",
      "epoch 13,step 1022000, training loss 0.0369377\n",
      "epoch 13,step 1029000, training loss 0.0391407\n",
      "epoch 13,step 1036000, training loss 0.0526845\n",
      "epoch 13,step 1043000, training loss 0.0315653\n",
      "epoch 13,step 1050000, training loss 0.0820643\n",
      "epoch 13,step 1057000, training loss 0.0347662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13,step 1064000, training loss 0.0345349\n",
      "epoch 13,step 1071000, training loss 0.0519843\n",
      "epoch 13,step 1078000, training loss 0.0284215\n",
      "epoch 13,step 1085000, training loss 0.041877\n",
      "epoch 13,step 1092000, training loss 0.0321042\n",
      "epoch 13,step 1099000, training loss 0.0377382\n",
      "epoch 13,step 1106000, training loss 0.0407308\n",
      "epoch 13,step 1113000, training loss 0.0368864\n",
      "epoch 13,step 1120000, training loss 0.0335963\n",
      "epoch 13,step 1127000, training loss 0.0439503\n",
      "epoch 13,step 1134000, training loss 0.0340546\n",
      "epoch 13,step 1141000, training loss 0.0448446\n",
      "epoch 13,step 1148000, training loss 0.0442297\n",
      "epoch 13,step 1155000, training loss 0.0550734\n",
      "epoch 13,step 1162000, training loss 0.0428498\n",
      "epoch 13,step 1169000, training loss 0.0360522\n",
      "epoch 13,step 1176000, training loss 0.0335867\n",
      "epoch 13,step 1183000, training loss 0.0491159\n",
      "epoch 13,step 1190000, training loss 0.0423061\n",
      "epoch 13,step 1197000, training loss 0.0481759\n",
      "epoch 13,step 1204000, training loss 0.0537156\n",
      "epoch 13,step 1211000, training loss 0.0334727\n",
      "epoch 13,step 1218000, training loss 0.0464567\n",
      "epoch 13,step 1225000, training loss 0.0388085\n",
      "epoch 13,step 1232000, training loss 0.0397267\n",
      "epoch 13,step 1239000, training loss 0.0394007\n",
      "epoch 13,step 1246000, training loss 0.0316949\n",
      "epoch 13,step 1253000, training loss 0.0504618\n",
      "epoch 13,step 1260000, training loss 0.0815916\n",
      "epoch 13,step 1267000, training loss 0.0384867\n",
      "epoch 13,step 1274000, training loss 0.034955\n",
      "epoch 13,step 1281000, training loss 0.0487137\n",
      "epoch 13,step 1288000, training loss 0.0394561\n",
      "epoch 13,step 1295000, training loss 0.029527\n",
      "epoch 13,step 1302000, training loss 0.0307936\n",
      "epoch 13,step 1309000, training loss 0.0542914\n",
      "epoch 13,step 1316000, training loss 0.0320358\n",
      "epoch 13,step 1323000, training loss 0.0393402\n",
      "epoch 13,step 1330000, training loss 0.0259586\n",
      "epoch 13,step 1337000, training loss 0.0304717\n",
      "epoch 13,step 1344000, training loss 0.0437825\n",
      "epoch 13,step 1351000, training loss 0.0306163\n",
      "epoch 13,step 1358000, training loss 0.0393465\n",
      "epoch 13,step 1365000, training loss 0.0516421\n",
      "epoch 13,step 1372000, training loss 0.0380284\n",
      "epoch 13,step 1379000, training loss 0.0322273\n",
      "epoch 13,step 1386000, training loss 0.0302962\n",
      "epoch 13,step 1393000, training loss 0.0290602\n",
      "epoch 13,training loss 0.0510835 ,test loss 0.0538016\n",
      "epoch 14,step 1500, training loss 0.14324\n",
      "epoch 14,step 3000, training loss 0.088926\n",
      "epoch 14,step 4500, training loss 0.0399339\n",
      "epoch 14,step 6000, training loss 0.0318877\n",
      "epoch 14,step 7500, training loss 0.0473489\n",
      "epoch 14,step 9000, training loss 0.0349011\n",
      "epoch 14,step 10500, training loss 0.031278\n",
      "epoch 14,step 12000, training loss 0.0297375\n",
      "epoch 14,step 13500, training loss 0.0400465\n",
      "epoch 14,step 15000, training loss 0.0593103\n",
      "epoch 14,step 16500, training loss 0.15298\n",
      "epoch 14,step 18000, training loss 0.0902866\n",
      "epoch 14,step 19500, training loss 0.0598508\n",
      "epoch 14,step 21000, training loss 0.0561306\n",
      "epoch 14,step 22500, training loss 0.0517124\n",
      "epoch 14,step 24000, training loss 0.031686\n",
      "epoch 14,step 25500, training loss 0.0460586\n",
      "epoch 14,step 27000, training loss 0.0377337\n",
      "epoch 14,step 28500, training loss 0.0690809\n",
      "epoch 14,step 30000, training loss 0.0304554\n",
      "epoch 14,step 31500, training loss 0.120902\n",
      "epoch 14,step 33000, training loss 0.0623281\n",
      "epoch 14,step 34500, training loss 0.0536319\n",
      "epoch 14,step 36000, training loss 0.0503455\n",
      "epoch 14,step 37500, training loss 0.0297928\n",
      "epoch 14,step 39000, training loss 0.0399776\n",
      "epoch 14,step 40500, training loss 0.0314391\n",
      "epoch 14,step 42000, training loss 0.0317478\n",
      "epoch 14,step 43500, training loss 0.0384175\n",
      "epoch 14,step 45000, training loss 0.0314606\n",
      "epoch 14,step 46500, training loss 0.109468\n",
      "epoch 14,step 48000, training loss 0.0544251\n",
      "epoch 14,step 49500, training loss 0.0365919\n",
      "epoch 14,step 51000, training loss 0.0343855\n",
      "epoch 14,step 52500, training loss 0.0357988\n",
      "epoch 14,step 54000, training loss 0.0594501\n",
      "epoch 14,step 55500, training loss 0.0371149\n",
      "epoch 14,step 57000, training loss 0.0322636\n",
      "epoch 14,step 58500, training loss 0.0255222\n",
      "epoch 14,step 60000, training loss 0.0330936\n",
      "epoch 14,step 61500, training loss 0.0986331\n",
      "epoch 14,step 63000, training loss 0.0537123\n",
      "epoch 14,step 64500, training loss 0.0344396\n",
      "epoch 14,step 66000, training loss 0.0510714\n",
      "epoch 14,step 67500, training loss 0.0389742\n",
      "epoch 14,step 69000, training loss 0.0346068\n",
      "epoch 14,step 70500, training loss 0.0493348\n",
      "epoch 14,step 72000, training loss 0.0333466\n",
      "epoch 14,step 73500, training loss 0.035821\n",
      "epoch 14,step 75000, training loss 0.0350642\n",
      "epoch 14,step 76500, training loss 0.110059\n",
      "epoch 14,step 78000, training loss 0.0865988\n",
      "epoch 14,step 79500, training loss 0.0383294\n",
      "epoch 14,step 81000, training loss 0.0340686\n",
      "epoch 14,step 82500, training loss 0.0345642\n",
      "epoch 14,step 84000, training loss 0.0329313\n",
      "epoch 14,step 85500, training loss 0.0313756\n",
      "epoch 14,step 87000, training loss 0.0516311\n",
      "epoch 14,step 88500, training loss 0.0504593\n",
      "epoch 14,step 90000, training loss 0.0445735\n",
      "epoch 14,step 91500, training loss 0.103258\n",
      "epoch 14,step 93000, training loss 0.0706532\n",
      "epoch 14,step 94500, training loss 0.0448398\n",
      "epoch 14,step 96000, training loss 0.0380582\n",
      "epoch 14,step 97500, training loss 0.0730412\n",
      "epoch 14,step 99000, training loss 0.0662301\n",
      "epoch 14,step 100500, training loss 0.0481479\n",
      "epoch 14,step 102000, training loss 0.0402239\n",
      "epoch 14,step 103500, training loss 0.0393241\n",
      "epoch 14,step 105000, training loss 0.0374118\n",
      "epoch 14,step 106500, training loss 0.102379\n",
      "epoch 14,step 108000, training loss 0.0492938\n",
      "epoch 14,step 109500, training loss 0.0374256\n",
      "epoch 14,step 111000, training loss 0.0295718\n",
      "epoch 14,step 112500, training loss 0.0529511\n",
      "epoch 14,step 114000, training loss 0.0404515\n",
      "epoch 14,step 115500, training loss 0.044236\n",
      "epoch 14,step 117000, training loss 0.0298895\n",
      "epoch 14,step 118500, training loss 0.0374278\n",
      "epoch 14,step 120000, training loss 0.0383274\n",
      "epoch 14,step 121500, training loss 0.13331\n",
      "epoch 14,step 123000, training loss 0.0594668\n",
      "epoch 14,step 124500, training loss 0.0884531\n",
      "epoch 14,step 126000, training loss 0.0379338\n",
      "epoch 14,step 127500, training loss 0.0343482\n",
      "epoch 14,step 129000, training loss 0.0351286\n",
      "epoch 14,step 130500, training loss 0.0387534\n",
      "epoch 14,step 132000, training loss 0.0349024\n",
      "epoch 14,step 133500, training loss 0.0274277\n",
      "epoch 14,step 135000, training loss 0.0300422\n",
      "epoch 14,step 136500, training loss 0.0969169\n",
      "epoch 14,step 138000, training loss 0.0812833\n",
      "epoch 14,step 139500, training loss 0.0384489\n",
      "epoch 14,step 141000, training loss 0.037176\n",
      "epoch 14,step 142500, training loss 0.0262556\n",
      "epoch 14,step 144000, training loss 0.0328179\n",
      "epoch 14,step 145500, training loss 0.0299852\n",
      "epoch 14,step 147000, training loss 0.0353854\n",
      "epoch 14,step 148500, training loss 0.0365473\n",
      "epoch 14,step 150000, training loss 0.0418253\n",
      "epoch 14,step 151500, training loss 0.112184\n",
      "epoch 14,step 153000, training loss 0.0565768\n",
      "epoch 14,step 154500, training loss 0.0387699\n",
      "epoch 14,step 156000, training loss 0.0288682\n",
      "epoch 14,step 157500, training loss 0.0332309\n",
      "epoch 14,step 159000, training loss 0.0359742\n",
      "epoch 14,step 160500, training loss 0.0254041\n",
      "epoch 14,step 162000, training loss 0.0472617\n",
      "epoch 14,step 163500, training loss 0.0476052\n",
      "epoch 14,step 165000, training loss 0.0393359\n",
      "epoch 14,step 166500, training loss 0.112871\n",
      "epoch 14,step 168000, training loss 0.0795262\n",
      "epoch 14,step 169500, training loss 0.0390865\n",
      "epoch 14,step 171000, training loss 0.0650258\n",
      "epoch 14,step 172500, training loss 0.0537918\n",
      "epoch 14,step 174000, training loss 0.0677962\n",
      "epoch 14,step 175500, training loss 0.0416662\n",
      "epoch 14,step 177000, training loss 0.0694549\n",
      "epoch 14,step 178500, training loss 0.0744799\n",
      "epoch 14,step 180000, training loss 0.0468016\n",
      "epoch 14,step 181500, training loss 0.1035\n",
      "epoch 14,step 183000, training loss 0.0682483\n",
      "epoch 14,step 184500, training loss 0.0494417\n",
      "epoch 14,step 186000, training loss 0.0432688\n",
      "epoch 14,step 187500, training loss 0.0434119\n",
      "epoch 14,step 189000, training loss 0.0288007\n",
      "epoch 14,step 190500, training loss 0.0436361\n",
      "epoch 14,step 192000, training loss 0.0402746\n",
      "epoch 14,step 193500, training loss 0.0325878\n",
      "epoch 14,step 195000, training loss 0.0263995\n",
      "epoch 14,step 196500, training loss 0.0933933\n",
      "epoch 14,step 198000, training loss 0.0924383\n",
      "epoch 14,step 199500, training loss 0.0398528\n",
      "epoch 14,step 201000, training loss 0.0391446\n",
      "epoch 14,step 202500, training loss 0.0638125\n",
      "epoch 14,step 204000, training loss 0.0316847\n",
      "epoch 14,step 205500, training loss 0.0577783\n",
      "epoch 14,step 207000, training loss 0.0309671\n",
      "epoch 14,step 208500, training loss 0.0492537\n",
      "epoch 14,step 210000, training loss 0.0403349\n",
      "epoch 14,step 211500, training loss 0.157104\n",
      "epoch 14,step 213000, training loss 0.100965\n",
      "epoch 14,step 214500, training loss 0.0562687\n",
      "epoch 14,step 216000, training loss 0.0360263\n",
      "epoch 14,step 217500, training loss 0.0452825\n",
      "epoch 14,step 219000, training loss 0.0445156\n",
      "epoch 14,step 220500, training loss 0.0544618\n",
      "epoch 14,step 222000, training loss 0.0366584\n",
      "epoch 14,step 223500, training loss 0.037272\n",
      "epoch 14,step 225000, training loss 0.0390461\n",
      "epoch 14,step 226500, training loss 0.100737\n",
      "epoch 14,step 228000, training loss 0.0818768\n",
      "epoch 14,step 229500, training loss 0.0581438\n",
      "epoch 14,step 231000, training loss 0.0540583\n",
      "epoch 14,step 232500, training loss 0.0618146\n",
      "epoch 14,step 234000, training loss 0.0673646\n",
      "epoch 14,step 235500, training loss 0.04658\n",
      "epoch 14,step 237000, training loss 0.0331648\n",
      "epoch 14,step 238500, training loss 0.0453606\n",
      "epoch 14,step 240000, training loss 0.0390398\n",
      "epoch 14,step 241500, training loss 0.101201\n",
      "epoch 14,step 243000, training loss 0.0698088\n",
      "epoch 14,step 244500, training loss 0.0439534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14,step 246000, training loss 0.035579\n",
      "epoch 14,step 247500, training loss 0.0427698\n",
      "epoch 14,step 249000, training loss 0.0348967\n",
      "epoch 14,step 250500, training loss 0.031944\n",
      "epoch 14,step 252000, training loss 0.0437935\n",
      "epoch 14,step 253500, training loss 0.0325496\n",
      "epoch 14,step 255000, training loss 0.0377781\n",
      "epoch 14,step 256500, training loss 0.138793\n",
      "epoch 14,step 258000, training loss 0.0645681\n",
      "epoch 14,step 259500, training loss 0.0473174\n",
      "epoch 14,step 261000, training loss 0.0661948\n",
      "epoch 14,step 262500, training loss 0.0530083\n",
      "epoch 14,step 264000, training loss 0.0421015\n",
      "epoch 14,step 265500, training loss 0.0607009\n",
      "epoch 14,step 267000, training loss 0.0394638\n",
      "epoch 14,step 268500, training loss 0.0385785\n",
      "epoch 14,step 270000, training loss 0.0341558\n",
      "epoch 14,step 271500, training loss 0.102566\n",
      "epoch 14,step 273000, training loss 0.0652099\n",
      "epoch 14,step 274500, training loss 0.0461865\n",
      "epoch 14,step 276000, training loss 0.0563621\n",
      "epoch 14,step 277500, training loss 0.0416458\n",
      "epoch 14,step 279000, training loss 0.0844153\n",
      "epoch 14,step 280500, training loss 0.0492977\n",
      "epoch 14,step 282000, training loss 0.0466215\n",
      "epoch 14,step 283500, training loss 0.0434771\n",
      "epoch 14,step 285000, training loss 0.0460943\n",
      "epoch 14,step 286500, training loss 0.106483\n",
      "epoch 14,step 288000, training loss 0.0522193\n",
      "epoch 14,step 289500, training loss 0.033288\n",
      "epoch 14,step 291000, training loss 0.0339651\n",
      "epoch 14,step 292500, training loss 0.0429229\n",
      "epoch 14,step 294000, training loss 0.0463569\n",
      "epoch 14,step 295500, training loss 0.0294281\n",
      "epoch 14,step 297000, training loss 0.0348508\n",
      "epoch 14,step 298500, training loss 0.0327901\n",
      "epoch 14,step 300000, training loss 0.0366868\n",
      "epoch 14,step 301500, training loss 0.106207\n",
      "epoch 14,step 303000, training loss 0.100725\n",
      "epoch 14,step 304500, training loss 0.0429316\n",
      "epoch 14,step 306000, training loss 0.0304939\n",
      "epoch 14,step 307500, training loss 0.0445084\n",
      "epoch 14,step 309000, training loss 0.0520713\n",
      "epoch 14,step 310500, training loss 0.0343146\n",
      "epoch 14,step 312000, training loss 0.0287513\n",
      "epoch 14,step 313500, training loss 0.0351508\n",
      "epoch 14,step 315000, training loss 0.0375058\n",
      "epoch 14,step 316500, training loss 0.109425\n",
      "epoch 14,step 318000, training loss 0.0576625\n",
      "epoch 14,step 319500, training loss 0.0424441\n",
      "epoch 14,step 321000, training loss 0.042769\n",
      "epoch 14,step 322500, training loss 0.0356551\n",
      "epoch 14,step 324000, training loss 0.0405441\n",
      "epoch 14,step 325500, training loss 0.0419836\n",
      "epoch 14,step 327000, training loss 0.0477377\n",
      "epoch 14,step 328500, training loss 0.0409257\n",
      "epoch 14,step 330000, training loss 0.0397754\n",
      "epoch 14,step 331500, training loss 0.13214\n",
      "epoch 14,step 333000, training loss 0.0603173\n",
      "epoch 14,step 334500, training loss 0.0431424\n",
      "epoch 14,step 336000, training loss 0.0356381\n",
      "epoch 14,step 337500, training loss 0.0357131\n",
      "epoch 14,step 339000, training loss 0.0300607\n",
      "epoch 14,step 340500, training loss 0.0343569\n",
      "epoch 14,step 342000, training loss 0.0356084\n",
      "epoch 14,step 343500, training loss 0.0335384\n",
      "epoch 14,step 345000, training loss 0.0434754\n",
      "epoch 14,step 346500, training loss 0.106703\n",
      "epoch 14,step 348000, training loss 0.0653744\n",
      "epoch 14,step 349500, training loss 0.0297726\n",
      "epoch 14,step 351000, training loss 0.0370496\n",
      "epoch 14,step 352500, training loss 0.0270145\n",
      "epoch 14,step 354000, training loss 0.0271772\n",
      "epoch 14,step 355500, training loss 0.039926\n",
      "epoch 14,step 357000, training loss 0.0356679\n",
      "epoch 14,step 358500, training loss 0.0341977\n",
      "epoch 14,step 360000, training loss 0.0346668\n",
      "epoch 14,step 361500, training loss 0.114148\n",
      "epoch 14,step 363000, training loss 0.0539367\n",
      "epoch 14,step 364500, training loss 0.0481998\n",
      "epoch 14,step 366000, training loss 0.0388588\n",
      "epoch 14,step 367500, training loss 0.0387926\n",
      "epoch 14,step 369000, training loss 0.0550259\n",
      "epoch 14,step 370500, training loss 0.035898\n",
      "epoch 14,step 372000, training loss 0.033069\n",
      "epoch 14,step 373500, training loss 0.0460671\n",
      "epoch 14,step 375000, training loss 0.0361005\n",
      "epoch 14,step 376500, training loss 0.175681\n",
      "epoch 14,step 378000, training loss 0.0569915\n",
      "epoch 14,step 379500, training loss 0.03444\n",
      "epoch 14,step 381000, training loss 0.0387723\n",
      "epoch 14,step 382500, training loss 0.0360969\n",
      "epoch 14,step 384000, training loss 0.0239598\n",
      "epoch 14,step 385500, training loss 0.043517\n",
      "epoch 14,step 387000, training loss 0.0363826\n",
      "epoch 14,step 388500, training loss 0.0541815\n",
      "epoch 14,step 390000, training loss 0.0456575\n",
      "epoch 14,step 391500, training loss 0.15599\n",
      "epoch 14,step 393000, training loss 0.0800777\n",
      "epoch 14,step 394500, training loss 0.0765092\n",
      "epoch 14,step 396000, training loss 0.0438198\n",
      "epoch 14,step 397500, training loss 0.0300527\n",
      "epoch 14,step 399000, training loss 0.0340306\n",
      "epoch 14,step 400500, training loss 0.0352287\n",
      "epoch 14,step 402000, training loss 0.0335746\n",
      "epoch 14,step 403500, training loss 0.032977\n",
      "epoch 14,step 405000, training loss 0.0398331\n",
      "epoch 14,step 406500, training loss 0.117152\n",
      "epoch 14,step 408000, training loss 0.0523343\n",
      "epoch 14,step 409500, training loss 0.0428504\n",
      "epoch 14,step 411000, training loss 0.0543447\n",
      "epoch 14,step 412500, training loss 0.0618829\n",
      "epoch 14,step 414000, training loss 0.0461463\n",
      "epoch 14,step 415500, training loss 0.0323386\n",
      "epoch 14,step 417000, training loss 0.0355164\n",
      "epoch 14,step 418500, training loss 0.0383984\n",
      "epoch 14,step 420000, training loss 0.0338099\n",
      "epoch 14,step 421500, training loss 0.146264\n",
      "epoch 14,step 423000, training loss 0.0694663\n",
      "epoch 14,step 424500, training loss 0.0316894\n",
      "epoch 14,step 426000, training loss 0.0389819\n",
      "epoch 14,step 427500, training loss 0.0328901\n",
      "epoch 14,step 429000, training loss 0.0331724\n",
      "epoch 14,step 430500, training loss 0.0412478\n",
      "epoch 14,step 432000, training loss 0.0284945\n",
      "epoch 14,step 433500, training loss 0.0368972\n",
      "epoch 14,step 435000, training loss 0.0428104\n",
      "epoch 14,step 436500, training loss 0.104923\n",
      "epoch 14,step 438000, training loss 0.0556689\n",
      "epoch 14,step 439500, training loss 0.0341103\n",
      "epoch 14,step 441000, training loss 0.0508557\n",
      "epoch 14,step 442500, training loss 0.0453423\n",
      "epoch 14,step 444000, training loss 0.0470857\n",
      "epoch 14,step 445500, training loss 0.0334848\n",
      "epoch 14,step 447000, training loss 0.0363682\n",
      "epoch 14,step 448500, training loss 0.0328639\n",
      "epoch 14,step 450000, training loss 0.0372311\n",
      "epoch 14,step 451500, training loss 0.104468\n",
      "epoch 14,step 453000, training loss 0.0683632\n",
      "epoch 14,step 454500, training loss 0.0397728\n",
      "epoch 14,step 456000, training loss 0.0384211\n",
      "epoch 14,step 457500, training loss 0.0366149\n",
      "epoch 14,step 459000, training loss 0.0447215\n",
      "epoch 14,step 460500, training loss 0.0347706\n",
      "epoch 14,step 462000, training loss 0.0522826\n",
      "epoch 14,step 463500, training loss 0.0279907\n",
      "epoch 14,step 465000, training loss 0.0362999\n",
      "epoch 14,step 466500, training loss 0.12549\n",
      "epoch 14,step 468000, training loss 0.0862357\n",
      "epoch 14,step 469500, training loss 0.0307969\n",
      "epoch 14,step 471000, training loss 0.0460805\n",
      "epoch 14,step 472500, training loss 0.0373672\n",
      "epoch 14,step 474000, training loss 0.0451184\n",
      "epoch 14,step 475500, training loss 0.0718343\n",
      "epoch 14,step 477000, training loss 0.0318395\n",
      "epoch 14,step 478500, training loss 0.0695377\n",
      "epoch 14,step 480000, training loss 0.0511184\n",
      "epoch 14,step 481500, training loss 0.112031\n",
      "epoch 14,step 483000, training loss 0.0838116\n",
      "epoch 14,step 484500, training loss 0.0442462\n",
      "epoch 14,step 486000, training loss 0.043827\n",
      "epoch 14,step 487500, training loss 0.0561866\n",
      "epoch 14,step 489000, training loss 0.0329014\n",
      "epoch 14,step 490500, training loss 0.0370573\n",
      "epoch 14,step 492000, training loss 0.031145\n",
      "epoch 14,step 493500, training loss 0.048372\n",
      "epoch 14,step 495000, training loss 0.0407399\n",
      "epoch 14,step 496500, training loss 0.100009\n",
      "epoch 14,step 498000, training loss 0.0989289\n",
      "epoch 14,step 499500, training loss 0.0353265\n",
      "epoch 14,step 501000, training loss 0.0504918\n",
      "epoch 14,step 502500, training loss 0.0371729\n",
      "epoch 14,step 504000, training loss 0.0282106\n",
      "epoch 14,step 505500, training loss 0.0354971\n",
      "epoch 14,step 507000, training loss 0.0332252\n",
      "epoch 14,step 508500, training loss 0.0357307\n",
      "epoch 14,step 510000, training loss 0.0538477\n",
      "epoch 14,step 511500, training loss 0.117894\n",
      "epoch 14,step 513000, training loss 0.0679782\n",
      "epoch 14,step 514500, training loss 0.0516398\n",
      "epoch 14,step 516000, training loss 0.0509838\n",
      "epoch 14,step 517500, training loss 0.0289604\n",
      "epoch 14,step 519000, training loss 0.032124\n",
      "epoch 14,step 520500, training loss 0.038006\n",
      "epoch 14,step 522000, training loss 0.0416767\n",
      "epoch 14,step 523500, training loss 0.0434663\n",
      "epoch 14,step 525000, training loss 0.0358032\n",
      "epoch 14,step 526500, training loss 0.109439\n",
      "epoch 14,step 528000, training loss 0.0600978\n",
      "epoch 14,step 529500, training loss 0.0416463\n",
      "epoch 14,step 531000, training loss 0.0357618\n",
      "epoch 14,step 532500, training loss 0.0461041\n",
      "epoch 14,step 534000, training loss 0.0347052\n",
      "epoch 14,step 535500, training loss 0.0338\n",
      "epoch 14,step 537000, training loss 0.029763\n",
      "epoch 14,step 538500, training loss 0.0287864\n",
      "epoch 14,step 540000, training loss 0.0303602\n",
      "epoch 14,step 541500, training loss 0.123187\n",
      "epoch 14,step 543000, training loss 0.0641323\n",
      "epoch 14,step 544500, training loss 0.0458275\n",
      "epoch 14,step 546000, training loss 0.037162\n",
      "epoch 14,step 547500, training loss 0.0593323\n",
      "epoch 14,step 549000, training loss 0.0509664\n",
      "epoch 14,step 550500, training loss 0.0453512\n",
      "epoch 14,step 552000, training loss 0.0460938\n",
      "epoch 14,step 553500, training loss 0.0421922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14,step 555000, training loss 0.0500717\n",
      "epoch 14,step 556500, training loss 0.126929\n",
      "epoch 14,step 558000, training loss 0.0683728\n",
      "epoch 14,step 559500, training loss 0.0358512\n",
      "epoch 14,step 561000, training loss 0.0605205\n",
      "epoch 14,step 562500, training loss 0.041121\n",
      "epoch 14,step 564000, training loss 0.0417162\n",
      "epoch 14,step 565500, training loss 0.0376785\n",
      "epoch 14,step 567000, training loss 0.0465852\n",
      "epoch 14,step 568500, training loss 0.0326068\n",
      "epoch 14,step 570000, training loss 0.0404878\n",
      "epoch 14,step 571500, training loss 0.119466\n",
      "epoch 14,step 573000, training loss 0.13423\n",
      "epoch 14,step 574500, training loss 0.0554872\n",
      "epoch 14,step 576000, training loss 0.0465385\n",
      "epoch 14,step 577500, training loss 0.0372303\n",
      "epoch 14,step 579000, training loss 0.0455738\n",
      "epoch 14,step 580500, training loss 0.0414743\n",
      "epoch 14,step 582000, training loss 0.0386097\n",
      "epoch 14,step 583500, training loss 0.0441601\n",
      "epoch 14,step 585000, training loss 0.0374133\n",
      "epoch 14,step 586500, training loss 0.108969\n",
      "epoch 14,step 588000, training loss 0.0694865\n",
      "epoch 14,step 589500, training loss 0.0348782\n",
      "epoch 14,step 591000, training loss 0.035492\n",
      "epoch 14,step 592500, training loss 0.0408365\n",
      "epoch 14,step 594000, training loss 0.0313066\n",
      "epoch 14,step 595500, training loss 0.0342827\n",
      "epoch 14,step 597000, training loss 0.0475954\n",
      "epoch 14,step 598500, training loss 0.0352331\n",
      "epoch 14,step 600000, training loss 0.0285641\n",
      "epoch 14,step 601500, training loss 0.134741\n",
      "epoch 14,step 603000, training loss 0.124562\n",
      "epoch 14,step 604500, training loss 0.0365932\n",
      "epoch 14,step 606000, training loss 0.0374225\n",
      "epoch 14,step 607500, training loss 0.0330907\n",
      "epoch 14,step 609000, training loss 0.0375373\n",
      "epoch 14,step 610500, training loss 0.0313461\n",
      "epoch 14,step 612000, training loss 0.0415221\n",
      "epoch 14,step 613500, training loss 0.0446369\n",
      "epoch 14,step 615000, training loss 0.0353129\n",
      "epoch 14,step 616500, training loss 0.105455\n",
      "epoch 14,step 618000, training loss 0.0615557\n",
      "epoch 14,step 619500, training loss 0.0423751\n",
      "epoch 14,step 621000, training loss 0.0448638\n",
      "epoch 14,step 622500, training loss 0.0345133\n",
      "epoch 14,step 624000, training loss 0.0470591\n",
      "epoch 14,step 625500, training loss 0.0430474\n",
      "epoch 14,step 627000, training loss 0.0409038\n",
      "epoch 14,step 628500, training loss 0.0288745\n",
      "epoch 14,step 630000, training loss 0.0336594\n",
      "epoch 14,step 631500, training loss 0.108791\n",
      "epoch 14,step 633000, training loss 0.0575641\n",
      "epoch 14,step 634500, training loss 0.0225344\n",
      "epoch 14,step 636000, training loss 0.0357673\n",
      "epoch 14,step 637500, training loss 0.0497338\n",
      "epoch 14,step 639000, training loss 0.047385\n",
      "epoch 14,step 640500, training loss 0.0431993\n",
      "epoch 14,step 642000, training loss 0.0496937\n",
      "epoch 14,step 643500, training loss 0.0326641\n",
      "epoch 14,step 645000, training loss 0.0384653\n",
      "epoch 14,step 646500, training loss 0.104652\n",
      "epoch 14,step 648000, training loss 0.0793912\n",
      "epoch 14,step 649500, training loss 0.0493028\n",
      "epoch 14,step 651000, training loss 0.0365595\n",
      "epoch 14,step 652500, training loss 0.0464623\n",
      "epoch 14,step 654000, training loss 0.0322824\n",
      "epoch 14,step 655500, training loss 0.0388424\n",
      "epoch 14,step 657000, training loss 0.0404234\n",
      "epoch 14,step 658500, training loss 0.0383455\n",
      "epoch 14,step 660000, training loss 0.0398183\n",
      "epoch 14,step 661500, training loss 0.113633\n",
      "epoch 14,step 663000, training loss 0.0555183\n",
      "epoch 14,step 664500, training loss 0.0412033\n",
      "epoch 14,step 666000, training loss 0.0274058\n",
      "epoch 14,step 667500, training loss 0.0257548\n",
      "epoch 14,step 669000, training loss 0.0304525\n",
      "epoch 14,step 670500, training loss 0.0398243\n",
      "epoch 14,step 672000, training loss 0.0320846\n",
      "epoch 14,step 673500, training loss 0.0359422\n",
      "epoch 14,step 675000, training loss 0.0335994\n",
      "epoch 14,step 676500, training loss 0.0979434\n",
      "epoch 14,step 678000, training loss 0.0527976\n",
      "epoch 14,step 679500, training loss 0.0423091\n",
      "epoch 14,step 681000, training loss 0.0433287\n",
      "epoch 14,step 682500, training loss 0.0321591\n",
      "epoch 14,step 684000, training loss 0.0285923\n",
      "epoch 14,step 685500, training loss 0.0348782\n",
      "epoch 14,step 687000, training loss 0.036878\n",
      "epoch 14,step 688500, training loss 0.0274039\n",
      "epoch 14,step 690000, training loss 0.036458\n",
      "epoch 14,step 691500, training loss 0.131561\n",
      "epoch 14,step 693000, training loss 0.0686513\n",
      "epoch 14,step 694500, training loss 0.0457\n",
      "epoch 14,step 696000, training loss 0.0483148\n",
      "epoch 14,step 697500, training loss 0.0532571\n",
      "epoch 14,step 699000, training loss 0.0389649\n",
      "epoch 14,step 700500, training loss 0.0591451\n",
      "epoch 14,step 702000, training loss 0.0418708\n",
      "epoch 14,step 703500, training loss 0.0386197\n",
      "epoch 14,step 705000, training loss 0.0386262\n",
      "epoch 14,step 706500, training loss 0.113975\n",
      "epoch 14,step 708000, training loss 0.123668\n",
      "epoch 14,step 709500, training loss 0.0486451\n",
      "epoch 14,step 711000, training loss 0.0434029\n",
      "epoch 14,step 712500, training loss 0.0321643\n",
      "epoch 14,step 714000, training loss 0.0573602\n",
      "epoch 14,step 715500, training loss 0.0409916\n",
      "epoch 14,step 717000, training loss 0.0466293\n",
      "epoch 14,step 718500, training loss 0.0377635\n",
      "epoch 14,step 720000, training loss 0.0412107\n",
      "epoch 14,step 721500, training loss 0.0976187\n",
      "epoch 14,step 723000, training loss 0.0521505\n",
      "epoch 14,step 724500, training loss 0.0444987\n",
      "epoch 14,step 726000, training loss 0.0423496\n",
      "epoch 14,step 727500, training loss 0.0364507\n",
      "epoch 14,step 729000, training loss 0.0426234\n",
      "epoch 14,step 730500, training loss 0.0361103\n",
      "epoch 14,step 732000, training loss 0.03719\n",
      "epoch 14,step 733500, training loss 0.0405083\n",
      "epoch 14,step 735000, training loss 0.0427177\n",
      "epoch 14,step 736500, training loss 0.110939\n",
      "epoch 14,step 738000, training loss 0.0588061\n",
      "epoch 14,step 739500, training loss 0.0359114\n",
      "epoch 14,step 741000, training loss 0.0569705\n",
      "epoch 14,step 742500, training loss 0.0430513\n",
      "epoch 14,step 744000, training loss 0.033697\n",
      "epoch 14,step 745500, training loss 0.0382895\n",
      "epoch 14,step 747000, training loss 0.0432046\n",
      "epoch 14,step 748500, training loss 0.0680026\n",
      "epoch 14,step 750000, training loss 0.0365541\n",
      "epoch 14,step 751500, training loss 0.137799\n",
      "epoch 14,step 753000, training loss 0.0731282\n",
      "epoch 14,step 754500, training loss 0.0425802\n",
      "epoch 14,step 756000, training loss 0.0462778\n",
      "epoch 14,step 757500, training loss 0.037503\n",
      "epoch 14,step 759000, training loss 0.0426338\n",
      "epoch 14,step 760500, training loss 0.031065\n",
      "epoch 14,step 762000, training loss 0.0408441\n",
      "epoch 14,step 763500, training loss 0.0422539\n",
      "epoch 14,step 765000, training loss 0.0315642\n",
      "epoch 14,step 766500, training loss 0.129275\n",
      "epoch 14,step 768000, training loss 0.12875\n",
      "epoch 14,step 769500, training loss 0.0474719\n",
      "epoch 14,step 771000, training loss 0.0454476\n",
      "epoch 14,step 772500, training loss 0.0442976\n",
      "epoch 14,step 774000, training loss 0.0493072\n",
      "epoch 14,step 775500, training loss 0.0475388\n",
      "epoch 14,step 777000, training loss 0.0414975\n",
      "epoch 14,step 778500, training loss 0.0532515\n",
      "epoch 14,step 780000, training loss 0.0335996\n",
      "epoch 14,step 781500, training loss 0.114098\n",
      "epoch 14,step 783000, training loss 0.083345\n",
      "epoch 14,step 784500, training loss 0.0438364\n",
      "epoch 14,step 786000, training loss 0.0385395\n",
      "epoch 14,step 787500, training loss 0.0470655\n",
      "epoch 14,step 789000, training loss 0.0360262\n",
      "epoch 14,step 790500, training loss 0.0456127\n",
      "epoch 14,step 792000, training loss 0.046726\n",
      "epoch 14,step 793500, training loss 0.0667355\n",
      "epoch 14,step 795000, training loss 0.0526742\n",
      "epoch 14,step 796500, training loss 0.15349\n",
      "epoch 14,step 798000, training loss 0.135778\n",
      "epoch 14,step 799500, training loss 0.0590943\n",
      "epoch 14,step 801000, training loss 0.0691685\n",
      "epoch 14,step 802500, training loss 0.0491096\n",
      "epoch 14,step 804000, training loss 0.040023\n",
      "epoch 14,step 805500, training loss 0.0452404\n",
      "epoch 14,step 807000, training loss 0.0429672\n",
      "epoch 14,step 808500, training loss 0.0436126\n",
      "epoch 14,step 810000, training loss 0.0400213\n",
      "epoch 14,step 811500, training loss 0.098272\n",
      "epoch 14,step 813000, training loss 0.10785\n",
      "epoch 14,step 814500, training loss 0.0695496\n",
      "epoch 14,step 816000, training loss 0.062719\n",
      "epoch 14,step 817500, training loss 0.0436738\n",
      "epoch 14,step 819000, training loss 0.0387258\n",
      "epoch 14,step 820500, training loss 0.03119\n",
      "epoch 14,step 822000, training loss 0.0447815\n",
      "epoch 14,step 823500, training loss 0.0386298\n",
      "epoch 14,step 825000, training loss 0.0568154\n",
      "epoch 14,step 826500, training loss 0.108463\n",
      "epoch 14,step 828000, training loss 0.0635027\n",
      "epoch 14,step 829500, training loss 0.0481167\n",
      "epoch 14,step 831000, training loss 0.0551967\n",
      "epoch 14,step 832500, training loss 0.0466048\n",
      "epoch 14,step 834000, training loss 0.0450356\n",
      "epoch 14,step 835500, training loss 0.034847\n",
      "epoch 14,step 837000, training loss 0.0510269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14,step 838500, training loss 0.038447\n",
      "epoch 14,step 840000, training loss 0.048685\n",
      "epoch 14,step 841500, training loss 0.118767\n",
      "epoch 14,step 843000, training loss 0.0903286\n",
      "epoch 14,step 844500, training loss 0.0659421\n",
      "epoch 14,step 846000, training loss 0.0462344\n",
      "epoch 14,step 847500, training loss 0.0392492\n",
      "epoch 14,step 849000, training loss 0.0355017\n",
      "epoch 14,step 850500, training loss 0.0385189\n",
      "epoch 14,step 852000, training loss 0.0408674\n",
      "epoch 14,step 853500, training loss 0.0379007\n",
      "epoch 14,step 855000, training loss 0.0338301\n",
      "epoch 14,step 856500, training loss 0.11919\n",
      "epoch 14,step 858000, training loss 0.0851182\n",
      "epoch 14,step 859500, training loss 0.0503617\n",
      "epoch 14,step 861000, training loss 0.0490598\n",
      "epoch 14,step 862500, training loss 0.0637256\n",
      "epoch 14,step 864000, training loss 0.0534194\n",
      "epoch 14,step 865500, training loss 0.0487696\n",
      "epoch 14,step 867000, training loss 0.0474976\n",
      "epoch 14,step 868500, training loss 0.0380782\n",
      "epoch 14,step 870000, training loss 0.0314624\n",
      "epoch 14,step 871500, training loss 0.118301\n",
      "epoch 14,step 873000, training loss 0.0632615\n",
      "epoch 14,step 874500, training loss 0.0405004\n",
      "epoch 14,step 876000, training loss 0.042151\n",
      "epoch 14,step 877500, training loss 0.0368059\n",
      "epoch 14,step 879000, training loss 0.0426528\n",
      "epoch 14,step 880500, training loss 0.0376801\n",
      "epoch 14,step 882000, training loss 0.0473194\n",
      "epoch 14,step 883500, training loss 0.041542\n",
      "epoch 14,step 885000, training loss 0.0301187\n",
      "epoch 14,step 886500, training loss 0.103093\n",
      "epoch 14,step 888000, training loss 0.058628\n",
      "epoch 14,step 889500, training loss 0.0423772\n",
      "epoch 14,step 891000, training loss 0.0446179\n",
      "epoch 14,step 892500, training loss 0.0344686\n",
      "epoch 14,step 894000, training loss 0.0395248\n",
      "epoch 14,step 895500, training loss 0.0287731\n",
      "epoch 14,step 897000, training loss 0.0349067\n",
      "epoch 14,step 898500, training loss 0.0402126\n",
      "epoch 14,step 900000, training loss 0.0487695\n",
      "epoch 14,step 901500, training loss 0.167083\n",
      "epoch 14,step 903000, training loss 0.0624665\n",
      "epoch 14,step 904500, training loss 0.0475653\n",
      "epoch 14,step 906000, training loss 0.0658197\n",
      "epoch 14,step 907500, training loss 0.0328513\n",
      "epoch 14,step 909000, training loss 0.0441214\n",
      "epoch 14,step 910500, training loss 0.0345108\n",
      "epoch 14,step 912000, training loss 0.03446\n",
      "epoch 14,step 913500, training loss 0.0407142\n",
      "epoch 14,step 915000, training loss 0.0306916\n",
      "epoch 14,step 916500, training loss 0.138771\n",
      "epoch 14,step 918000, training loss 0.078146\n",
      "epoch 14,step 919500, training loss 0.0435898\n",
      "epoch 14,step 921000, training loss 0.0308986\n",
      "epoch 14,step 922500, training loss 0.0338557\n",
      "epoch 14,step 924000, training loss 0.0397989\n",
      "epoch 14,step 925500, training loss 0.0327558\n",
      "epoch 14,step 927000, training loss 0.0283235\n",
      "epoch 14,step 928500, training loss 0.031831\n",
      "epoch 14,step 930000, training loss 0.0463276\n",
      "epoch 14,step 931500, training loss 0.153546\n",
      "epoch 14,step 933000, training loss 0.166535\n",
      "epoch 14,step 934500, training loss 0.0517436\n",
      "epoch 14,step 936000, training loss 0.0625723\n",
      "epoch 14,step 937500, training loss 0.0466965\n",
      "epoch 14,step 939000, training loss 0.0537281\n",
      "epoch 14,step 940500, training loss 0.0327246\n",
      "epoch 14,step 942000, training loss 0.040786\n",
      "epoch 14,step 943500, training loss 0.0347711\n",
      "epoch 14,step 945000, training loss 0.042953\n",
      "epoch 14,step 946500, training loss 0.119477\n",
      "epoch 14,step 948000, training loss 0.0633902\n",
      "epoch 14,step 949500, training loss 0.0392532\n",
      "epoch 14,step 951000, training loss 0.0456217\n",
      "epoch 14,step 952500, training loss 0.0320355\n",
      "epoch 14,step 954000, training loss 0.0290183\n",
      "epoch 14,step 955500, training loss 0.0310912\n",
      "epoch 14,step 957000, training loss 0.0502393\n",
      "epoch 14,step 958500, training loss 0.0394212\n",
      "epoch 14,step 960000, training loss 0.0426855\n",
      "epoch 14,step 961500, training loss 0.179491\n",
      "epoch 14,step 963000, training loss 0.088604\n",
      "epoch 14,step 964500, training loss 0.0345194\n",
      "epoch 14,step 966000, training loss 0.0372438\n",
      "epoch 14,step 967500, training loss 0.0474066\n",
      "epoch 14,step 969000, training loss 0.0479669\n",
      "epoch 14,step 970500, training loss 0.0992279\n",
      "epoch 14,step 972000, training loss 0.0349814\n",
      "epoch 14,step 973500, training loss 0.0325363\n",
      "epoch 14,step 975000, training loss 0.0430454\n",
      "epoch 14,step 976500, training loss 0.117922\n",
      "epoch 14,step 978000, training loss 0.0596916\n",
      "epoch 14,step 979500, training loss 0.0326619\n",
      "epoch 14,step 981000, training loss 0.0395737\n",
      "epoch 14,step 982500, training loss 0.0364593\n",
      "epoch 14,step 984000, training loss 0.0375161\n",
      "epoch 14,step 985500, training loss 0.0328142\n",
      "epoch 14,step 987000, training loss 0.0463416\n",
      "epoch 14,step 988500, training loss 0.0552524\n",
      "epoch 14,step 990000, training loss 0.036579\n",
      "epoch 14,step 991500, training loss 0.13845\n",
      "epoch 14,step 993000, training loss 0.077943\n",
      "epoch 14,step 994500, training loss 0.0471805\n",
      "epoch 14,step 996000, training loss 0.047439\n",
      "epoch 14,step 997500, training loss 0.0313485\n",
      "epoch 14,step 999000, training loss 0.0374333\n",
      "epoch 14,step 1000500, training loss 0.0432598\n",
      "epoch 14,step 1002000, training loss 0.0518441\n",
      "epoch 14,step 1003500, training loss 0.0425232\n",
      "epoch 14,step 1005000, training loss 0.0400646\n",
      "epoch 14,step 1006500, training loss 0.115517\n",
      "epoch 14,step 1008000, training loss 0.1303\n",
      "epoch 14,step 1009500, training loss 0.0458416\n",
      "epoch 14,step 1011000, training loss 0.0529617\n",
      "epoch 14,step 1012500, training loss 0.0339619\n",
      "epoch 14,step 1014000, training loss 0.0498783\n",
      "epoch 14,step 1015500, training loss 0.0519862\n",
      "epoch 14,step 1017000, training loss 0.041768\n",
      "epoch 14,step 1018500, training loss 0.0368122\n",
      "epoch 14,step 1020000, training loss 0.0407471\n",
      "epoch 14,step 1021500, training loss 0.130484\n",
      "epoch 14,step 1023000, training loss 0.0697879\n",
      "epoch 14,step 1024500, training loss 0.0492211\n",
      "epoch 14,step 1026000, training loss 0.0543407\n",
      "epoch 14,step 1027500, training loss 0.0400218\n",
      "epoch 14,step 1029000, training loss 0.0450938\n",
      "epoch 14,step 1030500, training loss 0.0396244\n",
      "epoch 14,step 1032000, training loss 0.0505391\n",
      "epoch 14,step 1033500, training loss 0.0427565\n",
      "epoch 14,step 1035000, training loss 0.040413\n",
      "epoch 14,step 1036500, training loss 0.121418\n",
      "epoch 14,step 1038000, training loss 0.0783486\n",
      "epoch 14,step 1039500, training loss 0.0375388\n",
      "epoch 14,step 1041000, training loss 0.0375219\n",
      "epoch 14,step 1042500, training loss 0.0395895\n",
      "epoch 14,step 1044000, training loss 0.0339799\n",
      "epoch 14,step 1045500, training loss 0.041202\n",
      "epoch 14,step 1047000, training loss 0.0473981\n",
      "epoch 14,step 1048500, training loss 0.0423812\n",
      "epoch 14,step 1050000, training loss 0.0325142\n",
      "epoch 14,step 1051500, training loss 0.129502\n",
      "epoch 14,step 1053000, training loss 0.0924693\n",
      "epoch 14,step 1054500, training loss 0.0499143\n",
      "epoch 14,step 1056000, training loss 0.0388644\n",
      "epoch 14,step 1057500, training loss 0.0423082\n",
      "epoch 14,step 1059000, training loss 0.0400268\n",
      "epoch 14,step 1060500, training loss 0.0424788\n",
      "epoch 14,step 1062000, training loss 0.0302187\n",
      "epoch 14,step 1063500, training loss 0.0323183\n",
      "epoch 14,step 1065000, training loss 0.0398949\n",
      "epoch 14,step 1066500, training loss 0.120469\n",
      "epoch 14,step 1068000, training loss 0.0742817\n",
      "epoch 14,step 1069500, training loss 0.0436418\n",
      "epoch 14,step 1071000, training loss 0.0418672\n",
      "epoch 14,step 1072500, training loss 0.0411487\n",
      "epoch 14,step 1074000, training loss 0.0359321\n",
      "epoch 14,step 1075500, training loss 0.0539045\n",
      "epoch 14,step 1077000, training loss 0.0571392\n",
      "epoch 14,step 1078500, training loss 0.0420133\n",
      "epoch 14,step 1080000, training loss 0.0382308\n",
      "epoch 14,step 1081500, training loss 0.092055\n",
      "epoch 14,step 1083000, training loss 0.0539363\n",
      "epoch 14,step 1084500, training loss 0.0548046\n",
      "epoch 14,step 1086000, training loss 0.0470318\n",
      "epoch 14,step 1087500, training loss 0.0444065\n",
      "epoch 14,step 1089000, training loss 0.0345087\n",
      "epoch 14,step 1090500, training loss 0.0529249\n",
      "epoch 14,step 1092000, training loss 0.0402133\n",
      "epoch 14,step 1093500, training loss 0.0344648\n",
      "epoch 14,step 1095000, training loss 0.0383128\n",
      "epoch 14,step 1096500, training loss 0.11482\n",
      "epoch 14,step 1098000, training loss 0.0651893\n",
      "epoch 14,step 1099500, training loss 0.03591\n",
      "epoch 14,step 1101000, training loss 0.0520077\n",
      "epoch 14,step 1102500, training loss 0.0380563\n",
      "epoch 14,step 1104000, training loss 0.0398129\n",
      "epoch 14,step 1105500, training loss 0.0363335\n",
      "epoch 14,step 1107000, training loss 0.0460685\n",
      "epoch 14,step 1108500, training loss 0.0735366\n",
      "epoch 14,step 1110000, training loss 0.0527664\n",
      "epoch 14,step 1111500, training loss 0.0903094\n",
      "epoch 14,step 1113000, training loss 0.0553108\n",
      "epoch 14,step 1114500, training loss 0.031065\n",
      "epoch 14,step 1116000, training loss 0.0282897\n",
      "epoch 14,step 1117500, training loss 0.0316363\n",
      "epoch 14,step 1119000, training loss 0.0366487\n",
      "epoch 14,step 1120500, training loss 0.0318519\n",
      "epoch 14,step 1122000, training loss 0.0293728\n",
      "epoch 14,step 1123500, training loss 0.032199\n",
      "epoch 14,step 1125000, training loss 0.0831623\n",
      "epoch 14,step 1126500, training loss 0.103172\n",
      "epoch 14,step 1128000, training loss 0.0517952\n",
      "epoch 14,step 1129500, training loss 0.0307191\n",
      "epoch 14,step 1131000, training loss 0.0337307\n",
      "epoch 14,step 1132500, training loss 0.0335944\n",
      "epoch 14,step 1134000, training loss 0.0367744\n",
      "epoch 14,step 1135500, training loss 0.0364487\n",
      "epoch 14,step 1137000, training loss 0.0352399\n",
      "epoch 14,step 1138500, training loss 0.0232044\n",
      "epoch 14,step 1140000, training loss 0.0321436\n",
      "epoch 14,step 1141500, training loss 0.106279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14,step 1143000, training loss 0.0933657\n",
      "epoch 14,step 1144500, training loss 0.0393599\n",
      "epoch 14,step 1146000, training loss 0.0302954\n",
      "epoch 14,step 1147500, training loss 0.0504305\n",
      "epoch 14,step 1149000, training loss 0.0600194\n",
      "epoch 14,step 1150500, training loss 0.0314898\n",
      "epoch 14,step 1152000, training loss 0.0501205\n",
      "epoch 14,step 1153500, training loss 0.0352125\n",
      "epoch 14,step 1155000, training loss 0.0289296\n",
      "epoch 14,step 1156500, training loss 0.137836\n",
      "epoch 14,step 1158000, training loss 0.104455\n",
      "epoch 14,step 1159500, training loss 0.0452968\n",
      "epoch 14,step 1161000, training loss 0.0458669\n",
      "epoch 14,step 1162500, training loss 0.0399037\n",
      "epoch 14,step 1164000, training loss 0.0318634\n",
      "epoch 14,step 1165500, training loss 0.032121\n",
      "epoch 14,step 1167000, training loss 0.0410657\n",
      "epoch 14,step 1168500, training loss 0.0422525\n",
      "epoch 14,step 1170000, training loss 0.0290648\n",
      "epoch 14,step 1171500, training loss 0.101471\n",
      "epoch 14,step 1173000, training loss 0.0911548\n",
      "epoch 14,step 1174500, training loss 0.0405433\n",
      "epoch 14,step 1176000, training loss 0.0295032\n",
      "epoch 14,step 1177500, training loss 0.0371351\n",
      "epoch 14,step 1179000, training loss 0.0319359\n",
      "epoch 14,step 1180500, training loss 0.065461\n",
      "epoch 14,step 1182000, training loss 0.0385032\n",
      "epoch 14,step 1183500, training loss 0.0316586\n",
      "epoch 14,step 1185000, training loss 0.0380026\n",
      "epoch 14,step 1186500, training loss 0.135809\n",
      "epoch 14,step 1188000, training loss 0.0716152\n",
      "epoch 14,step 1189500, training loss 0.06048\n",
      "epoch 14,step 1191000, training loss 0.0574398\n",
      "epoch 14,step 1192500, training loss 0.035645\n",
      "epoch 14,step 1194000, training loss 0.0412426\n",
      "epoch 14,step 1195500, training loss 0.030479\n",
      "epoch 14,step 1197000, training loss 0.0404707\n",
      "epoch 14,step 1198500, training loss 0.0327636\n",
      "epoch 14,step 1200000, training loss 0.033279\n",
      "epoch 14,step 1201500, training loss 0.122223\n",
      "epoch 14,step 1203000, training loss 0.0520501\n",
      "epoch 14,step 1204500, training loss 0.0452604\n",
      "epoch 14,step 1206000, training loss 0.0500104\n",
      "epoch 14,step 1207500, training loss 0.0400272\n",
      "epoch 14,step 1209000, training loss 0.0491469\n",
      "epoch 14,step 1210500, training loss 0.0371944\n",
      "epoch 14,step 1212000, training loss 0.0450802\n",
      "epoch 14,step 1213500, training loss 0.0375359\n",
      "epoch 14,step 1215000, training loss 0.0321289\n",
      "epoch 14,step 1216500, training loss 0.0986693\n",
      "epoch 14,step 1218000, training loss 0.0901261\n",
      "epoch 14,step 1219500, training loss 0.0399436\n",
      "epoch 14,step 1221000, training loss 0.0411332\n",
      "epoch 14,step 1222500, training loss 0.0434589\n",
      "epoch 14,step 1224000, training loss 0.0342339\n",
      "epoch 14,step 1225500, training loss 0.036262\n",
      "epoch 14,step 1227000, training loss 0.0432185\n",
      "epoch 14,step 1228500, training loss 0.0374954\n",
      "epoch 14,step 1230000, training loss 0.0394192\n",
      "epoch 14,step 1231500, training loss 0.12203\n",
      "epoch 14,step 1233000, training loss 0.0519309\n",
      "epoch 14,step 1234500, training loss 0.0319165\n",
      "epoch 14,step 1236000, training loss 0.0465679\n",
      "epoch 14,step 1237500, training loss 0.0526422\n",
      "epoch 14,step 1239000, training loss 0.0453414\n",
      "epoch 14,step 1240500, training loss 0.0405537\n",
      "epoch 14,step 1242000, training loss 0.0610985\n",
      "epoch 14,step 1243500, training loss 0.0494517\n",
      "epoch 14,step 1245000, training loss 0.0409447\n",
      "epoch 14,step 1246500, training loss 0.112813\n",
      "epoch 14,step 1248000, training loss 0.0820353\n",
      "epoch 14,step 1249500, training loss 0.0353759\n",
      "epoch 14,step 1251000, training loss 0.0367922\n",
      "epoch 14,step 1252500, training loss 0.0347638\n",
      "epoch 14,step 1254000, training loss 0.0474502\n",
      "epoch 14,step 1255500, training loss 0.0333561\n",
      "epoch 14,step 1257000, training loss 0.035432\n",
      "epoch 14,step 1258500, training loss 0.0473882\n",
      "epoch 14,step 1260000, training loss 0.0334837\n",
      "epoch 14,step 1261500, training loss 0.119974\n",
      "epoch 14,step 1263000, training loss 0.0939749\n",
      "epoch 14,step 1264500, training loss 0.0518792\n",
      "epoch 14,step 1266000, training loss 0.0393255\n",
      "epoch 14,step 1267500, training loss 0.0467576\n",
      "epoch 14,step 1269000, training loss 0.0424075\n",
      "epoch 14,step 1270500, training loss 0.057242\n",
      "epoch 14,step 1272000, training loss 0.0313614\n",
      "epoch 14,step 1273500, training loss 0.0420978\n",
      "epoch 14,step 1275000, training loss 0.0412845\n",
      "epoch 14,step 1276500, training loss 0.10314\n",
      "epoch 14,step 1278000, training loss 0.0647138\n",
      "epoch 14,step 1279500, training loss 0.040192\n",
      "epoch 14,step 1281000, training loss 0.0542663\n",
      "epoch 14,step 1282500, training loss 0.0486278\n",
      "epoch 14,step 1284000, training loss 0.0461786\n",
      "epoch 14,step 1285500, training loss 0.0374558\n",
      "epoch 14,step 1287000, training loss 0.0450341\n",
      "epoch 14,step 1288500, training loss 0.0420995\n",
      "epoch 14,step 1290000, training loss 0.0531533\n",
      "epoch 14,step 1291500, training loss 0.102228\n",
      "epoch 14,step 1293000, training loss 0.0721005\n",
      "epoch 14,step 1294500, training loss 0.03736\n",
      "epoch 14,step 1296000, training loss 0.0353192\n",
      "epoch 14,step 1297500, training loss 0.0324009\n",
      "epoch 14,step 1299000, training loss 0.0302872\n",
      "epoch 14,step 1300500, training loss 0.0392845\n",
      "epoch 14,step 1302000, training loss 0.0363847\n",
      "epoch 14,step 1303500, training loss 0.0474135\n",
      "epoch 14,step 1305000, training loss 0.0447866\n",
      "epoch 14,step 1306500, training loss 0.110268\n",
      "epoch 14,step 1308000, training loss 0.0631513\n",
      "epoch 14,step 1309500, training loss 0.0409985\n",
      "epoch 14,step 1311000, training loss 0.0362264\n",
      "epoch 14,step 1312500, training loss 0.0362036\n",
      "epoch 14,step 1314000, training loss 0.0546664\n",
      "epoch 14,step 1315500, training loss 0.0505752\n",
      "epoch 14,step 1317000, training loss 0.0306968\n",
      "epoch 14,step 1318500, training loss 0.0291303\n",
      "epoch 14,step 1320000, training loss 0.0386458\n",
      "epoch 14,step 1321500, training loss 0.153974\n",
      "epoch 14,step 1323000, training loss 0.0995714\n",
      "epoch 14,step 1324500, training loss 0.0609205\n",
      "epoch 14,step 1326000, training loss 0.0315489\n",
      "epoch 14,step 1327500, training loss 0.0363308\n",
      "epoch 14,step 1329000, training loss 0.0332125\n",
      "epoch 14,step 1330500, training loss 0.0325725\n",
      "epoch 14,step 1332000, training loss 0.0310751\n",
      "epoch 14,step 1333500, training loss 0.0629854\n",
      "epoch 14,step 1335000, training loss 0.0297305\n",
      "epoch 14,step 1336500, training loss 0.124345\n",
      "epoch 14,step 1338000, training loss 0.10532\n",
      "epoch 14,step 1339500, training loss 0.0480114\n",
      "epoch 14,step 1341000, training loss 0.0396644\n",
      "epoch 14,step 1342500, training loss 0.0503549\n",
      "epoch 14,step 1344000, training loss 0.0521569\n",
      "epoch 14,step 1345500, training loss 0.047318\n",
      "epoch 14,step 1347000, training loss 0.0427273\n",
      "epoch 14,step 1348500, training loss 0.0411713\n",
      "epoch 14,step 1350000, training loss 0.0793013\n",
      "epoch 14,step 1351500, training loss 0.109019\n",
      "epoch 14,step 1353000, training loss 0.0491212\n",
      "epoch 14,step 1354500, training loss 0.0356\n",
      "epoch 14,step 1356000, training loss 0.0549154\n",
      "epoch 14,step 1357500, training loss 0.0370787\n",
      "epoch 14,step 1359000, training loss 0.0356527\n",
      "epoch 14,step 1360500, training loss 0.0300699\n",
      "epoch 14,step 1362000, training loss 0.0324717\n",
      "epoch 14,step 1363500, training loss 0.0349522\n",
      "epoch 14,step 1365000, training loss 0.0345289\n",
      "epoch 14,step 1366500, training loss 0.108585\n",
      "epoch 14,step 1368000, training loss 0.0610658\n",
      "epoch 14,step 1369500, training loss 0.0409394\n",
      "epoch 14,step 1371000, training loss 0.0449282\n",
      "epoch 14,step 1372500, training loss 0.0443473\n",
      "epoch 14,step 1374000, training loss 0.0415246\n",
      "epoch 14,step 1375500, training loss 0.0381112\n",
      "epoch 14,step 1377000, training loss 0.0459199\n",
      "epoch 14,step 1378500, training loss 0.0475344\n",
      "epoch 14,step 1380000, training loss 0.038944\n",
      "epoch 14,step 1381500, training loss 0.10024\n",
      "epoch 14,step 1383000, training loss 0.0896461\n",
      "epoch 14,step 1384500, training loss 0.0401035\n",
      "epoch 14,step 1386000, training loss 0.0345201\n",
      "epoch 14,step 1387500, training loss 0.0293735\n",
      "epoch 14,step 1389000, training loss 0.0403168\n",
      "epoch 14,step 1390500, training loss 0.0351515\n",
      "epoch 14,step 1392000, training loss 0.0290565\n",
      "epoch 14,step 1393500, training loss 0.0336072\n",
      "epoch 14,step 1395000, training loss 0.0306602\n",
      "epoch 14,step 1396500, training loss 0.116036\n",
      "epoch 14,step 1398000, training loss 0.0583798\n",
      "epoch 14,step 1399500, training loss 0.0349248\n",
      "epoch 14,step 1401000, training loss 0.0424546\n",
      "epoch 14,step 1402500, training loss 0.0552441\n",
      "epoch 14,step 1404000, training loss 0.0354417\n",
      "epoch 14,step 1405500, training loss 0.0312718\n",
      "epoch 14,step 1407000, training loss 0.0459049\n",
      "epoch 14,step 1408500, training loss 0.0347667\n",
      "epoch 14,step 1410000, training loss 0.0308305\n",
      "epoch 14,step 1411500, training loss 0.125274\n",
      "epoch 14,step 1413000, training loss 0.0745212\n",
      "epoch 14,step 1414500, training loss 0.0332792\n",
      "epoch 14,step 1416000, training loss 0.0452336\n",
      "epoch 14,step 1417500, training loss 0.0386502\n",
      "epoch 14,step 1419000, training loss 0.042537\n",
      "epoch 14,step 1420500, training loss 0.0469068\n",
      "epoch 14,step 1422000, training loss 0.0297001\n",
      "epoch 14,step 1423500, training loss 0.0385132\n",
      "epoch 14,step 1425000, training loss 0.0270121\n",
      "epoch 14,step 1426500, training loss 0.116681\n",
      "epoch 14,step 1428000, training loss 0.0979909\n",
      "epoch 14,step 1429500, training loss 0.038235\n",
      "epoch 14,step 1431000, training loss 0.0321615\n",
      "epoch 14,step 1432500, training loss 0.0308329\n",
      "epoch 14,step 1434000, training loss 0.0419677\n",
      "epoch 14,step 1435500, training loss 0.0453364\n",
      "epoch 14,step 1437000, training loss 0.0522899\n",
      "epoch 14,step 1438500, training loss 0.0396021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14,step 1440000, training loss 0.0418782\n",
      "epoch 14,step 1441500, training loss 0.119857\n",
      "epoch 14,step 1443000, training loss 0.0703433\n",
      "epoch 14,step 1444500, training loss 0.0273472\n",
      "epoch 14,step 1446000, training loss 0.0285405\n",
      "epoch 14,step 1447500, training loss 0.031536\n",
      "epoch 14,step 1449000, training loss 0.0347942\n",
      "epoch 14,step 1450500, training loss 0.0340491\n",
      "epoch 14,step 1452000, training loss 0.033722\n",
      "epoch 14,step 1453500, training loss 0.0347248\n",
      "epoch 14,step 1455000, training loss 0.0393017\n",
      "epoch 14,step 1456500, training loss 0.133028\n",
      "epoch 14,step 1458000, training loss 0.064274\n",
      "epoch 14,step 1459500, training loss 0.0345628\n",
      "epoch 14,step 1461000, training loss 0.0381644\n",
      "epoch 14,step 1462500, training loss 0.0502197\n",
      "epoch 14,step 1464000, training loss 0.0340108\n",
      "epoch 14,step 1465500, training loss 0.0337718\n",
      "epoch 14,step 1467000, training loss 0.0346441\n",
      "epoch 14,step 1468500, training loss 0.0332484\n",
      "epoch 14,step 1470000, training loss 0.0370686\n",
      "epoch 14,step 1471500, training loss 0.10986\n",
      "epoch 14,step 1473000, training loss 0.0681917\n",
      "epoch 14,step 1474500, training loss 0.0492743\n",
      "epoch 14,step 1476000, training loss 0.0386318\n",
      "epoch 14,step 1477500, training loss 0.0295494\n",
      "epoch 14,step 1479000, training loss 0.069257\n",
      "epoch 14,step 1480500, training loss 0.0340453\n",
      "epoch 14,step 1482000, training loss 0.04163\n",
      "epoch 14,step 1483500, training loss 0.0323385\n",
      "epoch 14,step 1485000, training loss 0.0292731\n",
      "epoch 14,step 1486500, training loss 0.115547\n",
      "epoch 14,step 1488000, training loss 0.0516304\n",
      "epoch 14,step 1489500, training loss 0.0370246\n",
      "epoch 14,step 1491000, training loss 0.0259874\n",
      "epoch 14,step 1492500, training loss 0.0280229\n",
      "epoch 14,step 1494000, training loss 0.0342171\n",
      "epoch 14,step 1495500, training loss 0.0237782\n",
      "epoch 14,step 1497000, training loss 0.0331061\n",
      "epoch 14,step 1498500, training loss 0.0490653\n",
      "epoch 14,training loss 0.0490653 ,test loss 0.0518624\n",
      "epoch 15,step 8000, training loss 0.0449445\n",
      "epoch 15,step 16000, training loss 0.0543581\n",
      "epoch 15,step 24000, training loss 0.0475422\n",
      "epoch 15,step 32000, training loss 0.0300107\n",
      "epoch 15,step 40000, training loss 0.0294804\n",
      "epoch 15,step 48000, training loss 0.0302577\n",
      "epoch 15,step 56000, training loss 0.0341103\n",
      "epoch 15,step 64000, training loss 0.0316392\n",
      "epoch 15,step 72000, training loss 0.0374498\n",
      "epoch 15,step 80000, training loss 0.0344565\n",
      "epoch 15,step 88000, training loss 0.0325214\n",
      "epoch 15,step 96000, training loss 0.043012\n",
      "epoch 15,step 104000, training loss 0.0719922\n",
      "epoch 15,step 112000, training loss 0.0364433\n",
      "epoch 15,step 120000, training loss 0.0509828\n",
      "epoch 15,step 128000, training loss 0.0369229\n",
      "epoch 15,step 136000, training loss 0.033495\n",
      "epoch 15,step 144000, training loss 0.0295358\n",
      "epoch 15,step 152000, training loss 0.0247516\n",
      "epoch 15,step 160000, training loss 0.04059\n",
      "epoch 15,step 168000, training loss 0.0325218\n",
      "epoch 15,step 176000, training loss 0.0381005\n",
      "epoch 15,step 184000, training loss 0.0535383\n",
      "epoch 15,step 192000, training loss 0.0443179\n",
      "epoch 15,step 200000, training loss 0.0418251\n",
      "epoch 15,step 208000, training loss 0.0251759\n",
      "epoch 15,step 216000, training loss 0.0603135\n",
      "epoch 15,step 224000, training loss 0.0405543\n",
      "epoch 15,step 232000, training loss 0.0477234\n",
      "epoch 15,step 240000, training loss 0.0377122\n",
      "epoch 15,step 248000, training loss 0.0563533\n",
      "epoch 15,step 256000, training loss 0.0375527\n",
      "epoch 15,step 264000, training loss 0.0401285\n",
      "epoch 15,step 272000, training loss 0.0363759\n",
      "epoch 15,step 280000, training loss 0.0503518\n",
      "epoch 15,step 288000, training loss 0.0321816\n",
      "epoch 15,step 296000, training loss 0.0401913\n",
      "epoch 15,step 304000, training loss 0.0423303\n",
      "epoch 15,step 312000, training loss 0.0390339\n",
      "epoch 15,step 320000, training loss 0.0340279\n",
      "epoch 15,step 328000, training loss 0.0421431\n",
      "epoch 15,step 336000, training loss 0.034079\n",
      "epoch 15,step 344000, training loss 0.0361465\n",
      "epoch 15,step 352000, training loss 0.0385687\n",
      "epoch 15,step 360000, training loss 0.0333115\n",
      "epoch 15,step 368000, training loss 0.042002\n",
      "epoch 15,step 376000, training loss 0.0260728\n",
      "epoch 15,step 384000, training loss 0.0340712\n",
      "epoch 15,step 392000, training loss 0.0377416\n",
      "epoch 15,step 400000, training loss 0.0354833\n",
      "epoch 15,step 408000, training loss 0.036373\n",
      "epoch 15,step 416000, training loss 0.0436861\n",
      "epoch 15,step 424000, training loss 0.0276979\n",
      "epoch 15,step 432000, training loss 0.0377092\n",
      "epoch 15,step 440000, training loss 0.0595011\n",
      "epoch 15,step 448000, training loss 0.0318988\n",
      "epoch 15,step 456000, training loss 0.0319639\n",
      "epoch 15,step 464000, training loss 0.0420663\n",
      "epoch 15,step 472000, training loss 0.0428813\n",
      "epoch 15,step 480000, training loss 0.036268\n",
      "epoch 15,step 488000, training loss 0.0340993\n",
      "epoch 15,step 496000, training loss 0.0344428\n",
      "epoch 15,step 504000, training loss 0.0353789\n",
      "epoch 15,step 512000, training loss 0.0494417\n",
      "epoch 15,step 520000, training loss 0.0544429\n",
      "epoch 15,step 528000, training loss 0.0380588\n",
      "epoch 15,step 536000, training loss 0.0364321\n",
      "epoch 15,step 544000, training loss 0.0513451\n",
      "epoch 15,step 552000, training loss 0.0273186\n",
      "epoch 15,step 560000, training loss 0.0340243\n",
      "epoch 15,step 568000, training loss 0.0480399\n",
      "epoch 15,step 576000, training loss 0.0296422\n",
      "epoch 15,step 584000, training loss 0.0590263\n",
      "epoch 15,step 592000, training loss 0.0495119\n",
      "epoch 15,step 600000, training loss 0.0408135\n",
      "epoch 15,step 608000, training loss 0.0388496\n",
      "epoch 15,step 616000, training loss 0.0359173\n",
      "epoch 15,step 624000, training loss 0.0353856\n",
      "epoch 15,step 632000, training loss 0.0396728\n",
      "epoch 15,step 640000, training loss 0.0269947\n",
      "epoch 15,step 648000, training loss 0.0318305\n",
      "epoch 15,step 656000, training loss 0.0343889\n",
      "epoch 15,step 664000, training loss 0.0340798\n",
      "epoch 15,step 672000, training loss 0.0347145\n",
      "epoch 15,step 680000, training loss 0.048787\n",
      "epoch 15,step 688000, training loss 0.0392584\n",
      "epoch 15,step 696000, training loss 0.0458878\n",
      "epoch 15,step 704000, training loss 0.0407026\n",
      "epoch 15,step 712000, training loss 0.0253609\n",
      "epoch 15,step 720000, training loss 0.0341712\n",
      "epoch 15,step 728000, training loss 0.0318048\n",
      "epoch 15,step 736000, training loss 0.0380248\n",
      "epoch 15,step 744000, training loss 0.0490957\n",
      "epoch 15,step 752000, training loss 0.0387735\n",
      "epoch 15,step 760000, training loss 0.0318384\n",
      "epoch 15,step 768000, training loss 0.0386931\n",
      "epoch 15,step 776000, training loss 0.0357589\n",
      "epoch 15,step 784000, training loss 0.0418743\n",
      "epoch 15,step 792000, training loss 0.0417589\n",
      "epoch 15,step 800000, training loss 0.0345237\n",
      "epoch 15,step 808000, training loss 0.0353638\n",
      "epoch 15,step 816000, training loss 0.0319895\n",
      "epoch 15,step 824000, training loss 0.0414716\n",
      "epoch 15,step 832000, training loss 0.03207\n",
      "epoch 15,step 840000, training loss 0.0423483\n",
      "epoch 15,step 848000, training loss 0.0525346\n",
      "epoch 15,step 856000, training loss 0.0464497\n",
      "epoch 15,step 864000, training loss 0.0420766\n",
      "epoch 15,step 872000, training loss 0.0463161\n",
      "epoch 15,step 880000, training loss 0.0575081\n",
      "epoch 15,step 888000, training loss 0.0474805\n",
      "epoch 15,step 896000, training loss 0.050806\n",
      "epoch 15,step 904000, training loss 0.03677\n",
      "epoch 15,step 912000, training loss 0.0326472\n",
      "epoch 15,step 920000, training loss 0.0602269\n",
      "epoch 15,step 928000, training loss 0.0325244\n",
      "epoch 15,step 936000, training loss 0.0367536\n",
      "epoch 15,step 944000, training loss 0.0300436\n",
      "epoch 15,step 952000, training loss 0.0365886\n",
      "epoch 15,step 960000, training loss 0.0465804\n",
      "epoch 15,step 968000, training loss 0.0312685\n",
      "epoch 15,step 976000, training loss 0.0308115\n",
      "epoch 15,step 984000, training loss 0.0334253\n",
      "epoch 15,step 992000, training loss 0.045104\n",
      "epoch 15,step 1000000, training loss 0.047552\n",
      "epoch 15,step 1008000, training loss 0.0413608\n",
      "epoch 15,step 1016000, training loss 0.0315442\n",
      "epoch 15,step 1024000, training loss 0.0423448\n",
      "epoch 15,step 1032000, training loss 0.0466938\n",
      "epoch 15,step 1040000, training loss 0.041272\n",
      "epoch 15,step 1048000, training loss 0.037659\n",
      "epoch 15,step 1056000, training loss 0.0364962\n",
      "epoch 15,step 1064000, training loss 0.0308786\n",
      "epoch 15,step 1072000, training loss 0.0386999\n",
      "epoch 15,step 1080000, training loss 0.031177\n",
      "epoch 15,step 1088000, training loss 0.0390361\n",
      "epoch 15,step 1096000, training loss 0.03794\n",
      "epoch 15,step 1104000, training loss 0.0384401\n",
      "epoch 15,step 1112000, training loss 0.0411175\n",
      "epoch 15,step 1120000, training loss 0.0309526\n",
      "epoch 15,step 1128000, training loss 0.0393133\n",
      "epoch 15,step 1136000, training loss 0.0372166\n",
      "epoch 15,step 1144000, training loss 0.0396484\n",
      "epoch 15,step 1152000, training loss 0.0377944\n",
      "epoch 15,step 1160000, training loss 0.0419412\n",
      "epoch 15,step 1168000, training loss 0.0379857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15,step 1176000, training loss 0.0376146\n",
      "epoch 15,step 1184000, training loss 0.0510242\n",
      "epoch 15,step 1192000, training loss 0.0325502\n",
      "epoch 15,step 1200000, training loss 0.080808\n",
      "epoch 15,step 1208000, training loss 0.0326652\n",
      "epoch 15,step 1216000, training loss 0.031319\n",
      "epoch 15,step 1224000, training loss 0.0492141\n",
      "epoch 15,step 1232000, training loss 0.0259716\n",
      "epoch 15,step 1240000, training loss 0.0385902\n",
      "epoch 15,step 1248000, training loss 0.0294018\n",
      "epoch 15,step 1256000, training loss 0.0375613\n",
      "epoch 15,step 1264000, training loss 0.0393046\n",
      "epoch 15,step 1272000, training loss 0.0352832\n",
      "epoch 15,step 1280000, training loss 0.0332009\n",
      "epoch 15,step 1288000, training loss 0.0408406\n",
      "epoch 15,step 1296000, training loss 0.0317786\n",
      "epoch 15,step 1304000, training loss 0.0441765\n",
      "epoch 15,step 1312000, training loss 0.0397343\n",
      "epoch 15,step 1320000, training loss 0.0545062\n",
      "epoch 15,step 1328000, training loss 0.0404829\n",
      "epoch 15,step 1336000, training loss 0.0347541\n",
      "epoch 15,step 1344000, training loss 0.0331492\n",
      "epoch 15,step 1352000, training loss 0.0461724\n",
      "epoch 15,step 1360000, training loss 0.0406106\n",
      "epoch 15,step 1368000, training loss 0.0464692\n",
      "epoch 15,step 1376000, training loss 0.0539898\n",
      "epoch 15,step 1384000, training loss 0.0326118\n",
      "epoch 15,step 1392000, training loss 0.0423243\n",
      "epoch 15,step 1400000, training loss 0.0360378\n",
      "epoch 15,step 1408000, training loss 0.035366\n",
      "epoch 15,step 1416000, training loss 0.034875\n",
      "epoch 15,step 1424000, training loss 0.0289529\n",
      "epoch 15,step 1432000, training loss 0.0477245\n",
      "epoch 15,step 1440000, training loss 0.0761568\n",
      "epoch 15,step 1448000, training loss 0.0361804\n",
      "epoch 15,step 1456000, training loss 0.0333732\n",
      "epoch 15,step 1464000, training loss 0.0441743\n",
      "epoch 15,step 1472000, training loss 0.0366892\n",
      "epoch 15,step 1480000, training loss 0.029395\n",
      "epoch 15,step 1488000, training loss 0.0282939\n",
      "epoch 15,step 1496000, training loss 0.0515314\n",
      "epoch 15,step 1504000, training loss 0.0293626\n",
      "epoch 15,step 1512000, training loss 0.0379004\n",
      "epoch 15,step 1520000, training loss 0.0264716\n",
      "epoch 15,step 1528000, training loss 0.0281497\n",
      "epoch 15,step 1536000, training loss 0.0403076\n",
      "epoch 15,step 1544000, training loss 0.029993\n",
      "epoch 15,step 1552000, training loss 0.0366707\n",
      "epoch 15,step 1560000, training loss 0.0439407\n",
      "epoch 15,step 1568000, training loss 0.0361626\n",
      "epoch 15,step 1576000, training loss 0.0284863\n",
      "epoch 15,step 1584000, training loss 0.0274419\n",
      "epoch 15,step 1592000, training loss 0.0273281\n",
      "epoch 15,training loss 0.0487554 ,test loss 0.0497267\n",
      "epoch 16,step 8500, training loss 0.0449796\n",
      "epoch 16,step 17000, training loss 0.0527151\n",
      "epoch 16,step 25500, training loss 0.0467841\n",
      "epoch 16,step 34000, training loss 0.029132\n",
      "epoch 16,step 42500, training loss 0.0297631\n",
      "epoch 16,step 51000, training loss 0.0299839\n",
      "epoch 16,step 59500, training loss 0.0338603\n",
      "epoch 16,step 68000, training loss 0.0302563\n",
      "epoch 16,step 76500, training loss 0.0360091\n",
      "epoch 16,step 85000, training loss 0.033129\n",
      "epoch 16,step 93500, training loss 0.0321062\n",
      "epoch 16,step 102000, training loss 0.0412454\n",
      "epoch 16,step 110500, training loss 0.0714628\n",
      "epoch 16,step 119000, training loss 0.0358213\n",
      "epoch 16,step 127500, training loss 0.0499909\n",
      "epoch 16,step 136000, training loss 0.0349098\n",
      "epoch 16,step 144500, training loss 0.0341117\n",
      "epoch 16,step 153000, training loss 0.0283678\n",
      "epoch 16,step 161500, training loss 0.0249851\n",
      "epoch 16,step 170000, training loss 0.0395509\n",
      "epoch 16,step 178500, training loss 0.0306841\n",
      "epoch 16,step 187000, training loss 0.0375652\n",
      "epoch 16,step 195500, training loss 0.051264\n",
      "epoch 16,step 204000, training loss 0.0411762\n",
      "epoch 16,step 212500, training loss 0.0404868\n",
      "epoch 16,step 221000, training loss 0.0246463\n",
      "epoch 16,step 229500, training loss 0.0591844\n",
      "epoch 16,step 238000, training loss 0.036672\n",
      "epoch 16,step 246500, training loss 0.0431198\n",
      "epoch 16,step 255000, training loss 0.0362602\n",
      "epoch 16,step 263500, training loss 0.0565439\n",
      "epoch 16,step 272000, training loss 0.0350533\n",
      "epoch 16,step 280500, training loss 0.0383851\n",
      "epoch 16,step 289000, training loss 0.0343107\n",
      "epoch 16,step 297500, training loss 0.0466448\n",
      "epoch 16,step 306000, training loss 0.0323131\n",
      "epoch 16,step 314500, training loss 0.0362317\n",
      "epoch 16,step 323000, training loss 0.0407146\n",
      "epoch 16,step 331500, training loss 0.0362811\n",
      "epoch 16,step 340000, training loss 0.032025\n",
      "epoch 16,step 348500, training loss 0.0394622\n",
      "epoch 16,step 357000, training loss 0.0329473\n",
      "epoch 16,step 365500, training loss 0.0335924\n",
      "epoch 16,step 374000, training loss 0.0367054\n",
      "epoch 16,step 382500, training loss 0.0306136\n",
      "epoch 16,step 391000, training loss 0.0393973\n",
      "epoch 16,step 399500, training loss 0.0252509\n",
      "epoch 16,step 408000, training loss 0.0336492\n",
      "epoch 16,step 416500, training loss 0.0369425\n",
      "epoch 16,step 425000, training loss 0.0361247\n",
      "epoch 16,step 433500, training loss 0.0343902\n",
      "epoch 16,step 442000, training loss 0.0415223\n",
      "epoch 16,step 450500, training loss 0.0278976\n",
      "epoch 16,step 459000, training loss 0.0363704\n",
      "epoch 16,step 467500, training loss 0.0606005\n",
      "epoch 16,step 476000, training loss 0.0307545\n",
      "epoch 16,step 484500, training loss 0.0303346\n",
      "epoch 16,step 493000, training loss 0.0410112\n",
      "epoch 16,step 501500, training loss 0.0428004\n",
      "epoch 16,step 510000, training loss 0.035203\n",
      "epoch 16,step 518500, training loss 0.0339852\n",
      "epoch 16,step 527000, training loss 0.0347619\n",
      "epoch 16,step 535500, training loss 0.0352542\n",
      "epoch 16,step 544000, training loss 0.0505175\n",
      "epoch 16,step 552500, training loss 0.0525299\n",
      "epoch 16,step 561000, training loss 0.036661\n",
      "epoch 16,step 569500, training loss 0.0350347\n",
      "epoch 16,step 578000, training loss 0.0484906\n",
      "epoch 16,step 586500, training loss 0.0284144\n",
      "epoch 16,step 595000, training loss 0.0340868\n",
      "epoch 16,step 603500, training loss 0.0437897\n",
      "epoch 16,step 612000, training loss 0.0261477\n",
      "epoch 16,step 620500, training loss 0.0573216\n",
      "epoch 16,step 629000, training loss 0.0483582\n",
      "epoch 16,step 637500, training loss 0.0384574\n",
      "epoch 16,step 646000, training loss 0.036357\n",
      "epoch 16,step 654500, training loss 0.0353247\n",
      "epoch 16,step 663000, training loss 0.0352902\n",
      "epoch 16,step 671500, training loss 0.036629\n",
      "epoch 16,step 680000, training loss 0.0252101\n",
      "epoch 16,step 688500, training loss 0.0343707\n",
      "epoch 16,step 697000, training loss 0.0352117\n",
      "epoch 16,step 705500, training loss 0.0326233\n",
      "epoch 16,step 714000, training loss 0.0311186\n",
      "epoch 16,step 722500, training loss 0.0461093\n",
      "epoch 16,step 731000, training loss 0.0381607\n",
      "epoch 16,step 739500, training loss 0.0450448\n",
      "epoch 16,step 748000, training loss 0.037807\n",
      "epoch 16,step 756500, training loss 0.0240031\n",
      "epoch 16,step 765000, training loss 0.0328304\n",
      "epoch 16,step 773500, training loss 0.0304042\n",
      "epoch 16,step 782000, training loss 0.0356814\n",
      "epoch 16,step 790500, training loss 0.0492427\n",
      "epoch 16,step 799000, training loss 0.0371684\n",
      "epoch 16,step 807500, training loss 0.030514\n",
      "epoch 16,step 816000, training loss 0.0382968\n",
      "epoch 16,step 824500, training loss 0.0345576\n",
      "epoch 16,step 833000, training loss 0.0392907\n",
      "epoch 16,step 841500, training loss 0.0397492\n",
      "epoch 16,step 850000, training loss 0.0334688\n",
      "epoch 16,step 858500, training loss 0.0347454\n",
      "epoch 16,step 867000, training loss 0.0301994\n",
      "epoch 16,step 875500, training loss 0.0428838\n",
      "epoch 16,step 884000, training loss 0.0305084\n",
      "epoch 16,step 892500, training loss 0.0419968\n",
      "epoch 16,step 901000, training loss 0.051643\n",
      "epoch 16,step 909500, training loss 0.0461961\n",
      "epoch 16,step 918000, training loss 0.0409499\n",
      "epoch 16,step 926500, training loss 0.0408412\n",
      "epoch 16,step 935000, training loss 0.0534621\n",
      "epoch 16,step 943500, training loss 0.045875\n",
      "epoch 16,step 952000, training loss 0.0488008\n",
      "epoch 16,step 960500, training loss 0.0365276\n",
      "epoch 16,step 969000, training loss 0.0317219\n",
      "epoch 16,step 977500, training loss 0.0611295\n",
      "epoch 16,step 986000, training loss 0.029365\n",
      "epoch 16,step 994500, training loss 0.0357903\n",
      "epoch 16,step 1003000, training loss 0.0296569\n",
      "epoch 16,step 1011500, training loss 0.0354659\n",
      "epoch 16,step 1020000, training loss 0.0453868\n",
      "epoch 16,step 1028500, training loss 0.0314738\n",
      "epoch 16,step 1037000, training loss 0.0301122\n",
      "epoch 16,step 1045500, training loss 0.0313892\n",
      "epoch 16,step 1054000, training loss 0.0430927\n",
      "epoch 16,step 1062500, training loss 0.0434737\n",
      "epoch 16,step 1071000, training loss 0.0398833\n",
      "epoch 16,step 1079500, training loss 0.0308616\n",
      "epoch 16,step 1088000, training loss 0.0409234\n",
      "epoch 16,step 1096500, training loss 0.043689\n",
      "epoch 16,step 1105000, training loss 0.0401653\n",
      "epoch 16,step 1113500, training loss 0.0348592\n",
      "epoch 16,step 1122000, training loss 0.0353629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16,step 1130500, training loss 0.0293234\n",
      "epoch 16,step 1139000, training loss 0.0363854\n",
      "epoch 16,step 1147500, training loss 0.0305363\n",
      "epoch 16,step 1156000, training loss 0.0366555\n",
      "epoch 16,step 1164500, training loss 0.0363249\n",
      "epoch 16,step 1173000, training loss 0.0384561\n",
      "epoch 16,step 1181500, training loss 0.0401149\n",
      "epoch 16,step 1190000, training loss 0.0311924\n",
      "epoch 16,step 1198500, training loss 0.039261\n",
      "epoch 16,step 1207000, training loss 0.036336\n",
      "epoch 16,step 1215500, training loss 0.0397675\n",
      "epoch 16,step 1224000, training loss 0.0369565\n",
      "epoch 16,step 1232500, training loss 0.0412338\n",
      "epoch 16,step 1241000, training loss 0.0374919\n",
      "epoch 16,step 1249500, training loss 0.0371396\n",
      "epoch 16,step 1258000, training loss 0.0504919\n",
      "epoch 16,step 1266500, training loss 0.0331114\n",
      "epoch 16,step 1275000, training loss 0.0807982\n",
      "epoch 16,step 1283500, training loss 0.0333634\n",
      "epoch 16,step 1292000, training loss 0.031339\n",
      "epoch 16,step 1300500, training loss 0.0484933\n",
      "epoch 16,step 1309000, training loss 0.0269631\n",
      "epoch 16,step 1317500, training loss 0.0374087\n",
      "epoch 16,step 1326000, training loss 0.0295054\n",
      "epoch 16,step 1334500, training loss 0.0377506\n",
      "epoch 16,step 1343000, training loss 0.0386977\n",
      "epoch 16,step 1351500, training loss 0.0349944\n",
      "epoch 16,step 1360000, training loss 0.0320723\n",
      "epoch 16,step 1368500, training loss 0.0396204\n",
      "epoch 16,step 1377000, training loss 0.0323394\n",
      "epoch 16,step 1385500, training loss 0.0427926\n",
      "epoch 16,step 1394000, training loss 0.0386804\n",
      "epoch 16,step 1402500, training loss 0.0543935\n",
      "epoch 16,step 1411000, training loss 0.0387981\n",
      "epoch 16,step 1419500, training loss 0.0348796\n",
      "epoch 16,step 1428000, training loss 0.0333642\n",
      "epoch 16,step 1436500, training loss 0.0451917\n",
      "epoch 16,step 1445000, training loss 0.0396393\n",
      "epoch 16,step 1453500, training loss 0.0445482\n",
      "epoch 16,step 1462000, training loss 0.051043\n",
      "epoch 16,step 1470500, training loss 0.0303612\n",
      "epoch 16,step 1479000, training loss 0.0405547\n",
      "epoch 16,step 1487500, training loss 0.0332069\n",
      "epoch 16,step 1496000, training loss 0.033617\n",
      "epoch 16,step 1504500, training loss 0.0341384\n",
      "epoch 16,step 1513000, training loss 0.0284027\n",
      "epoch 16,step 1521500, training loss 0.0481732\n",
      "epoch 16,step 1530000, training loss 0.0753603\n",
      "epoch 16,step 1538500, training loss 0.0344869\n",
      "epoch 16,step 1547000, training loss 0.0315767\n",
      "epoch 16,step 1555500, training loss 0.0432748\n",
      "epoch 16,step 1564000, training loss 0.0356359\n",
      "epoch 16,step 1572500, training loss 0.0281963\n",
      "epoch 16,step 1581000, training loss 0.0273716\n",
      "epoch 16,step 1589500, training loss 0.0503144\n",
      "epoch 16,step 1598000, training loss 0.0291316\n",
      "epoch 16,step 1606500, training loss 0.0363796\n",
      "epoch 16,step 1615000, training loss 0.0256758\n",
      "epoch 16,step 1623500, training loss 0.0288553\n",
      "epoch 16,step 1632000, training loss 0.0397756\n",
      "epoch 16,step 1640500, training loss 0.0285467\n",
      "epoch 16,step 1649000, training loss 0.0364732\n",
      "epoch 16,step 1657500, training loss 0.0441007\n",
      "epoch 16,step 1666000, training loss 0.0364163\n",
      "epoch 16,step 1674500, training loss 0.027546\n",
      "epoch 16,step 1683000, training loss 0.0259762\n",
      "epoch 16,step 1691500, training loss 0.0262633\n",
      "epoch 16,training loss 0.0475774 ,test loss 0.0481978\n",
      "epoch 17,step 9000, training loss 0.0420113\n",
      "epoch 17,step 18000, training loss 0.0502722\n",
      "epoch 17,step 27000, training loss 0.0464359\n",
      "epoch 17,step 36000, training loss 0.029515\n",
      "epoch 17,step 45000, training loss 0.028668\n",
      "epoch 17,step 54000, training loss 0.0284013\n",
      "epoch 17,step 63000, training loss 0.0329034\n",
      "epoch 17,step 72000, training loss 0.0294456\n",
      "epoch 17,step 81000, training loss 0.0346393\n",
      "epoch 17,step 90000, training loss 0.0312796\n",
      "epoch 17,step 99000, training loss 0.0311012\n",
      "epoch 17,step 108000, training loss 0.0413699\n",
      "epoch 17,step 117000, training loss 0.068937\n",
      "epoch 17,step 126000, training loss 0.0354676\n",
      "epoch 17,step 135000, training loss 0.0477385\n",
      "epoch 17,step 144000, training loss 0.0347929\n",
      "epoch 17,step 153000, training loss 0.0326279\n",
      "epoch 17,step 162000, training loss 0.0277324\n",
      "epoch 17,step 171000, training loss 0.0237945\n",
      "epoch 17,step 180000, training loss 0.0396284\n",
      "epoch 17,step 189000, training loss 0.0300723\n",
      "epoch 17,step 198000, training loss 0.0369895\n",
      "epoch 17,step 207000, training loss 0.0506677\n",
      "epoch 17,step 216000, training loss 0.0424825\n",
      "epoch 17,step 225000, training loss 0.0390148\n",
      "epoch 17,step 234000, training loss 0.0232801\n",
      "epoch 17,step 243000, training loss 0.0588375\n",
      "epoch 17,step 252000, training loss 0.0366876\n",
      "epoch 17,step 261000, training loss 0.0432907\n",
      "epoch 17,step 270000, training loss 0.0353558\n",
      "epoch 17,step 279000, training loss 0.060676\n",
      "epoch 17,step 288000, training loss 0.038298\n",
      "epoch 17,step 297000, training loss 0.0382348\n",
      "epoch 17,step 306000, training loss 0.0355777\n",
      "epoch 17,step 315000, training loss 0.0477361\n",
      "epoch 17,step 324000, training loss 0.0310969\n",
      "epoch 17,step 333000, training loss 0.0355808\n",
      "epoch 17,step 342000, training loss 0.0394086\n",
      "epoch 17,step 351000, training loss 0.0343796\n",
      "epoch 17,step 360000, training loss 0.0307466\n",
      "epoch 17,step 369000, training loss 0.037703\n",
      "epoch 17,step 378000, training loss 0.0304918\n",
      "epoch 17,step 387000, training loss 0.0324527\n",
      "epoch 17,step 396000, training loss 0.0359553\n",
      "epoch 17,step 405000, training loss 0.0308222\n",
      "epoch 17,step 414000, training loss 0.0397253\n",
      "epoch 17,step 423000, training loss 0.0254623\n",
      "epoch 17,step 432000, training loss 0.0329944\n",
      "epoch 17,step 441000, training loss 0.0369028\n",
      "epoch 17,step 450000, training loss 0.0335697\n",
      "epoch 17,step 459000, training loss 0.0326038\n",
      "epoch 17,step 468000, training loss 0.0405976\n",
      "epoch 17,step 477000, training loss 0.0262281\n",
      "epoch 17,step 486000, training loss 0.0351143\n",
      "epoch 17,step 495000, training loss 0.0582779\n",
      "epoch 17,step 504000, training loss 0.0293388\n",
      "epoch 17,step 513000, training loss 0.0282011\n",
      "epoch 17,step 522000, training loss 0.0401526\n",
      "epoch 17,step 531000, training loss 0.0413743\n",
      "epoch 17,step 540000, training loss 0.0337302\n",
      "epoch 17,step 549000, training loss 0.0358464\n",
      "epoch 17,step 558000, training loss 0.0361208\n",
      "epoch 17,step 567000, training loss 0.0343906\n",
      "epoch 17,step 576000, training loss 0.0479366\n",
      "epoch 17,step 585000, training loss 0.0520519\n",
      "epoch 17,step 594000, training loss 0.0350887\n",
      "epoch 17,step 603000, training loss 0.0358067\n",
      "epoch 17,step 612000, training loss 0.0499156\n",
      "epoch 17,step 621000, training loss 0.0266068\n",
      "epoch 17,step 630000, training loss 0.0335204\n",
      "epoch 17,step 639000, training loss 0.0430674\n",
      "epoch 17,step 648000, training loss 0.0262271\n",
      "epoch 17,step 657000, training loss 0.0569134\n",
      "epoch 17,step 666000, training loss 0.0480037\n",
      "epoch 17,step 675000, training loss 0.0395766\n",
      "epoch 17,step 684000, training loss 0.0367089\n",
      "epoch 17,step 693000, training loss 0.0356523\n",
      "epoch 17,step 702000, training loss 0.034109\n",
      "epoch 17,step 711000, training loss 0.0364478\n",
      "epoch 17,step 720000, training loss 0.0237937\n",
      "epoch 17,step 729000, training loss 0.0304242\n",
      "epoch 17,step 738000, training loss 0.0324925\n",
      "epoch 17,step 747000, training loss 0.0320947\n",
      "epoch 17,step 756000, training loss 0.0308545\n",
      "epoch 17,step 765000, training loss 0.0449658\n",
      "epoch 17,step 774000, training loss 0.0359361\n",
      "epoch 17,step 783000, training loss 0.0451488\n",
      "epoch 17,step 792000, training loss 0.0366553\n",
      "epoch 17,step 801000, training loss 0.0235727\n",
      "epoch 17,step 810000, training loss 0.0312729\n",
      "epoch 17,step 819000, training loss 0.0291412\n",
      "epoch 17,step 828000, training loss 0.0347005\n",
      "epoch 17,step 837000, training loss 0.047775\n",
      "epoch 17,step 846000, training loss 0.0358538\n",
      "epoch 17,step 855000, training loss 0.027567\n",
      "epoch 17,step 864000, training loss 0.0355628\n",
      "epoch 17,step 873000, training loss 0.0329632\n",
      "epoch 17,step 882000, training loss 0.0380206\n",
      "epoch 17,step 891000, training loss 0.0391918\n",
      "epoch 17,step 900000, training loss 0.0323727\n",
      "epoch 17,step 909000, training loss 0.0335757\n",
      "epoch 17,step 918000, training loss 0.0294282\n",
      "epoch 17,step 927000, training loss 0.0409356\n",
      "epoch 17,step 936000, training loss 0.0287424\n",
      "epoch 17,step 945000, training loss 0.0417861\n",
      "epoch 17,step 954000, training loss 0.0489721\n",
      "epoch 17,step 963000, training loss 0.0448856\n",
      "epoch 17,step 972000, training loss 0.0400316\n",
      "epoch 17,step 981000, training loss 0.0427765\n",
      "epoch 17,step 990000, training loss 0.0548635\n",
      "epoch 17,step 999000, training loss 0.0445413\n",
      "epoch 17,step 1008000, training loss 0.0457583\n",
      "epoch 17,step 1017000, training loss 0.0333796\n",
      "epoch 17,step 1026000, training loss 0.0295508\n",
      "epoch 17,step 1035000, training loss 0.0586277\n",
      "epoch 17,step 1044000, training loss 0.0296211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17,step 1053000, training loss 0.0349497\n",
      "epoch 17,step 1062000, training loss 0.0296847\n",
      "epoch 17,step 1071000, training loss 0.0346547\n",
      "epoch 17,step 1080000, training loss 0.0415553\n",
      "epoch 17,step 1089000, training loss 0.0285674\n",
      "epoch 17,step 1098000, training loss 0.0294857\n",
      "epoch 17,step 1107000, training loss 0.0304558\n",
      "epoch 17,step 1116000, training loss 0.0416302\n",
      "epoch 17,step 1125000, training loss 0.0420567\n",
      "epoch 17,step 1134000, training loss 0.0514569\n",
      "epoch 17,step 1143000, training loss 0.0299427\n",
      "epoch 17,step 1152000, training loss 0.0412656\n",
      "epoch 17,step 1161000, training loss 0.0458054\n",
      "epoch 17,step 1170000, training loss 0.0393053\n",
      "epoch 17,step 1179000, training loss 0.0335801\n",
      "epoch 17,step 1188000, training loss 0.0349447\n",
      "epoch 17,step 1197000, training loss 0.0285494\n",
      "epoch 17,step 1206000, training loss 0.0376302\n",
      "epoch 17,step 1215000, training loss 0.0283862\n",
      "epoch 17,step 1224000, training loss 0.0365118\n",
      "epoch 17,step 1233000, training loss 0.0340899\n",
      "epoch 17,step 1242000, training loss 0.0382726\n",
      "epoch 17,step 1251000, training loss 0.0376874\n",
      "epoch 17,step 1260000, training loss 0.029906\n",
      "epoch 17,step 1269000, training loss 0.0382886\n",
      "epoch 17,step 1278000, training loss 0.033953\n",
      "epoch 17,step 1287000, training loss 0.0389859\n",
      "epoch 17,step 1296000, training loss 0.0347398\n",
      "epoch 17,step 1305000, training loss 0.036803\n",
      "epoch 17,step 1314000, training loss 0.0338051\n",
      "epoch 17,step 1323000, training loss 0.03489\n",
      "epoch 17,step 1332000, training loss 0.047798\n",
      "epoch 17,step 1341000, training loss 0.0306728\n",
      "epoch 17,step 1350000, training loss 0.0761338\n",
      "epoch 17,step 1359000, training loss 0.0295519\n",
      "epoch 17,step 1368000, training loss 0.0284581\n",
      "epoch 17,step 1377000, training loss 0.050143\n",
      "epoch 17,step 1386000, training loss 0.0252621\n",
      "epoch 17,step 1395000, training loss 0.0379769\n",
      "epoch 17,step 1404000, training loss 0.0276663\n",
      "epoch 17,step 1413000, training loss 0.0361172\n",
      "epoch 17,step 1422000, training loss 0.0372688\n",
      "epoch 17,step 1431000, training loss 0.03416\n",
      "epoch 17,step 1440000, training loss 0.0307613\n",
      "epoch 17,step 1449000, training loss 0.0391288\n",
      "epoch 17,step 1458000, training loss 0.0315841\n",
      "epoch 17,step 1467000, training loss 0.0436701\n",
      "epoch 17,step 1476000, training loss 0.0384097\n",
      "epoch 17,step 1485000, training loss 0.0499402\n",
      "epoch 17,step 1494000, training loss 0.0376815\n",
      "epoch 17,step 1503000, training loss 0.0336831\n",
      "epoch 17,step 1512000, training loss 0.0321788\n",
      "epoch 17,step 1521000, training loss 0.0428515\n",
      "epoch 17,step 1530000, training loss 0.0387217\n",
      "epoch 17,step 1539000, training loss 0.0447058\n",
      "epoch 17,step 1548000, training loss 0.0507897\n",
      "epoch 17,step 1557000, training loss 0.0307539\n",
      "epoch 17,step 1566000, training loss 0.0398906\n",
      "epoch 17,step 1575000, training loss 0.0346815\n",
      "epoch 17,step 1584000, training loss 0.0337111\n",
      "epoch 17,step 1593000, training loss 0.0336132\n",
      "epoch 17,step 1602000, training loss 0.0285865\n",
      "epoch 17,step 1611000, training loss 0.0457782\n",
      "epoch 17,step 1620000, training loss 0.0742102\n",
      "epoch 17,step 1629000, training loss 0.0330006\n",
      "epoch 17,step 1638000, training loss 0.0309066\n",
      "epoch 17,step 1647000, training loss 0.0433999\n",
      "epoch 17,step 1656000, training loss 0.0348767\n",
      "epoch 17,step 1665000, training loss 0.0275005\n",
      "epoch 17,step 1674000, training loss 0.027785\n",
      "epoch 17,step 1683000, training loss 0.0483552\n",
      "epoch 17,step 1692000, training loss 0.0275472\n",
      "epoch 17,step 1701000, training loss 0.0348674\n",
      "epoch 17,step 1710000, training loss 0.0248673\n",
      "epoch 17,step 1719000, training loss 0.0278875\n",
      "epoch 17,step 1728000, training loss 0.0381512\n",
      "epoch 17,step 1737000, training loss 0.0276907\n",
      "epoch 17,step 1746000, training loss 0.03517\n",
      "epoch 17,step 1755000, training loss 0.0445789\n",
      "epoch 17,step 1764000, training loss 0.0357293\n",
      "epoch 17,step 1773000, training loss 0.0271502\n",
      "epoch 17,step 1782000, training loss 0.0261562\n",
      "epoch 17,step 1791000, training loss 0.026658\n",
      "epoch 17,training loss 0.0444584 ,test loss 0.0471968\n",
      "epoch 18,step 9500, training loss 0.0418141\n",
      "epoch 18,step 19000, training loss 0.0488423\n",
      "epoch 18,step 28500, training loss 0.0434169\n",
      "epoch 18,step 38000, training loss 0.0286277\n",
      "epoch 18,step 47500, training loss 0.0279542\n",
      "epoch 18,step 57000, training loss 0.0279881\n",
      "epoch 18,step 66500, training loss 0.0336994\n",
      "epoch 18,step 76000, training loss 0.0276102\n",
      "epoch 18,step 85500, training loss 0.0343583\n",
      "epoch 18,step 95000, training loss 0.0320681\n",
      "epoch 18,step 104500, training loss 0.0309218\n",
      "epoch 18,step 114000, training loss 0.0398991\n",
      "epoch 18,step 123500, training loss 0.0644583\n",
      "epoch 18,step 133000, training loss 0.0340255\n",
      "epoch 18,step 142500, training loss 0.0470953\n",
      "epoch 18,step 152000, training loss 0.031884\n",
      "epoch 18,step 161500, training loss 0.0319001\n",
      "epoch 18,step 171000, training loss 0.0266478\n",
      "epoch 18,step 180500, training loss 0.0228108\n",
      "epoch 18,step 190000, training loss 0.0391866\n",
      "epoch 18,step 199500, training loss 0.0295412\n",
      "epoch 18,step 209000, training loss 0.0362467\n",
      "epoch 18,step 218500, training loss 0.0506808\n",
      "epoch 18,step 228000, training loss 0.0411609\n",
      "epoch 18,step 237500, training loss 0.0400994\n",
      "epoch 18,step 247000, training loss 0.0233576\n",
      "epoch 18,step 256500, training loss 0.0559106\n",
      "epoch 18,step 266000, training loss 0.0356282\n",
      "epoch 18,step 275500, training loss 0.0411405\n",
      "epoch 18,step 285000, training loss 0.0338082\n",
      "epoch 18,step 294500, training loss 0.0494108\n",
      "epoch 18,step 304000, training loss 0.0333055\n",
      "epoch 18,step 313500, training loss 0.0362447\n",
      "epoch 18,step 323000, training loss 0.0331936\n",
      "epoch 18,step 332500, training loss 0.0461783\n",
      "epoch 18,step 342000, training loss 0.0308636\n",
      "epoch 18,step 351500, training loss 0.0362291\n",
      "epoch 18,step 361000, training loss 0.0377859\n",
      "epoch 18,step 370500, training loss 0.0333413\n",
      "epoch 18,step 380000, training loss 0.0305394\n",
      "epoch 18,step 389500, training loss 0.03775\n",
      "epoch 18,step 399000, training loss 0.0312604\n",
      "epoch 18,step 408500, training loss 0.0307829\n",
      "epoch 18,step 418000, training loss 0.0359198\n",
      "epoch 18,step 427500, training loss 0.0301699\n",
      "epoch 18,step 437000, training loss 0.0375596\n",
      "epoch 18,step 446500, training loss 0.0240193\n",
      "epoch 18,step 456000, training loss 0.0307971\n",
      "epoch 18,step 465500, training loss 0.0348035\n",
      "epoch 18,step 475000, training loss 0.0326632\n",
      "epoch 18,step 484500, training loss 0.0326115\n",
      "epoch 18,step 494000, training loss 0.0399394\n",
      "epoch 18,step 503500, training loss 0.026286\n",
      "epoch 18,step 513000, training loss 0.0329774\n",
      "epoch 18,step 522500, training loss 0.0576152\n",
      "epoch 18,step 532000, training loss 0.0300089\n",
      "epoch 18,step 541500, training loss 0.0282202\n",
      "epoch 18,step 551000, training loss 0.0383741\n",
      "epoch 18,step 560500, training loss 0.0405501\n",
      "epoch 18,step 570000, training loss 0.0339241\n",
      "epoch 18,step 579500, training loss 0.0361788\n",
      "epoch 18,step 589000, training loss 0.0365324\n",
      "epoch 18,step 598500, training loss 0.0334192\n",
      "epoch 18,step 608000, training loss 0.0461384\n",
      "epoch 18,step 617500, training loss 0.052013\n",
      "epoch 18,step 627000, training loss 0.0342812\n",
      "epoch 18,step 636500, training loss 0.0331904\n",
      "epoch 18,step 646000, training loss 0.0470549\n",
      "epoch 18,step 655500, training loss 0.025758\n",
      "epoch 18,step 665000, training loss 0.0329048\n",
      "epoch 18,step 674500, training loss 0.041807\n",
      "epoch 18,step 684000, training loss 0.0254729\n",
      "epoch 18,step 693500, training loss 0.0550783\n",
      "epoch 18,step 703000, training loss 0.0466169\n",
      "epoch 18,step 712500, training loss 0.039943\n",
      "epoch 18,step 722000, training loss 0.0353774\n",
      "epoch 18,step 731500, training loss 0.0354215\n",
      "epoch 18,step 741000, training loss 0.0331836\n",
      "epoch 18,step 750500, training loss 0.0352013\n",
      "epoch 18,step 760000, training loss 0.0231975\n",
      "epoch 18,step 769500, training loss 0.0318009\n",
      "epoch 18,step 779000, training loss 0.0322501\n",
      "epoch 18,step 788500, training loss 0.0324219\n",
      "epoch 18,step 798000, training loss 0.030918\n",
      "epoch 18,step 807500, training loss 0.0455523\n",
      "epoch 18,step 817000, training loss 0.0364742\n",
      "epoch 18,step 826500, training loss 0.045431\n",
      "epoch 18,step 836000, training loss 0.0356776\n",
      "epoch 18,step 845500, training loss 0.0246473\n",
      "epoch 18,step 855000, training loss 0.0312808\n",
      "epoch 18,step 864500, training loss 0.0291403\n",
      "epoch 18,step 874000, training loss 0.0343172\n",
      "epoch 18,step 883500, training loss 0.0478828\n",
      "epoch 18,step 893000, training loss 0.035261\n",
      "epoch 18,step 902500, training loss 0.0275038\n",
      "epoch 18,step 912000, training loss 0.0353741\n",
      "epoch 18,step 921500, training loss 0.0323381\n",
      "epoch 18,step 931000, training loss 0.0387504\n",
      "epoch 18,step 940500, training loss 0.039421\n",
      "epoch 18,step 950000, training loss 0.0328157\n",
      "epoch 18,step 959500, training loss 0.0334506\n",
      "epoch 18,step 969000, training loss 0.0290531\n",
      "epoch 18,step 978500, training loss 0.0410323\n",
      "epoch 18,step 988000, training loss 0.0284978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18,step 997500, training loss 0.0399322\n",
      "epoch 18,step 1007000, training loss 0.0462511\n",
      "epoch 18,step 1016500, training loss 0.0436442\n",
      "epoch 18,step 1026000, training loss 0.0380043\n",
      "epoch 18,step 1035500, training loss 0.0415625\n",
      "epoch 18,step 1045000, training loss 0.0552818\n",
      "epoch 18,step 1054500, training loss 0.0435135\n",
      "epoch 18,step 1064000, training loss 0.0473425\n",
      "epoch 18,step 1073500, training loss 0.0335184\n",
      "epoch 18,step 1083000, training loss 0.0297007\n",
      "epoch 18,step 1092500, training loss 0.0561206\n",
      "epoch 18,step 1102000, training loss 0.0284545\n",
      "epoch 18,step 1111500, training loss 0.0364109\n",
      "epoch 18,step 1121000, training loss 0.0291076\n",
      "epoch 18,step 1130500, training loss 0.0332106\n",
      "epoch 18,step 1140000, training loss 0.0398057\n",
      "epoch 18,step 1149500, training loss 0.0276029\n",
      "epoch 18,step 1159000, training loss 0.0279535\n",
      "epoch 18,step 1168500, training loss 0.0300776\n",
      "epoch 18,step 1178000, training loss 0.0408445\n",
      "epoch 18,step 1187500, training loss 0.0411883\n",
      "epoch 18,step 1197000, training loss 0.0358981\n",
      "epoch 18,step 1206500, training loss 0.027679\n",
      "epoch 18,step 1216000, training loss 0.0356531\n",
      "epoch 18,step 1225500, training loss 0.0399071\n",
      "epoch 18,step 1235000, training loss 0.0365113\n",
      "epoch 18,step 1244500, training loss 0.0328275\n",
      "epoch 18,step 1254000, training loss 0.0308388\n",
      "epoch 18,step 1263500, training loss 0.0282173\n",
      "epoch 18,step 1273000, training loss 0.035594\n",
      "epoch 18,step 1282500, training loss 0.0277971\n",
      "epoch 18,step 1292000, training loss 0.0361229\n",
      "epoch 18,step 1301500, training loss 0.0330948\n",
      "epoch 18,step 1311000, training loss 0.036265\n",
      "epoch 18,step 1320500, training loss 0.0378562\n",
      "epoch 18,step 1330000, training loss 0.0308222\n",
      "epoch 18,step 1339500, training loss 0.0346863\n",
      "epoch 18,step 1349000, training loss 0.0330049\n",
      "epoch 18,step 1358500, training loss 0.0359716\n",
      "epoch 18,step 1368000, training loss 0.0335435\n",
      "epoch 18,step 1377500, training loss 0.034984\n",
      "epoch 18,step 1387000, training loss 0.0310046\n",
      "epoch 18,step 1396500, training loss 0.032299\n",
      "epoch 18,step 1406000, training loss 0.0449171\n",
      "epoch 18,step 1415500, training loss 0.0282443\n",
      "epoch 18,step 1425000, training loss 0.0721682\n",
      "epoch 18,step 1434500, training loss 0.0289295\n",
      "epoch 18,step 1444000, training loss 0.0277869\n",
      "epoch 18,step 1453500, training loss 0.046691\n",
      "epoch 18,step 1463000, training loss 0.0226943\n",
      "epoch 18,step 1472500, training loss 0.0351848\n",
      "epoch 18,step 1482000, training loss 0.0253872\n",
      "epoch 18,step 1491500, training loss 0.034183\n",
      "epoch 18,step 1501000, training loss 0.0360997\n",
      "epoch 18,step 1510500, training loss 0.0340151\n",
      "epoch 18,step 1520000, training loss 0.0296409\n",
      "epoch 18,step 1529500, training loss 0.0371332\n",
      "epoch 18,step 1539000, training loss 0.0294933\n",
      "epoch 18,step 1548500, training loss 0.0430809\n",
      "epoch 18,step 1558000, training loss 0.0395692\n",
      "epoch 18,step 1567500, training loss 0.0524334\n",
      "epoch 18,step 1577000, training loss 0.0389591\n",
      "epoch 18,step 1586500, training loss 0.033298\n",
      "epoch 18,step 1596000, training loss 0.0319522\n",
      "epoch 18,step 1605500, training loss 0.0444475\n",
      "epoch 18,step 1615000, training loss 0.0379658\n",
      "epoch 18,step 1624500, training loss 0.0427473\n",
      "epoch 18,step 1634000, training loss 0.0488307\n",
      "epoch 18,step 1643500, training loss 0.0304814\n",
      "epoch 18,step 1653000, training loss 0.0386941\n",
      "epoch 18,step 1662500, training loss 0.0327192\n",
      "epoch 18,step 1672000, training loss 0.0322365\n",
      "epoch 18,step 1681500, training loss 0.0316524\n",
      "epoch 18,step 1691000, training loss 0.0263892\n",
      "epoch 18,step 1700500, training loss 0.0437292\n",
      "epoch 18,step 1710000, training loss 0.0729212\n",
      "epoch 18,step 1719500, training loss 0.0323863\n",
      "epoch 18,step 1729000, training loss 0.0289803\n",
      "epoch 18,step 1738500, training loss 0.0413153\n",
      "epoch 18,step 1748000, training loss 0.03304\n",
      "epoch 18,step 1757500, training loss 0.0268211\n",
      "epoch 18,step 1767000, training loss 0.0270735\n",
      "epoch 18,step 1776500, training loss 0.0459164\n",
      "epoch 18,step 1786000, training loss 0.0271956\n",
      "epoch 18,step 1795500, training loss 0.0340679\n",
      "epoch 18,step 1805000, training loss 0.0239249\n",
      "epoch 18,step 1814500, training loss 0.0272779\n",
      "epoch 18,step 1824000, training loss 0.0370216\n",
      "epoch 18,step 1833500, training loss 0.026749\n",
      "epoch 18,step 1843000, training loss 0.0348658\n",
      "epoch 18,step 1852500, training loss 0.0422579\n",
      "epoch 18,step 1862000, training loss 0.0338191\n",
      "epoch 18,step 1871500, training loss 0.0256557\n",
      "epoch 18,step 1881000, training loss 0.0247314\n",
      "epoch 18,step 1890500, training loss 0.0266397\n",
      "epoch 18,training loss 0.0440051 ,test loss 0.0464986\n",
      "epoch 19,step 2000, training loss 0.120217\n",
      "epoch 19,step 4000, training loss 0.078449\n",
      "epoch 19,step 6000, training loss 0.0391724\n",
      "epoch 19,step 8000, training loss 0.0281473\n",
      "epoch 19,step 10000, training loss 0.0405708\n",
      "epoch 19,step 12000, training loss 0.0304033\n",
      "epoch 19,step 14000, training loss 0.027774\n",
      "epoch 19,step 16000, training loss 0.0265347\n",
      "epoch 19,step 18000, training loss 0.0347317\n",
      "epoch 19,step 20000, training loss 0.0491741\n",
      "epoch 19,step 22000, training loss 0.130725\n",
      "epoch 19,step 24000, training loss 0.0768696\n",
      "epoch 19,step 26000, training loss 0.0487766\n",
      "epoch 19,step 28000, training loss 0.0482989\n",
      "epoch 19,step 30000, training loss 0.0420348\n",
      "epoch 19,step 32000, training loss 0.0298227\n",
      "epoch 19,step 34000, training loss 0.0408168\n",
      "epoch 19,step 36000, training loss 0.0320774\n",
      "epoch 19,step 38000, training loss 0.0600622\n",
      "epoch 19,step 40000, training loss 0.0272523\n",
      "epoch 19,step 42000, training loss 0.100996\n",
      "epoch 19,step 44000, training loss 0.0530653\n",
      "epoch 19,step 46000, training loss 0.0472396\n",
      "epoch 19,step 48000, training loss 0.0435935\n",
      "epoch 19,step 50000, training loss 0.0275379\n",
      "epoch 19,step 52000, training loss 0.0367151\n",
      "epoch 19,step 54000, training loss 0.0293906\n",
      "epoch 19,step 56000, training loss 0.0292302\n",
      "epoch 19,step 58000, training loss 0.0343351\n",
      "epoch 19,step 60000, training loss 0.02786\n",
      "epoch 19,step 62000, training loss 0.0904375\n",
      "epoch 19,step 64000, training loss 0.0483332\n",
      "epoch 19,step 66000, training loss 0.0338401\n",
      "epoch 19,step 68000, training loss 0.0317011\n",
      "epoch 19,step 70000, training loss 0.0323459\n",
      "epoch 19,step 72000, training loss 0.0521663\n",
      "epoch 19,step 74000, training loss 0.0320987\n",
      "epoch 19,step 76000, training loss 0.0277324\n",
      "epoch 19,step 78000, training loss 0.0229454\n",
      "epoch 19,step 80000, training loss 0.0265867\n",
      "epoch 19,step 82000, training loss 0.0897882\n",
      "epoch 19,step 84000, training loss 0.0440308\n",
      "epoch 19,step 86000, training loss 0.031094\n",
      "epoch 19,step 88000, training loss 0.0456745\n",
      "epoch 19,step 90000, training loss 0.0335014\n",
      "epoch 19,step 92000, training loss 0.0288198\n",
      "epoch 19,step 94000, training loss 0.0416903\n",
      "epoch 19,step 96000, training loss 0.032564\n",
      "epoch 19,step 98000, training loss 0.0306366\n",
      "epoch 19,step 100000, training loss 0.0300232\n",
      "epoch 19,step 102000, training loss 0.0955206\n",
      "epoch 19,step 104000, training loss 0.0771614\n",
      "epoch 19,step 106000, training loss 0.0333435\n",
      "epoch 19,step 108000, training loss 0.0307547\n",
      "epoch 19,step 110000, training loss 0.0298207\n",
      "epoch 19,step 112000, training loss 0.0287562\n",
      "epoch 19,step 114000, training loss 0.0258732\n",
      "epoch 19,step 116000, training loss 0.0430226\n",
      "epoch 19,step 118000, training loss 0.0443293\n",
      "epoch 19,step 120000, training loss 0.0399316\n",
      "epoch 19,step 122000, training loss 0.0942175\n",
      "epoch 19,step 124000, training loss 0.0618155\n",
      "epoch 19,step 126000, training loss 0.0399897\n",
      "epoch 19,step 128000, training loss 0.0368328\n",
      "epoch 19,step 130000, training loss 0.0637032\n",
      "epoch 19,step 132000, training loss 0.0605017\n",
      "epoch 19,step 134000, training loss 0.0430488\n",
      "epoch 19,step 136000, training loss 0.0356492\n",
      "epoch 19,step 138000, training loss 0.0336154\n",
      "epoch 19,step 140000, training loss 0.0341312\n",
      "epoch 19,step 142000, training loss 0.0958811\n",
      "epoch 19,step 144000, training loss 0.0443509\n",
      "epoch 19,step 146000, training loss 0.0334536\n",
      "epoch 19,step 148000, training loss 0.0277263\n",
      "epoch 19,step 150000, training loss 0.0475114\n",
      "epoch 19,step 152000, training loss 0.0364934\n",
      "epoch 19,step 154000, training loss 0.0389679\n",
      "epoch 19,step 156000, training loss 0.0287395\n",
      "epoch 19,step 158000, training loss 0.0351173\n",
      "epoch 19,step 160000, training loss 0.0319082\n",
      "epoch 19,step 162000, training loss 0.109919\n",
      "epoch 19,step 164000, training loss 0.0506768\n",
      "epoch 19,step 166000, training loss 0.0820444\n",
      "epoch 19,step 168000, training loss 0.0347081\n",
      "epoch 19,step 170000, training loss 0.0306632\n",
      "epoch 19,step 172000, training loss 0.03045\n",
      "epoch 19,step 174000, training loss 0.0327129\n",
      "epoch 19,step 176000, training loss 0.0295319\n",
      "epoch 19,step 178000, training loss 0.0255156\n",
      "epoch 19,step 180000, training loss 0.026773\n",
      "epoch 19,step 182000, training loss 0.0902478\n",
      "epoch 19,step 184000, training loss 0.0731121\n",
      "epoch 19,step 186000, training loss 0.0367547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19,step 188000, training loss 0.0350941\n",
      "epoch 19,step 190000, training loss 0.0243236\n",
      "epoch 19,step 192000, training loss 0.0295795\n",
      "epoch 19,step 194000, training loss 0.0283542\n",
      "epoch 19,step 196000, training loss 0.0299114\n",
      "epoch 19,step 198000, training loss 0.0324551\n",
      "epoch 19,step 200000, training loss 0.0383752\n",
      "epoch 19,step 202000, training loss 0.107009\n",
      "epoch 19,step 204000, training loss 0.050587\n",
      "epoch 19,step 206000, training loss 0.0322947\n",
      "epoch 19,step 208000, training loss 0.0234081\n",
      "epoch 19,step 210000, training loss 0.0306876\n",
      "epoch 19,step 212000, training loss 0.0347744\n",
      "epoch 19,step 214000, training loss 0.0231789\n",
      "epoch 19,step 216000, training loss 0.0417647\n",
      "epoch 19,step 218000, training loss 0.0377626\n",
      "epoch 19,step 220000, training loss 0.0357441\n",
      "epoch 19,step 222000, training loss 0.0969863\n",
      "epoch 19,step 224000, training loss 0.0694628\n",
      "epoch 19,step 226000, training loss 0.0351526\n",
      "epoch 19,step 228000, training loss 0.0563726\n",
      "epoch 19,step 230000, training loss 0.0479147\n",
      "epoch 19,step 232000, training loss 0.0611567\n",
      "epoch 19,step 234000, training loss 0.0370057\n",
      "epoch 19,step 236000, training loss 0.0615228\n",
      "epoch 19,step 238000, training loss 0.0673368\n",
      "epoch 19,step 240000, training loss 0.0380004\n",
      "epoch 19,step 242000, training loss 0.0899414\n",
      "epoch 19,step 244000, training loss 0.0600868\n",
      "epoch 19,step 246000, training loss 0.0420943\n",
      "epoch 19,step 248000, training loss 0.0384996\n",
      "epoch 19,step 250000, training loss 0.0380475\n",
      "epoch 19,step 252000, training loss 0.0259771\n",
      "epoch 19,step 254000, training loss 0.0380623\n",
      "epoch 19,step 256000, training loss 0.032968\n",
      "epoch 19,step 258000, training loss 0.0283297\n",
      "epoch 19,step 260000, training loss 0.0227021\n",
      "epoch 19,step 262000, training loss 0.0811535\n",
      "epoch 19,step 264000, training loss 0.0768519\n",
      "epoch 19,step 266000, training loss 0.0335026\n",
      "epoch 19,step 268000, training loss 0.0319085\n",
      "epoch 19,step 270000, training loss 0.0534808\n",
      "epoch 19,step 272000, training loss 0.0272808\n",
      "epoch 19,step 274000, training loss 0.0510469\n",
      "epoch 19,step 276000, training loss 0.0265525\n",
      "epoch 19,step 278000, training loss 0.0418747\n",
      "epoch 19,step 280000, training loss 0.0344319\n",
      "epoch 19,step 282000, training loss 0.141893\n",
      "epoch 19,step 284000, training loss 0.0878326\n",
      "epoch 19,step 286000, training loss 0.0502472\n",
      "epoch 19,step 288000, training loss 0.0332744\n",
      "epoch 19,step 290000, training loss 0.041164\n",
      "epoch 19,step 292000, training loss 0.0395234\n",
      "epoch 19,step 294000, training loss 0.0463034\n",
      "epoch 19,step 296000, training loss 0.0291227\n",
      "epoch 19,step 298000, training loss 0.0321647\n",
      "epoch 19,step 300000, training loss 0.0334757\n",
      "epoch 19,step 302000, training loss 0.0867925\n",
      "epoch 19,step 304000, training loss 0.0718627\n",
      "epoch 19,step 306000, training loss 0.046855\n",
      "epoch 19,step 308000, training loss 0.0474061\n",
      "epoch 19,step 310000, training loss 0.0538391\n",
      "epoch 19,step 312000, training loss 0.0561031\n",
      "epoch 19,step 314000, training loss 0.0403042\n",
      "epoch 19,step 316000, training loss 0.0258745\n",
      "epoch 19,step 318000, training loss 0.0391167\n",
      "epoch 19,step 320000, training loss 0.0316698\n",
      "epoch 19,step 322000, training loss 0.0895816\n",
      "epoch 19,step 324000, training loss 0.0575702\n",
      "epoch 19,step 326000, training loss 0.0402829\n",
      "epoch 19,step 328000, training loss 0.0295269\n",
      "epoch 19,step 330000, training loss 0.0363205\n",
      "epoch 19,step 332000, training loss 0.0275547\n",
      "epoch 19,step 334000, training loss 0.0289974\n",
      "epoch 19,step 336000, training loss 0.0390771\n",
      "epoch 19,step 338000, training loss 0.0293938\n",
      "epoch 19,step 340000, training loss 0.034088\n",
      "epoch 19,step 342000, training loss 0.125784\n",
      "epoch 19,step 344000, training loss 0.0571635\n",
      "epoch 19,step 346000, training loss 0.0408151\n",
      "epoch 19,step 348000, training loss 0.0565942\n",
      "epoch 19,step 350000, training loss 0.0446935\n",
      "epoch 19,step 352000, training loss 0.0352268\n",
      "epoch 19,step 354000, training loss 0.050957\n",
      "epoch 19,step 356000, training loss 0.0329887\n",
      "epoch 19,step 358000, training loss 0.0336021\n",
      "epoch 19,step 360000, training loss 0.0314455\n",
      "epoch 19,step 362000, training loss 0.0908677\n",
      "epoch 19,step 364000, training loss 0.0581424\n",
      "epoch 19,step 366000, training loss 0.0374742\n",
      "epoch 19,step 368000, training loss 0.0421865\n",
      "epoch 19,step 370000, training loss 0.0340337\n",
      "epoch 19,step 372000, training loss 0.0765739\n",
      "epoch 19,step 374000, training loss 0.0425806\n",
      "epoch 19,step 376000, training loss 0.0377949\n",
      "epoch 19,step 378000, training loss 0.0342834\n",
      "epoch 19,step 380000, training loss 0.036794\n",
      "epoch 19,step 382000, training loss 0.0857808\n",
      "epoch 19,step 384000, training loss 0.0460969\n",
      "epoch 19,step 386000, training loss 0.0243979\n",
      "epoch 19,step 388000, training loss 0.0264865\n",
      "epoch 19,step 390000, training loss 0.0316675\n",
      "epoch 19,step 392000, training loss 0.0379854\n",
      "epoch 19,step 394000, training loss 0.0242523\n",
      "epoch 19,step 396000, training loss 0.0274463\n",
      "epoch 19,step 398000, training loss 0.0289434\n",
      "epoch 19,step 400000, training loss 0.0290794\n",
      "epoch 19,step 402000, training loss 0.0881152\n",
      "epoch 19,step 404000, training loss 0.0817785\n",
      "epoch 19,step 406000, training loss 0.0390033\n",
      "epoch 19,step 408000, training loss 0.0253623\n",
      "epoch 19,step 410000, training loss 0.0362552\n",
      "epoch 19,step 412000, training loss 0.043147\n",
      "epoch 19,step 414000, training loss 0.0291026\n",
      "epoch 19,step 416000, training loss 0.0252543\n",
      "epoch 19,step 418000, training loss 0.0282545\n",
      "epoch 19,step 420000, training loss 0.0293125\n",
      "epoch 19,step 422000, training loss 0.0938313\n",
      "epoch 19,step 424000, training loss 0.0519362\n",
      "epoch 19,step 426000, training loss 0.0389132\n",
      "epoch 19,step 428000, training loss 0.0343422\n",
      "epoch 19,step 430000, training loss 0.0308419\n",
      "epoch 19,step 432000, training loss 0.0335783\n",
      "epoch 19,step 434000, training loss 0.0344693\n",
      "epoch 19,step 436000, training loss 0.0392537\n",
      "epoch 19,step 438000, training loss 0.0337502\n",
      "epoch 19,step 440000, training loss 0.034363\n",
      "epoch 19,step 442000, training loss 0.113627\n",
      "epoch 19,step 444000, training loss 0.0548815\n",
      "epoch 19,step 446000, training loss 0.0383648\n",
      "epoch 19,step 448000, training loss 0.0277313\n",
      "epoch 19,step 450000, training loss 0.0302994\n",
      "epoch 19,step 452000, training loss 0.0256085\n",
      "epoch 19,step 454000, training loss 0.0298287\n",
      "epoch 19,step 456000, training loss 0.02931\n",
      "epoch 19,step 458000, training loss 0.0279678\n",
      "epoch 19,step 460000, training loss 0.0360339\n",
      "epoch 19,step 462000, training loss 0.0915156\n",
      "epoch 19,step 464000, training loss 0.0527777\n",
      "epoch 19,step 466000, training loss 0.0249381\n",
      "epoch 19,step 468000, training loss 0.0322553\n",
      "epoch 19,step 470000, training loss 0.0244268\n",
      "epoch 19,step 472000, training loss 0.023971\n",
      "epoch 19,step 474000, training loss 0.0341245\n",
      "epoch 19,step 476000, training loss 0.0289236\n",
      "epoch 19,step 478000, training loss 0.0289079\n",
      "epoch 19,step 480000, training loss 0.0306614\n",
      "epoch 19,step 482000, training loss 0.102446\n",
      "epoch 19,step 484000, training loss 0.0474158\n",
      "epoch 19,step 486000, training loss 0.038479\n",
      "epoch 19,step 488000, training loss 0.0339947\n",
      "epoch 19,step 490000, training loss 0.0332962\n",
      "epoch 19,step 492000, training loss 0.0457245\n",
      "epoch 19,step 494000, training loss 0.032561\n",
      "epoch 19,step 496000, training loss 0.02667\n",
      "epoch 19,step 498000, training loss 0.0374666\n",
      "epoch 19,step 500000, training loss 0.0310847\n",
      "epoch 19,step 502000, training loss 0.172261\n",
      "epoch 19,step 504000, training loss 0.0493448\n",
      "epoch 19,step 506000, training loss 0.0281468\n",
      "epoch 19,step 508000, training loss 0.0335042\n",
      "epoch 19,step 510000, training loss 0.0320715\n",
      "epoch 19,step 512000, training loss 0.020797\n",
      "epoch 19,step 514000, training loss 0.0371718\n",
      "epoch 19,step 516000, training loss 0.0302021\n",
      "epoch 19,step 518000, training loss 0.0486736\n",
      "epoch 19,step 520000, training loss 0.0384543\n",
      "epoch 19,step 522000, training loss 0.141989\n",
      "epoch 19,step 524000, training loss 0.0720897\n",
      "epoch 19,step 526000, training loss 0.0663546\n",
      "epoch 19,step 528000, training loss 0.0363941\n",
      "epoch 19,step 530000, training loss 0.0257311\n",
      "epoch 19,step 532000, training loss 0.0300201\n",
      "epoch 19,step 534000, training loss 0.0312557\n",
      "epoch 19,step 536000, training loss 0.0290067\n",
      "epoch 19,step 538000, training loss 0.0271457\n",
      "epoch 19,step 540000, training loss 0.0323713\n",
      "epoch 19,step 542000, training loss 0.102076\n",
      "epoch 19,step 544000, training loss 0.0440743\n",
      "epoch 19,step 546000, training loss 0.0388724\n",
      "epoch 19,step 548000, training loss 0.0504033\n",
      "epoch 19,step 550000, training loss 0.0565588\n",
      "epoch 19,step 552000, training loss 0.0398747\n",
      "epoch 19,step 554000, training loss 0.0281914\n",
      "epoch 19,step 556000, training loss 0.0306931\n",
      "epoch 19,step 558000, training loss 0.0325529\n",
      "epoch 19,step 560000, training loss 0.0286559\n",
      "epoch 19,step 562000, training loss 0.122494\n",
      "epoch 19,step 564000, training loss 0.0614926\n",
      "epoch 19,step 566000, training loss 0.0291456\n",
      "epoch 19,step 568000, training loss 0.0308344\n",
      "epoch 19,step 570000, training loss 0.0270167\n",
      "epoch 19,step 572000, training loss 0.0283904\n",
      "epoch 19,step 574000, training loss 0.0389687\n",
      "epoch 19,step 576000, training loss 0.0257909\n",
      "epoch 19,step 578000, training loss 0.0324523\n",
      "epoch 19,step 580000, training loss 0.0374174\n",
      "epoch 19,step 582000, training loss 0.0945662\n",
      "epoch 19,step 584000, training loss 0.0473818\n",
      "epoch 19,step 586000, training loss 0.0309218\n",
      "epoch 19,step 588000, training loss 0.0448037\n",
      "epoch 19,step 590000, training loss 0.0395679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19,step 592000, training loss 0.0414546\n",
      "epoch 19,step 594000, training loss 0.0286412\n",
      "epoch 19,step 596000, training loss 0.036459\n",
      "epoch 19,step 598000, training loss 0.0301872\n",
      "epoch 19,step 600000, training loss 0.0333118\n",
      "epoch 19,step 602000, training loss 0.092157\n",
      "epoch 19,step 604000, training loss 0.0610271\n",
      "epoch 19,step 606000, training loss 0.0380714\n",
      "epoch 19,step 608000, training loss 0.0361764\n",
      "epoch 19,step 610000, training loss 0.0348994\n",
      "epoch 19,step 612000, training loss 0.0398493\n",
      "epoch 19,step 614000, training loss 0.0320425\n",
      "epoch 19,step 616000, training loss 0.0508265\n",
      "epoch 19,step 618000, training loss 0.0274451\n",
      "epoch 19,step 620000, training loss 0.0349093\n",
      "epoch 19,step 622000, training loss 0.11409\n",
      "epoch 19,step 624000, training loss 0.0779673\n",
      "epoch 19,step 626000, training loss 0.0287726\n",
      "epoch 19,step 628000, training loss 0.0392613\n",
      "epoch 19,step 630000, training loss 0.0332429\n",
      "epoch 19,step 632000, training loss 0.0419941\n",
      "epoch 19,step 634000, training loss 0.0631218\n",
      "epoch 19,step 636000, training loss 0.0291444\n",
      "epoch 19,step 638000, training loss 0.0600374\n",
      "epoch 19,step 640000, training loss 0.0443477\n",
      "epoch 19,step 642000, training loss 0.0978863\n",
      "epoch 19,step 644000, training loss 0.0720051\n",
      "epoch 19,step 646000, training loss 0.0419139\n",
      "epoch 19,step 648000, training loss 0.0413438\n",
      "epoch 19,step 650000, training loss 0.0497587\n",
      "epoch 19,step 652000, training loss 0.0297482\n",
      "epoch 19,step 654000, training loss 0.0331837\n",
      "epoch 19,step 656000, training loss 0.026211\n",
      "epoch 19,step 658000, training loss 0.0422381\n",
      "epoch 19,step 660000, training loss 0.0326011\n",
      "epoch 19,step 662000, training loss 0.0893996\n",
      "epoch 19,step 664000, training loss 0.0831452\n",
      "epoch 19,step 666000, training loss 0.0315073\n",
      "epoch 19,step 668000, training loss 0.0500245\n",
      "epoch 19,step 670000, training loss 0.0324462\n",
      "epoch 19,step 672000, training loss 0.0242977\n",
      "epoch 19,step 674000, training loss 0.0320493\n",
      "epoch 19,step 676000, training loss 0.0310087\n",
      "epoch 19,step 678000, training loss 0.0312678\n",
      "epoch 19,step 680000, training loss 0.0463022\n",
      "epoch 19,step 682000, training loss 0.101095\n",
      "epoch 19,step 684000, training loss 0.0607777\n",
      "epoch 19,step 686000, training loss 0.0491379\n",
      "epoch 19,step 688000, training loss 0.0457864\n",
      "epoch 19,step 690000, training loss 0.0254182\n",
      "epoch 19,step 692000, training loss 0.0286828\n",
      "epoch 19,step 694000, training loss 0.0340891\n",
      "epoch 19,step 696000, training loss 0.0376006\n",
      "epoch 19,step 698000, training loss 0.0372609\n",
      "epoch 19,step 700000, training loss 0.0321244\n",
      "epoch 19,step 702000, training loss 0.0975641\n",
      "epoch 19,step 704000, training loss 0.0502911\n",
      "epoch 19,step 706000, training loss 0.0403627\n",
      "epoch 19,step 708000, training loss 0.0334417\n",
      "epoch 19,step 710000, training loss 0.0411163\n",
      "epoch 19,step 712000, training loss 0.030685\n",
      "epoch 19,step 714000, training loss 0.0277603\n",
      "epoch 19,step 716000, training loss 0.0255922\n",
      "epoch 19,step 718000, training loss 0.0259028\n",
      "epoch 19,step 720000, training loss 0.0244053\n",
      "epoch 19,step 722000, training loss 0.104426\n",
      "epoch 19,step 724000, training loss 0.0572157\n",
      "epoch 19,step 726000, training loss 0.0396708\n",
      "epoch 19,step 728000, training loss 0.0291167\n",
      "epoch 19,step 730000, training loss 0.0520068\n",
      "epoch 19,step 732000, training loss 0.0430205\n",
      "epoch 19,step 734000, training loss 0.0385312\n",
      "epoch 19,step 736000, training loss 0.0390097\n",
      "epoch 19,step 738000, training loss 0.0346403\n",
      "epoch 19,step 740000, training loss 0.0449004\n",
      "epoch 19,step 742000, training loss 0.110189\n",
      "epoch 19,step 744000, training loss 0.0580522\n",
      "epoch 19,step 746000, training loss 0.0301859\n",
      "epoch 19,step 748000, training loss 0.0529214\n",
      "epoch 19,step 750000, training loss 0.0381191\n",
      "epoch 19,step 752000, training loss 0.0363543\n",
      "epoch 19,step 754000, training loss 0.0333442\n",
      "epoch 19,step 756000, training loss 0.0452251\n",
      "epoch 19,step 758000, training loss 0.0279262\n",
      "epoch 19,step 760000, training loss 0.0328061\n",
      "epoch 19,step 762000, training loss 0.0989164\n",
      "epoch 19,step 764000, training loss 0.119708\n",
      "epoch 19,step 766000, training loss 0.0518111\n",
      "epoch 19,step 768000, training loss 0.0398749\n",
      "epoch 19,step 770000, training loss 0.0330884\n",
      "epoch 19,step 772000, training loss 0.0372405\n",
      "epoch 19,step 774000, training loss 0.0335419\n",
      "epoch 19,step 776000, training loss 0.0322439\n",
      "epoch 19,step 778000, training loss 0.0375706\n",
      "epoch 19,step 780000, training loss 0.0331944\n",
      "epoch 19,step 782000, training loss 0.090954\n",
      "epoch 19,step 784000, training loss 0.0604987\n",
      "epoch 19,step 786000, training loss 0.0296572\n",
      "epoch 19,step 788000, training loss 0.0309278\n",
      "epoch 19,step 790000, training loss 0.0350428\n",
      "epoch 19,step 792000, training loss 0.0258108\n",
      "epoch 19,step 794000, training loss 0.0287112\n",
      "epoch 19,step 796000, training loss 0.0416746\n",
      "epoch 19,step 798000, training loss 0.0336116\n",
      "epoch 19,step 800000, training loss 0.0230194\n",
      "epoch 19,step 802000, training loss 0.116076\n",
      "epoch 19,step 804000, training loss 0.121676\n",
      "epoch 19,step 806000, training loss 0.0345156\n",
      "epoch 19,step 808000, training loss 0.0350163\n",
      "epoch 19,step 810000, training loss 0.0289836\n",
      "epoch 19,step 812000, training loss 0.0331752\n",
      "epoch 19,step 814000, training loss 0.0283011\n",
      "epoch 19,step 816000, training loss 0.0358827\n",
      "epoch 19,step 818000, training loss 0.0405598\n",
      "epoch 19,step 820000, training loss 0.0301312\n",
      "epoch 19,step 822000, training loss 0.0884207\n",
      "epoch 19,step 824000, training loss 0.0541177\n",
      "epoch 19,step 826000, training loss 0.0367721\n",
      "epoch 19,step 828000, training loss 0.0385158\n",
      "epoch 19,step 830000, training loss 0.0309493\n",
      "epoch 19,step 832000, training loss 0.043657\n",
      "epoch 19,step 834000, training loss 0.038364\n",
      "epoch 19,step 836000, training loss 0.0340316\n",
      "epoch 19,step 838000, training loss 0.0227949\n",
      "epoch 19,step 840000, training loss 0.0296116\n",
      "epoch 19,step 842000, training loss 0.0952984\n",
      "epoch 19,step 844000, training loss 0.0520623\n",
      "epoch 19,step 846000, training loss 0.0198464\n",
      "epoch 19,step 848000, training loss 0.030527\n",
      "epoch 19,step 850000, training loss 0.0431264\n",
      "epoch 19,step 852000, training loss 0.0411553\n",
      "epoch 19,step 854000, training loss 0.0405554\n",
      "epoch 19,step 856000, training loss 0.045547\n",
      "epoch 19,step 858000, training loss 0.0287595\n",
      "epoch 19,step 860000, training loss 0.034025\n",
      "epoch 19,step 862000, training loss 0.0980463\n",
      "epoch 19,step 864000, training loss 0.0737532\n",
      "epoch 19,step 866000, training loss 0.0437056\n",
      "epoch 19,step 868000, training loss 0.0310806\n",
      "epoch 19,step 870000, training loss 0.0426209\n",
      "epoch 19,step 872000, training loss 0.0300668\n",
      "epoch 19,step 874000, training loss 0.0376831\n",
      "epoch 19,step 876000, training loss 0.0359177\n",
      "epoch 19,step 878000, training loss 0.0331511\n",
      "epoch 19,step 880000, training loss 0.0340461\n",
      "epoch 19,step 882000, training loss 0.113012\n",
      "epoch 19,step 884000, training loss 0.0534411\n",
      "epoch 19,step 886000, training loss 0.0403251\n",
      "epoch 19,step 888000, training loss 0.0263535\n",
      "epoch 19,step 890000, training loss 0.024081\n",
      "epoch 19,step 892000, training loss 0.0266384\n",
      "epoch 19,step 894000, training loss 0.0362942\n",
      "epoch 19,step 896000, training loss 0.0297774\n",
      "epoch 19,step 898000, training loss 0.0338802\n",
      "epoch 19,step 900000, training loss 0.0315459\n",
      "epoch 19,step 902000, training loss 0.0907238\n",
      "epoch 19,step 904000, training loss 0.0474244\n",
      "epoch 19,step 906000, training loss 0.0420273\n",
      "epoch 19,step 908000, training loss 0.038901\n",
      "epoch 19,step 910000, training loss 0.0287863\n",
      "epoch 19,step 912000, training loss 0.0264309\n",
      "epoch 19,step 914000, training loss 0.0319983\n",
      "epoch 19,step 916000, training loss 0.0350328\n",
      "epoch 19,step 918000, training loss 0.0231814\n",
      "epoch 19,step 920000, training loss 0.0328067\n",
      "epoch 19,step 922000, training loss 0.11069\n",
      "epoch 19,step 924000, training loss 0.0612235\n",
      "epoch 19,step 926000, training loss 0.0418897\n",
      "epoch 19,step 928000, training loss 0.0415914\n",
      "epoch 19,step 930000, training loss 0.0448652\n",
      "epoch 19,step 932000, training loss 0.0352607\n",
      "epoch 19,step 934000, training loss 0.0538775\n",
      "epoch 19,step 936000, training loss 0.0360115\n",
      "epoch 19,step 938000, training loss 0.0327897\n",
      "epoch 19,step 940000, training loss 0.0339931\n",
      "epoch 19,step 942000, training loss 0.0945137\n",
      "epoch 19,step 944000, training loss 0.102449\n",
      "epoch 19,step 946000, training loss 0.0414588\n",
      "epoch 19,step 948000, training loss 0.0327534\n",
      "epoch 19,step 950000, training loss 0.0267491\n",
      "epoch 19,step 952000, training loss 0.0501538\n",
      "epoch 19,step 954000, training loss 0.0335409\n",
      "epoch 19,step 956000, training loss 0.0374392\n",
      "epoch 19,step 958000, training loss 0.0308746\n",
      "epoch 19,step 960000, training loss 0.0341115\n",
      "epoch 19,step 962000, training loss 0.0819853\n",
      "epoch 19,step 964000, training loss 0.0439942\n",
      "epoch 19,step 966000, training loss 0.0379832\n",
      "epoch 19,step 968000, training loss 0.0374562\n",
      "epoch 19,step 970000, training loss 0.0310307\n",
      "epoch 19,step 972000, training loss 0.035302\n",
      "epoch 19,step 974000, training loss 0.0291606\n",
      "epoch 19,step 976000, training loss 0.0311814\n",
      "epoch 19,step 978000, training loss 0.032787\n",
      "epoch 19,step 980000, training loss 0.0369776\n",
      "epoch 19,step 982000, training loss 0.0987631\n",
      "epoch 19,step 984000, training loss 0.0492158\n",
      "epoch 19,step 986000, training loss 0.0302225\n",
      "epoch 19,step 988000, training loss 0.0474327\n",
      "epoch 19,step 990000, training loss 0.0380215\n",
      "epoch 19,step 992000, training loss 0.0295529\n",
      "epoch 19,step 994000, training loss 0.0322915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19,step 996000, training loss 0.0337187\n",
      "epoch 19,step 998000, training loss 0.0597874\n",
      "epoch 19,step 1000000, training loss 0.0313463\n",
      "epoch 19,step 1002000, training loss 0.111975\n",
      "epoch 19,step 1004000, training loss 0.0602852\n",
      "epoch 19,step 1006000, training loss 0.0384326\n",
      "epoch 19,step 1008000, training loss 0.0417869\n",
      "epoch 19,step 1010000, training loss 0.0336633\n",
      "epoch 19,step 1012000, training loss 0.0350468\n",
      "epoch 19,step 1014000, training loss 0.0242965\n",
      "epoch 19,step 1016000, training loss 0.0343729\n",
      "epoch 19,step 1018000, training loss 0.0366847\n",
      "epoch 19,step 1020000, training loss 0.0293662\n",
      "epoch 19,step 1022000, training loss 0.111068\n",
      "epoch 19,step 1024000, training loss 0.113789\n",
      "epoch 19,step 1026000, training loss 0.0334803\n",
      "epoch 19,step 1028000, training loss 0.0352136\n",
      "epoch 19,step 1030000, training loss 0.0388843\n",
      "epoch 19,step 1032000, training loss 0.039849\n",
      "epoch 19,step 1034000, training loss 0.0391643\n",
      "epoch 19,step 1036000, training loss 0.0321713\n",
      "epoch 19,step 1038000, training loss 0.0407217\n",
      "epoch 19,step 1040000, training loss 0.0267224\n",
      "epoch 19,step 1042000, training loss 0.0997007\n",
      "epoch 19,step 1044000, training loss 0.0708788\n",
      "epoch 19,step 1046000, training loss 0.034852\n",
      "epoch 19,step 1048000, training loss 0.0309465\n",
      "epoch 19,step 1050000, training loss 0.0391212\n",
      "epoch 19,step 1052000, training loss 0.0302943\n",
      "epoch 19,step 1054000, training loss 0.036638\n",
      "epoch 19,step 1056000, training loss 0.0400895\n",
      "epoch 19,step 1058000, training loss 0.058761\n",
      "epoch 19,step 1060000, training loss 0.0456848\n",
      "epoch 19,step 1062000, training loss 0.129832\n",
      "epoch 19,step 1064000, training loss 0.112585\n",
      "epoch 19,step 1066000, training loss 0.0506292\n",
      "epoch 19,step 1068000, training loss 0.0578823\n",
      "epoch 19,step 1070000, training loss 0.0431205\n",
      "epoch 19,step 1072000, training loss 0.0353269\n",
      "epoch 19,step 1074000, training loss 0.035778\n",
      "epoch 19,step 1076000, training loss 0.0356508\n",
      "epoch 19,step 1078000, training loss 0.0386634\n",
      "epoch 19,step 1080000, training loss 0.0381921\n",
      "epoch 19,step 1082000, training loss 0.0912955\n",
      "epoch 19,step 1084000, training loss 0.0915044\n",
      "epoch 19,step 1086000, training loss 0.059328\n",
      "epoch 19,step 1088000, training loss 0.0565976\n",
      "epoch 19,step 1090000, training loss 0.0402414\n",
      "epoch 19,step 1092000, training loss 0.0340465\n",
      "epoch 19,step 1094000, training loss 0.027099\n",
      "epoch 19,step 1096000, training loss 0.0379115\n",
      "epoch 19,step 1098000, training loss 0.0330249\n",
      "epoch 19,step 1100000, training loss 0.0545921\n",
      "epoch 19,step 1102000, training loss 0.0999124\n",
      "epoch 19,step 1104000, training loss 0.0564477\n",
      "epoch 19,step 1106000, training loss 0.0418939\n",
      "epoch 19,step 1108000, training loss 0.0496261\n",
      "epoch 19,step 1110000, training loss 0.0415966\n",
      "epoch 19,step 1112000, training loss 0.0380395\n",
      "epoch 19,step 1114000, training loss 0.0313698\n",
      "epoch 19,step 1116000, training loss 0.0450209\n",
      "epoch 19,step 1118000, training loss 0.0332126\n",
      "epoch 19,step 1120000, training loss 0.0455544\n",
      "epoch 19,step 1122000, training loss 0.108522\n",
      "epoch 19,step 1124000, training loss 0.0760892\n",
      "epoch 19,step 1126000, training loss 0.0554751\n",
      "epoch 19,step 1128000, training loss 0.0383948\n",
      "epoch 19,step 1130000, training loss 0.0320037\n",
      "epoch 19,step 1132000, training loss 0.0307882\n",
      "epoch 19,step 1134000, training loss 0.0336648\n",
      "epoch 19,step 1136000, training loss 0.0354188\n",
      "epoch 19,step 1138000, training loss 0.0316311\n",
      "epoch 19,step 1140000, training loss 0.028848\n",
      "epoch 19,step 1142000, training loss 0.108299\n",
      "epoch 19,step 1144000, training loss 0.0774559\n",
      "epoch 19,step 1146000, training loss 0.0465269\n",
      "epoch 19,step 1148000, training loss 0.0425429\n",
      "epoch 19,step 1150000, training loss 0.0560927\n",
      "epoch 19,step 1152000, training loss 0.04532\n",
      "epoch 19,step 1154000, training loss 0.042071\n",
      "epoch 19,step 1156000, training loss 0.0422122\n",
      "epoch 19,step 1158000, training loss 0.0331144\n",
      "epoch 19,step 1160000, training loss 0.0277135\n",
      "epoch 19,step 1162000, training loss 0.107285\n",
      "epoch 19,step 1164000, training loss 0.0570791\n",
      "epoch 19,step 1166000, training loss 0.0344133\n",
      "epoch 19,step 1168000, training loss 0.0397087\n",
      "epoch 19,step 1170000, training loss 0.0345911\n",
      "epoch 19,step 1172000, training loss 0.0363562\n",
      "epoch 19,step 1174000, training loss 0.0333202\n",
      "epoch 19,step 1176000, training loss 0.0424634\n",
      "epoch 19,step 1178000, training loss 0.0381026\n",
      "epoch 19,step 1180000, training loss 0.0289919\n",
      "epoch 19,step 1182000, training loss 0.0862606\n",
      "epoch 19,step 1184000, training loss 0.0519811\n",
      "epoch 19,step 1186000, training loss 0.0372047\n",
      "epoch 19,step 1188000, training loss 0.0384274\n",
      "epoch 19,step 1190000, training loss 0.0333785\n",
      "epoch 19,step 1192000, training loss 0.0337408\n",
      "epoch 19,step 1194000, training loss 0.026148\n",
      "epoch 19,step 1196000, training loss 0.0307569\n",
      "epoch 19,step 1198000, training loss 0.033117\n",
      "epoch 19,step 1200000, training loss 0.0373989\n",
      "epoch 19,step 1202000, training loss 0.151206\n",
      "epoch 19,step 1204000, training loss 0.0574896\n",
      "epoch 19,step 1206000, training loss 0.040401\n",
      "epoch 19,step 1208000, training loss 0.0569307\n",
      "epoch 19,step 1210000, training loss 0.0244814\n",
      "epoch 19,step 1212000, training loss 0.0378691\n",
      "epoch 19,step 1214000, training loss 0.0318922\n",
      "epoch 19,step 1216000, training loss 0.03173\n",
      "epoch 19,step 1218000, training loss 0.0323906\n",
      "epoch 19,step 1220000, training loss 0.0256009\n",
      "epoch 19,step 1222000, training loss 0.124042\n",
      "epoch 19,step 1224000, training loss 0.0692719\n",
      "epoch 19,step 1226000, training loss 0.0393936\n",
      "epoch 19,step 1228000, training loss 0.0289019\n",
      "epoch 19,step 1230000, training loss 0.0292726\n",
      "epoch 19,step 1232000, training loss 0.0331006\n",
      "epoch 19,step 1234000, training loss 0.0274858\n",
      "epoch 19,step 1236000, training loss 0.025857\n",
      "epoch 19,step 1238000, training loss 0.0294555\n",
      "epoch 19,step 1240000, training loss 0.0406836\n",
      "epoch 19,step 1242000, training loss 0.133844\n",
      "epoch 19,step 1244000, training loss 0.131144\n",
      "epoch 19,step 1246000, training loss 0.0444447\n",
      "epoch 19,step 1248000, training loss 0.0549798\n",
      "epoch 19,step 1250000, training loss 0.0405986\n",
      "epoch 19,step 1252000, training loss 0.0478856\n",
      "epoch 19,step 1254000, training loss 0.0287169\n",
      "epoch 19,step 1256000, training loss 0.034652\n",
      "epoch 19,step 1258000, training loss 0.02791\n",
      "epoch 19,step 1260000, training loss 0.0367632\n",
      "epoch 19,step 1262000, training loss 0.0963078\n",
      "epoch 19,step 1264000, training loss 0.0573394\n",
      "epoch 19,step 1266000, training loss 0.0333794\n",
      "epoch 19,step 1268000, training loss 0.036413\n",
      "epoch 19,step 1270000, training loss 0.025798\n",
      "epoch 19,step 1272000, training loss 0.0257908\n",
      "epoch 19,step 1274000, training loss 0.0263451\n",
      "epoch 19,step 1276000, training loss 0.043676\n",
      "epoch 19,step 1278000, training loss 0.0316199\n",
      "epoch 19,step 1280000, training loss 0.0346588\n",
      "epoch 19,step 1282000, training loss 0.145564\n",
      "epoch 19,step 1284000, training loss 0.0740434\n",
      "epoch 19,step 1286000, training loss 0.0317243\n",
      "epoch 19,step 1288000, training loss 0.0312419\n",
      "epoch 19,step 1290000, training loss 0.0402004\n",
      "epoch 19,step 1292000, training loss 0.0401842\n",
      "epoch 19,step 1294000, training loss 0.0857517\n",
      "epoch 19,step 1296000, training loss 0.028631\n",
      "epoch 19,step 1298000, training loss 0.0266356\n",
      "epoch 19,step 1300000, training loss 0.0354797\n",
      "epoch 19,step 1302000, training loss 0.107466\n",
      "epoch 19,step 1304000, training loss 0.0467469\n",
      "epoch 19,step 1306000, training loss 0.0243373\n",
      "epoch 19,step 1308000, training loss 0.0307542\n",
      "epoch 19,step 1310000, training loss 0.0316592\n",
      "epoch 19,step 1312000, training loss 0.0319578\n",
      "epoch 19,step 1314000, training loss 0.0243798\n",
      "epoch 19,step 1316000, training loss 0.0356195\n",
      "epoch 19,step 1318000, training loss 0.048127\n",
      "epoch 19,step 1320000, training loss 0.0301523\n",
      "epoch 19,step 1322000, training loss 0.119972\n",
      "epoch 19,step 1324000, training loss 0.0639808\n",
      "epoch 19,step 1326000, training loss 0.0367265\n",
      "epoch 19,step 1328000, training loss 0.0390233\n",
      "epoch 19,step 1330000, training loss 0.0273511\n",
      "epoch 19,step 1332000, training loss 0.0347571\n",
      "epoch 19,step 1334000, training loss 0.0348891\n",
      "epoch 19,step 1336000, training loss 0.0439839\n",
      "epoch 19,step 1338000, training loss 0.0338656\n",
      "epoch 19,step 1340000, training loss 0.0353782\n",
      "epoch 19,step 1342000, training loss 0.104347\n",
      "epoch 19,step 1344000, training loss 0.111031\n",
      "epoch 19,step 1346000, training loss 0.0390062\n",
      "epoch 19,step 1348000, training loss 0.043418\n",
      "epoch 19,step 1350000, training loss 0.0263047\n",
      "epoch 19,step 1352000, training loss 0.0438902\n",
      "epoch 19,step 1354000, training loss 0.0464548\n",
      "epoch 19,step 1356000, training loss 0.0331172\n",
      "epoch 19,step 1358000, training loss 0.0294293\n",
      "epoch 19,step 1360000, training loss 0.0344539\n",
      "epoch 19,step 1362000, training loss 0.110394\n",
      "epoch 19,step 1364000, training loss 0.0584578\n",
      "epoch 19,step 1366000, training loss 0.0445987\n",
      "epoch 19,step 1368000, training loss 0.0472662\n",
      "epoch 19,step 1370000, training loss 0.0330153\n",
      "epoch 19,step 1372000, training loss 0.03506\n",
      "epoch 19,step 1374000, training loss 0.0357616\n",
      "epoch 19,step 1376000, training loss 0.043496\n",
      "epoch 19,step 1378000, training loss 0.0391696\n",
      "epoch 19,step 1380000, training loss 0.0349268\n",
      "epoch 19,step 1382000, training loss 0.103851\n",
      "epoch 19,step 1384000, training loss 0.0660691\n",
      "epoch 19,step 1386000, training loss 0.03256\n",
      "epoch 19,step 1388000, training loss 0.0329653\n",
      "epoch 19,step 1390000, training loss 0.037686\n",
      "epoch 19,step 1392000, training loss 0.0308538\n",
      "epoch 19,step 1394000, training loss 0.0348248\n",
      "epoch 19,step 1396000, training loss 0.0382596\n",
      "epoch 19,step 1398000, training loss 0.036205\n",
      "epoch 19,step 1400000, training loss 0.031006\n",
      "epoch 19,step 1402000, training loss 0.109617\n",
      "epoch 19,step 1404000, training loss 0.0773068\n",
      "epoch 19,step 1406000, training loss 0.0412665\n",
      "epoch 19,step 1408000, training loss 0.0346719\n",
      "epoch 19,step 1410000, training loss 0.0343742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19,step 1412000, training loss 0.0334024\n",
      "epoch 19,step 1414000, training loss 0.0385481\n",
      "epoch 19,step 1416000, training loss 0.0258501\n",
      "epoch 19,step 1418000, training loss 0.0279183\n",
      "epoch 19,step 1420000, training loss 0.0326389\n",
      "epoch 19,step 1422000, training loss 0.103723\n",
      "epoch 19,step 1424000, training loss 0.0674931\n",
      "epoch 19,step 1426000, training loss 0.0388127\n",
      "epoch 19,step 1428000, training loss 0.0332891\n",
      "epoch 19,step 1430000, training loss 0.0368287\n",
      "epoch 19,step 1432000, training loss 0.031771\n",
      "epoch 19,step 1434000, training loss 0.0471868\n",
      "epoch 19,step 1436000, training loss 0.0468862\n",
      "epoch 19,step 1438000, training loss 0.0321866\n",
      "epoch 19,step 1440000, training loss 0.0327727\n",
      "epoch 19,step 1442000, training loss 0.0792479\n",
      "epoch 19,step 1444000, training loss 0.0496109\n",
      "epoch 19,step 1446000, training loss 0.04817\n",
      "epoch 19,step 1448000, training loss 0.0360721\n",
      "epoch 19,step 1450000, training loss 0.0339849\n",
      "epoch 19,step 1452000, training loss 0.0294548\n",
      "epoch 19,step 1454000, training loss 0.046404\n",
      "epoch 19,step 1456000, training loss 0.0348592\n",
      "epoch 19,step 1458000, training loss 0.0292169\n",
      "epoch 19,step 1460000, training loss 0.0312664\n",
      "epoch 19,step 1462000, training loss 0.102325\n",
      "epoch 19,step 1464000, training loss 0.0547152\n",
      "epoch 19,step 1466000, training loss 0.0317783\n",
      "epoch 19,step 1468000, training loss 0.045265\n",
      "epoch 19,step 1470000, training loss 0.0340899\n",
      "epoch 19,step 1472000, training loss 0.035583\n",
      "epoch 19,step 1474000, training loss 0.0308011\n",
      "epoch 19,step 1476000, training loss 0.0404999\n",
      "epoch 19,step 1478000, training loss 0.0638495\n",
      "epoch 19,step 1480000, training loss 0.0445908\n",
      "epoch 19,step 1482000, training loss 0.0808192\n",
      "epoch 19,step 1484000, training loss 0.0490544\n",
      "epoch 19,step 1486000, training loss 0.0283929\n",
      "epoch 19,step 1488000, training loss 0.0251224\n",
      "epoch 19,step 1490000, training loss 0.0293002\n",
      "epoch 19,step 1492000, training loss 0.0308647\n",
      "epoch 19,step 1494000, training loss 0.0287189\n",
      "epoch 19,step 1496000, training loss 0.0275218\n",
      "epoch 19,step 1498000, training loss 0.0277185\n",
      "epoch 19,step 1500000, training loss 0.0693652\n",
      "epoch 19,step 1502000, training loss 0.0926546\n",
      "epoch 19,step 1504000, training loss 0.0459316\n",
      "epoch 19,step 1506000, training loss 0.0326065\n",
      "epoch 19,step 1508000, training loss 0.0304152\n",
      "epoch 19,step 1510000, training loss 0.0282962\n",
      "epoch 19,step 1512000, training loss 0.0319826\n",
      "epoch 19,step 1514000, training loss 0.0318581\n",
      "epoch 19,step 1516000, training loss 0.0293006\n",
      "epoch 19,step 1518000, training loss 0.0204209\n",
      "epoch 19,step 1520000, training loss 0.0275239\n",
      "epoch 19,step 1522000, training loss 0.0934551\n",
      "epoch 19,step 1524000, training loss 0.0845013\n",
      "epoch 19,step 1526000, training loss 0.0353511\n",
      "epoch 19,step 1528000, training loss 0.0271789\n",
      "epoch 19,step 1530000, training loss 0.044918\n",
      "epoch 19,step 1532000, training loss 0.0519907\n",
      "epoch 19,step 1534000, training loss 0.0275189\n",
      "epoch 19,step 1536000, training loss 0.0414483\n",
      "epoch 19,step 1538000, training loss 0.0322518\n",
      "epoch 19,step 1540000, training loss 0.0224161\n",
      "epoch 19,step 1542000, training loss 0.126809\n",
      "epoch 19,step 1544000, training loss 0.092897\n",
      "epoch 19,step 1546000, training loss 0.0403153\n",
      "epoch 19,step 1548000, training loss 0.0407511\n",
      "epoch 19,step 1550000, training loss 0.0350772\n",
      "epoch 19,step 1552000, training loss 0.0269378\n",
      "epoch 19,step 1554000, training loss 0.0287117\n",
      "epoch 19,step 1556000, training loss 0.037259\n",
      "epoch 19,step 1558000, training loss 0.0401054\n",
      "epoch 19,step 1560000, training loss 0.0252704\n",
      "epoch 19,step 1562000, training loss 0.0926749\n",
      "epoch 19,step 1564000, training loss 0.0780812\n",
      "epoch 19,step 1566000, training loss 0.0426316\n",
      "epoch 19,step 1568000, training loss 0.0303216\n",
      "epoch 19,step 1570000, training loss 0.0340742\n",
      "epoch 19,step 1572000, training loss 0.0278915\n",
      "epoch 19,step 1574000, training loss 0.0594124\n",
      "epoch 19,step 1576000, training loss 0.0389007\n",
      "epoch 19,step 1578000, training loss 0.0303491\n",
      "epoch 19,step 1580000, training loss 0.0362116\n",
      "epoch 19,step 1582000, training loss 0.12056\n",
      "epoch 19,step 1584000, training loss 0.0647241\n",
      "epoch 19,step 1586000, training loss 0.0515432\n",
      "epoch 19,step 1588000, training loss 0.0524644\n",
      "epoch 19,step 1590000, training loss 0.0349686\n",
      "epoch 19,step 1592000, training loss 0.0370606\n",
      "epoch 19,step 1594000, training loss 0.0285616\n",
      "epoch 19,step 1596000, training loss 0.0390908\n",
      "epoch 19,step 1598000, training loss 0.029218\n",
      "epoch 19,step 1600000, training loss 0.028886\n",
      "epoch 19,step 1602000, training loss 0.11521\n",
      "epoch 19,step 1604000, training loss 0.047807\n",
      "epoch 19,step 1606000, training loss 0.0460865\n",
      "epoch 19,step 1608000, training loss 0.0493626\n",
      "epoch 19,step 1610000, training loss 0.0343199\n",
      "epoch 19,step 1612000, training loss 0.0406066\n",
      "epoch 19,step 1614000, training loss 0.0348728\n",
      "epoch 19,step 1616000, training loss 0.0453434\n",
      "epoch 19,step 1618000, training loss 0.0361528\n",
      "epoch 19,step 1620000, training loss 0.0286848\n",
      "epoch 19,step 1622000, training loss 0.0989682\n",
      "epoch 19,step 1624000, training loss 0.0800532\n",
      "epoch 19,step 1626000, training loss 0.0417063\n",
      "epoch 19,step 1628000, training loss 0.0432258\n",
      "epoch 19,step 1630000, training loss 0.0419842\n",
      "epoch 19,step 1632000, training loss 0.0332414\n",
      "epoch 19,step 1634000, training loss 0.0334544\n",
      "epoch 19,step 1636000, training loss 0.0400769\n",
      "epoch 19,step 1638000, training loss 0.0358285\n",
      "epoch 19,step 1640000, training loss 0.0389382\n",
      "epoch 19,step 1642000, training loss 0.114597\n",
      "epoch 19,step 1644000, training loss 0.0470619\n",
      "epoch 19,step 1646000, training loss 0.0293547\n",
      "epoch 19,step 1648000, training loss 0.0404463\n",
      "epoch 19,step 1650000, training loss 0.0488869\n",
      "epoch 19,step 1652000, training loss 0.040068\n",
      "epoch 19,step 1654000, training loss 0.0357139\n",
      "epoch 19,step 1656000, training loss 0.0578305\n",
      "epoch 19,step 1658000, training loss 0.0467899\n",
      "epoch 19,step 1660000, training loss 0.034733\n",
      "epoch 19,step 1662000, training loss 0.106237\n",
      "epoch 19,step 1664000, training loss 0.0732455\n",
      "epoch 19,step 1666000, training loss 0.0340047\n",
      "epoch 19,step 1668000, training loss 0.0375289\n",
      "epoch 19,step 1670000, training loss 0.0354729\n",
      "epoch 19,step 1672000, training loss 0.0424564\n",
      "epoch 19,step 1674000, training loss 0.0317663\n",
      "epoch 19,step 1676000, training loss 0.0331118\n",
      "epoch 19,step 1678000, training loss 0.0443495\n",
      "epoch 19,step 1680000, training loss 0.0325351\n",
      "epoch 19,step 1682000, training loss 0.113247\n",
      "epoch 19,step 1684000, training loss 0.0834739\n",
      "epoch 19,step 1686000, training loss 0.0441834\n",
      "epoch 19,step 1688000, training loss 0.0359874\n",
      "epoch 19,step 1690000, training loss 0.0416266\n",
      "epoch 19,step 1692000, training loss 0.036881\n",
      "epoch 19,step 1694000, training loss 0.0526055\n",
      "epoch 19,step 1696000, training loss 0.0287207\n",
      "epoch 19,step 1698000, training loss 0.0356595\n",
      "epoch 19,step 1700000, training loss 0.0373673\n",
      "epoch 19,step 1702000, training loss 0.0941133\n",
      "epoch 19,step 1704000, training loss 0.0576734\n",
      "epoch 19,step 1706000, training loss 0.0404929\n",
      "epoch 19,step 1708000, training loss 0.0503474\n",
      "epoch 19,step 1710000, training loss 0.0429536\n",
      "epoch 19,step 1712000, training loss 0.0417631\n",
      "epoch 19,step 1714000, training loss 0.0342437\n",
      "epoch 19,step 1716000, training loss 0.0409318\n",
      "epoch 19,step 1718000, training loss 0.037046\n",
      "epoch 19,step 1720000, training loss 0.0496574\n",
      "epoch 19,step 1722000, training loss 0.0920331\n",
      "epoch 19,step 1724000, training loss 0.0646837\n",
      "epoch 19,step 1726000, training loss 0.0321783\n",
      "epoch 19,step 1728000, training loss 0.0309704\n",
      "epoch 19,step 1730000, training loss 0.0304365\n",
      "epoch 19,step 1732000, training loss 0.0277366\n",
      "epoch 19,step 1734000, training loss 0.0334389\n",
      "epoch 19,step 1736000, training loss 0.0313971\n",
      "epoch 19,step 1738000, training loss 0.0401416\n",
      "epoch 19,step 1740000, training loss 0.0384628\n",
      "epoch 19,step 1742000, training loss 0.0963376\n",
      "epoch 19,step 1744000, training loss 0.0549639\n",
      "epoch 19,step 1746000, training loss 0.0305278\n",
      "epoch 19,step 1748000, training loss 0.0310999\n",
      "epoch 19,step 1750000, training loss 0.0321401\n",
      "epoch 19,step 1752000, training loss 0.047718\n",
      "epoch 19,step 1754000, training loss 0.0457691\n",
      "epoch 19,step 1756000, training loss 0.0272015\n",
      "epoch 19,step 1758000, training loss 0.0235824\n",
      "epoch 19,step 1760000, training loss 0.0316197\n",
      "epoch 19,step 1762000, training loss 0.142346\n",
      "epoch 19,step 1764000, training loss 0.0907792\n",
      "epoch 19,step 1766000, training loss 0.0563413\n",
      "epoch 19,step 1768000, training loss 0.0303158\n",
      "epoch 19,step 1770000, training loss 0.0321887\n",
      "epoch 19,step 1772000, training loss 0.0294595\n",
      "epoch 19,step 1774000, training loss 0.0281793\n",
      "epoch 19,step 1776000, training loss 0.0279793\n",
      "epoch 19,step 1778000, training loss 0.0584649\n",
      "epoch 19,step 1780000, training loss 0.0265283\n",
      "epoch 19,step 1782000, training loss 0.107072\n",
      "epoch 19,step 1784000, training loss 0.0935313\n",
      "epoch 19,step 1786000, training loss 0.0459853\n",
      "epoch 19,step 1788000, training loss 0.0348138\n",
      "epoch 19,step 1790000, training loss 0.0425859\n",
      "epoch 19,step 1792000, training loss 0.044274\n",
      "epoch 19,step 1794000, training loss 0.0384357\n",
      "epoch 19,step 1796000, training loss 0.0368244\n",
      "epoch 19,step 1798000, training loss 0.0352875\n",
      "epoch 19,step 1800000, training loss 0.0706514\n",
      "epoch 19,step 1802000, training loss 0.0936201\n",
      "epoch 19,step 1804000, training loss 0.0414719\n",
      "epoch 19,step 1806000, training loss 0.029148\n",
      "epoch 19,step 1808000, training loss 0.0466583\n",
      "epoch 19,step 1810000, training loss 0.0310591\n",
      "epoch 19,step 1812000, training loss 0.028678\n",
      "epoch 19,step 1814000, training loss 0.0266162\n",
      "epoch 19,step 1816000, training loss 0.028024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19,step 1818000, training loss 0.0305715\n",
      "epoch 19,step 1820000, training loss 0.0288714\n",
      "epoch 19,step 1822000, training loss 0.0899087\n",
      "epoch 19,step 1824000, training loss 0.0546037\n",
      "epoch 19,step 1826000, training loss 0.0369236\n",
      "epoch 19,step 1828000, training loss 0.0410815\n",
      "epoch 19,step 1830000, training loss 0.0391753\n",
      "epoch 19,step 1832000, training loss 0.0311753\n",
      "epoch 19,step 1834000, training loss 0.0322383\n",
      "epoch 19,step 1836000, training loss 0.0410583\n",
      "epoch 19,step 1838000, training loss 0.0401628\n",
      "epoch 19,step 1840000, training loss 0.0335231\n",
      "epoch 19,step 1842000, training loss 0.0862631\n",
      "epoch 19,step 1844000, training loss 0.0741446\n",
      "epoch 19,step 1846000, training loss 0.0330032\n",
      "epoch 19,step 1848000, training loss 0.0304983\n",
      "epoch 19,step 1850000, training loss 0.025777\n",
      "epoch 19,step 1852000, training loss 0.0356489\n",
      "epoch 19,step 1854000, training loss 0.0301328\n",
      "epoch 19,step 1856000, training loss 0.022821\n",
      "epoch 19,step 1858000, training loss 0.0292425\n",
      "epoch 19,step 1860000, training loss 0.026409\n",
      "epoch 19,step 1862000, training loss 0.0936475\n",
      "epoch 19,step 1864000, training loss 0.0502482\n",
      "epoch 19,step 1866000, training loss 0.0291419\n",
      "epoch 19,step 1868000, training loss 0.0344548\n",
      "epoch 19,step 1870000, training loss 0.044805\n",
      "epoch 19,step 1872000, training loss 0.0305758\n",
      "epoch 19,step 1874000, training loss 0.0258458\n",
      "epoch 19,step 1876000, training loss 0.041088\n",
      "epoch 19,step 1878000, training loss 0.028019\n",
      "epoch 19,step 1880000, training loss 0.0265518\n",
      "epoch 19,step 1882000, training loss 0.105682\n",
      "epoch 19,step 1884000, training loss 0.0656455\n",
      "epoch 19,step 1886000, training loss 0.0290806\n",
      "epoch 19,step 1888000, training loss 0.0392798\n",
      "epoch 19,step 1890000, training loss 0.0349295\n",
      "epoch 19,step 1892000, training loss 0.0396202\n",
      "epoch 19,step 1894000, training loss 0.0417535\n",
      "epoch 19,step 1896000, training loss 0.0255388\n",
      "epoch 19,step 1898000, training loss 0.0355045\n",
      "epoch 19,step 1900000, training loss 0.0240154\n",
      "epoch 19,step 1902000, training loss 0.103619\n",
      "epoch 19,step 1904000, training loss 0.0851527\n",
      "epoch 19,step 1906000, training loss 0.0308249\n",
      "epoch 19,step 1908000, training loss 0.0240224\n",
      "epoch 19,step 1910000, training loss 0.0267722\n",
      "epoch 19,step 1912000, training loss 0.0368726\n",
      "epoch 19,step 1914000, training loss 0.036837\n",
      "epoch 19,step 1916000, training loss 0.0429387\n",
      "epoch 19,step 1918000, training loss 0.0310662\n",
      "epoch 19,step 1920000, training loss 0.0353301\n",
      "epoch 19,step 1922000, training loss 0.102832\n",
      "epoch 19,step 1924000, training loss 0.0603945\n",
      "epoch 19,step 1926000, training loss 0.0226205\n",
      "epoch 19,step 1928000, training loss 0.0230423\n",
      "epoch 19,step 1930000, training loss 0.0258018\n",
      "epoch 19,step 1932000, training loss 0.0300429\n",
      "epoch 19,step 1934000, training loss 0.0288288\n",
      "epoch 19,step 1936000, training loss 0.0290179\n",
      "epoch 19,step 1938000, training loss 0.0290291\n",
      "epoch 19,step 1940000, training loss 0.0341472\n",
      "epoch 19,step 1942000, training loss 0.113617\n",
      "epoch 19,step 1944000, training loss 0.0544823\n",
      "epoch 19,step 1946000, training loss 0.0290096\n",
      "epoch 19,step 1948000, training loss 0.0323345\n",
      "epoch 19,step 1950000, training loss 0.0412807\n",
      "epoch 19,step 1952000, training loss 0.0277047\n",
      "epoch 19,step 1954000, training loss 0.0280814\n",
      "epoch 19,step 1956000, training loss 0.0298583\n",
      "epoch 19,step 1958000, training loss 0.0277956\n",
      "epoch 19,step 1960000, training loss 0.0328977\n",
      "epoch 19,step 1962000, training loss 0.093872\n",
      "epoch 19,step 1964000, training loss 0.0591437\n",
      "epoch 19,step 1966000, training loss 0.0443546\n",
      "epoch 19,step 1968000, training loss 0.0307082\n",
      "epoch 19,step 1970000, training loss 0.0248273\n",
      "epoch 19,step 1972000, training loss 0.0597596\n",
      "epoch 19,step 1974000, training loss 0.0293989\n",
      "epoch 19,step 1976000, training loss 0.0343754\n",
      "epoch 19,step 1978000, training loss 0.028814\n",
      "epoch 19,step 1980000, training loss 0.0240543\n",
      "epoch 19,step 1982000, training loss 0.096911\n",
      "epoch 19,step 1984000, training loss 0.0461592\n",
      "epoch 19,step 1986000, training loss 0.0324178\n",
      "epoch 19,step 1988000, training loss 0.0233549\n",
      "epoch 19,step 1990000, training loss 0.0262889\n",
      "epoch 19,step 1992000, training loss 0.029852\n",
      "epoch 19,step 1994000, training loss 0.0224138\n",
      "epoch 19,step 1996000, training loss 0.0296959\n",
      "epoch 19,step 1998000, training loss 0.0431673\n",
      "epoch 19,training loss 0.0431673 ,test loss 0.0450923\n",
      "epoch 20,step 10500, training loss 0.0383995\n",
      "epoch 20,step 21000, training loss 0.0476808\n",
      "epoch 20,step 31500, training loss 0.0404961\n",
      "epoch 20,step 42000, training loss 0.0269275\n",
      "epoch 20,step 52500, training loss 0.0275375\n",
      "epoch 20,step 63000, training loss 0.0275497\n",
      "epoch 20,step 73500, training loss 0.0310141\n",
      "epoch 20,step 84000, training loss 0.0257762\n",
      "epoch 20,step 94500, training loss 0.0322262\n",
      "epoch 20,step 105000, training loss 0.0292244\n",
      "epoch 20,step 115500, training loss 0.0283781\n",
      "epoch 20,step 126000, training loss 0.0391889\n",
      "epoch 20,step 136500, training loss 0.0605424\n",
      "epoch 20,step 147000, training loss 0.0332159\n",
      "epoch 20,step 157500, training loss 0.04515\n",
      "epoch 20,step 168000, training loss 0.0314\n",
      "epoch 20,step 178500, training loss 0.0293368\n",
      "epoch 20,step 189000, training loss 0.025848\n",
      "epoch 20,step 199500, training loss 0.0218182\n",
      "epoch 20,step 210000, training loss 0.0375497\n",
      "epoch 20,step 220500, training loss 0.0288398\n",
      "epoch 20,step 231000, training loss 0.0340608\n",
      "epoch 20,step 241500, training loss 0.0468776\n",
      "epoch 20,step 252000, training loss 0.0381889\n",
      "epoch 20,step 262500, training loss 0.0374057\n",
      "epoch 20,step 273000, training loss 0.0229833\n",
      "epoch 20,step 283500, training loss 0.0497187\n",
      "epoch 20,step 294000, training loss 0.0329095\n",
      "epoch 20,step 304500, training loss 0.0412232\n",
      "epoch 20,step 315000, training loss 0.032404\n",
      "epoch 20,step 325500, training loss 0.0466064\n",
      "epoch 20,step 336000, training loss 0.0298195\n",
      "epoch 20,step 346500, training loss 0.0344723\n",
      "epoch 20,step 357000, training loss 0.0325442\n",
      "epoch 20,step 367500, training loss 0.0442311\n",
      "epoch 20,step 378000, training loss 0.0311143\n",
      "epoch 20,step 388500, training loss 0.0347822\n",
      "epoch 20,step 399000, training loss 0.0360395\n",
      "epoch 20,step 409500, training loss 0.0318042\n",
      "epoch 20,step 420000, training loss 0.0283371\n",
      "epoch 20,step 430500, training loss 0.0371526\n",
      "epoch 20,step 441000, training loss 0.0285101\n",
      "epoch 20,step 451500, training loss 0.0306567\n",
      "epoch 20,step 462000, training loss 0.0339656\n",
      "epoch 20,step 472500, training loss 0.0296587\n",
      "epoch 20,step 483000, training loss 0.0358832\n",
      "epoch 20,step 493500, training loss 0.0233985\n",
      "epoch 20,step 504000, training loss 0.0301557\n",
      "epoch 20,step 514500, training loss 0.0330045\n",
      "epoch 20,step 525000, training loss 0.0306138\n",
      "epoch 20,step 535500, training loss 0.03131\n",
      "epoch 20,step 546000, training loss 0.0373287\n",
      "epoch 20,step 556500, training loss 0.0259989\n",
      "epoch 20,step 567000, training loss 0.0320837\n",
      "epoch 20,step 577500, training loss 0.054399\n",
      "epoch 20,step 588000, training loss 0.028747\n",
      "epoch 20,step 598500, training loss 0.0257975\n",
      "epoch 20,step 609000, training loss 0.0356537\n",
      "epoch 20,step 619500, training loss 0.0385103\n",
      "epoch 20,step 630000, training loss 0.0316319\n",
      "epoch 20,step 640500, training loss 0.0339041\n",
      "epoch 20,step 651000, training loss 0.0335339\n",
      "epoch 20,step 661500, training loss 0.0304758\n",
      "epoch 20,step 672000, training loss 0.0435487\n",
      "epoch 20,step 682500, training loss 0.0488726\n",
      "epoch 20,step 693000, training loss 0.0326615\n",
      "epoch 20,step 703500, training loss 0.0334999\n",
      "epoch 20,step 714000, training loss 0.0466775\n",
      "epoch 20,step 724500, training loss 0.0252529\n",
      "epoch 20,step 735000, training loss 0.0311409\n",
      "epoch 20,step 745500, training loss 0.0391355\n",
      "epoch 20,step 756000, training loss 0.0236776\n",
      "epoch 20,step 766500, training loss 0.0525672\n",
      "epoch 20,step 777000, training loss 0.0452192\n",
      "epoch 20,step 787500, training loss 0.0381849\n",
      "epoch 20,step 798000, training loss 0.0324292\n",
      "epoch 20,step 808500, training loss 0.0346045\n",
      "epoch 20,step 819000, training loss 0.0318497\n",
      "epoch 20,step 829500, training loss 0.0356969\n",
      "epoch 20,step 840000, training loss 0.0225289\n",
      "epoch 20,step 850500, training loss 0.0295445\n",
      "epoch 20,step 861000, training loss 0.0306219\n",
      "epoch 20,step 871500, training loss 0.0308453\n",
      "epoch 20,step 882000, training loss 0.0282313\n",
      "epoch 20,step 892500, training loss 0.0422551\n",
      "epoch 20,step 903000, training loss 0.0334403\n",
      "epoch 20,step 913500, training loss 0.0425527\n",
      "epoch 20,step 924000, training loss 0.0332404\n",
      "epoch 20,step 934500, training loss 0.0230499\n",
      "epoch 20,step 945000, training loss 0.0311873\n",
      "epoch 20,step 955500, training loss 0.0282102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20,step 966000, training loss 0.0323329\n",
      "epoch 20,step 976500, training loss 0.045806\n",
      "epoch 20,step 987000, training loss 0.0335554\n",
      "epoch 20,step 997500, training loss 0.0256133\n",
      "epoch 20,step 1008000, training loss 0.0328555\n",
      "epoch 20,step 1018500, training loss 0.0308283\n",
      "epoch 20,step 1029000, training loss 0.0367136\n",
      "epoch 20,step 1039500, training loss 0.0377921\n",
      "epoch 20,step 1050000, training loss 0.0301383\n",
      "epoch 20,step 1060500, training loss 0.0321007\n",
      "epoch 20,step 1071000, training loss 0.0282181\n",
      "epoch 20,step 1081500, training loss 0.0376199\n",
      "epoch 20,step 1092000, training loss 0.0257681\n",
      "epoch 20,step 1102500, training loss 0.0381446\n",
      "epoch 20,step 1113000, training loss 0.0428391\n",
      "epoch 20,step 1123500, training loss 0.0430754\n",
      "epoch 20,step 1134000, training loss 0.0371992\n",
      "epoch 20,step 1144500, training loss 0.0406183\n",
      "epoch 20,step 1155000, training loss 0.0530689\n",
      "epoch 20,step 1165500, training loss 0.041099\n",
      "epoch 20,step 1176000, training loss 0.0465982\n",
      "epoch 20,step 1186500, training loss 0.0328836\n",
      "epoch 20,step 1197000, training loss 0.0288392\n",
      "epoch 20,step 1207500, training loss 0.0529209\n",
      "epoch 20,step 1218000, training loss 0.0265183\n",
      "epoch 20,step 1228500, training loss 0.0356232\n",
      "epoch 20,step 1239000, training loss 0.0286357\n",
      "epoch 20,step 1249500, training loss 0.0342617\n",
      "epoch 20,step 1260000, training loss 0.0384421\n",
      "epoch 20,step 1270500, training loss 0.0240094\n",
      "epoch 20,step 1281000, training loss 0.0258955\n",
      "epoch 20,step 1291500, training loss 0.0301388\n",
      "epoch 20,step 1302000, training loss 0.0403041\n",
      "epoch 20,step 1312500, training loss 0.0394588\n",
      "epoch 20,step 1323000, training loss 0.0358864\n",
      "epoch 20,step 1333500, training loss 0.0261472\n",
      "epoch 20,step 1344000, training loss 0.0326936\n",
      "epoch 20,step 1354500, training loss 0.0397545\n",
      "epoch 20,step 1365000, training loss 0.0360916\n",
      "epoch 20,step 1375500, training loss 0.0316977\n",
      "epoch 20,step 1386000, training loss 0.0292257\n",
      "epoch 20,step 1396500, training loss 0.0252261\n",
      "epoch 20,step 1407000, training loss 0.0340561\n",
      "epoch 20,step 1417500, training loss 0.0255653\n",
      "epoch 20,step 1428000, training loss 0.0336891\n",
      "epoch 20,step 1438500, training loss 0.0330742\n",
      "epoch 20,step 1449000, training loss 0.0338269\n",
      "epoch 20,step 1459500, training loss 0.0355889\n",
      "epoch 20,step 1470000, training loss 0.030188\n",
      "epoch 20,step 1480500, training loss 0.033076\n",
      "epoch 20,step 1491000, training loss 0.0323844\n",
      "epoch 20,step 1501500, training loss 0.0348669\n",
      "epoch 20,step 1512000, training loss 0.0322205\n",
      "epoch 20,step 1522500, training loss 0.0338416\n",
      "epoch 20,step 1533000, training loss 0.0290906\n",
      "epoch 20,step 1543500, training loss 0.0320292\n",
      "epoch 20,step 1554000, training loss 0.0430472\n",
      "epoch 20,step 1564500, training loss 0.0285966\n",
      "epoch 20,step 1575000, training loss 0.0679458\n",
      "epoch 20,step 1585500, training loss 0.028341\n",
      "epoch 20,step 1596000, training loss 0.0262133\n",
      "epoch 20,step 1606500, training loss 0.0446302\n",
      "epoch 20,step 1617000, training loss 0.0219494\n",
      "epoch 20,step 1627500, training loss 0.0340188\n",
      "epoch 20,step 1638000, training loss 0.024894\n",
      "epoch 20,step 1648500, training loss 0.0324042\n",
      "epoch 20,step 1659000, training loss 0.0351813\n",
      "epoch 20,step 1669500, training loss 0.0334005\n",
      "epoch 20,step 1680000, training loss 0.0278261\n",
      "epoch 20,step 1690500, training loss 0.0339706\n",
      "epoch 20,step 1701000, training loss 0.028695\n",
      "epoch 20,step 1711500, training loss 0.0401594\n",
      "epoch 20,step 1722000, training loss 0.0361151\n",
      "epoch 20,step 1732500, training loss 0.0489009\n",
      "epoch 20,step 1743000, training loss 0.0363542\n",
      "epoch 20,step 1753500, training loss 0.0321734\n",
      "epoch 20,step 1764000, training loss 0.0310255\n",
      "epoch 20,step 1774500, training loss 0.0415873\n",
      "epoch 20,step 1785000, training loss 0.0363749\n",
      "epoch 20,step 1795500, training loss 0.0431031\n",
      "epoch 20,step 1806000, training loss 0.0489272\n",
      "epoch 20,step 1816500, training loss 0.0300016\n",
      "epoch 20,step 1827000, training loss 0.0380789\n",
      "epoch 20,step 1837500, training loss 0.0318633\n",
      "epoch 20,step 1848000, training loss 0.0324225\n",
      "epoch 20,step 1858500, training loss 0.0322586\n",
      "epoch 20,step 1869000, training loss 0.02577\n",
      "epoch 20,step 1879500, training loss 0.0426996\n",
      "epoch 20,step 1890000, training loss 0.0692626\n",
      "epoch 20,step 1900500, training loss 0.0298679\n",
      "epoch 20,step 1911000, training loss 0.0295481\n",
      "epoch 20,step 1921500, training loss 0.0401965\n",
      "epoch 20,step 1932000, training loss 0.0324727\n",
      "epoch 20,step 1942500, training loss 0.0247604\n",
      "epoch 20,step 1953000, training loss 0.0264697\n",
      "epoch 20,step 1963500, training loss 0.0454501\n",
      "epoch 20,step 1974000, training loss 0.0259308\n",
      "epoch 20,step 1984500, training loss 0.0331698\n",
      "epoch 20,step 1995000, training loss 0.0234549\n",
      "epoch 20,step 2005500, training loss 0.0267276\n",
      "epoch 20,step 2016000, training loss 0.0345486\n",
      "epoch 20,step 2026500, training loss 0.0255169\n",
      "epoch 20,step 2037000, training loss 0.0330097\n",
      "epoch 20,step 2047500, training loss 0.0402537\n",
      "epoch 20,step 2058000, training loss 0.0327318\n",
      "epoch 20,step 2068500, training loss 0.0234687\n",
      "epoch 20,step 2079000, training loss 0.0231833\n",
      "epoch 20,step 2089500, training loss 0.025682\n",
      "epoch 20,training loss 0.041086 ,test loss 0.0444279\n",
      "epoch 21,step 11000, training loss 0.0373797\n",
      "epoch 21,step 22000, training loss 0.0458306\n",
      "epoch 21,step 33000, training loss 0.0398084\n",
      "epoch 21,step 44000, training loss 0.0252094\n",
      "epoch 21,step 55000, training loss 0.0265361\n",
      "epoch 21,step 66000, training loss 0.0274865\n",
      "epoch 21,step 77000, training loss 0.0312922\n",
      "epoch 21,step 88000, training loss 0.0259351\n",
      "epoch 21,step 99000, training loss 0.032318\n",
      "epoch 21,step 110000, training loss 0.0298356\n",
      "epoch 21,step 121000, training loss 0.0298344\n",
      "epoch 21,step 132000, training loss 0.0386755\n",
      "epoch 21,step 143000, training loss 0.0613535\n",
      "epoch 21,step 154000, training loss 0.0314149\n",
      "epoch 21,step 165000, training loss 0.0461911\n",
      "epoch 21,step 176000, training loss 0.0298767\n",
      "epoch 21,step 187000, training loss 0.0283931\n",
      "epoch 21,step 198000, training loss 0.0255872\n",
      "epoch 21,step 209000, training loss 0.02128\n",
      "epoch 21,step 220000, training loss 0.0360745\n",
      "epoch 21,step 231000, training loss 0.0280069\n",
      "epoch 21,step 242000, training loss 0.0343263\n",
      "epoch 21,step 253000, training loss 0.0446017\n",
      "epoch 21,step 264000, training loss 0.0369955\n",
      "epoch 21,step 275000, training loss 0.0373488\n",
      "epoch 21,step 286000, training loss 0.0218538\n",
      "epoch 21,step 297000, training loss 0.0507531\n",
      "epoch 21,step 308000, training loss 0.0324182\n",
      "epoch 21,step 319000, training loss 0.0390303\n",
      "epoch 21,step 330000, training loss 0.0313455\n",
      "epoch 21,step 341000, training loss 0.0472701\n",
      "epoch 21,step 352000, training loss 0.0296212\n",
      "epoch 21,step 363000, training loss 0.0337637\n",
      "epoch 21,step 374000, training loss 0.0313937\n",
      "epoch 21,step 385000, training loss 0.0442505\n",
      "epoch 21,step 396000, training loss 0.0310363\n",
      "epoch 21,step 407000, training loss 0.0356505\n",
      "epoch 21,step 418000, training loss 0.0351835\n",
      "epoch 21,step 429000, training loss 0.0303175\n",
      "epoch 21,step 440000, training loss 0.0266764\n",
      "epoch 21,step 451000, training loss 0.037222\n",
      "epoch 21,step 462000, training loss 0.0283722\n",
      "epoch 21,step 473000, training loss 0.0311321\n",
      "epoch 21,step 484000, training loss 0.0344658\n",
      "epoch 21,step 495000, training loss 0.0283228\n",
      "epoch 21,step 506000, training loss 0.0372005\n",
      "epoch 21,step 517000, training loss 0.0232365\n",
      "epoch 21,step 528000, training loss 0.0303626\n",
      "epoch 21,step 539000, training loss 0.0328577\n",
      "epoch 21,step 550000, training loss 0.0298259\n",
      "epoch 21,step 561000, training loss 0.031509\n",
      "epoch 21,step 572000, training loss 0.0371689\n",
      "epoch 21,step 583000, training loss 0.0251277\n",
      "epoch 21,step 594000, training loss 0.0321011\n",
      "epoch 21,step 605000, training loss 0.0537815\n",
      "epoch 21,step 616000, training loss 0.0274305\n",
      "epoch 21,step 627000, training loss 0.0247774\n",
      "epoch 21,step 638000, training loss 0.0347603\n",
      "epoch 21,step 649000, training loss 0.0397189\n",
      "epoch 21,step 660000, training loss 0.0317565\n",
      "epoch 21,step 671000, training loss 0.0335507\n",
      "epoch 21,step 682000, training loss 0.0324429\n",
      "epoch 21,step 693000, training loss 0.0306088\n",
      "epoch 21,step 704000, training loss 0.0425284\n",
      "epoch 21,step 715000, training loss 0.0486855\n",
      "epoch 21,step 726000, training loss 0.0324344\n",
      "epoch 21,step 737000, training loss 0.0317268\n",
      "epoch 21,step 748000, training loss 0.0453836\n",
      "epoch 21,step 759000, training loss 0.0248254\n",
      "epoch 21,step 770000, training loss 0.0303874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21,step 781000, training loss 0.039228\n",
      "epoch 21,step 792000, training loss 0.0234273\n",
      "epoch 21,step 803000, training loss 0.0503848\n",
      "epoch 21,step 814000, training loss 0.0428574\n",
      "epoch 21,step 825000, training loss 0.0365453\n",
      "epoch 21,step 836000, training loss 0.0312754\n",
      "epoch 21,step 847000, training loss 0.0333311\n",
      "epoch 21,step 858000, training loss 0.0304734\n",
      "epoch 21,step 869000, training loss 0.0343894\n",
      "epoch 21,step 880000, training loss 0.0215507\n",
      "epoch 21,step 891000, training loss 0.0269143\n",
      "epoch 21,step 902000, training loss 0.0294373\n",
      "epoch 21,step 913000, training loss 0.02995\n",
      "epoch 21,step 924000, training loss 0.028534\n",
      "epoch 21,step 935000, training loss 0.0397399\n",
      "epoch 21,step 946000, training loss 0.0322771\n",
      "epoch 21,step 957000, training loss 0.0410921\n",
      "epoch 21,step 968000, training loss 0.0322854\n",
      "epoch 21,step 979000, training loss 0.023709\n",
      "epoch 21,step 990000, training loss 0.030614\n",
      "epoch 21,step 1001000, training loss 0.028529\n",
      "epoch 21,step 1012000, training loss 0.0310552\n",
      "epoch 21,step 1023000, training loss 0.0454105\n",
      "epoch 21,step 1034000, training loss 0.033121\n",
      "epoch 21,step 1045000, training loss 0.0248661\n",
      "epoch 21,step 1056000, training loss 0.0331567\n",
      "epoch 21,step 1067000, training loss 0.0296779\n",
      "epoch 21,step 1078000, training loss 0.0360196\n",
      "epoch 21,step 1089000, training loss 0.0376367\n",
      "epoch 21,step 1100000, training loss 0.0293739\n",
      "epoch 21,step 1111000, training loss 0.0324062\n",
      "epoch 21,step 1122000, training loss 0.0273627\n",
      "epoch 21,step 1133000, training loss 0.0361589\n",
      "epoch 21,step 1144000, training loss 0.024658\n",
      "epoch 21,step 1155000, training loss 0.037513\n",
      "epoch 21,step 1166000, training loss 0.0433816\n",
      "epoch 21,step 1177000, training loss 0.0421544\n",
      "epoch 21,step 1188000, training loss 0.0360676\n",
      "epoch 21,step 1199000, training loss 0.0383622\n",
      "epoch 21,step 1210000, training loss 0.0513055\n",
      "epoch 21,step 1221000, training loss 0.0397862\n",
      "epoch 21,step 1232000, training loss 0.042937\n",
      "epoch 21,step 1243000, training loss 0.0325303\n",
      "epoch 21,step 1254000, training loss 0.028016\n",
      "epoch 21,step 1265000, training loss 0.0523959\n",
      "epoch 21,step 1276000, training loss 0.025556\n",
      "epoch 21,step 1287000, training loss 0.0342502\n",
      "epoch 21,step 1298000, training loss 0.0277926\n",
      "epoch 21,step 1309000, training loss 0.0325391\n",
      "epoch 21,step 1320000, training loss 0.0383085\n",
      "epoch 21,step 1331000, training loss 0.0242669\n",
      "epoch 21,step 1342000, training loss 0.0246613\n",
      "epoch 21,step 1353000, training loss 0.0291953\n",
      "epoch 21,step 1364000, training loss 0.0401892\n",
      "epoch 21,step 1375000, training loss 0.0387779\n",
      "epoch 21,step 1386000, training loss 0.0355804\n",
      "epoch 21,step 1397000, training loss 0.0251865\n",
      "epoch 21,step 1408000, training loss 0.0322965\n",
      "epoch 21,step 1419000, training loss 0.0386907\n",
      "epoch 21,step 1430000, training loss 0.0344705\n",
      "epoch 21,step 1441000, training loss 0.0302615\n",
      "epoch 21,step 1452000, training loss 0.0282264\n",
      "epoch 21,step 1463000, training loss 0.0242669\n",
      "epoch 21,step 1474000, training loss 0.0328218\n",
      "epoch 21,step 1485000, training loss 0.0243178\n",
      "epoch 21,step 1496000, training loss 0.0332635\n",
      "epoch 21,step 1507000, training loss 0.0322446\n",
      "epoch 21,step 1518000, training loss 0.0327556\n",
      "epoch 21,step 1529000, training loss 0.0358472\n",
      "epoch 21,step 1540000, training loss 0.0294412\n",
      "epoch 21,step 1551000, training loss 0.0331509\n",
      "epoch 21,step 1562000, training loss 0.0312461\n",
      "epoch 21,step 1573000, training loss 0.0339793\n",
      "epoch 21,step 1584000, training loss 0.0311771\n",
      "epoch 21,step 1595000, training loss 0.0327683\n",
      "epoch 21,step 1606000, training loss 0.0288771\n",
      "epoch 21,step 1617000, training loss 0.0316766\n",
      "epoch 21,step 1628000, training loss 0.0422645\n",
      "epoch 21,step 1639000, training loss 0.0283228\n",
      "epoch 21,step 1650000, training loss 0.0651085\n",
      "epoch 21,step 1661000, training loss 0.0277929\n",
      "epoch 21,step 1672000, training loss 0.0257693\n",
      "epoch 21,step 1683000, training loss 0.0439076\n",
      "epoch 21,step 1694000, training loss 0.0209433\n",
      "epoch 21,step 1705000, training loss 0.032669\n",
      "epoch 21,step 1716000, training loss 0.023739\n",
      "epoch 21,step 1727000, training loss 0.0308958\n",
      "epoch 21,step 1738000, training loss 0.0336617\n",
      "epoch 21,step 1749000, training loss 0.0327185\n",
      "epoch 21,step 1760000, training loss 0.0278113\n",
      "epoch 21,step 1771000, training loss 0.0341399\n",
      "epoch 21,step 1782000, training loss 0.0272044\n",
      "epoch 21,step 1793000, training loss 0.0388825\n",
      "epoch 21,step 1804000, training loss 0.035589\n",
      "epoch 21,step 1815000, training loss 0.0481373\n",
      "epoch 21,step 1826000, training loss 0.0362229\n",
      "epoch 21,step 1837000, training loss 0.0324532\n",
      "epoch 21,step 1848000, training loss 0.0307\n",
      "epoch 21,step 1859000, training loss 0.0392923\n",
      "epoch 21,step 1870000, training loss 0.035531\n",
      "epoch 21,step 1881000, training loss 0.0423777\n",
      "epoch 21,step 1892000, training loss 0.0472727\n",
      "epoch 21,step 1903000, training loss 0.0304963\n",
      "epoch 21,step 1914000, training loss 0.0382952\n",
      "epoch 21,step 1925000, training loss 0.0307397\n",
      "epoch 21,step 1936000, training loss 0.0305852\n",
      "epoch 21,step 1947000, training loss 0.0304325\n",
      "epoch 21,step 1958000, training loss 0.0247601\n",
      "epoch 21,step 1969000, training loss 0.0399994\n",
      "epoch 21,step 1980000, training loss 0.0670081\n",
      "epoch 21,step 1991000, training loss 0.0303418\n",
      "epoch 21,step 2002000, training loss 0.0289645\n",
      "epoch 21,step 2013000, training loss 0.038031\n",
      "epoch 21,step 2024000, training loss 0.0322362\n",
      "epoch 21,step 2035000, training loss 0.0256034\n",
      "epoch 21,step 2046000, training loss 0.025685\n",
      "epoch 21,step 2057000, training loss 0.0442342\n",
      "epoch 21,step 2068000, training loss 0.0261556\n",
      "epoch 21,step 2079000, training loss 0.0326372\n",
      "epoch 21,step 2090000, training loss 0.0234528\n",
      "epoch 21,step 2101000, training loss 0.0266179\n",
      "epoch 21,step 2112000, training loss 0.034424\n",
      "epoch 21,step 2123000, training loss 0.0247178\n",
      "epoch 21,step 2134000, training loss 0.0328144\n",
      "epoch 21,step 2145000, training loss 0.040101\n",
      "epoch 21,step 2156000, training loss 0.0322955\n",
      "epoch 21,step 2167000, training loss 0.0244155\n",
      "epoch 21,step 2178000, training loss 0.0232109\n",
      "epoch 21,step 2189000, training loss 0.0265983\n",
      "epoch 21,training loss 0.0400936 ,test loss 0.0439503\n",
      "epoch 22,step 11500, training loss 0.0371929\n",
      "epoch 22,step 23000, training loss 0.0470296\n",
      "epoch 22,step 34500, training loss 0.0392403\n",
      "epoch 22,step 46000, training loss 0.0252232\n",
      "epoch 22,step 57500, training loss 0.0260885\n",
      "epoch 22,step 69000, training loss 0.026685\n",
      "epoch 22,step 80500, training loss 0.0296629\n",
      "epoch 22,step 92000, training loss 0.0250133\n",
      "epoch 22,step 103500, training loss 0.0317213\n",
      "epoch 22,step 115000, training loss 0.0286288\n",
      "epoch 22,step 126500, training loss 0.028967\n",
      "epoch 22,step 138000, training loss 0.0386656\n",
      "epoch 22,step 149500, training loss 0.0591866\n",
      "epoch 22,step 161000, training loss 0.0303411\n",
      "epoch 22,step 172500, training loss 0.0441311\n",
      "epoch 22,step 184000, training loss 0.0293588\n",
      "epoch 22,step 195500, training loss 0.0291257\n",
      "epoch 22,step 207000, training loss 0.0255315\n",
      "epoch 22,step 218500, training loss 0.0220645\n",
      "epoch 22,step 230000, training loss 0.0362156\n",
      "epoch 22,step 241500, training loss 0.0274252\n",
      "epoch 22,step 253000, training loss 0.0336228\n",
      "epoch 22,step 264500, training loss 0.0441591\n",
      "epoch 22,step 276000, training loss 0.0361841\n",
      "epoch 22,step 287500, training loss 0.0349301\n",
      "epoch 22,step 299000, training loss 0.0210812\n",
      "epoch 22,step 310500, training loss 0.0494757\n",
      "epoch 22,step 322000, training loss 0.0308425\n",
      "epoch 22,step 333500, training loss 0.0384872\n",
      "epoch 22,step 345000, training loss 0.0312696\n",
      "epoch 22,step 356500, training loss 0.0465545\n",
      "epoch 22,step 368000, training loss 0.0291433\n",
      "epoch 22,step 379500, training loss 0.0322477\n",
      "epoch 22,step 391000, training loss 0.0305514\n",
      "epoch 22,step 402500, training loss 0.0434174\n",
      "epoch 22,step 414000, training loss 0.0309833\n",
      "epoch 22,step 425500, training loss 0.0346916\n",
      "epoch 22,step 437000, training loss 0.0343141\n",
      "epoch 22,step 448500, training loss 0.0303312\n",
      "epoch 22,step 460000, training loss 0.0257836\n",
      "epoch 22,step 471500, training loss 0.0353146\n",
      "epoch 22,step 483000, training loss 0.0273628\n",
      "epoch 22,step 494500, training loss 0.0329013\n",
      "epoch 22,step 506000, training loss 0.0343114\n",
      "epoch 22,step 517500, training loss 0.0269716\n",
      "epoch 22,step 529000, training loss 0.0355254\n",
      "epoch 22,step 540500, training loss 0.0221913\n",
      "epoch 22,step 552000, training loss 0.0293185\n",
      "epoch 22,step 563500, training loss 0.0317251\n",
      "epoch 22,step 575000, training loss 0.0284593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22,step 586500, training loss 0.0308908\n",
      "epoch 22,step 598000, training loss 0.035183\n",
      "epoch 22,step 609500, training loss 0.0247945\n",
      "epoch 22,step 621000, training loss 0.031365\n",
      "epoch 22,step 632500, training loss 0.0533018\n",
      "epoch 22,step 644000, training loss 0.0270751\n",
      "epoch 22,step 655500, training loss 0.0243657\n",
      "epoch 22,step 667000, training loss 0.0334525\n",
      "epoch 22,step 678500, training loss 0.0384372\n",
      "epoch 22,step 690000, training loss 0.0308627\n",
      "epoch 22,step 701500, training loss 0.0344026\n",
      "epoch 22,step 713000, training loss 0.0332605\n",
      "epoch 22,step 724500, training loss 0.0304074\n",
      "epoch 22,step 736000, training loss 0.0421144\n",
      "epoch 22,step 747500, training loss 0.0472315\n",
      "epoch 22,step 759000, training loss 0.0314799\n",
      "epoch 22,step 770500, training loss 0.0323256\n",
      "epoch 22,step 782000, training loss 0.0460597\n",
      "epoch 22,step 793500, training loss 0.0245979\n",
      "epoch 22,step 805000, training loss 0.0313778\n",
      "epoch 22,step 816500, training loss 0.0381664\n",
      "epoch 22,step 828000, training loss 0.0228723\n",
      "epoch 22,step 839500, training loss 0.0483806\n",
      "epoch 22,step 851000, training loss 0.0417509\n",
      "epoch 22,step 862500, training loss 0.0372864\n",
      "epoch 22,step 874000, training loss 0.0311962\n",
      "epoch 22,step 885500, training loss 0.0340433\n",
      "epoch 22,step 897000, training loss 0.0297754\n",
      "epoch 22,step 908500, training loss 0.0340909\n",
      "epoch 22,step 920000, training loss 0.021752\n",
      "epoch 22,step 931500, training loss 0.0266159\n",
      "epoch 22,step 943000, training loss 0.0289914\n",
      "epoch 22,step 954500, training loss 0.0291961\n",
      "epoch 22,step 966000, training loss 0.0272414\n",
      "epoch 22,step 977500, training loss 0.0396164\n",
      "epoch 22,step 989000, training loss 0.032368\n",
      "epoch 22,step 1000500, training loss 0.0421032\n",
      "epoch 22,step 1012000, training loss 0.031904\n",
      "epoch 22,step 1023500, training loss 0.0222859\n",
      "epoch 22,step 1035000, training loss 0.0306059\n",
      "epoch 22,step 1046500, training loss 0.0268653\n",
      "epoch 22,step 1058000, training loss 0.0301393\n",
      "epoch 22,step 1069500, training loss 0.0441129\n",
      "epoch 22,step 1081000, training loss 0.0333998\n",
      "epoch 22,step 1092500, training loss 0.0240907\n",
      "epoch 22,step 1104000, training loss 0.032137\n",
      "epoch 22,step 1115500, training loss 0.0290932\n",
      "epoch 22,step 1127000, training loss 0.0358763\n",
      "epoch 22,step 1138500, training loss 0.037046\n",
      "epoch 22,step 1150000, training loss 0.0281808\n",
      "epoch 22,step 1161500, training loss 0.0302162\n",
      "epoch 22,step 1173000, training loss 0.0258692\n",
      "epoch 22,step 1184500, training loss 0.0352392\n",
      "epoch 22,step 1196000, training loss 0.0243522\n",
      "epoch 22,step 1207500, training loss 0.0362148\n",
      "epoch 22,step 1219000, training loss 0.0412267\n",
      "epoch 22,step 1230500, training loss 0.0420952\n",
      "epoch 22,step 1242000, training loss 0.0354906\n",
      "epoch 22,step 1253500, training loss 0.0400268\n",
      "epoch 22,step 1265000, training loss 0.0509567\n",
      "epoch 22,step 1276500, training loss 0.0399135\n",
      "epoch 22,step 1288000, training loss 0.0425086\n",
      "epoch 22,step 1299500, training loss 0.0308897\n",
      "epoch 22,step 1311000, training loss 0.026464\n",
      "epoch 22,step 1322500, training loss 0.0516115\n",
      "epoch 22,step 1334000, training loss 0.0248661\n",
      "epoch 22,step 1345500, training loss 0.0339051\n",
      "epoch 22,step 1357000, training loss 0.0264236\n",
      "epoch 22,step 1368500, training loss 0.0331174\n",
      "epoch 22,step 1380000, training loss 0.0381874\n",
      "epoch 22,step 1391500, training loss 0.0238837\n",
      "epoch 22,step 1403000, training loss 0.0240045\n",
      "epoch 22,step 1414500, training loss 0.0288761\n",
      "epoch 22,step 1426000, training loss 0.0399735\n",
      "epoch 22,step 1437500, training loss 0.0393691\n",
      "epoch 22,step 1449000, training loss 0.0360309\n",
      "epoch 22,step 1460500, training loss 0.0246226\n",
      "epoch 22,step 1472000, training loss 0.0316949\n",
      "epoch 22,step 1483500, training loss 0.0374926\n",
      "epoch 22,step 1495000, training loss 0.0335004\n",
      "epoch 22,step 1506500, training loss 0.0296795\n",
      "epoch 22,step 1518000, training loss 0.0274802\n",
      "epoch 22,step 1529500, training loss 0.0244126\n",
      "epoch 22,step 1541000, training loss 0.0327502\n",
      "epoch 22,step 1552500, training loss 0.0240681\n",
      "epoch 22,step 1564000, training loss 0.032349\n",
      "epoch 22,step 1575500, training loss 0.0311036\n",
      "epoch 22,step 1587000, training loss 0.0336617\n",
      "epoch 22,step 1598500, training loss 0.0349739\n",
      "epoch 22,step 1610000, training loss 0.0290441\n",
      "epoch 22,step 1621500, training loss 0.032354\n",
      "epoch 22,step 1633000, training loss 0.0309734\n",
      "epoch 22,step 1644500, training loss 0.0345385\n",
      "epoch 22,step 1656000, training loss 0.0298523\n",
      "epoch 22,step 1667500, training loss 0.0339223\n",
      "epoch 22,step 1679000, training loss 0.0286668\n",
      "epoch 22,step 1690500, training loss 0.0313425\n",
      "epoch 22,step 1702000, training loss 0.0417861\n",
      "epoch 22,step 1713500, training loss 0.0284173\n",
      "epoch 22,step 1725000, training loss 0.0641837\n",
      "epoch 22,step 1736500, training loss 0.0262672\n",
      "epoch 22,step 1748000, training loss 0.0262812\n",
      "epoch 22,step 1759500, training loss 0.0427103\n",
      "epoch 22,step 1771000, training loss 0.0207449\n",
      "epoch 22,step 1782500, training loss 0.0328513\n",
      "epoch 22,step 1794000, training loss 0.0238396\n",
      "epoch 22,step 1805500, training loss 0.0321589\n",
      "epoch 22,step 1817000, training loss 0.0331281\n",
      "epoch 22,step 1828500, training loss 0.0331725\n",
      "epoch 22,step 1840000, training loss 0.0273073\n",
      "epoch 22,step 1851500, training loss 0.0332939\n",
      "epoch 22,step 1863000, training loss 0.0267007\n",
      "epoch 22,step 1874500, training loss 0.0387335\n",
      "epoch 22,step 1886000, training loss 0.0371345\n",
      "epoch 22,step 1897500, training loss 0.0483307\n",
      "epoch 22,step 1909000, training loss 0.0369407\n",
      "epoch 22,step 1920500, training loss 0.0309487\n",
      "epoch 22,step 1932000, training loss 0.0299594\n",
      "epoch 22,step 1943500, training loss 0.0405843\n",
      "epoch 22,step 1955000, training loss 0.0358889\n",
      "epoch 22,step 1966500, training loss 0.0434755\n",
      "epoch 22,step 1978000, training loss 0.0474407\n",
      "epoch 22,step 1989500, training loss 0.0309368\n",
      "epoch 22,step 2001000, training loss 0.0378787\n",
      "epoch 22,step 2012500, training loss 0.0303866\n",
      "epoch 22,step 2024000, training loss 0.030836\n",
      "epoch 22,step 2035500, training loss 0.030775\n",
      "epoch 22,step 2047000, training loss 0.0248451\n",
      "epoch 22,step 2058500, training loss 0.0394422\n",
      "epoch 22,step 2070000, training loss 0.0664361\n",
      "epoch 22,step 2081500, training loss 0.0296357\n",
      "epoch 22,step 2093000, training loss 0.028884\n",
      "epoch 22,step 2104500, training loss 0.0390282\n",
      "epoch 22,step 2116000, training loss 0.0319516\n",
      "epoch 22,step 2127500, training loss 0.0248808\n",
      "epoch 22,step 2139000, training loss 0.0252199\n",
      "epoch 22,step 2150500, training loss 0.0424544\n",
      "epoch 22,step 2162000, training loss 0.025857\n",
      "epoch 22,step 2173500, training loss 0.0322006\n",
      "epoch 22,step 2185000, training loss 0.0232241\n",
      "epoch 22,step 2196500, training loss 0.0263773\n",
      "epoch 22,step 2208000, training loss 0.0338439\n",
      "epoch 22,step 2219500, training loss 0.0253308\n",
      "epoch 22,step 2231000, training loss 0.0328034\n",
      "epoch 22,step 2242500, training loss 0.0390308\n",
      "epoch 22,step 2254000, training loss 0.0304808\n",
      "epoch 22,step 2265500, training loss 0.0231823\n",
      "epoch 22,step 2277000, training loss 0.0234544\n",
      "epoch 22,step 2288500, training loss 0.0259147\n",
      "epoch 22,training loss 0.0401338 ,test loss 0.0434075\n",
      "epoch 23,step 12000, training loss 0.0371016\n",
      "epoch 23,step 24000, training loss 0.0464199\n",
      "epoch 23,step 36000, training loss 0.0370116\n",
      "epoch 23,step 48000, training loss 0.0240939\n",
      "epoch 23,step 60000, training loss 0.0249001\n",
      "epoch 23,step 72000, training loss 0.0254048\n",
      "epoch 23,step 84000, training loss 0.0289744\n",
      "epoch 23,step 96000, training loss 0.0244856\n",
      "epoch 23,step 108000, training loss 0.0305452\n",
      "epoch 23,step 120000, training loss 0.0281547\n",
      "epoch 23,step 132000, training loss 0.0277023\n",
      "epoch 23,step 144000, training loss 0.0382006\n",
      "epoch 23,step 156000, training loss 0.0583677\n",
      "epoch 23,step 168000, training loss 0.0303902\n",
      "epoch 23,step 180000, training loss 0.0447162\n",
      "epoch 23,step 192000, training loss 0.0292168\n",
      "epoch 23,step 204000, training loss 0.0286148\n",
      "epoch 23,step 216000, training loss 0.024699\n",
      "epoch 23,step 228000, training loss 0.0212662\n",
      "epoch 23,step 240000, training loss 0.0361491\n",
      "epoch 23,step 252000, training loss 0.0276745\n",
      "epoch 23,step 264000, training loss 0.0331231\n",
      "epoch 23,step 276000, training loss 0.043974\n",
      "epoch 23,step 288000, training loss 0.0345186\n",
      "epoch 23,step 300000, training loss 0.0350077\n",
      "epoch 23,step 312000, training loss 0.0200484\n",
      "epoch 23,step 324000, training loss 0.047778\n",
      "epoch 23,step 336000, training loss 0.0295239\n",
      "epoch 23,step 348000, training loss 0.0385422\n",
      "epoch 23,step 360000, training loss 0.0314172\n",
      "epoch 23,step 372000, training loss 0.0442484\n",
      "epoch 23,step 384000, training loss 0.0282333\n",
      "epoch 23,step 396000, training loss 0.0316074\n",
      "epoch 23,step 408000, training loss 0.0310001\n",
      "epoch 23,step 420000, training loss 0.0426114\n",
      "epoch 23,step 432000, training loss 0.0308513\n",
      "epoch 23,step 444000, training loss 0.0361651\n",
      "epoch 23,step 456000, training loss 0.0347016\n",
      "epoch 23,step 468000, training loss 0.0314581\n",
      "epoch 23,step 480000, training loss 0.0262116\n",
      "epoch 23,step 492000, training loss 0.0361123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23,step 504000, training loss 0.0274574\n",
      "epoch 23,step 516000, training loss 0.0314811\n",
      "epoch 23,step 528000, training loss 0.034081\n",
      "epoch 23,step 540000, training loss 0.0275432\n",
      "epoch 23,step 552000, training loss 0.0353151\n",
      "epoch 23,step 564000, training loss 0.0213143\n",
      "epoch 23,step 576000, training loss 0.028902\n",
      "epoch 23,step 588000, training loss 0.0315937\n",
      "epoch 23,step 600000, training loss 0.0277781\n",
      "epoch 23,step 612000, training loss 0.030644\n",
      "epoch 23,step 624000, training loss 0.0349511\n",
      "epoch 23,step 636000, training loss 0.0239246\n",
      "epoch 23,step 648000, training loss 0.030053\n",
      "epoch 23,step 660000, training loss 0.0518435\n",
      "epoch 23,step 672000, training loss 0.0265481\n",
      "epoch 23,step 684000, training loss 0.0236447\n",
      "epoch 23,step 696000, training loss 0.0337851\n",
      "epoch 23,step 708000, training loss 0.0392712\n",
      "epoch 23,step 720000, training loss 0.0313223\n",
      "epoch 23,step 732000, training loss 0.0330366\n",
      "epoch 23,step 744000, training loss 0.0319438\n",
      "epoch 23,step 756000, training loss 0.0293178\n",
      "epoch 23,step 768000, training loss 0.0408575\n",
      "epoch 23,step 780000, training loss 0.0451221\n",
      "epoch 23,step 792000, training loss 0.0296417\n",
      "epoch 23,step 804000, training loss 0.0314211\n",
      "epoch 23,step 816000, training loss 0.0421887\n",
      "epoch 23,step 828000, training loss 0.023134\n",
      "epoch 23,step 840000, training loss 0.0302362\n",
      "epoch 23,step 852000, training loss 0.0393521\n",
      "epoch 23,step 864000, training loss 0.021967\n",
      "epoch 23,step 876000, training loss 0.0472997\n",
      "epoch 23,step 888000, training loss 0.0403856\n",
      "epoch 23,step 900000, training loss 0.0353503\n",
      "epoch 23,step 912000, training loss 0.0303817\n",
      "epoch 23,step 924000, training loss 0.0318422\n",
      "epoch 23,step 936000, training loss 0.0284695\n",
      "epoch 23,step 948000, training loss 0.0344468\n",
      "epoch 23,step 960000, training loss 0.0215638\n",
      "epoch 23,step 972000, training loss 0.0264256\n",
      "epoch 23,step 984000, training loss 0.028968\n",
      "epoch 23,step 996000, training loss 0.0286652\n",
      "epoch 23,step 1008000, training loss 0.0261793\n",
      "epoch 23,step 1020000, training loss 0.0401267\n",
      "epoch 23,step 1032000, training loss 0.031191\n",
      "epoch 23,step 1044000, training loss 0.0398823\n",
      "epoch 23,step 1056000, training loss 0.0306748\n",
      "epoch 23,step 1068000, training loss 0.0218018\n",
      "epoch 23,step 1080000, training loss 0.0307542\n",
      "epoch 23,step 1092000, training loss 0.0263216\n",
      "epoch 23,step 1104000, training loss 0.0287972\n",
      "epoch 23,step 1116000, training loss 0.0423171\n",
      "epoch 23,step 1128000, training loss 0.0315092\n",
      "epoch 23,step 1140000, training loss 0.0235238\n",
      "epoch 23,step 1152000, training loss 0.0323448\n",
      "epoch 23,step 1164000, training loss 0.0284141\n",
      "epoch 23,step 1176000, training loss 0.0350194\n",
      "epoch 23,step 1188000, training loss 0.0365911\n",
      "epoch 23,step 1200000, training loss 0.0284958\n",
      "epoch 23,step 1212000, training loss 0.0307524\n",
      "epoch 23,step 1224000, training loss 0.0266636\n",
      "epoch 23,step 1236000, training loss 0.0364174\n",
      "epoch 23,step 1248000, training loss 0.0246477\n",
      "epoch 23,step 1260000, training loss 0.0369154\n",
      "epoch 23,step 1272000, training loss 0.0411766\n",
      "epoch 23,step 1284000, training loss 0.0416666\n",
      "epoch 23,step 1296000, training loss 0.0343456\n",
      "epoch 23,step 1308000, training loss 0.0409485\n",
      "epoch 23,step 1320000, training loss 0.0503099\n",
      "epoch 23,step 1332000, training loss 0.038114\n",
      "epoch 23,step 1344000, training loss 0.0408016\n",
      "epoch 23,step 1356000, training loss 0.0320313\n",
      "epoch 23,step 1368000, training loss 0.0277478\n",
      "epoch 23,step 1380000, training loss 0.0519689\n",
      "epoch 23,step 1392000, training loss 0.0245981\n",
      "epoch 23,step 1404000, training loss 0.0315575\n",
      "epoch 23,step 1416000, training loss 0.0256345\n",
      "epoch 23,step 1428000, training loss 0.0345376\n",
      "epoch 23,step 1440000, training loss 0.0356164\n",
      "epoch 23,step 1452000, training loss 0.0235571\n",
      "epoch 23,step 1464000, training loss 0.0242586\n",
      "epoch 23,step 1476000, training loss 0.0295029\n",
      "epoch 23,step 1488000, training loss 0.0389062\n",
      "epoch 23,step 1500000, training loss 0.0386743\n",
      "epoch 23,step 1512000, training loss 0.0324566\n",
      "epoch 23,step 1524000, training loss 0.0248897\n",
      "epoch 23,step 1536000, training loss 0.0321262\n",
      "epoch 23,step 1548000, training loss 0.0383948\n",
      "epoch 23,step 1560000, training loss 0.0348764\n",
      "epoch 23,step 1572000, training loss 0.0311208\n",
      "epoch 23,step 1584000, training loss 0.0277922\n",
      "epoch 23,step 1596000, training loss 0.0236922\n",
      "epoch 23,step 1608000, training loss 0.0321496\n",
      "epoch 23,step 1620000, training loss 0.024573\n",
      "epoch 23,step 1632000, training loss 0.0326845\n",
      "epoch 23,step 1644000, training loss 0.0306851\n",
      "epoch 23,step 1656000, training loss 0.0320844\n",
      "epoch 23,step 1668000, training loss 0.0356909\n",
      "epoch 23,step 1680000, training loss 0.0295823\n",
      "epoch 23,step 1692000, training loss 0.03181\n",
      "epoch 23,step 1704000, training loss 0.0297831\n",
      "epoch 23,step 1716000, training loss 0.0338546\n",
      "epoch 23,step 1728000, training loss 0.0294138\n",
      "epoch 23,step 1740000, training loss 0.0329808\n",
      "epoch 23,step 1752000, training loss 0.0280216\n",
      "epoch 23,step 1764000, training loss 0.0290328\n",
      "epoch 23,step 1776000, training loss 0.0409022\n",
      "epoch 23,step 1788000, training loss 0.0265563\n",
      "epoch 23,step 1800000, training loss 0.0632904\n",
      "epoch 23,step 1812000, training loss 0.0257876\n",
      "epoch 23,step 1824000, training loss 0.0246938\n",
      "epoch 23,step 1836000, training loss 0.0403061\n",
      "epoch 23,step 1848000, training loss 0.0198812\n",
      "epoch 23,step 1860000, training loss 0.0321523\n",
      "epoch 23,step 1872000, training loss 0.0224894\n",
      "epoch 23,step 1884000, training loss 0.0304568\n",
      "epoch 23,step 1896000, training loss 0.0314271\n",
      "epoch 23,step 1908000, training loss 0.0311447\n",
      "epoch 23,step 1920000, training loss 0.0263312\n",
      "epoch 23,step 1932000, training loss 0.0314059\n",
      "epoch 23,step 1944000, training loss 0.0256448\n",
      "epoch 23,step 1956000, training loss 0.0379637\n",
      "epoch 23,step 1968000, training loss 0.0343339\n",
      "epoch 23,step 1980000, training loss 0.0471666\n",
      "epoch 23,step 1992000, training loss 0.0353061\n",
      "epoch 23,step 2004000, training loss 0.0321894\n",
      "epoch 23,step 2016000, training loss 0.0308044\n",
      "epoch 23,step 2028000, training loss 0.0389655\n",
      "epoch 23,step 2040000, training loss 0.0344225\n",
      "epoch 23,step 2052000, training loss 0.0419233\n",
      "epoch 23,step 2064000, training loss 0.046793\n",
      "epoch 23,step 2076000, training loss 0.0283654\n",
      "epoch 23,step 2088000, training loss 0.0365643\n",
      "epoch 23,step 2100000, training loss 0.0306173\n",
      "epoch 23,step 2112000, training loss 0.0303625\n",
      "epoch 23,step 2124000, training loss 0.030509\n",
      "epoch 23,step 2136000, training loss 0.0245671\n",
      "epoch 23,step 2148000, training loss 0.0394084\n",
      "epoch 23,step 2160000, training loss 0.06455\n",
      "epoch 23,step 2172000, training loss 0.028875\n",
      "epoch 23,step 2184000, training loss 0.0282147\n",
      "epoch 23,step 2196000, training loss 0.0374779\n",
      "epoch 23,step 2208000, training loss 0.0303245\n",
      "epoch 23,step 2220000, training loss 0.0239876\n",
      "epoch 23,step 2232000, training loss 0.0245306\n",
      "epoch 23,step 2244000, training loss 0.0414564\n",
      "epoch 23,step 2256000, training loss 0.0243785\n",
      "epoch 23,step 2268000, training loss 0.0319906\n",
      "epoch 23,step 2280000, training loss 0.0228193\n",
      "epoch 23,step 2292000, training loss 0.0262839\n",
      "epoch 23,step 2304000, training loss 0.0327816\n",
      "epoch 23,step 2316000, training loss 0.0236983\n",
      "epoch 23,step 2328000, training loss 0.0316929\n",
      "epoch 23,step 2340000, training loss 0.0367804\n",
      "epoch 23,step 2352000, training loss 0.0292997\n",
      "epoch 23,step 2364000, training loss 0.0228432\n",
      "epoch 23,step 2376000, training loss 0.0229953\n",
      "epoch 23,step 2388000, training loss 0.0241235\n",
      "epoch 23,training loss 0.0377562 ,test loss 0.0424154\n",
      "epoch 24,step 2500, training loss 0.107186\n",
      "epoch 24,step 5000, training loss 0.0715869\n",
      "epoch 24,step 7500, training loss 0.0354259\n",
      "epoch 24,step 10000, training loss 0.0260053\n",
      "epoch 24,step 12500, training loss 0.0359442\n",
      "epoch 24,step 15000, training loss 0.0254848\n",
      "epoch 24,step 17500, training loss 0.0242331\n",
      "epoch 24,step 20000, training loss 0.0246487\n",
      "epoch 24,step 22500, training loss 0.0315709\n",
      "epoch 24,step 25000, training loss 0.0446888\n",
      "epoch 24,step 27500, training loss 0.125419\n",
      "epoch 24,step 30000, training loss 0.0668967\n",
      "epoch 24,step 32500, training loss 0.044613\n",
      "epoch 24,step 35000, training loss 0.0440913\n",
      "epoch 24,step 37500, training loss 0.0364868\n",
      "epoch 24,step 40000, training loss 0.0270779\n",
      "epoch 24,step 42500, training loss 0.0373189\n",
      "epoch 24,step 45000, training loss 0.029194\n",
      "epoch 24,step 47500, training loss 0.0524219\n",
      "epoch 24,step 50000, training loss 0.0233241\n",
      "epoch 24,step 52500, training loss 0.0894816\n",
      "epoch 24,step 55000, training loss 0.0497572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24,step 57500, training loss 0.0410214\n",
      "epoch 24,step 60000, training loss 0.0379815\n",
      "epoch 24,step 62500, training loss 0.0240071\n",
      "epoch 24,step 65000, training loss 0.0304626\n",
      "epoch 24,step 67500, training loss 0.0249563\n",
      "epoch 24,step 70000, training loss 0.0251109\n",
      "epoch 24,step 72500, training loss 0.0294321\n",
      "epoch 24,step 75000, training loss 0.0253028\n",
      "epoch 24,step 77500, training loss 0.0787645\n",
      "epoch 24,step 80000, training loss 0.0416838\n",
      "epoch 24,step 82500, training loss 0.0306566\n",
      "epoch 24,step 85000, training loss 0.0283984\n",
      "epoch 24,step 87500, training loss 0.028324\n",
      "epoch 24,step 90000, training loss 0.0455623\n",
      "epoch 24,step 92500, training loss 0.0283876\n",
      "epoch 24,step 95000, training loss 0.0256356\n",
      "epoch 24,step 97500, training loss 0.021505\n",
      "epoch 24,step 100000, training loss 0.0239221\n",
      "epoch 24,step 102500, training loss 0.079765\n",
      "epoch 24,step 105000, training loss 0.038773\n",
      "epoch 24,step 107500, training loss 0.0264904\n",
      "epoch 24,step 110000, training loss 0.0420984\n",
      "epoch 24,step 112500, training loss 0.0290349\n",
      "epoch 24,step 115000, training loss 0.0257883\n",
      "epoch 24,step 117500, training loss 0.0361737\n",
      "epoch 24,step 120000, training loss 0.0279671\n",
      "epoch 24,step 122500, training loss 0.0265369\n",
      "epoch 24,step 125000, training loss 0.0268237\n",
      "epoch 24,step 127500, training loss 0.086789\n",
      "epoch 24,step 130000, training loss 0.0676719\n",
      "epoch 24,step 132500, training loss 0.0316804\n",
      "epoch 24,step 135000, training loss 0.028478\n",
      "epoch 24,step 137500, training loss 0.0270528\n",
      "epoch 24,step 140000, training loss 0.0258349\n",
      "epoch 24,step 142500, training loss 0.0236305\n",
      "epoch 24,step 145000, training loss 0.0381933\n",
      "epoch 24,step 147500, training loss 0.0413301\n",
      "epoch 24,step 150000, training loss 0.0368706\n",
      "epoch 24,step 152500, training loss 0.085052\n",
      "epoch 24,step 155000, training loss 0.0584684\n",
      "epoch 24,step 157500, training loss 0.0371727\n",
      "epoch 24,step 160000, training loss 0.0307098\n",
      "epoch 24,step 162500, training loss 0.0562603\n",
      "epoch 24,step 165000, training loss 0.0532257\n",
      "epoch 24,step 167500, training loss 0.0396381\n",
      "epoch 24,step 170000, training loss 0.0319061\n",
      "epoch 24,step 172500, training loss 0.0294832\n",
      "epoch 24,step 175000, training loss 0.0297314\n",
      "epoch 24,step 177500, training loss 0.0907835\n",
      "epoch 24,step 180000, training loss 0.0393236\n",
      "epoch 24,step 182500, training loss 0.0288889\n",
      "epoch 24,step 185000, training loss 0.0238995\n",
      "epoch 24,step 187500, training loss 0.0428458\n",
      "epoch 24,step 190000, training loss 0.0328198\n",
      "epoch 24,step 192500, training loss 0.0366764\n",
      "epoch 24,step 195000, training loss 0.0263173\n",
      "epoch 24,step 197500, training loss 0.0306261\n",
      "epoch 24,step 200000, training loss 0.0278927\n",
      "epoch 24,step 202500, training loss 0.096417\n",
      "epoch 24,step 205000, training loss 0.0468613\n",
      "epoch 24,step 207500, training loss 0.0757133\n",
      "epoch 24,step 210000, training loss 0.0277967\n",
      "epoch 24,step 212500, training loss 0.027641\n",
      "epoch 24,step 215000, training loss 0.0272236\n",
      "epoch 24,step 217500, training loss 0.029502\n",
      "epoch 24,step 220000, training loss 0.027406\n",
      "epoch 24,step 222500, training loss 0.0213828\n",
      "epoch 24,step 225000, training loss 0.0239888\n",
      "epoch 24,step 227500, training loss 0.0782294\n",
      "epoch 24,step 230000, training loss 0.0682387\n",
      "epoch 24,step 232500, training loss 0.0317675\n",
      "epoch 24,step 235000, training loss 0.0298337\n",
      "epoch 24,step 237500, training loss 0.0208737\n",
      "epoch 24,step 240000, training loss 0.0266567\n",
      "epoch 24,step 242500, training loss 0.0256028\n",
      "epoch 24,step 245000, training loss 0.0262715\n",
      "epoch 24,step 247500, training loss 0.0298662\n",
      "epoch 24,step 250000, training loss 0.035482\n",
      "epoch 24,step 252500, training loss 0.103222\n",
      "epoch 24,step 255000, training loss 0.0470065\n",
      "epoch 24,step 257500, training loss 0.0314682\n",
      "epoch 24,step 260000, training loss 0.0224196\n",
      "epoch 24,step 262500, training loss 0.0282565\n",
      "epoch 24,step 265000, training loss 0.0298367\n",
      "epoch 24,step 267500, training loss 0.0212018\n",
      "epoch 24,step 270000, training loss 0.0383672\n",
      "epoch 24,step 272500, training loss 0.0355708\n",
      "epoch 24,step 275000, training loss 0.033797\n",
      "epoch 24,step 277500, training loss 0.0931984\n",
      "epoch 24,step 280000, training loss 0.0614274\n",
      "epoch 24,step 282500, training loss 0.031866\n",
      "epoch 24,step 285000, training loss 0.0493637\n",
      "epoch 24,step 287500, training loss 0.0433773\n",
      "epoch 24,step 290000, training loss 0.0572365\n",
      "epoch 24,step 292500, training loss 0.036263\n",
      "epoch 24,step 295000, training loss 0.0563866\n",
      "epoch 24,step 297500, training loss 0.0629471\n",
      "epoch 24,step 300000, training loss 0.0365142\n",
      "epoch 24,step 302500, training loss 0.0887875\n",
      "epoch 24,step 305000, training loss 0.0557433\n",
      "epoch 24,step 307500, training loss 0.0388106\n",
      "epoch 24,step 310000, training loss 0.0355945\n",
      "epoch 24,step 312500, training loss 0.0353018\n",
      "epoch 24,step 315000, training loss 0.0248163\n",
      "epoch 24,step 317500, training loss 0.0332874\n",
      "epoch 24,step 320000, training loss 0.0297748\n",
      "epoch 24,step 322500, training loss 0.0245686\n",
      "epoch 24,step 325000, training loss 0.0200709\n",
      "epoch 24,step 327500, training loss 0.0786371\n",
      "epoch 24,step 330000, training loss 0.0700293\n",
      "epoch 24,step 332500, training loss 0.0312591\n",
      "epoch 24,step 335000, training loss 0.0292975\n",
      "epoch 24,step 337500, training loss 0.0473165\n",
      "epoch 24,step 340000, training loss 0.0235026\n",
      "epoch 24,step 342500, training loss 0.0451487\n",
      "epoch 24,step 345000, training loss 0.025308\n",
      "epoch 24,step 347500, training loss 0.0381413\n",
      "epoch 24,step 350000, training loss 0.0298184\n",
      "epoch 24,step 352500, training loss 0.131599\n",
      "epoch 24,step 355000, training loss 0.0808239\n",
      "epoch 24,step 357500, training loss 0.0471355\n",
      "epoch 24,step 360000, training loss 0.0314919\n",
      "epoch 24,step 362500, training loss 0.0385138\n",
      "epoch 24,step 365000, training loss 0.0310833\n",
      "epoch 24,step 367500, training loss 0.0401106\n",
      "epoch 24,step 370000, training loss 0.0280228\n",
      "epoch 24,step 372500, training loss 0.0296502\n",
      "epoch 24,step 375000, training loss 0.0307183\n",
      "epoch 24,step 377500, training loss 0.0810656\n",
      "epoch 24,step 380000, training loss 0.0664547\n",
      "epoch 24,step 382500, training loss 0.0433699\n",
      "epoch 24,step 385000, training loss 0.0402658\n",
      "epoch 24,step 387500, training loss 0.045351\n",
      "epoch 24,step 390000, training loss 0.0472673\n",
      "epoch 24,step 392500, training loss 0.0378941\n",
      "epoch 24,step 395000, training loss 0.0241975\n",
      "epoch 24,step 397500, training loss 0.0312444\n",
      "epoch 24,step 400000, training loss 0.0288756\n",
      "epoch 24,step 402500, training loss 0.0798211\n",
      "epoch 24,step 405000, training loss 0.0550405\n",
      "epoch 24,step 407500, training loss 0.0372475\n",
      "epoch 24,step 410000, training loss 0.0263538\n",
      "epoch 24,step 412500, training loss 0.0319831\n",
      "epoch 24,step 415000, training loss 0.0232253\n",
      "epoch 24,step 417500, training loss 0.0278678\n",
      "epoch 24,step 420000, training loss 0.0364103\n",
      "epoch 24,step 422500, training loss 0.0275647\n",
      "epoch 24,step 425000, training loss 0.0302046\n",
      "epoch 24,step 427500, training loss 0.115119\n",
      "epoch 24,step 430000, training loss 0.0528516\n",
      "epoch 24,step 432500, training loss 0.0383807\n",
      "epoch 24,step 435000, training loss 0.0527911\n",
      "epoch 24,step 437500, training loss 0.0409126\n",
      "epoch 24,step 440000, training loss 0.0311648\n",
      "epoch 24,step 442500, training loss 0.0454208\n",
      "epoch 24,step 445000, training loss 0.0306615\n",
      "epoch 24,step 447500, training loss 0.0311285\n",
      "epoch 24,step 450000, training loss 0.0296409\n",
      "epoch 24,step 452500, training loss 0.0796463\n",
      "epoch 24,step 455000, training loss 0.0562763\n",
      "epoch 24,step 457500, training loss 0.035215\n",
      "epoch 24,step 460000, training loss 0.0406181\n",
      "epoch 24,step 462500, training loss 0.034204\n",
      "epoch 24,step 465000, training loss 0.0690285\n",
      "epoch 24,step 467500, training loss 0.0392412\n",
      "epoch 24,step 470000, training loss 0.0359273\n",
      "epoch 24,step 472500, training loss 0.0337104\n",
      "epoch 24,step 475000, training loss 0.0336014\n",
      "epoch 24,step 477500, training loss 0.0810652\n",
      "epoch 24,step 480000, training loss 0.0422577\n",
      "epoch 24,step 482500, training loss 0.0229866\n",
      "epoch 24,step 485000, training loss 0.0246684\n",
      "epoch 24,step 487500, training loss 0.0300113\n",
      "epoch 24,step 490000, training loss 0.036392\n",
      "epoch 24,step 492500, training loss 0.0234022\n",
      "epoch 24,step 495000, training loss 0.0258377\n",
      "epoch 24,step 497500, training loss 0.0269312\n",
      "epoch 24,step 500000, training loss 0.0252548\n",
      "epoch 24,step 502500, training loss 0.0800269\n",
      "epoch 24,step 505000, training loss 0.0747156\n",
      "epoch 24,step 507500, training loss 0.0377839\n",
      "epoch 24,step 510000, training loss 0.024722\n",
      "epoch 24,step 512500, training loss 0.0345495\n",
      "epoch 24,step 515000, training loss 0.0404118\n",
      "epoch 24,step 517500, training loss 0.026522\n",
      "epoch 24,step 520000, training loss 0.023762\n",
      "epoch 24,step 522500, training loss 0.0255979\n",
      "epoch 24,step 525000, training loss 0.0269596\n",
      "epoch 24,step 527500, training loss 0.0948616\n",
      "epoch 24,step 530000, training loss 0.0484013\n",
      "epoch 24,step 532500, training loss 0.0352896\n",
      "epoch 24,step 535000, training loss 0.0321458\n",
      "epoch 24,step 537500, training loss 0.0302415\n",
      "epoch 24,step 540000, training loss 0.0314135\n",
      "epoch 24,step 542500, training loss 0.0339917\n",
      "epoch 24,step 545000, training loss 0.0358484\n",
      "epoch 24,step 547500, training loss 0.0317232\n",
      "epoch 24,step 550000, training loss 0.0326691\n",
      "epoch 24,step 552500, training loss 0.103211\n",
      "epoch 24,step 555000, training loss 0.0497416\n",
      "epoch 24,step 557500, training loss 0.0360367\n",
      "epoch 24,step 560000, training loss 0.0284548\n",
      "epoch 24,step 562500, training loss 0.026253\n",
      "epoch 24,step 565000, training loss 0.0253223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24,step 567500, training loss 0.0277525\n",
      "epoch 24,step 570000, training loss 0.0273579\n",
      "epoch 24,step 572500, training loss 0.0252464\n",
      "epoch 24,step 575000, training loss 0.033961\n",
      "epoch 24,step 577500, training loss 0.0820769\n",
      "epoch 24,step 580000, training loss 0.0478868\n",
      "epoch 24,step 582500, training loss 0.024153\n",
      "epoch 24,step 585000, training loss 0.0295878\n",
      "epoch 24,step 587500, training loss 0.0217767\n",
      "epoch 24,step 590000, training loss 0.0228082\n",
      "epoch 24,step 592500, training loss 0.0311245\n",
      "epoch 24,step 595000, training loss 0.0270137\n",
      "epoch 24,step 597500, training loss 0.0262615\n",
      "epoch 24,step 600000, training loss 0.0285633\n",
      "epoch 24,step 602500, training loss 0.0943176\n",
      "epoch 24,step 605000, training loss 0.0409489\n",
      "epoch 24,step 607500, training loss 0.0367576\n",
      "epoch 24,step 610000, training loss 0.033314\n",
      "epoch 24,step 612500, training loss 0.031559\n",
      "epoch 24,step 615000, training loss 0.0412284\n",
      "epoch 24,step 617500, training loss 0.028725\n",
      "epoch 24,step 620000, training loss 0.0252589\n",
      "epoch 24,step 622500, training loss 0.0341426\n",
      "epoch 24,step 625000, training loss 0.0277347\n",
      "epoch 24,step 627500, training loss 0.153664\n",
      "epoch 24,step 630000, training loss 0.0448093\n",
      "epoch 24,step 632500, training loss 0.0241859\n",
      "epoch 24,step 635000, training loss 0.0310504\n",
      "epoch 24,step 637500, training loss 0.0307581\n",
      "epoch 24,step 640000, training loss 0.0194761\n",
      "epoch 24,step 642500, training loss 0.0344279\n",
      "epoch 24,step 645000, training loss 0.0285615\n",
      "epoch 24,step 647500, training loss 0.0454132\n",
      "epoch 24,step 650000, training loss 0.034572\n",
      "epoch 24,step 652500, training loss 0.124103\n",
      "epoch 24,step 655000, training loss 0.0673619\n",
      "epoch 24,step 657500, training loss 0.0620143\n",
      "epoch 24,step 660000, training loss 0.0363178\n",
      "epoch 24,step 662500, training loss 0.0242393\n",
      "epoch 24,step 665000, training loss 0.0257222\n",
      "epoch 24,step 667500, training loss 0.0290683\n",
      "epoch 24,step 670000, training loss 0.0276354\n",
      "epoch 24,step 672500, training loss 0.0253705\n",
      "epoch 24,step 675000, training loss 0.0300915\n",
      "epoch 24,step 677500, training loss 0.0958068\n",
      "epoch 24,step 680000, training loss 0.0399344\n",
      "epoch 24,step 682500, training loss 0.0357996\n",
      "epoch 24,step 685000, training loss 0.0447065\n",
      "epoch 24,step 687500, training loss 0.0523966\n",
      "epoch 24,step 690000, training loss 0.0371108\n",
      "epoch 24,step 692500, training loss 0.0267108\n",
      "epoch 24,step 695000, training loss 0.0286347\n",
      "epoch 24,step 697500, training loss 0.0288857\n",
      "epoch 24,step 700000, training loss 0.0262282\n",
      "epoch 24,step 702500, training loss 0.111476\n",
      "epoch 24,step 705000, training loss 0.0553264\n",
      "epoch 24,step 707500, training loss 0.0275342\n",
      "epoch 24,step 710000, training loss 0.0285111\n",
      "epoch 24,step 712500, training loss 0.0234557\n",
      "epoch 24,step 715000, training loss 0.0256618\n",
      "epoch 24,step 717500, training loss 0.0353995\n",
      "epoch 24,step 720000, training loss 0.02415\n",
      "epoch 24,step 722500, training loss 0.0300241\n",
      "epoch 24,step 725000, training loss 0.0329293\n",
      "epoch 24,step 727500, training loss 0.0945595\n",
      "epoch 24,step 730000, training loss 0.0405944\n",
      "epoch 24,step 732500, training loss 0.0302262\n",
      "epoch 24,step 735000, training loss 0.0415887\n",
      "epoch 24,step 737500, training loss 0.0387634\n",
      "epoch 24,step 740000, training loss 0.0410672\n",
      "epoch 24,step 742500, training loss 0.0265421\n",
      "epoch 24,step 745000, training loss 0.033898\n",
      "epoch 24,step 747500, training loss 0.0277528\n",
      "epoch 24,step 750000, training loss 0.0311239\n",
      "epoch 24,step 752500, training loss 0.0881866\n",
      "epoch 24,step 755000, training loss 0.0555071\n",
      "epoch 24,step 757500, training loss 0.0343155\n",
      "epoch 24,step 760000, training loss 0.0341773\n",
      "epoch 24,step 762500, training loss 0.0327364\n",
      "epoch 24,step 765000, training loss 0.0365533\n",
      "epoch 24,step 767500, training loss 0.0304243\n",
      "epoch 24,step 770000, training loss 0.0471308\n",
      "epoch 24,step 772500, training loss 0.0257103\n",
      "epoch 24,step 775000, training loss 0.0321107\n",
      "epoch 24,step 777500, training loss 0.0996766\n",
      "epoch 24,step 780000, training loss 0.0728854\n",
      "epoch 24,step 782500, training loss 0.0253259\n",
      "epoch 24,step 785000, training loss 0.0354939\n",
      "epoch 24,step 787500, training loss 0.0294425\n",
      "epoch 24,step 790000, training loss 0.0378239\n",
      "epoch 24,step 792500, training loss 0.0561213\n",
      "epoch 24,step 795000, training loss 0.0272777\n",
      "epoch 24,step 797500, training loss 0.0545985\n",
      "epoch 24,step 800000, training loss 0.0406246\n",
      "epoch 24,step 802500, training loss 0.0907922\n",
      "epoch 24,step 805000, training loss 0.0644766\n",
      "epoch 24,step 807500, training loss 0.0367028\n",
      "epoch 24,step 810000, training loss 0.03698\n",
      "epoch 24,step 812500, training loss 0.0447654\n",
      "epoch 24,step 815000, training loss 0.0280937\n",
      "epoch 24,step 817500, training loss 0.0297019\n",
      "epoch 24,step 820000, training loss 0.0241994\n",
      "epoch 24,step 822500, training loss 0.0368591\n",
      "epoch 24,step 825000, training loss 0.0297685\n",
      "epoch 24,step 827500, training loss 0.0794656\n",
      "epoch 24,step 830000, training loss 0.0767312\n",
      "epoch 24,step 832500, training loss 0.0291116\n",
      "epoch 24,step 835000, training loss 0.0490823\n",
      "epoch 24,step 837500, training loss 0.0306841\n",
      "epoch 24,step 840000, training loss 0.0227111\n",
      "epoch 24,step 842500, training loss 0.0286421\n",
      "epoch 24,step 845000, training loss 0.0293862\n",
      "epoch 24,step 847500, training loss 0.0303265\n",
      "epoch 24,step 850000, training loss 0.0413944\n",
      "epoch 24,step 852500, training loss 0.0951416\n",
      "epoch 24,step 855000, training loss 0.0572897\n",
      "epoch 24,step 857500, training loss 0.0468424\n",
      "epoch 24,step 860000, training loss 0.0426796\n",
      "epoch 24,step 862500, training loss 0.023765\n",
      "epoch 24,step 865000, training loss 0.0279591\n",
      "epoch 24,step 867500, training loss 0.0311435\n",
      "epoch 24,step 870000, training loss 0.0357081\n",
      "epoch 24,step 872500, training loss 0.0348359\n",
      "epoch 24,step 875000, training loss 0.0307498\n",
      "epoch 24,step 877500, training loss 0.0882145\n",
      "epoch 24,step 880000, training loss 0.0447597\n",
      "epoch 24,step 882500, training loss 0.0405264\n",
      "epoch 24,step 885000, training loss 0.0348911\n",
      "epoch 24,step 887500, training loss 0.0384221\n",
      "epoch 24,step 890000, training loss 0.0277674\n",
      "epoch 24,step 892500, training loss 0.0271519\n",
      "epoch 24,step 895000, training loss 0.0271928\n",
      "epoch 24,step 897500, training loss 0.0253569\n",
      "epoch 24,step 900000, training loss 0.022854\n",
      "epoch 24,step 902500, training loss 0.095694\n",
      "epoch 24,step 905000, training loss 0.05131\n",
      "epoch 24,step 907500, training loss 0.0377442\n",
      "epoch 24,step 910000, training loss 0.0280618\n",
      "epoch 24,step 912500, training loss 0.0481527\n",
      "epoch 24,step 915000, training loss 0.0380411\n",
      "epoch 24,step 917500, training loss 0.0344753\n",
      "epoch 24,step 920000, training loss 0.0358259\n",
      "epoch 24,step 922500, training loss 0.0326701\n",
      "epoch 24,step 925000, training loss 0.040926\n",
      "epoch 24,step 927500, training loss 0.0983902\n",
      "epoch 24,step 930000, training loss 0.0533082\n",
      "epoch 24,step 932500, training loss 0.0263885\n",
      "epoch 24,step 935000, training loss 0.0470589\n",
      "epoch 24,step 937500, training loss 0.0358981\n",
      "epoch 24,step 940000, training loss 0.0339537\n",
      "epoch 24,step 942500, training loss 0.0291904\n",
      "epoch 24,step 945000, training loss 0.0385269\n",
      "epoch 24,step 947500, training loss 0.0250672\n",
      "epoch 24,step 950000, training loss 0.0305833\n",
      "epoch 24,step 952500, training loss 0.0897271\n",
      "epoch 24,step 955000, training loss 0.109082\n",
      "epoch 24,step 957500, training loss 0.0471897\n",
      "epoch 24,step 960000, training loss 0.0354709\n",
      "epoch 24,step 962500, training loss 0.0313306\n",
      "epoch 24,step 965000, training loss 0.0338872\n",
      "epoch 24,step 967500, training loss 0.0316754\n",
      "epoch 24,step 970000, training loss 0.0282254\n",
      "epoch 24,step 972500, training loss 0.0355356\n",
      "epoch 24,step 975000, training loss 0.0280326\n",
      "epoch 24,step 977500, training loss 0.0850098\n",
      "epoch 24,step 980000, training loss 0.0552576\n",
      "epoch 24,step 982500, training loss 0.0248577\n",
      "epoch 24,step 985000, training loss 0.0283116\n",
      "epoch 24,step 987500, training loss 0.0324743\n",
      "epoch 24,step 990000, training loss 0.0240362\n",
      "epoch 24,step 992500, training loss 0.0256978\n",
      "epoch 24,step 995000, training loss 0.0390631\n",
      "epoch 24,step 997500, training loss 0.0293749\n",
      "epoch 24,step 1000000, training loss 0.0204413\n",
      "epoch 24,step 1002500, training loss 0.109358\n",
      "epoch 24,step 1005000, training loss 0.118343\n",
      "epoch 24,step 1007500, training loss 0.031321\n",
      "epoch 24,step 1010000, training loss 0.0301983\n",
      "epoch 24,step 1012500, training loss 0.0257667\n",
      "epoch 24,step 1015000, training loss 0.0287175\n",
      "epoch 24,step 1017500, training loss 0.0247019\n",
      "epoch 24,step 1020000, training loss 0.0321851\n",
      "epoch 24,step 1022500, training loss 0.0392428\n",
      "epoch 24,step 1025000, training loss 0.0282953\n",
      "epoch 24,step 1027500, training loss 0.0828016\n",
      "epoch 24,step 1030000, training loss 0.047501\n",
      "epoch 24,step 1032500, training loss 0.0331478\n",
      "epoch 24,step 1035000, training loss 0.0339213\n",
      "epoch 24,step 1037500, training loss 0.027344\n",
      "epoch 24,step 1040000, training loss 0.0392654\n",
      "epoch 24,step 1042500, training loss 0.0350333\n",
      "epoch 24,step 1045000, training loss 0.0309659\n",
      "epoch 24,step 1047500, training loss 0.0208638\n",
      "epoch 24,step 1050000, training loss 0.0255134\n",
      "epoch 24,step 1052500, training loss 0.0893828\n",
      "epoch 24,step 1055000, training loss 0.0460707\n",
      "epoch 24,step 1057500, training loss 0.0189238\n",
      "epoch 24,step 1060000, training loss 0.0302757\n",
      "epoch 24,step 1062500, training loss 0.0387359\n",
      "epoch 24,step 1065000, training loss 0.0373664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24,step 1067500, training loss 0.0372658\n",
      "epoch 24,step 1070000, training loss 0.0417683\n",
      "epoch 24,step 1072500, training loss 0.0275797\n",
      "epoch 24,step 1075000, training loss 0.0313901\n",
      "epoch 24,step 1077500, training loss 0.0895785\n",
      "epoch 24,step 1080000, training loss 0.0672098\n",
      "epoch 24,step 1082500, training loss 0.0388708\n",
      "epoch 24,step 1085000, training loss 0.0298831\n",
      "epoch 24,step 1087500, training loss 0.0399915\n",
      "epoch 24,step 1090000, training loss 0.0295351\n",
      "epoch 24,step 1092500, training loss 0.0354633\n",
      "epoch 24,step 1095000, training loss 0.0327889\n",
      "epoch 24,step 1097500, training loss 0.0300746\n",
      "epoch 24,step 1100000, training loss 0.0311193\n",
      "epoch 24,step 1102500, training loss 0.103198\n",
      "epoch 24,step 1105000, training loss 0.0502789\n",
      "epoch 24,step 1107500, training loss 0.0383018\n",
      "epoch 24,step 1110000, training loss 0.0257857\n",
      "epoch 24,step 1112500, training loss 0.0217863\n",
      "epoch 24,step 1115000, training loss 0.0243531\n",
      "epoch 24,step 1117500, training loss 0.0337395\n",
      "epoch 24,step 1120000, training loss 0.0268987\n",
      "epoch 24,step 1122500, training loss 0.0299145\n",
      "epoch 24,step 1125000, training loss 0.0301511\n",
      "epoch 24,step 1127500, training loss 0.0853556\n",
      "epoch 24,step 1130000, training loss 0.0429772\n",
      "epoch 24,step 1132500, training loss 0.0394426\n",
      "epoch 24,step 1135000, training loss 0.0349151\n",
      "epoch 24,step 1137500, training loss 0.0261467\n",
      "epoch 24,step 1140000, training loss 0.0252195\n",
      "epoch 24,step 1142500, training loss 0.0309984\n",
      "epoch 24,step 1145000, training loss 0.0314592\n",
      "epoch 24,step 1147500, training loss 0.0204196\n",
      "epoch 24,step 1150000, training loss 0.0287767\n",
      "epoch 24,step 1152500, training loss 0.101775\n",
      "epoch 24,step 1155000, training loss 0.0571374\n",
      "epoch 24,step 1157500, training loss 0.0408999\n",
      "epoch 24,step 1160000, training loss 0.0373809\n",
      "epoch 24,step 1162500, training loss 0.0412389\n",
      "epoch 24,step 1165000, training loss 0.030197\n",
      "epoch 24,step 1167500, training loss 0.0474393\n",
      "epoch 24,step 1170000, training loss 0.0341079\n",
      "epoch 24,step 1172500, training loss 0.0296525\n",
      "epoch 24,step 1175000, training loss 0.0314941\n",
      "epoch 24,step 1177500, training loss 0.0889924\n",
      "epoch 24,step 1180000, training loss 0.0946896\n",
      "epoch 24,step 1182500, training loss 0.0373249\n",
      "epoch 24,step 1185000, training loss 0.0293835\n",
      "epoch 24,step 1187500, training loss 0.023509\n",
      "epoch 24,step 1190000, training loss 0.0472742\n",
      "epoch 24,step 1192500, training loss 0.0325213\n",
      "epoch 24,step 1195000, training loss 0.0346313\n",
      "epoch 24,step 1197500, training loss 0.0272612\n",
      "epoch 24,step 1200000, training loss 0.0305247\n",
      "epoch 24,step 1202500, training loss 0.0733588\n",
      "epoch 24,step 1205000, training loss 0.0376755\n",
      "epoch 24,step 1207500, training loss 0.0371671\n",
      "epoch 24,step 1210000, training loss 0.035899\n",
      "epoch 24,step 1212500, training loss 0.0280931\n",
      "epoch 24,step 1215000, training loss 0.0321647\n",
      "epoch 24,step 1217500, training loss 0.0284768\n",
      "epoch 24,step 1220000, training loss 0.0291606\n",
      "epoch 24,step 1222500, training loss 0.0295669\n",
      "epoch 24,step 1225000, training loss 0.0342191\n",
      "epoch 24,step 1227500, training loss 0.0956912\n",
      "epoch 24,step 1230000, training loss 0.0448057\n",
      "epoch 24,step 1232500, training loss 0.0286901\n",
      "epoch 24,step 1235000, training loss 0.0453489\n",
      "epoch 24,step 1237500, training loss 0.0364654\n",
      "epoch 24,step 1240000, training loss 0.0278074\n",
      "epoch 24,step 1242500, training loss 0.0297957\n",
      "epoch 24,step 1245000, training loss 0.0301447\n",
      "epoch 24,step 1247500, training loss 0.0552387\n",
      "epoch 24,step 1250000, training loss 0.0289777\n",
      "epoch 24,step 1252500, training loss 0.102196\n",
      "epoch 24,step 1255000, training loss 0.0576173\n",
      "epoch 24,step 1257500, training loss 0.0359931\n",
      "epoch 24,step 1260000, training loss 0.0364928\n",
      "epoch 24,step 1262500, training loss 0.0297923\n",
      "epoch 24,step 1265000, training loss 0.0303733\n",
      "epoch 24,step 1267500, training loss 0.0223095\n",
      "epoch 24,step 1270000, training loss 0.0315442\n",
      "epoch 24,step 1272500, training loss 0.0344585\n",
      "epoch 24,step 1275000, training loss 0.0256504\n",
      "epoch 24,step 1277500, training loss 0.104091\n",
      "epoch 24,step 1280000, training loss 0.10687\n",
      "epoch 24,step 1282500, training loss 0.031053\n",
      "epoch 24,step 1285000, training loss 0.0333079\n",
      "epoch 24,step 1287500, training loss 0.0358599\n",
      "epoch 24,step 1290000, training loss 0.0370235\n",
      "epoch 24,step 1292500, training loss 0.0347511\n",
      "epoch 24,step 1295000, training loss 0.030476\n",
      "epoch 24,step 1297500, training loss 0.0375051\n",
      "epoch 24,step 1300000, training loss 0.0234362\n",
      "epoch 24,step 1302500, training loss 0.0982979\n",
      "epoch 24,step 1305000, training loss 0.062531\n",
      "epoch 24,step 1307500, training loss 0.0303033\n",
      "epoch 24,step 1310000, training loss 0.0281365\n",
      "epoch 24,step 1312500, training loss 0.0362501\n",
      "epoch 24,step 1315000, training loss 0.0262986\n",
      "epoch 24,step 1317500, training loss 0.033181\n",
      "epoch 24,step 1320000, training loss 0.0350013\n",
      "epoch 24,step 1322500, training loss 0.0540165\n",
      "epoch 24,step 1325000, training loss 0.0408353\n",
      "epoch 24,step 1327500, training loss 0.129291\n",
      "epoch 24,step 1330000, training loss 0.100509\n",
      "epoch 24,step 1332500, training loss 0.0488282\n",
      "epoch 24,step 1335000, training loss 0.0534143\n",
      "epoch 24,step 1337500, training loss 0.0419376\n",
      "epoch 24,step 1340000, training loss 0.0316193\n",
      "epoch 24,step 1342500, training loss 0.0300362\n",
      "epoch 24,step 1345000, training loss 0.0335166\n",
      "epoch 24,step 1347500, training loss 0.0363397\n",
      "epoch 24,step 1350000, training loss 0.034269\n",
      "epoch 24,step 1352500, training loss 0.0786195\n",
      "epoch 24,step 1355000, training loss 0.0823649\n",
      "epoch 24,step 1357500, training loss 0.0500672\n",
      "epoch 24,step 1360000, training loss 0.0526614\n",
      "epoch 24,step 1362500, training loss 0.0426452\n",
      "epoch 24,step 1365000, training loss 0.0321478\n",
      "epoch 24,step 1367500, training loss 0.0254328\n",
      "epoch 24,step 1370000, training loss 0.0333659\n",
      "epoch 24,step 1372500, training loss 0.0288559\n",
      "epoch 24,step 1375000, training loss 0.0503102\n",
      "epoch 24,step 1377500, training loss 0.0929037\n",
      "epoch 24,step 1380000, training loss 0.0533685\n",
      "epoch 24,step 1382500, training loss 0.0386459\n",
      "epoch 24,step 1385000, training loss 0.0438657\n",
      "epoch 24,step 1387500, training loss 0.0372404\n",
      "epoch 24,step 1390000, training loss 0.0347993\n",
      "epoch 24,step 1392500, training loss 0.030434\n",
      "epoch 24,step 1395000, training loss 0.0438649\n",
      "epoch 24,step 1397500, training loss 0.0323638\n",
      "epoch 24,step 1400000, training loss 0.0403985\n",
      "epoch 24,step 1402500, training loss 0.0989505\n",
      "epoch 24,step 1405000, training loss 0.0744933\n",
      "epoch 24,step 1407500, training loss 0.050256\n",
      "epoch 24,step 1410000, training loss 0.0377227\n",
      "epoch 24,step 1412500, training loss 0.0319352\n",
      "epoch 24,step 1415000, training loss 0.0289796\n",
      "epoch 24,step 1417500, training loss 0.0302646\n",
      "epoch 24,step 1420000, training loss 0.030928\n",
      "epoch 24,step 1422500, training loss 0.0279575\n",
      "epoch 24,step 1425000, training loss 0.0279504\n",
      "epoch 24,step 1427500, training loss 0.0975404\n",
      "epoch 24,step 1430000, training loss 0.0685328\n",
      "epoch 24,step 1432500, training loss 0.04394\n",
      "epoch 24,step 1435000, training loss 0.0380216\n",
      "epoch 24,step 1437500, training loss 0.0500444\n",
      "epoch 24,step 1440000, training loss 0.0405297\n",
      "epoch 24,step 1442500, training loss 0.0366605\n",
      "epoch 24,step 1445000, training loss 0.0372184\n",
      "epoch 24,step 1447500, training loss 0.0288616\n",
      "epoch 24,step 1450000, training loss 0.0230832\n",
      "epoch 24,step 1452500, training loss 0.0984284\n",
      "epoch 24,step 1455000, training loss 0.0528717\n",
      "epoch 24,step 1457500, training loss 0.0303471\n",
      "epoch 24,step 1460000, training loss 0.0343704\n",
      "epoch 24,step 1462500, training loss 0.0315344\n",
      "epoch 24,step 1465000, training loss 0.035582\n",
      "epoch 24,step 1467500, training loss 0.0294802\n",
      "epoch 24,step 1470000, training loss 0.0389687\n",
      "epoch 24,step 1472500, training loss 0.0353477\n",
      "epoch 24,step 1475000, training loss 0.0250613\n",
      "epoch 24,step 1477500, training loss 0.0862382\n",
      "epoch 24,step 1480000, training loss 0.0479132\n",
      "epoch 24,step 1482500, training loss 0.0343517\n",
      "epoch 24,step 1485000, training loss 0.0380247\n",
      "epoch 24,step 1487500, training loss 0.0327036\n",
      "epoch 24,step 1490000, training loss 0.0338974\n",
      "epoch 24,step 1492500, training loss 0.0239588\n",
      "epoch 24,step 1495000, training loss 0.0292923\n",
      "epoch 24,step 1497500, training loss 0.0304655\n",
      "epoch 24,step 1500000, training loss 0.03796\n",
      "epoch 24,step 1502500, training loss 0.147623\n",
      "epoch 24,step 1505000, training loss 0.0562601\n",
      "epoch 24,step 1507500, training loss 0.0394022\n",
      "epoch 24,step 1510000, training loss 0.0528366\n",
      "epoch 24,step 1512500, training loss 0.0232806\n",
      "epoch 24,step 1515000, training loss 0.0358966\n",
      "epoch 24,step 1517500, training loss 0.0297822\n",
      "epoch 24,step 1520000, training loss 0.0277576\n",
      "epoch 24,step 1522500, training loss 0.0288016\n",
      "epoch 24,step 1525000, training loss 0.0239831\n",
      "epoch 24,step 1527500, training loss 0.115921\n",
      "epoch 24,step 1530000, training loss 0.0652214\n",
      "epoch 24,step 1532500, training loss 0.0367389\n",
      "epoch 24,step 1535000, training loss 0.027596\n",
      "epoch 24,step 1537500, training loss 0.0283772\n",
      "epoch 24,step 1540000, training loss 0.0310765\n",
      "epoch 24,step 1542500, training loss 0.025075\n",
      "epoch 24,step 1545000, training loss 0.0256509\n",
      "epoch 24,step 1547500, training loss 0.0285112\n",
      "epoch 24,step 1550000, training loss 0.0378251\n",
      "epoch 24,step 1552500, training loss 0.126711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24,step 1555000, training loss 0.121249\n",
      "epoch 24,step 1557500, training loss 0.0420603\n",
      "epoch 24,step 1560000, training loss 0.0502372\n",
      "epoch 24,step 1562500, training loss 0.0382545\n",
      "epoch 24,step 1565000, training loss 0.0465852\n",
      "epoch 24,step 1567500, training loss 0.0268356\n",
      "epoch 24,step 1570000, training loss 0.0327942\n",
      "epoch 24,step 1572500, training loss 0.0274165\n",
      "epoch 24,step 1575000, training loss 0.0343982\n",
      "epoch 24,step 1577500, training loss 0.0938271\n",
      "epoch 24,step 1580000, training loss 0.0554278\n",
      "epoch 24,step 1582500, training loss 0.0314065\n",
      "epoch 24,step 1585000, training loss 0.0356047\n",
      "epoch 24,step 1587500, training loss 0.0250638\n",
      "epoch 24,step 1590000, training loss 0.0247921\n",
      "epoch 24,step 1592500, training loss 0.0253336\n",
      "epoch 24,step 1595000, training loss 0.0401091\n",
      "epoch 24,step 1597500, training loss 0.0287228\n",
      "epoch 24,step 1600000, training loss 0.0314736\n",
      "epoch 24,step 1602500, training loss 0.135284\n",
      "epoch 24,step 1605000, training loss 0.0697086\n",
      "epoch 24,step 1607500, training loss 0.0318658\n",
      "epoch 24,step 1610000, training loss 0.0300462\n",
      "epoch 24,step 1612500, training loss 0.0372654\n",
      "epoch 24,step 1615000, training loss 0.0368445\n",
      "epoch 24,step 1617500, training loss 0.0784202\n",
      "epoch 24,step 1620000, training loss 0.0256826\n",
      "epoch 24,step 1622500, training loss 0.0235623\n",
      "epoch 24,step 1625000, training loss 0.0333958\n",
      "epoch 24,step 1627500, training loss 0.0963287\n",
      "epoch 24,step 1630000, training loss 0.042568\n",
      "epoch 24,step 1632500, training loss 0.0230354\n",
      "epoch 24,step 1635000, training loss 0.0287576\n",
      "epoch 24,step 1637500, training loss 0.0296635\n",
      "epoch 24,step 1640000, training loss 0.0291522\n",
      "epoch 24,step 1642500, training loss 0.021209\n",
      "epoch 24,step 1645000, training loss 0.0313276\n",
      "epoch 24,step 1647500, training loss 0.0453539\n",
      "epoch 24,step 1650000, training loss 0.027235\n",
      "epoch 24,step 1652500, training loss 0.116726\n",
      "epoch 24,step 1655000, training loss 0.0584553\n",
      "epoch 24,step 1657500, training loss 0.0297115\n",
      "epoch 24,step 1660000, training loss 0.0315716\n",
      "epoch 24,step 1662500, training loss 0.023364\n",
      "epoch 24,step 1665000, training loss 0.0284132\n",
      "epoch 24,step 1667500, training loss 0.0317197\n",
      "epoch 24,step 1670000, training loss 0.0393121\n",
      "epoch 24,step 1672500, training loss 0.0297848\n",
      "epoch 24,step 1675000, training loss 0.032469\n",
      "epoch 24,step 1677500, training loss 0.105294\n",
      "epoch 24,step 1680000, training loss 0.0983743\n",
      "epoch 24,step 1682500, training loss 0.0357661\n",
      "epoch 24,step 1685000, training loss 0.0438961\n",
      "epoch 24,step 1687500, training loss 0.0239533\n",
      "epoch 24,step 1690000, training loss 0.0392308\n",
      "epoch 24,step 1692500, training loss 0.0414665\n",
      "epoch 24,step 1695000, training loss 0.0312959\n",
      "epoch 24,step 1697500, training loss 0.0272376\n",
      "epoch 24,step 1700000, training loss 0.0323048\n",
      "epoch 24,step 1702500, training loss 0.103852\n",
      "epoch 24,step 1705000, training loss 0.0557708\n",
      "epoch 24,step 1707500, training loss 0.0412176\n",
      "epoch 24,step 1710000, training loss 0.043764\n",
      "epoch 24,step 1712500, training loss 0.0308237\n",
      "epoch 24,step 1715000, training loss 0.032263\n",
      "epoch 24,step 1717500, training loss 0.0350001\n",
      "epoch 24,step 1720000, training loss 0.0406114\n",
      "epoch 24,step 1722500, training loss 0.0352889\n",
      "epoch 24,step 1725000, training loss 0.0323998\n",
      "epoch 24,step 1727500, training loss 0.0991432\n",
      "epoch 24,step 1730000, training loss 0.0613687\n",
      "epoch 24,step 1732500, training loss 0.0309559\n",
      "epoch 24,step 1735000, training loss 0.0345707\n",
      "epoch 24,step 1737500, training loss 0.0352924\n",
      "epoch 24,step 1740000, training loss 0.028046\n",
      "epoch 24,step 1742500, training loss 0.0311074\n",
      "epoch 24,step 1745000, training loss 0.0345992\n",
      "epoch 24,step 1747500, training loss 0.0346472\n",
      "epoch 24,step 1750000, training loss 0.0292574\n",
      "epoch 24,step 1752500, training loss 0.101439\n",
      "epoch 24,step 1755000, training loss 0.071433\n",
      "epoch 24,step 1757500, training loss 0.0358338\n",
      "epoch 24,step 1760000, training loss 0.0306695\n",
      "epoch 24,step 1762500, training loss 0.0309642\n",
      "epoch 24,step 1765000, training loss 0.0296823\n",
      "epoch 24,step 1767500, training loss 0.0337168\n",
      "epoch 24,step 1770000, training loss 0.0232088\n",
      "epoch 24,step 1772500, training loss 0.0256113\n",
      "epoch 24,step 1775000, training loss 0.0289838\n",
      "epoch 24,step 1777500, training loss 0.0922305\n",
      "epoch 24,step 1780000, training loss 0.0610023\n",
      "epoch 24,step 1782500, training loss 0.0350566\n",
      "epoch 24,step 1785000, training loss 0.030095\n",
      "epoch 24,step 1787500, training loss 0.0328438\n",
      "epoch 24,step 1790000, training loss 0.0292713\n",
      "epoch 24,step 1792500, training loss 0.042631\n",
      "epoch 24,step 1795000, training loss 0.0419756\n",
      "epoch 24,step 1797500, training loss 0.0290479\n",
      "epoch 24,step 1800000, training loss 0.0293169\n",
      "epoch 24,step 1802500, training loss 0.0737032\n",
      "epoch 24,step 1805000, training loss 0.0466003\n",
      "epoch 24,step 1807500, training loss 0.0429097\n",
      "epoch 24,step 1810000, training loss 0.0344891\n",
      "epoch 24,step 1812500, training loss 0.0337157\n",
      "epoch 24,step 1815000, training loss 0.0273463\n",
      "epoch 24,step 1817500, training loss 0.0423705\n",
      "epoch 24,step 1820000, training loss 0.0311\n",
      "epoch 24,step 1822500, training loss 0.0276975\n",
      "epoch 24,step 1825000, training loss 0.0273308\n",
      "epoch 24,step 1827500, training loss 0.0950091\n",
      "epoch 24,step 1830000, training loss 0.0520077\n",
      "epoch 24,step 1832500, training loss 0.0292955\n",
      "epoch 24,step 1835000, training loss 0.0427232\n",
      "epoch 24,step 1837500, training loss 0.0286038\n",
      "epoch 24,step 1840000, training loss 0.0315439\n",
      "epoch 24,step 1842500, training loss 0.0287197\n",
      "epoch 24,step 1845000, training loss 0.038675\n",
      "epoch 24,step 1847500, training loss 0.0587446\n",
      "epoch 24,step 1850000, training loss 0.0397793\n",
      "epoch 24,step 1852500, training loss 0.0724443\n",
      "epoch 24,step 1855000, training loss 0.0439626\n",
      "epoch 24,step 1857500, training loss 0.0252517\n",
      "epoch 24,step 1860000, training loss 0.0238856\n",
      "epoch 24,step 1862500, training loss 0.0267279\n",
      "epoch 24,step 1865000, training loss 0.029933\n",
      "epoch 24,step 1867500, training loss 0.0258576\n",
      "epoch 24,step 1870000, training loss 0.024316\n",
      "epoch 24,step 1872500, training loss 0.0251565\n",
      "epoch 24,step 1875000, training loss 0.0622551\n",
      "epoch 24,step 1877500, training loss 0.0811001\n",
      "epoch 24,step 1880000, training loss 0.0392722\n",
      "epoch 24,step 1882500, training loss 0.0276156\n",
      "epoch 24,step 1885000, training loss 0.0268791\n",
      "epoch 24,step 1887500, training loss 0.0263908\n",
      "epoch 24,step 1890000, training loss 0.0297278\n",
      "epoch 24,step 1892500, training loss 0.0269964\n",
      "epoch 24,step 1895000, training loss 0.0259015\n",
      "epoch 24,step 1897500, training loss 0.0186896\n",
      "epoch 24,step 1900000, training loss 0.0248065\n",
      "epoch 24,step 1902500, training loss 0.0847692\n",
      "epoch 24,step 1905000, training loss 0.0784732\n",
      "epoch 24,step 1907500, training loss 0.0301683\n",
      "epoch 24,step 1910000, training loss 0.0255635\n",
      "epoch 24,step 1912500, training loss 0.0398489\n",
      "epoch 24,step 1915000, training loss 0.0462098\n",
      "epoch 24,step 1917500, training loss 0.0249992\n",
      "epoch 24,step 1920000, training loss 0.0374338\n",
      "epoch 24,step 1922500, training loss 0.0297102\n",
      "epoch 24,step 1925000, training loss 0.0198567\n",
      "epoch 24,step 1927500, training loss 0.113664\n",
      "epoch 24,step 1930000, training loss 0.0821074\n",
      "epoch 24,step 1932500, training loss 0.0370545\n",
      "epoch 24,step 1935000, training loss 0.0366737\n",
      "epoch 24,step 1937500, training loss 0.0316672\n",
      "epoch 24,step 1940000, training loss 0.0237695\n",
      "epoch 24,step 1942500, training loss 0.026529\n",
      "epoch 24,step 1945000, training loss 0.0346677\n",
      "epoch 24,step 1947500, training loss 0.0346294\n",
      "epoch 24,step 1950000, training loss 0.0230063\n",
      "epoch 24,step 1952500, training loss 0.0792697\n",
      "epoch 24,step 1955000, training loss 0.0679891\n",
      "epoch 24,step 1957500, training loss 0.0359298\n",
      "epoch 24,step 1960000, training loss 0.0253904\n",
      "epoch 24,step 1962500, training loss 0.0297975\n",
      "epoch 24,step 1965000, training loss 0.0248893\n",
      "epoch 24,step 1967500, training loss 0.0514102\n",
      "epoch 24,step 1970000, training loss 0.0339876\n",
      "epoch 24,step 1972500, training loss 0.0262839\n",
      "epoch 24,step 1975000, training loss 0.031536\n",
      "epoch 24,step 1977500, training loss 0.112636\n",
      "epoch 24,step 1980000, training loss 0.0592891\n",
      "epoch 24,step 1982500, training loss 0.0484126\n",
      "epoch 24,step 1985000, training loss 0.04747\n",
      "epoch 24,step 1987500, training loss 0.0300553\n",
      "epoch 24,step 1990000, training loss 0.0322614\n",
      "epoch 24,step 1992500, training loss 0.0231526\n",
      "epoch 24,step 1995000, training loss 0.0340084\n",
      "epoch 24,step 1997500, training loss 0.024693\n",
      "epoch 24,step 2000000, training loss 0.0260073\n",
      "epoch 24,step 2002500, training loss 0.0996945\n",
      "epoch 24,step 2005000, training loss 0.0409375\n",
      "epoch 24,step 2007500, training loss 0.0383532\n",
      "epoch 24,step 2010000, training loss 0.04121\n",
      "epoch 24,step 2012500, training loss 0.0318695\n",
      "epoch 24,step 2015000, training loss 0.0376625\n",
      "epoch 24,step 2017500, training loss 0.0314535\n",
      "epoch 24,step 2020000, training loss 0.0394183\n",
      "epoch 24,step 2022500, training loss 0.0315922\n",
      "epoch 24,step 2025000, training loss 0.0257756\n",
      "epoch 24,step 2027500, training loss 0.0869265\n",
      "epoch 24,step 2030000, training loss 0.0698749\n",
      "epoch 24,step 2032500, training loss 0.0357562\n",
      "epoch 24,step 2035000, training loss 0.0364182\n",
      "epoch 24,step 2037500, training loss 0.0368705\n",
      "epoch 24,step 2040000, training loss 0.0297149\n",
      "epoch 24,step 2042500, training loss 0.0302578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24,step 2045000, training loss 0.0356279\n",
      "epoch 24,step 2047500, training loss 0.0321112\n",
      "epoch 24,step 2050000, training loss 0.0344522\n",
      "epoch 24,step 2052500, training loss 0.104163\n",
      "epoch 24,step 2055000, training loss 0.0428148\n",
      "epoch 24,step 2057500, training loss 0.0286427\n",
      "epoch 24,step 2060000, training loss 0.0406096\n",
      "epoch 24,step 2062500, training loss 0.0466434\n",
      "epoch 24,step 2065000, training loss 0.0362417\n",
      "epoch 24,step 2067500, training loss 0.03049\n",
      "epoch 24,step 2070000, training loss 0.0521862\n",
      "epoch 24,step 2072500, training loss 0.0450034\n",
      "epoch 24,step 2075000, training loss 0.0355499\n",
      "epoch 24,step 2077500, training loss 0.100341\n",
      "epoch 24,step 2080000, training loss 0.0647476\n",
      "epoch 24,step 2082500, training loss 0.0292276\n",
      "epoch 24,step 2085000, training loss 0.033159\n",
      "epoch 24,step 2087500, training loss 0.0307742\n",
      "epoch 24,step 2090000, training loss 0.0389637\n",
      "epoch 24,step 2092500, training loss 0.0287561\n",
      "epoch 24,step 2095000, training loss 0.0285467\n",
      "epoch 24,step 2097500, training loss 0.0385986\n",
      "epoch 24,step 2100000, training loss 0.0293126\n",
      "epoch 24,step 2102500, training loss 0.100393\n",
      "epoch 24,step 2105000, training loss 0.0754486\n",
      "epoch 24,step 2107500, training loss 0.037906\n",
      "epoch 24,step 2110000, training loss 0.0321421\n",
      "epoch 24,step 2112500, training loss 0.0371616\n",
      "epoch 24,step 2115000, training loss 0.0341787\n",
      "epoch 24,step 2117500, training loss 0.0475445\n",
      "epoch 24,step 2120000, training loss 0.0251684\n",
      "epoch 24,step 2122500, training loss 0.0312695\n",
      "epoch 24,step 2125000, training loss 0.0339621\n",
      "epoch 24,step 2127500, training loss 0.0871618\n",
      "epoch 24,step 2130000, training loss 0.0508779\n",
      "epoch 24,step 2132500, training loss 0.032142\n",
      "epoch 24,step 2135000, training loss 0.044639\n",
      "epoch 24,step 2137500, training loss 0.0411433\n",
      "epoch 24,step 2140000, training loss 0.0382367\n",
      "epoch 24,step 2142500, training loss 0.0306393\n",
      "epoch 24,step 2145000, training loss 0.035758\n",
      "epoch 24,step 2147500, training loss 0.0350022\n",
      "epoch 24,step 2150000, training loss 0.0452886\n",
      "epoch 24,step 2152500, training loss 0.0832941\n",
      "epoch 24,step 2155000, training loss 0.0571692\n",
      "epoch 24,step 2157500, training loss 0.0283652\n",
      "epoch 24,step 2160000, training loss 0.0289666\n",
      "epoch 24,step 2162500, training loss 0.0276068\n",
      "epoch 24,step 2165000, training loss 0.0256603\n",
      "epoch 24,step 2167500, training loss 0.0297741\n",
      "epoch 24,step 2170000, training loss 0.0280664\n",
      "epoch 24,step 2172500, training loss 0.0350827\n",
      "epoch 24,step 2175000, training loss 0.0359787\n",
      "epoch 24,step 2177500, training loss 0.0926801\n",
      "epoch 24,step 2180000, training loss 0.0492186\n",
      "epoch 24,step 2182500, training loss 0.0277246\n",
      "epoch 24,step 2185000, training loss 0.0303075\n",
      "epoch 24,step 2187500, training loss 0.0287113\n",
      "epoch 24,step 2190000, training loss 0.0437815\n",
      "epoch 24,step 2192500, training loss 0.0429413\n",
      "epoch 24,step 2195000, training loss 0.0253606\n",
      "epoch 24,step 2197500, training loss 0.0219652\n",
      "epoch 24,step 2200000, training loss 0.0287255\n",
      "epoch 24,step 2202500, training loss 0.136808\n",
      "epoch 24,step 2205000, training loss 0.0809656\n",
      "epoch 24,step 2207500, training loss 0.0508998\n",
      "epoch 24,step 2210000, training loss 0.0294174\n",
      "epoch 24,step 2212500, training loss 0.0300056\n",
      "epoch 24,step 2215000, training loss 0.0266305\n",
      "epoch 24,step 2217500, training loss 0.0260975\n",
      "epoch 24,step 2220000, training loss 0.0262591\n",
      "epoch 24,step 2222500, training loss 0.0537273\n",
      "epoch 24,step 2225000, training loss 0.0250281\n",
      "epoch 24,step 2227500, training loss 0.104091\n",
      "epoch 24,step 2230000, training loss 0.0887678\n",
      "epoch 24,step 2232500, training loss 0.0429249\n",
      "epoch 24,step 2235000, training loss 0.032353\n",
      "epoch 24,step 2237500, training loss 0.0389474\n",
      "epoch 24,step 2240000, training loss 0.0404333\n",
      "epoch 24,step 2242500, training loss 0.0362008\n",
      "epoch 24,step 2245000, training loss 0.0322273\n",
      "epoch 24,step 2247500, training loss 0.0306991\n",
      "epoch 24,step 2250000, training loss 0.0649319\n",
      "epoch 24,step 2252500, training loss 0.0860208\n",
      "epoch 24,step 2255000, training loss 0.0360529\n",
      "epoch 24,step 2257500, training loss 0.0271652\n",
      "epoch 24,step 2260000, training loss 0.0451648\n",
      "epoch 24,step 2262500, training loss 0.0289236\n",
      "epoch 24,step 2265000, training loss 0.0273693\n",
      "epoch 24,step 2267500, training loss 0.024186\n",
      "epoch 24,step 2270000, training loss 0.0253248\n",
      "epoch 24,step 2272500, training loss 0.028979\n",
      "epoch 24,step 2275000, training loss 0.0279437\n",
      "epoch 24,step 2277500, training loss 0.0826951\n",
      "epoch 24,step 2280000, training loss 0.0506456\n",
      "epoch 24,step 2282500, training loss 0.0329347\n",
      "epoch 24,step 2285000, training loss 0.0363733\n",
      "epoch 24,step 2287500, training loss 0.0363317\n",
      "epoch 24,step 2290000, training loss 0.0288981\n",
      "epoch 24,step 2292500, training loss 0.0299713\n",
      "epoch 24,step 2295000, training loss 0.0371071\n",
      "epoch 24,step 2297500, training loss 0.0353478\n",
      "epoch 24,step 2300000, training loss 0.0295219\n",
      "epoch 24,step 2302500, training loss 0.079328\n",
      "epoch 24,step 2305000, training loss 0.0674592\n",
      "epoch 24,step 2307500, training loss 0.0309954\n",
      "epoch 24,step 2310000, training loss 0.0271991\n",
      "epoch 24,step 2312500, training loss 0.0235098\n",
      "epoch 24,step 2315000, training loss 0.0326581\n",
      "epoch 24,step 2317500, training loss 0.0279152\n",
      "epoch 24,step 2320000, training loss 0.022133\n",
      "epoch 24,step 2322500, training loss 0.0268475\n",
      "epoch 24,step 2325000, training loss 0.023591\n",
      "epoch 24,step 2327500, training loss 0.0891432\n",
      "epoch 24,step 2330000, training loss 0.0470121\n",
      "epoch 24,step 2332500, training loss 0.0272981\n",
      "epoch 24,step 2335000, training loss 0.0320523\n",
      "epoch 24,step 2337500, training loss 0.0404478\n",
      "epoch 24,step 2340000, training loss 0.0288428\n",
      "epoch 24,step 2342500, training loss 0.0233934\n",
      "epoch 24,step 2345000, training loss 0.037934\n",
      "epoch 24,step 2347500, training loss 0.0269841\n",
      "epoch 24,step 2350000, training loss 0.0256933\n",
      "epoch 24,step 2352500, training loss 0.0992549\n",
      "epoch 24,step 2355000, training loss 0.0601171\n",
      "epoch 24,step 2357500, training loss 0.0281647\n",
      "epoch 24,step 2360000, training loss 0.0350816\n",
      "epoch 24,step 2362500, training loss 0.0308045\n",
      "epoch 24,step 2365000, training loss 0.0368371\n",
      "epoch 24,step 2367500, training loss 0.041302\n",
      "epoch 24,step 2370000, training loss 0.0245427\n",
      "epoch 24,step 2372500, training loss 0.0331019\n",
      "epoch 24,step 2375000, training loss 0.0218958\n",
      "epoch 24,step 2377500, training loss 0.0953057\n",
      "epoch 24,step 2380000, training loss 0.0820273\n",
      "epoch 24,step 2382500, training loss 0.0309085\n",
      "epoch 24,step 2385000, training loss 0.0237807\n",
      "epoch 24,step 2387500, training loss 0.0256615\n",
      "epoch 24,step 2390000, training loss 0.0342188\n",
      "epoch 24,step 2392500, training loss 0.0329417\n",
      "epoch 24,step 2395000, training loss 0.0397527\n",
      "epoch 24,step 2397500, training loss 0.0295425\n",
      "epoch 24,step 2400000, training loss 0.0329691\n",
      "epoch 24,step 2402500, training loss 0.0929518\n",
      "epoch 24,step 2405000, training loss 0.0524714\n",
      "epoch 24,step 2407500, training loss 0.0214981\n",
      "epoch 24,step 2410000, training loss 0.0218317\n",
      "epoch 24,step 2412500, training loss 0.0231283\n",
      "epoch 24,step 2415000, training loss 0.0273222\n",
      "epoch 24,step 2417500, training loss 0.0262593\n",
      "epoch 24,step 2420000, training loss 0.0262648\n",
      "epoch 24,step 2422500, training loss 0.0260366\n",
      "epoch 24,step 2425000, training loss 0.0317446\n",
      "epoch 24,step 2427500, training loss 0.101526\n",
      "epoch 24,step 2430000, training loss 0.048865\n",
      "epoch 24,step 2432500, training loss 0.0256211\n",
      "epoch 24,step 2435000, training loss 0.0281131\n",
      "epoch 24,step 2437500, training loss 0.0359007\n",
      "epoch 24,step 2440000, training loss 0.024832\n",
      "epoch 24,step 2442500, training loss 0.025114\n",
      "epoch 24,step 2445000, training loss 0.0280928\n",
      "epoch 24,step 2447500, training loss 0.025323\n",
      "epoch 24,step 2450000, training loss 0.0284241\n",
      "epoch 24,step 2452500, training loss 0.086887\n",
      "epoch 24,step 2455000, training loss 0.0548948\n",
      "epoch 24,step 2457500, training loss 0.040325\n",
      "epoch 24,step 2460000, training loss 0.0275701\n",
      "epoch 24,step 2462500, training loss 0.0224208\n",
      "epoch 24,step 2465000, training loss 0.0542849\n",
      "epoch 24,step 2467500, training loss 0.0279815\n",
      "epoch 24,step 2470000, training loss 0.0314485\n",
      "epoch 24,step 2472500, training loss 0.0254429\n",
      "epoch 24,step 2475000, training loss 0.0228491\n",
      "epoch 24,step 2477500, training loss 0.0902961\n",
      "epoch 24,step 2480000, training loss 0.0413731\n",
      "epoch 24,step 2482500, training loss 0.030312\n",
      "epoch 24,step 2485000, training loss 0.0223929\n",
      "epoch 24,step 2487500, training loss 0.0248688\n",
      "epoch 24,step 2490000, training loss 0.027178\n",
      "epoch 24,step 2492500, training loss 0.0208217\n",
      "epoch 24,step 2495000, training loss 0.0274346\n",
      "epoch 24,step 2497500, training loss 0.0386269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24,training loss 0.0386269 ,test loss 0.0417044\n",
      "epoch 25,step 13000, training loss 0.0353683\n",
      "epoch 25,step 26000, training loss 0.0438335\n",
      "epoch 25,step 39000, training loss 0.0363686\n",
      "epoch 25,step 52000, training loss 0.022872\n",
      "epoch 25,step 65000, training loss 0.0247725\n",
      "epoch 25,step 78000, training loss 0.0243369\n",
      "epoch 25,step 91000, training loss 0.0281835\n",
      "epoch 25,step 104000, training loss 0.0236224\n",
      "epoch 25,step 117000, training loss 0.0287832\n",
      "epoch 25,step 130000, training loss 0.0264608\n",
      "epoch 25,step 143000, training loss 0.0270907\n",
      "epoch 25,step 156000, training loss 0.0353801\n",
      "epoch 25,step 169000, training loss 0.0561855\n",
      "epoch 25,step 182000, training loss 0.0290556\n",
      "epoch 25,step 195000, training loss 0.0432435\n",
      "epoch 25,step 208000, training loss 0.0275987\n",
      "epoch 25,step 221000, training loss 0.0284099\n",
      "epoch 25,step 234000, training loss 0.024056\n",
      "epoch 25,step 247000, training loss 0.0211882\n",
      "epoch 25,step 260000, training loss 0.034615\n",
      "epoch 25,step 273000, training loss 0.0269638\n",
      "epoch 25,step 286000, training loss 0.032888\n",
      "epoch 25,step 299000, training loss 0.0434992\n",
      "epoch 25,step 312000, training loss 0.0353686\n",
      "epoch 25,step 325000, training loss 0.0352409\n",
      "epoch 25,step 338000, training loss 0.01958\n",
      "epoch 25,step 351000, training loss 0.0485648\n",
      "epoch 25,step 364000, training loss 0.0299448\n",
      "epoch 25,step 377000, training loss 0.0388591\n",
      "epoch 25,step 390000, training loss 0.0309104\n",
      "epoch 25,step 403000, training loss 0.0475541\n",
      "epoch 25,step 416000, training loss 0.0296073\n",
      "epoch 25,step 429000, training loss 0.0309671\n",
      "epoch 25,step 442000, training loss 0.0312217\n",
      "epoch 25,step 455000, training loss 0.0406548\n",
      "epoch 25,step 468000, training loss 0.0282829\n",
      "epoch 25,step 481000, training loss 0.0329559\n",
      "epoch 25,step 494000, training loss 0.0318476\n",
      "epoch 25,step 507000, training loss 0.0289975\n",
      "epoch 25,step 520000, training loss 0.0260519\n",
      "epoch 25,step 533000, training loss 0.0342\n",
      "epoch 25,step 546000, training loss 0.026118\n",
      "epoch 25,step 559000, training loss 0.0289647\n",
      "epoch 25,step 572000, training loss 0.0318778\n",
      "epoch 25,step 585000, training loss 0.0258261\n",
      "epoch 25,step 598000, training loss 0.0335731\n",
      "epoch 25,step 611000, training loss 0.0207917\n",
      "epoch 25,step 624000, training loss 0.0276759\n",
      "epoch 25,step 637000, training loss 0.0298016\n",
      "epoch 25,step 650000, training loss 0.0269546\n",
      "epoch 25,step 663000, training loss 0.0299516\n",
      "epoch 25,step 676000, training loss 0.0344099\n",
      "epoch 25,step 689000, training loss 0.0235562\n",
      "epoch 25,step 702000, training loss 0.028928\n",
      "epoch 25,step 715000, training loss 0.04943\n",
      "epoch 25,step 728000, training loss 0.0260052\n",
      "epoch 25,step 741000, training loss 0.0228366\n",
      "epoch 25,step 754000, training loss 0.0324978\n",
      "epoch 25,step 767000, training loss 0.0378871\n",
      "epoch 25,step 780000, training loss 0.0300129\n",
      "epoch 25,step 793000, training loss 0.0318301\n",
      "epoch 25,step 806000, training loss 0.0296201\n",
      "epoch 25,step 819000, training loss 0.0275266\n",
      "epoch 25,step 832000, training loss 0.0394167\n",
      "epoch 25,step 845000, training loss 0.0437939\n",
      "epoch 25,step 858000, training loss 0.0279406\n",
      "epoch 25,step 871000, training loss 0.0298044\n",
      "epoch 25,step 884000, training loss 0.0409716\n",
      "epoch 25,step 897000, training loss 0.0232205\n",
      "epoch 25,step 910000, training loss 0.03033\n",
      "epoch 25,step 923000, training loss 0.0374418\n",
      "epoch 25,step 936000, training loss 0.0218127\n",
      "epoch 25,step 949000, training loss 0.0470183\n",
      "epoch 25,step 962000, training loss 0.0403802\n",
      "epoch 25,step 975000, training loss 0.036262\n",
      "epoch 25,step 988000, training loss 0.0298181\n",
      "epoch 25,step 1001000, training loss 0.0313379\n",
      "epoch 25,step 1014000, training loss 0.0277326\n",
      "epoch 25,step 1027000, training loss 0.0328293\n",
      "epoch 25,step 1040000, training loss 0.0204474\n",
      "epoch 25,step 1053000, training loss 0.0261374\n",
      "epoch 25,step 1066000, training loss 0.027057\n",
      "epoch 25,step 1079000, training loss 0.0272057\n",
      "epoch 25,step 1092000, training loss 0.0252342\n",
      "epoch 25,step 1105000, training loss 0.0379744\n",
      "epoch 25,step 1118000, training loss 0.0301743\n",
      "epoch 25,step 1131000, training loss 0.038834\n",
      "epoch 25,step 1144000, training loss 0.0302098\n",
      "epoch 25,step 1157000, training loss 0.0213579\n",
      "epoch 25,step 1170000, training loss 0.0292592\n",
      "epoch 25,step 1183000, training loss 0.0249394\n",
      "epoch 25,step 1196000, training loss 0.0279291\n",
      "epoch 25,step 1209000, training loss 0.0425275\n",
      "epoch 25,step 1222000, training loss 0.0315541\n",
      "epoch 25,step 1235000, training loss 0.0244909\n",
      "epoch 25,step 1248000, training loss 0.0316054\n",
      "epoch 25,step 1261000, training loss 0.0277988\n",
      "epoch 25,step 1274000, training loss 0.03486\n",
      "epoch 25,step 1287000, training loss 0.0376724\n",
      "epoch 25,step 1300000, training loss 0.0298682\n",
      "epoch 25,step 1313000, training loss 0.031096\n",
      "epoch 25,step 1326000, training loss 0.0263214\n",
      "epoch 25,step 1339000, training loss 0.035458\n",
      "epoch 25,step 1352000, training loss 0.0231971\n",
      "epoch 25,step 1365000, training loss 0.0363514\n",
      "epoch 25,step 1378000, training loss 0.0408517\n",
      "epoch 25,step 1391000, training loss 0.0408874\n",
      "epoch 25,step 1404000, training loss 0.0329821\n",
      "epoch 25,step 1417000, training loss 0.0412052\n",
      "epoch 25,step 1430000, training loss 0.0513129\n",
      "epoch 25,step 1443000, training loss 0.036934\n",
      "epoch 25,step 1456000, training loss 0.0399463\n",
      "epoch 25,step 1469000, training loss 0.0315142\n",
      "epoch 25,step 1482000, training loss 0.0268927\n",
      "epoch 25,step 1495000, training loss 0.0500733\n",
      "epoch 25,step 1508000, training loss 0.0228942\n",
      "epoch 25,step 1521000, training loss 0.0301711\n",
      "epoch 25,step 1534000, training loss 0.0245774\n",
      "epoch 25,step 1547000, training loss 0.0321033\n",
      "epoch 25,step 1560000, training loss 0.0363924\n",
      "epoch 25,step 1573000, training loss 0.0228067\n",
      "epoch 25,step 1586000, training loss 0.0235649\n",
      "epoch 25,step 1599000, training loss 0.0276245\n",
      "epoch 25,step 1612000, training loss 0.0374454\n",
      "epoch 25,step 1625000, training loss 0.0379021\n",
      "epoch 25,step 1638000, training loss 0.0330508\n",
      "epoch 25,step 1651000, training loss 0.0250402\n",
      "epoch 25,step 1664000, training loss 0.0311545\n",
      "epoch 25,step 1677000, training loss 0.036767\n",
      "epoch 25,step 1690000, training loss 0.0332647\n",
      "epoch 25,step 1703000, training loss 0.0298649\n",
      "epoch 25,step 1716000, training loss 0.0268973\n",
      "epoch 25,step 1729000, training loss 0.0239996\n",
      "epoch 25,step 1742000, training loss 0.0317761\n",
      "epoch 25,step 1755000, training loss 0.0239151\n",
      "epoch 25,step 1768000, training loss 0.0332276\n",
      "epoch 25,step 1781000, training loss 0.0286705\n",
      "epoch 25,step 1794000, training loss 0.0312512\n",
      "epoch 25,step 1807000, training loss 0.0333616\n",
      "epoch 25,step 1820000, training loss 0.028056\n",
      "epoch 25,step 1833000, training loss 0.0299939\n",
      "epoch 25,step 1846000, training loss 0.0272491\n",
      "epoch 25,step 1859000, training loss 0.0333975\n",
      "epoch 25,step 1872000, training loss 0.0281629\n",
      "epoch 25,step 1885000, training loss 0.0342543\n",
      "epoch 25,step 1898000, training loss 0.0272805\n",
      "epoch 25,step 1911000, training loss 0.0290851\n",
      "epoch 25,step 1924000, training loss 0.0400353\n",
      "epoch 25,step 1937000, training loss 0.0254856\n",
      "epoch 25,step 1950000, training loss 0.060895\n",
      "epoch 25,step 1963000, training loss 0.0257172\n",
      "epoch 25,step 1976000, training loss 0.0241971\n",
      "epoch 25,step 1989000, training loss 0.0392251\n",
      "epoch 25,step 2002000, training loss 0.0188768\n",
      "epoch 25,step 2015000, training loss 0.0309873\n",
      "epoch 25,step 2028000, training loss 0.0222574\n",
      "epoch 25,step 2041000, training loss 0.0295082\n",
      "epoch 25,step 2054000, training loss 0.0316249\n",
      "epoch 25,step 2067000, training loss 0.0306752\n",
      "epoch 25,step 2080000, training loss 0.0261932\n",
      "epoch 25,step 2093000, training loss 0.0335091\n",
      "epoch 25,step 2106000, training loss 0.026124\n",
      "epoch 25,step 2119000, training loss 0.0362674\n",
      "epoch 25,step 2132000, training loss 0.033778\n",
      "epoch 25,step 2145000, training loss 0.0462104\n",
      "epoch 25,step 2158000, training loss 0.0354614\n",
      "epoch 25,step 2171000, training loss 0.0300498\n",
      "epoch 25,step 2184000, training loss 0.0291067\n",
      "epoch 25,step 2197000, training loss 0.0373223\n",
      "epoch 25,step 2210000, training loss 0.033428\n",
      "epoch 25,step 2223000, training loss 0.0408603\n",
      "epoch 25,step 2236000, training loss 0.0456859\n",
      "epoch 25,step 2249000, training loss 0.0286937\n",
      "epoch 25,step 2262000, training loss 0.0356987\n",
      "epoch 25,step 2275000, training loss 0.0292406\n",
      "epoch 25,step 2288000, training loss 0.0289176\n",
      "epoch 25,step 2301000, training loss 0.0295027\n",
      "epoch 25,step 2314000, training loss 0.024696\n",
      "epoch 25,step 2327000, training loss 0.0387738\n",
      "epoch 25,step 2340000, training loss 0.0637778\n",
      "epoch 25,step 2353000, training loss 0.0285211\n",
      "epoch 25,step 2366000, training loss 0.0284286\n",
      "epoch 25,step 2379000, training loss 0.035193\n",
      "epoch 25,step 2392000, training loss 0.0285134\n",
      "epoch 25,step 2405000, training loss 0.0242658\n",
      "epoch 25,step 2418000, training loss 0.0232896\n",
      "epoch 25,step 2431000, training loss 0.0406043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25,step 2444000, training loss 0.0248208\n",
      "epoch 25,step 2457000, training loss 0.030732\n",
      "epoch 25,step 2470000, training loss 0.0221344\n",
      "epoch 25,step 2483000, training loss 0.0264399\n",
      "epoch 25,step 2496000, training loss 0.0329614\n",
      "epoch 25,step 2509000, training loss 0.0235712\n",
      "epoch 25,step 2522000, training loss 0.0308482\n",
      "epoch 25,step 2535000, training loss 0.0363363\n",
      "epoch 25,step 2548000, training loss 0.0287975\n",
      "epoch 25,step 2561000, training loss 0.0228933\n",
      "epoch 25,step 2574000, training loss 0.0223835\n",
      "epoch 25,step 2587000, training loss 0.0244082\n",
      "epoch 25,training loss 0.0383422 ,test loss 0.0417892\n",
      "epoch 26,step 13500, training loss 0.0358181\n",
      "epoch 26,step 27000, training loss 0.0436656\n",
      "epoch 26,step 40500, training loss 0.0367326\n",
      "epoch 26,step 54000, training loss 0.0225775\n",
      "epoch 26,step 67500, training loss 0.0245368\n",
      "epoch 26,step 81000, training loss 0.0241284\n",
      "epoch 26,step 94500, training loss 0.0278184\n",
      "epoch 26,step 108000, training loss 0.0235387\n",
      "epoch 26,step 121500, training loss 0.0282686\n",
      "epoch 26,step 135000, training loss 0.0265007\n",
      "epoch 26,step 148500, training loss 0.0262869\n",
      "epoch 26,step 162000, training loss 0.0353991\n",
      "epoch 26,step 175500, training loss 0.0555341\n",
      "epoch 26,step 189000, training loss 0.0289262\n",
      "epoch 26,step 202500, training loss 0.0423086\n",
      "epoch 26,step 216000, training loss 0.0273905\n",
      "epoch 26,step 229500, training loss 0.0272111\n",
      "epoch 26,step 243000, training loss 0.0238111\n",
      "epoch 26,step 256500, training loss 0.0210862\n",
      "epoch 26,step 270000, training loss 0.0340469\n",
      "epoch 26,step 283500, training loss 0.0266367\n",
      "epoch 26,step 297000, training loss 0.0318876\n",
      "epoch 26,step 310500, training loss 0.0433659\n",
      "epoch 26,step 324000, training loss 0.0345852\n",
      "epoch 26,step 337500, training loss 0.0345723\n",
      "epoch 26,step 351000, training loss 0.0190433\n",
      "epoch 26,step 364500, training loss 0.04721\n",
      "epoch 26,step 378000, training loss 0.0295468\n",
      "epoch 26,step 391500, training loss 0.0376628\n",
      "epoch 26,step 405000, training loss 0.0299863\n",
      "epoch 26,step 418500, training loss 0.0449734\n",
      "epoch 26,step 432000, training loss 0.0276068\n",
      "epoch 26,step 445500, training loss 0.0299702\n",
      "epoch 26,step 459000, training loss 0.0292382\n",
      "epoch 26,step 472500, training loss 0.0393783\n",
      "epoch 26,step 486000, training loss 0.0288115\n",
      "epoch 26,step 499500, training loss 0.0327925\n",
      "epoch 26,step 513000, training loss 0.0317045\n",
      "epoch 26,step 526500, training loss 0.0288986\n",
      "epoch 26,step 540000, training loss 0.0254868\n",
      "epoch 26,step 553500, training loss 0.0332795\n",
      "epoch 26,step 567000, training loss 0.0263503\n",
      "epoch 26,step 580500, training loss 0.0281004\n",
      "epoch 26,step 594000, training loss 0.0317116\n",
      "epoch 26,step 607500, training loss 0.0252067\n",
      "epoch 26,step 621000, training loss 0.0335113\n",
      "epoch 26,step 634500, training loss 0.0215001\n",
      "epoch 26,step 648000, training loss 0.0273504\n",
      "epoch 26,step 661500, training loss 0.029506\n",
      "epoch 26,step 675000, training loss 0.0276097\n",
      "epoch 26,step 688500, training loss 0.030843\n",
      "epoch 26,step 702000, training loss 0.0336745\n",
      "epoch 26,step 715500, training loss 0.0232663\n",
      "epoch 26,step 729000, training loss 0.0285192\n",
      "epoch 26,step 742500, training loss 0.0490215\n",
      "epoch 26,step 756000, training loss 0.0258217\n",
      "epoch 26,step 769500, training loss 0.0218421\n",
      "epoch 26,step 783000, training loss 0.0314071\n",
      "epoch 26,step 796500, training loss 0.0370613\n",
      "epoch 26,step 810000, training loss 0.0294306\n",
      "epoch 26,step 823500, training loss 0.0313852\n",
      "epoch 26,step 837000, training loss 0.0299242\n",
      "epoch 26,step 850500, training loss 0.0280417\n",
      "epoch 26,step 864000, training loss 0.0382694\n",
      "epoch 26,step 877500, training loss 0.0429832\n",
      "epoch 26,step 891000, training loss 0.0277609\n",
      "epoch 26,step 904500, training loss 0.0288929\n",
      "epoch 26,step 918000, training loss 0.039888\n",
      "epoch 26,step 931500, training loss 0.0218857\n",
      "epoch 26,step 945000, training loss 0.0297261\n",
      "epoch 26,step 958500, training loss 0.0377422\n",
      "epoch 26,step 972000, training loss 0.0214777\n",
      "epoch 26,step 985500, training loss 0.0466567\n",
      "epoch 26,step 999000, training loss 0.0402923\n",
      "epoch 26,step 1012500, training loss 0.0363707\n",
      "epoch 26,step 1026000, training loss 0.0295107\n",
      "epoch 26,step 1039500, training loss 0.0294283\n",
      "epoch 26,step 1053000, training loss 0.0275384\n",
      "epoch 26,step 1066500, training loss 0.0324819\n",
      "epoch 26,step 1080000, training loss 0.0203273\n",
      "epoch 26,step 1093500, training loss 0.0250655\n",
      "epoch 26,step 1107000, training loss 0.0267709\n",
      "epoch 26,step 1120500, training loss 0.0272439\n",
      "epoch 26,step 1134000, training loss 0.0245026\n",
      "epoch 26,step 1147500, training loss 0.0377667\n",
      "epoch 26,step 1161000, training loss 0.0301476\n",
      "epoch 26,step 1174500, training loss 0.038214\n",
      "epoch 26,step 1188000, training loss 0.0293446\n",
      "epoch 26,step 1201500, training loss 0.0214381\n",
      "epoch 26,step 1215000, training loss 0.0294407\n",
      "epoch 26,step 1228500, training loss 0.0256111\n",
      "epoch 26,step 1242000, training loss 0.0279143\n",
      "epoch 26,step 1255500, training loss 0.0410208\n",
      "epoch 26,step 1269000, training loss 0.0304412\n",
      "epoch 26,step 1282500, training loss 0.0237747\n",
      "epoch 26,step 1296000, training loss 0.0310375\n",
      "epoch 26,step 1309500, training loss 0.0270691\n",
      "epoch 26,step 1323000, training loss 0.0328776\n",
      "epoch 26,step 1336500, training loss 0.036521\n",
      "epoch 26,step 1350000, training loss 0.0283258\n",
      "epoch 26,step 1363500, training loss 0.0302132\n",
      "epoch 26,step 1377000, training loss 0.0258375\n",
      "epoch 26,step 1390500, training loss 0.0341305\n",
      "epoch 26,step 1404000, training loss 0.022669\n",
      "epoch 26,step 1417500, training loss 0.0362531\n",
      "epoch 26,step 1431000, training loss 0.0408822\n",
      "epoch 26,step 1444500, training loss 0.0394268\n",
      "epoch 26,step 1458000, training loss 0.0318821\n",
      "epoch 26,step 1471500, training loss 0.0409559\n",
      "epoch 26,step 1485000, training loss 0.0486109\n",
      "epoch 26,step 1498500, training loss 0.0362303\n",
      "epoch 26,step 1512000, training loss 0.038756\n",
      "epoch 26,step 1525500, training loss 0.0303336\n",
      "epoch 26,step 1539000, training loss 0.0269874\n",
      "epoch 26,step 1552500, training loss 0.0496128\n",
      "epoch 26,step 1566000, training loss 0.0232579\n",
      "epoch 26,step 1579500, training loss 0.0290148\n",
      "epoch 26,step 1593000, training loss 0.0242257\n",
      "epoch 26,step 1606500, training loss 0.0315439\n",
      "epoch 26,step 1620000, training loss 0.0333476\n",
      "epoch 26,step 1633500, training loss 0.0228526\n",
      "epoch 26,step 1647000, training loss 0.0231752\n",
      "epoch 26,step 1660500, training loss 0.0268204\n",
      "epoch 26,step 1674000, training loss 0.0362677\n",
      "epoch 26,step 1687500, training loss 0.0362357\n",
      "epoch 26,step 1701000, training loss 0.032724\n",
      "epoch 26,step 1714500, training loss 0.0243435\n",
      "epoch 26,step 1728000, training loss 0.0301942\n",
      "epoch 26,step 1741500, training loss 0.0359989\n",
      "epoch 26,step 1755000, training loss 0.0334091\n",
      "epoch 26,step 1768500, training loss 0.0307606\n",
      "epoch 26,step 1782000, training loss 0.0271039\n",
      "epoch 26,step 1795500, training loss 0.0232296\n",
      "epoch 26,step 1809000, training loss 0.0313981\n",
      "epoch 26,step 1822500, training loss 0.0228343\n",
      "epoch 26,step 1836000, training loss 0.0318428\n",
      "epoch 26,step 1849500, training loss 0.0288372\n",
      "epoch 26,step 1863000, training loss 0.0298799\n",
      "epoch 26,step 1876500, training loss 0.0330153\n",
      "epoch 26,step 1890000, training loss 0.0281977\n",
      "epoch 26,step 1903500, training loss 0.0277956\n",
      "epoch 26,step 1917000, training loss 0.0278405\n",
      "epoch 26,step 1930500, training loss 0.0321725\n",
      "epoch 26,step 1944000, training loss 0.02768\n",
      "epoch 26,step 1957500, training loss 0.0344481\n",
      "epoch 26,step 1971000, training loss 0.027081\n",
      "epoch 26,step 1984500, training loss 0.0292595\n",
      "epoch 26,step 1998000, training loss 0.0386372\n",
      "epoch 26,step 2011500, training loss 0.0255717\n",
      "epoch 26,step 2025000, training loss 0.0599353\n",
      "epoch 26,step 2038500, training loss 0.0265473\n",
      "epoch 26,step 2052000, training loss 0.0237652\n",
      "epoch 26,step 2065500, training loss 0.0385814\n",
      "epoch 26,step 2079000, training loss 0.0192209\n",
      "epoch 26,step 2092500, training loss 0.0318343\n",
      "epoch 26,step 2106000, training loss 0.0226337\n",
      "epoch 26,step 2119500, training loss 0.029836\n",
      "epoch 26,step 2133000, training loss 0.0317651\n",
      "epoch 26,step 2146500, training loss 0.0298865\n",
      "epoch 26,step 2160000, training loss 0.0249851\n",
      "epoch 26,step 2173500, training loss 0.0324827\n",
      "epoch 26,step 2187000, training loss 0.0256556\n",
      "epoch 26,step 2200500, training loss 0.0350956\n",
      "epoch 26,step 2214000, training loss 0.0323508\n",
      "epoch 26,step 2227500, training loss 0.0455066\n",
      "epoch 26,step 2241000, training loss 0.0345404\n",
      "epoch 26,step 2254500, training loss 0.0296143\n",
      "epoch 26,step 2268000, training loss 0.0287242\n",
      "epoch 26,step 2281500, training loss 0.0369956\n",
      "epoch 26,step 2295000, training loss 0.0322288\n",
      "epoch 26,step 2308500, training loss 0.0400993\n",
      "epoch 26,step 2322000, training loss 0.0447\n",
      "epoch 26,step 2335500, training loss 0.0291949\n",
      "epoch 26,step 2349000, training loss 0.0356186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26,step 2362500, training loss 0.0277749\n",
      "epoch 26,step 2376000, training loss 0.0280632\n",
      "epoch 26,step 2389500, training loss 0.0307551\n",
      "epoch 26,step 2403000, training loss 0.024854\n",
      "epoch 26,step 2416500, training loss 0.0384044\n",
      "epoch 26,step 2430000, training loss 0.0641627\n",
      "epoch 26,step 2443500, training loss 0.0283084\n",
      "epoch 26,step 2457000, training loss 0.0280812\n",
      "epoch 26,step 2470500, training loss 0.0346885\n",
      "epoch 26,step 2484000, training loss 0.0283054\n",
      "epoch 26,step 2497500, training loss 0.0231605\n",
      "epoch 26,step 2511000, training loss 0.022455\n",
      "epoch 26,step 2524500, training loss 0.0387716\n",
      "epoch 26,step 2538000, training loss 0.0243119\n",
      "epoch 26,step 2551500, training loss 0.0298507\n",
      "epoch 26,step 2565000, training loss 0.0213074\n",
      "epoch 26,step 2578500, training loss 0.0258816\n",
      "epoch 26,step 2592000, training loss 0.0315997\n",
      "epoch 26,step 2605500, training loss 0.0225988\n",
      "epoch 26,step 2619000, training loss 0.0301467\n",
      "epoch 26,step 2632500, training loss 0.0350256\n",
      "epoch 26,step 2646000, training loss 0.0284504\n",
      "epoch 26,step 2659500, training loss 0.0226838\n",
      "epoch 26,step 2673000, training loss 0.0227656\n",
      "epoch 26,step 2686500, training loss 0.0242071\n",
      "epoch 26,training loss 0.0379355 ,test loss 0.0414437\n",
      "epoch 27,step 14000, training loss 0.0344897\n",
      "epoch 27,step 28000, training loss 0.0423658\n",
      "epoch 27,step 42000, training loss 0.0380401\n",
      "epoch 27,step 56000, training loss 0.0218281\n",
      "epoch 27,step 70000, training loss 0.0240158\n",
      "epoch 27,step 84000, training loss 0.0234351\n",
      "epoch 27,step 98000, training loss 0.0271498\n",
      "epoch 27,step 112000, training loss 0.022879\n",
      "epoch 27,step 126000, training loss 0.0270566\n",
      "epoch 27,step 140000, training loss 0.0250889\n",
      "epoch 27,step 154000, training loss 0.0254537\n",
      "epoch 27,step 168000, training loss 0.0347247\n",
      "epoch 27,step 182000, training loss 0.0544126\n",
      "epoch 27,step 196000, training loss 0.0284139\n",
      "epoch 27,step 210000, training loss 0.0418907\n",
      "epoch 27,step 224000, training loss 0.0273862\n",
      "epoch 27,step 238000, training loss 0.0274182\n",
      "epoch 27,step 252000, training loss 0.0240556\n",
      "epoch 27,step 266000, training loss 0.0206702\n",
      "epoch 27,step 280000, training loss 0.0335528\n",
      "epoch 27,step 294000, training loss 0.0274597\n",
      "epoch 27,step 308000, training loss 0.0316122\n",
      "epoch 27,step 322000, training loss 0.0428225\n",
      "epoch 27,step 336000, training loss 0.0343483\n",
      "epoch 27,step 350000, training loss 0.0338017\n",
      "epoch 27,step 364000, training loss 0.019213\n",
      "epoch 27,step 378000, training loss 0.0460367\n",
      "epoch 27,step 392000, training loss 0.0287037\n",
      "epoch 27,step 406000, training loss 0.0366058\n",
      "epoch 27,step 420000, training loss 0.0299344\n",
      "epoch 27,step 434000, training loss 0.0436226\n",
      "epoch 27,step 448000, training loss 0.027661\n",
      "epoch 27,step 462000, training loss 0.0295192\n",
      "epoch 27,step 476000, training loss 0.0299217\n",
      "epoch 27,step 490000, training loss 0.0392589\n",
      "epoch 27,step 504000, training loss 0.0289634\n",
      "epoch 27,step 518000, training loss 0.0332702\n",
      "epoch 27,step 532000, training loss 0.0316499\n",
      "epoch 27,step 546000, training loss 0.028841\n",
      "epoch 27,step 560000, training loss 0.025175\n",
      "epoch 27,step 574000, training loss 0.033307\n",
      "epoch 27,step 588000, training loss 0.0268654\n",
      "epoch 27,step 602000, training loss 0.0286511\n",
      "epoch 27,step 616000, training loss 0.0317293\n",
      "epoch 27,step 630000, training loss 0.0249462\n",
      "epoch 27,step 644000, training loss 0.033943\n",
      "epoch 27,step 658000, training loss 0.0206336\n",
      "epoch 27,step 672000, training loss 0.0267375\n",
      "epoch 27,step 686000, training loss 0.0288947\n",
      "epoch 27,step 700000, training loss 0.0267642\n",
      "epoch 27,step 714000, training loss 0.0305831\n",
      "epoch 27,step 728000, training loss 0.0330571\n",
      "epoch 27,step 742000, training loss 0.0224179\n",
      "epoch 27,step 756000, training loss 0.0285145\n",
      "epoch 27,step 770000, training loss 0.0494999\n",
      "epoch 27,step 784000, training loss 0.0252404\n",
      "epoch 27,step 798000, training loss 0.0215805\n",
      "epoch 27,step 812000, training loss 0.0310372\n",
      "epoch 27,step 826000, training loss 0.036416\n",
      "epoch 27,step 840000, training loss 0.0293866\n",
      "epoch 27,step 854000, training loss 0.0308294\n",
      "epoch 27,step 868000, training loss 0.029913\n",
      "epoch 27,step 882000, training loss 0.0277743\n",
      "epoch 27,step 896000, training loss 0.0393107\n",
      "epoch 27,step 910000, training loss 0.0424567\n",
      "epoch 27,step 924000, training loss 0.0275896\n",
      "epoch 27,step 938000, training loss 0.0294149\n",
      "epoch 27,step 952000, training loss 0.0392758\n",
      "epoch 27,step 966000, training loss 0.0216437\n",
      "epoch 27,step 980000, training loss 0.0284982\n",
      "epoch 27,step 994000, training loss 0.0372535\n",
      "epoch 27,step 1008000, training loss 0.0212806\n",
      "epoch 27,step 1022000, training loss 0.0461208\n",
      "epoch 27,step 1036000, training loss 0.039582\n",
      "epoch 27,step 1050000, training loss 0.0354094\n",
      "epoch 27,step 1064000, training loss 0.0295886\n",
      "epoch 27,step 1078000, training loss 0.0284874\n",
      "epoch 27,step 1092000, training loss 0.0264861\n",
      "epoch 27,step 1106000, training loss 0.0315798\n",
      "epoch 27,step 1120000, training loss 0.0200762\n",
      "epoch 27,step 1134000, training loss 0.0250271\n",
      "epoch 27,step 1148000, training loss 0.0273188\n",
      "epoch 27,step 1162000, training loss 0.0268743\n",
      "epoch 27,step 1176000, training loss 0.0241989\n",
      "epoch 27,step 1190000, training loss 0.0373335\n",
      "epoch 27,step 1204000, training loss 0.0297412\n",
      "epoch 27,step 1218000, training loss 0.0369613\n",
      "epoch 27,step 1232000, training loss 0.0290882\n",
      "epoch 27,step 1246000, training loss 0.0215637\n",
      "epoch 27,step 1260000, training loss 0.0286619\n",
      "epoch 27,step 1274000, training loss 0.0254235\n",
      "epoch 27,step 1288000, training loss 0.0275363\n",
      "epoch 27,step 1302000, training loss 0.0404656\n",
      "epoch 27,step 1316000, training loss 0.0303242\n",
      "epoch 27,step 1330000, training loss 0.0231466\n",
      "epoch 27,step 1344000, training loss 0.0300489\n",
      "epoch 27,step 1358000, training loss 0.0267676\n",
      "epoch 27,step 1372000, training loss 0.0327382\n",
      "epoch 27,step 1386000, training loss 0.0356676\n",
      "epoch 27,step 1400000, training loss 0.028308\n",
      "epoch 27,step 1414000, training loss 0.0289374\n",
      "epoch 27,step 1428000, training loss 0.0250261\n",
      "epoch 27,step 1442000, training loss 0.0340636\n",
      "epoch 27,step 1456000, training loss 0.0221954\n",
      "epoch 27,step 1470000, training loss 0.0339026\n",
      "epoch 27,step 1484000, training loss 0.0391967\n",
      "epoch 27,step 1498000, training loss 0.0378074\n",
      "epoch 27,step 1512000, training loss 0.0300917\n",
      "epoch 27,step 1526000, training loss 0.0403616\n",
      "epoch 27,step 1540000, training loss 0.0467442\n",
      "epoch 27,step 1554000, training loss 0.0347467\n",
      "epoch 27,step 1568000, training loss 0.0389645\n",
      "epoch 27,step 1582000, training loss 0.0299811\n",
      "epoch 27,step 1596000, training loss 0.0267401\n",
      "epoch 27,step 1610000, training loss 0.0491352\n",
      "epoch 27,step 1624000, training loss 0.0220367\n",
      "epoch 27,step 1638000, training loss 0.0271331\n",
      "epoch 27,step 1652000, training loss 0.0229121\n",
      "epoch 27,step 1666000, training loss 0.0303588\n",
      "epoch 27,step 1680000, training loss 0.0359055\n",
      "epoch 27,step 1694000, training loss 0.022864\n",
      "epoch 27,step 1708000, training loss 0.0229589\n",
      "epoch 27,step 1722000, training loss 0.0263529\n",
      "epoch 27,step 1736000, training loss 0.0356506\n",
      "epoch 27,step 1750000, training loss 0.036664\n",
      "epoch 27,step 1764000, training loss 0.0323057\n",
      "epoch 27,step 1778000, training loss 0.0251089\n",
      "epoch 27,step 1792000, training loss 0.0305855\n",
      "epoch 27,step 1806000, training loss 0.0365872\n",
      "epoch 27,step 1820000, training loss 0.0328297\n",
      "epoch 27,step 1834000, training loss 0.0300184\n",
      "epoch 27,step 1848000, training loss 0.0270568\n",
      "epoch 27,step 1862000, training loss 0.0241268\n",
      "epoch 27,step 1876000, training loss 0.0303257\n",
      "epoch 27,step 1890000, training loss 0.0226911\n",
      "epoch 27,step 1904000, training loss 0.0309872\n",
      "epoch 27,step 1918000, training loss 0.0273496\n",
      "epoch 27,step 1932000, training loss 0.0299823\n",
      "epoch 27,step 1946000, training loss 0.031665\n",
      "epoch 27,step 1960000, training loss 0.0271416\n",
      "epoch 27,step 1974000, training loss 0.0274885\n",
      "epoch 27,step 1988000, training loss 0.0263806\n",
      "epoch 27,step 2002000, training loss 0.0321479\n",
      "epoch 27,step 2016000, training loss 0.0270822\n",
      "epoch 27,step 2030000, training loss 0.0338493\n",
      "epoch 27,step 2044000, training loss 0.0253745\n",
      "epoch 27,step 2058000, training loss 0.0286247\n",
      "epoch 27,step 2072000, training loss 0.0393376\n",
      "epoch 27,step 2086000, training loss 0.0250079\n",
      "epoch 27,step 2100000, training loss 0.0601322\n",
      "epoch 27,step 2114000, training loss 0.025018\n",
      "epoch 27,step 2128000, training loss 0.0226018\n",
      "epoch 27,step 2142000, training loss 0.0370727\n",
      "epoch 27,step 2156000, training loss 0.0186883\n",
      "epoch 27,step 2170000, training loss 0.0305947\n",
      "epoch 27,step 2184000, training loss 0.0223918\n",
      "epoch 27,step 2198000, training loss 0.028253\n",
      "epoch 27,step 2212000, training loss 0.0295355\n",
      "epoch 27,step 2226000, training loss 0.0296081\n",
      "epoch 27,step 2240000, training loss 0.0247154\n",
      "epoch 27,step 2254000, training loss 0.0310421\n",
      "epoch 27,step 2268000, training loss 0.025539\n",
      "epoch 27,step 2282000, training loss 0.0340867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27,step 2296000, training loss 0.0318818\n",
      "epoch 27,step 2310000, training loss 0.0457948\n",
      "epoch 27,step 2324000, training loss 0.0339452\n",
      "epoch 27,step 2338000, training loss 0.0301843\n",
      "epoch 27,step 2352000, training loss 0.0277868\n",
      "epoch 27,step 2366000, training loss 0.0361603\n",
      "epoch 27,step 2380000, training loss 0.0329635\n",
      "epoch 27,step 2394000, training loss 0.0398768\n",
      "epoch 27,step 2408000, training loss 0.0443816\n",
      "epoch 27,step 2422000, training loss 0.0280799\n",
      "epoch 27,step 2436000, training loss 0.0357068\n",
      "epoch 27,step 2450000, training loss 0.0280815\n",
      "epoch 27,step 2464000, training loss 0.0277567\n",
      "epoch 27,step 2478000, training loss 0.0293724\n",
      "epoch 27,step 2492000, training loss 0.0245362\n",
      "epoch 27,step 2506000, training loss 0.0363277\n",
      "epoch 27,step 2520000, training loss 0.062271\n",
      "epoch 27,step 2534000, training loss 0.0276104\n",
      "epoch 27,step 2548000, training loss 0.0272076\n",
      "epoch 27,step 2562000, training loss 0.0361061\n",
      "epoch 27,step 2576000, training loss 0.0272612\n",
      "epoch 27,step 2590000, training loss 0.0229826\n",
      "epoch 27,step 2604000, training loss 0.022174\n",
      "epoch 27,step 2618000, training loss 0.0382888\n",
      "epoch 27,step 2632000, training loss 0.0234423\n",
      "epoch 27,step 2646000, training loss 0.0302375\n",
      "epoch 27,step 2660000, training loss 0.0216245\n",
      "epoch 27,step 2674000, training loss 0.0265898\n",
      "epoch 27,step 2688000, training loss 0.0317563\n",
      "epoch 27,step 2702000, training loss 0.0226714\n",
      "epoch 27,step 2716000, training loss 0.0303174\n",
      "epoch 27,step 2730000, training loss 0.0332838\n",
      "epoch 27,step 2744000, training loss 0.0273109\n",
      "epoch 27,step 2758000, training loss 0.021428\n",
      "epoch 27,step 2772000, training loss 0.0224225\n",
      "epoch 27,step 2786000, training loss 0.0229593\n",
      "epoch 27,training loss 0.0361554 ,test loss 0.040718\n",
      "epoch 28,step 14500, training loss 0.034068\n",
      "epoch 28,step 29000, training loss 0.0419878\n",
      "epoch 28,step 43500, training loss 0.0361489\n",
      "epoch 28,step 58000, training loss 0.0211895\n",
      "epoch 28,step 72500, training loss 0.023941\n",
      "epoch 28,step 87000, training loss 0.0234568\n",
      "epoch 28,step 101500, training loss 0.0270562\n",
      "epoch 28,step 116000, training loss 0.0227638\n",
      "epoch 28,step 130500, training loss 0.0276912\n",
      "epoch 28,step 145000, training loss 0.0255195\n",
      "epoch 28,step 159500, training loss 0.025184\n",
      "epoch 28,step 174000, training loss 0.0342881\n",
      "epoch 28,step 188500, training loss 0.0535128\n",
      "epoch 28,step 203000, training loss 0.0274394\n",
      "epoch 28,step 217500, training loss 0.0415809\n",
      "epoch 28,step 232000, training loss 0.027621\n",
      "epoch 28,step 246500, training loss 0.0271611\n",
      "epoch 28,step 261000, training loss 0.023514\n",
      "epoch 28,step 275500, training loss 0.0207981\n",
      "epoch 28,step 290000, training loss 0.0334156\n",
      "epoch 28,step 304500, training loss 0.0270955\n",
      "epoch 28,step 319000, training loss 0.0319242\n",
      "epoch 28,step 333500, training loss 0.0441276\n",
      "epoch 28,step 348000, training loss 0.0352904\n",
      "epoch 28,step 362500, training loss 0.0350892\n",
      "epoch 28,step 377000, training loss 0.0188013\n",
      "epoch 28,step 391500, training loss 0.0478457\n",
      "epoch 28,step 406000, training loss 0.0295754\n",
      "epoch 28,step 420500, training loss 0.0364998\n",
      "epoch 28,step 435000, training loss 0.028787\n",
      "epoch 28,step 449500, training loss 0.0475143\n",
      "epoch 28,step 464000, training loss 0.0289372\n",
      "epoch 28,step 478500, training loss 0.0295625\n",
      "epoch 28,step 493000, training loss 0.0303071\n",
      "epoch 28,step 507500, training loss 0.0394166\n",
      "epoch 28,step 522000, training loss 0.0281539\n",
      "epoch 28,step 536500, training loss 0.0311479\n",
      "epoch 28,step 551000, training loss 0.0310718\n",
      "epoch 28,step 565500, training loss 0.0281878\n",
      "epoch 28,step 580000, training loss 0.0247327\n",
      "epoch 28,step 594500, training loss 0.0327515\n",
      "epoch 28,step 609000, training loss 0.0256618\n",
      "epoch 28,step 623500, training loss 0.0271384\n",
      "epoch 28,step 638000, training loss 0.0304919\n",
      "epoch 28,step 652500, training loss 0.0255712\n",
      "epoch 28,step 667000, training loss 0.0339793\n",
      "epoch 28,step 681500, training loss 0.0203932\n",
      "epoch 28,step 696000, training loss 0.0268575\n",
      "epoch 28,step 710500, training loss 0.0289019\n",
      "epoch 28,step 725000, training loss 0.0262268\n",
      "epoch 28,step 739500, training loss 0.0299773\n",
      "epoch 28,step 754000, training loss 0.0324242\n",
      "epoch 28,step 768500, training loss 0.0225027\n",
      "epoch 28,step 783000, training loss 0.028405\n",
      "epoch 28,step 797500, training loss 0.0489026\n",
      "epoch 28,step 812000, training loss 0.0251753\n",
      "epoch 28,step 826500, training loss 0.0220474\n",
      "epoch 28,step 841000, training loss 0.0307543\n",
      "epoch 28,step 855500, training loss 0.0371404\n",
      "epoch 28,step 870000, training loss 0.028947\n",
      "epoch 28,step 884500, training loss 0.0307881\n",
      "epoch 28,step 899000, training loss 0.0297157\n",
      "epoch 28,step 913500, training loss 0.0271464\n",
      "epoch 28,step 928000, training loss 0.0381411\n",
      "epoch 28,step 942500, training loss 0.042306\n",
      "epoch 28,step 957000, training loss 0.0269751\n",
      "epoch 28,step 971500, training loss 0.0287347\n",
      "epoch 28,step 986000, training loss 0.039591\n",
      "epoch 28,step 1000500, training loss 0.0215034\n",
      "epoch 28,step 1015000, training loss 0.0278363\n",
      "epoch 28,step 1029500, training loss 0.03573\n",
      "epoch 28,step 1044000, training loss 0.0206242\n",
      "epoch 28,step 1058500, training loss 0.0458952\n",
      "epoch 28,step 1073000, training loss 0.0394076\n",
      "epoch 28,step 1087500, training loss 0.0348165\n",
      "epoch 28,step 1102000, training loss 0.0287917\n",
      "epoch 28,step 1116500, training loss 0.0287131\n",
      "epoch 28,step 1131000, training loss 0.026047\n",
      "epoch 28,step 1145500, training loss 0.032295\n",
      "epoch 28,step 1160000, training loss 0.0202517\n",
      "epoch 28,step 1174500, training loss 0.0239372\n",
      "epoch 28,step 1189000, training loss 0.0264351\n",
      "epoch 28,step 1203500, training loss 0.0261699\n",
      "epoch 28,step 1218000, training loss 0.0245203\n",
      "epoch 28,step 1232500, training loss 0.0369644\n",
      "epoch 28,step 1247000, training loss 0.0295296\n",
      "epoch 28,step 1261500, training loss 0.0374453\n",
      "epoch 28,step 1276000, training loss 0.0291151\n",
      "epoch 28,step 1290500, training loss 0.0198057\n",
      "epoch 28,step 1305000, training loss 0.0275581\n",
      "epoch 28,step 1319500, training loss 0.0241685\n",
      "epoch 28,step 1334000, training loss 0.026545\n",
      "epoch 28,step 1348500, training loss 0.0394836\n",
      "epoch 28,step 1363000, training loss 0.0306192\n",
      "epoch 28,step 1377500, training loss 0.0225898\n",
      "epoch 28,step 1392000, training loss 0.0296201\n",
      "epoch 28,step 1406500, training loss 0.0262007\n",
      "epoch 28,step 1421000, training loss 0.0318836\n",
      "epoch 28,step 1435500, training loss 0.0354365\n",
      "epoch 28,step 1450000, training loss 0.0272278\n",
      "epoch 28,step 1464500, training loss 0.0283616\n",
      "epoch 28,step 1479000, training loss 0.0253416\n",
      "epoch 28,step 1493500, training loss 0.0332808\n",
      "epoch 28,step 1508000, training loss 0.0225404\n",
      "epoch 28,step 1522500, training loss 0.0339171\n",
      "epoch 28,step 1537000, training loss 0.0391223\n",
      "epoch 28,step 1551500, training loss 0.0373941\n",
      "epoch 28,step 1566000, training loss 0.0293286\n",
      "epoch 28,step 1580500, training loss 0.0399549\n",
      "epoch 28,step 1595000, training loss 0.0477817\n",
      "epoch 28,step 1609500, training loss 0.0348096\n",
      "epoch 28,step 1624000, training loss 0.0389064\n",
      "epoch 28,step 1638500, training loss 0.0301433\n",
      "epoch 28,step 1653000, training loss 0.0260714\n",
      "epoch 28,step 1667500, training loss 0.0500561\n",
      "epoch 28,step 1682000, training loss 0.0217347\n",
      "epoch 28,step 1696500, training loss 0.0279303\n",
      "epoch 28,step 1711000, training loss 0.0235397\n",
      "epoch 28,step 1725500, training loss 0.0308506\n",
      "epoch 28,step 1740000, training loss 0.0323524\n",
      "epoch 28,step 1754500, training loss 0.0225037\n",
      "epoch 28,step 1769000, training loss 0.0222841\n",
      "epoch 28,step 1783500, training loss 0.0251718\n",
      "epoch 28,step 1798000, training loss 0.0344146\n",
      "epoch 28,step 1812500, training loss 0.0353024\n",
      "epoch 28,step 1827000, training loss 0.03094\n",
      "epoch 28,step 1841500, training loss 0.0250139\n",
      "epoch 28,step 1856000, training loss 0.0293112\n",
      "epoch 28,step 1870500, training loss 0.0349963\n",
      "epoch 28,step 1885000, training loss 0.0326611\n",
      "epoch 28,step 1899500, training loss 0.0289911\n",
      "epoch 28,step 1914000, training loss 0.0260967\n",
      "epoch 28,step 1928500, training loss 0.0231543\n",
      "epoch 28,step 1943000, training loss 0.0308919\n",
      "epoch 28,step 1957500, training loss 0.0227793\n",
      "epoch 28,step 1972000, training loss 0.0301166\n",
      "epoch 28,step 1986500, training loss 0.0266889\n",
      "epoch 28,step 2001000, training loss 0.029791\n",
      "epoch 28,step 2015500, training loss 0.0318317\n",
      "epoch 28,step 2030000, training loss 0.026474\n",
      "epoch 28,step 2044500, training loss 0.0268208\n",
      "epoch 28,step 2059000, training loss 0.0264844\n",
      "epoch 28,step 2073500, training loss 0.0307621\n",
      "epoch 28,step 2088000, training loss 0.0272149\n",
      "epoch 28,step 2102500, training loss 0.0352894\n",
      "epoch 28,step 2117000, training loss 0.0264993\n",
      "epoch 28,step 2131500, training loss 0.0284302\n",
      "epoch 28,step 2146000, training loss 0.0378299\n",
      "epoch 28,step 2160500, training loss 0.0244785\n",
      "epoch 28,step 2175000, training loss 0.0594566\n",
      "epoch 28,step 2189500, training loss 0.0249694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28,step 2204000, training loss 0.0230249\n",
      "epoch 28,step 2218500, training loss 0.0368461\n",
      "epoch 28,step 2233000, training loss 0.0188684\n",
      "epoch 28,step 2247500, training loss 0.0305075\n",
      "epoch 28,step 2262000, training loss 0.0227402\n",
      "epoch 28,step 2276500, training loss 0.0281635\n",
      "epoch 28,step 2291000, training loss 0.0295482\n",
      "epoch 28,step 2305500, training loss 0.029876\n",
      "epoch 28,step 2320000, training loss 0.0263204\n",
      "epoch 28,step 2334500, training loss 0.0334688\n",
      "epoch 28,step 2349000, training loss 0.0253664\n",
      "epoch 28,step 2363500, training loss 0.0341071\n",
      "epoch 28,step 2378000, training loss 0.0329496\n",
      "epoch 28,step 2392500, training loss 0.0462664\n",
      "epoch 28,step 2407000, training loss 0.0355357\n",
      "epoch 28,step 2421500, training loss 0.0311317\n",
      "epoch 28,step 2436000, training loss 0.0275922\n",
      "epoch 28,step 2450500, training loss 0.0360203\n",
      "epoch 28,step 2465000, training loss 0.0333448\n",
      "epoch 28,step 2479500, training loss 0.0393992\n",
      "epoch 28,step 2494000, training loss 0.0432627\n",
      "epoch 28,step 2508500, training loss 0.027868\n",
      "epoch 28,step 2523000, training loss 0.0355088\n",
      "epoch 28,step 2537500, training loss 0.0274228\n",
      "epoch 28,step 2552000, training loss 0.0275507\n",
      "epoch 28,step 2566500, training loss 0.0299973\n",
      "epoch 28,step 2581000, training loss 0.0245578\n",
      "epoch 28,step 2595500, training loss 0.0383621\n",
      "epoch 28,step 2610000, training loss 0.0633524\n",
      "epoch 28,step 2624500, training loss 0.0280991\n",
      "epoch 28,step 2639000, training loss 0.0276253\n",
      "epoch 28,step 2653500, training loss 0.0354141\n",
      "epoch 28,step 2668000, training loss 0.0271954\n",
      "epoch 28,step 2682500, training loss 0.0226308\n",
      "epoch 28,step 2697000, training loss 0.0221045\n",
      "epoch 28,step 2711500, training loss 0.0384616\n",
      "epoch 28,step 2726000, training loss 0.0232541\n",
      "epoch 28,step 2740500, training loss 0.0294629\n",
      "epoch 28,step 2755000, training loss 0.020261\n",
      "epoch 28,step 2769500, training loss 0.0253548\n",
      "epoch 28,step 2784000, training loss 0.0310326\n",
      "epoch 28,step 2798500, training loss 0.0219202\n",
      "epoch 28,step 2813000, training loss 0.0293223\n",
      "epoch 28,step 2827500, training loss 0.0322027\n",
      "epoch 28,step 2842000, training loss 0.0267429\n",
      "epoch 28,step 2856500, training loss 0.0219652\n",
      "epoch 28,step 2871000, training loss 0.0223935\n",
      "epoch 28,step 2885500, training loss 0.0223699\n",
      "epoch 28,training loss 0.0368303 ,test loss 0.0400433\n",
      "epoch 29,step 3000, training loss 0.0992763\n",
      "epoch 29,step 6000, training loss 0.0673661\n",
      "epoch 29,step 9000, training loss 0.0332378\n",
      "epoch 29,step 12000, training loss 0.0226224\n",
      "epoch 29,step 15000, training loss 0.0330735\n",
      "epoch 29,step 18000, training loss 0.0234276\n",
      "epoch 29,step 21000, training loss 0.0225057\n",
      "epoch 29,step 24000, training loss 0.0226379\n",
      "epoch 29,step 27000, training loss 0.0294732\n",
      "epoch 29,step 30000, training loss 0.0421488\n",
      "epoch 29,step 33000, training loss 0.118536\n",
      "epoch 29,step 36000, training loss 0.0652297\n",
      "epoch 29,step 39000, training loss 0.043316\n",
      "epoch 29,step 42000, training loss 0.0422826\n",
      "epoch 29,step 45000, training loss 0.0353053\n",
      "epoch 29,step 48000, training loss 0.0261877\n",
      "epoch 29,step 51000, training loss 0.0371511\n",
      "epoch 29,step 54000, training loss 0.0286516\n",
      "epoch 29,step 57000, training loss 0.0496219\n",
      "epoch 29,step 60000, training loss 0.021194\n",
      "epoch 29,step 63000, training loss 0.0888655\n",
      "epoch 29,step 66000, training loss 0.0458957\n",
      "epoch 29,step 69000, training loss 0.0394832\n",
      "epoch 29,step 72000, training loss 0.0364682\n",
      "epoch 29,step 75000, training loss 0.0238758\n",
      "epoch 29,step 78000, training loss 0.0295069\n",
      "epoch 29,step 81000, training loss 0.0249128\n",
      "epoch 29,step 84000, training loss 0.0242398\n",
      "epoch 29,step 87000, training loss 0.0272877\n",
      "epoch 29,step 90000, training loss 0.0230573\n",
      "epoch 29,step 93000, training loss 0.0746038\n",
      "epoch 29,step 96000, training loss 0.0390637\n",
      "epoch 29,step 99000, training loss 0.0309045\n",
      "epoch 29,step 102000, training loss 0.0282777\n",
      "epoch 29,step 105000, training loss 0.0274269\n",
      "epoch 29,step 108000, training loss 0.043192\n",
      "epoch 29,step 111000, training loss 0.0262346\n",
      "epoch 29,step 114000, training loss 0.0247234\n",
      "epoch 29,step 117000, training loss 0.0205646\n",
      "epoch 29,step 120000, training loss 0.0227988\n",
      "epoch 29,step 123000, training loss 0.0747675\n",
      "epoch 29,step 126000, training loss 0.0358486\n",
      "epoch 29,step 129000, training loss 0.0240276\n",
      "epoch 29,step 132000, training loss 0.0380629\n",
      "epoch 29,step 135000, training loss 0.0272307\n",
      "epoch 29,step 138000, training loss 0.0248062\n",
      "epoch 29,step 141000, training loss 0.0345345\n",
      "epoch 29,step 144000, training loss 0.025785\n",
      "epoch 29,step 147000, training loss 0.0248349\n",
      "epoch 29,step 150000, training loss 0.0258989\n",
      "epoch 29,step 153000, training loss 0.0832255\n",
      "epoch 29,step 156000, training loss 0.0643069\n",
      "epoch 29,step 159000, training loss 0.0325551\n",
      "epoch 29,step 162000, training loss 0.0289489\n",
      "epoch 29,step 165000, training loss 0.0254201\n",
      "epoch 29,step 168000, training loss 0.0246603\n",
      "epoch 29,step 171000, training loss 0.0229857\n",
      "epoch 29,step 174000, training loss 0.0358209\n",
      "epoch 29,step 177000, training loss 0.0405402\n",
      "epoch 29,step 180000, training loss 0.0343532\n",
      "epoch 29,step 183000, training loss 0.0824207\n",
      "epoch 29,step 186000, training loss 0.055174\n",
      "epoch 29,step 189000, training loss 0.03386\n",
      "epoch 29,step 192000, training loss 0.02944\n",
      "epoch 29,step 195000, training loss 0.0539092\n",
      "epoch 29,step 198000, training loss 0.0487527\n",
      "epoch 29,step 201000, training loss 0.0367628\n",
      "epoch 29,step 204000, training loss 0.0304551\n",
      "epoch 29,step 207000, training loss 0.0269787\n",
      "epoch 29,step 210000, training loss 0.0274173\n",
      "epoch 29,step 213000, training loss 0.0839431\n",
      "epoch 29,step 216000, training loss 0.0374534\n",
      "epoch 29,step 219000, training loss 0.0283124\n",
      "epoch 29,step 222000, training loss 0.0232061\n",
      "epoch 29,step 225000, training loss 0.0416869\n",
      "epoch 29,step 228000, training loss 0.0325006\n",
      "epoch 29,step 231000, training loss 0.034488\n",
      "epoch 29,step 234000, training loss 0.0250678\n",
      "epoch 29,step 237000, training loss 0.0301767\n",
      "epoch 29,step 240000, training loss 0.0275608\n",
      "epoch 29,step 243000, training loss 0.0956995\n",
      "epoch 29,step 246000, training loss 0.0427524\n",
      "epoch 29,step 249000, training loss 0.0698404\n",
      "epoch 29,step 252000, training loss 0.0273777\n",
      "epoch 29,step 255000, training loss 0.0271391\n",
      "epoch 29,step 258000, training loss 0.0254476\n",
      "epoch 29,step 261000, training loss 0.0265945\n",
      "epoch 29,step 264000, training loss 0.0260056\n",
      "epoch 29,step 267000, training loss 0.0206489\n",
      "epoch 29,step 270000, training loss 0.0236618\n",
      "epoch 29,step 273000, training loss 0.0785996\n",
      "epoch 29,step 276000, training loss 0.0642041\n",
      "epoch 29,step 279000, training loss 0.0315325\n",
      "epoch 29,step 282000, training loss 0.0293738\n",
      "epoch 29,step 285000, training loss 0.0209579\n",
      "epoch 29,step 288000, training loss 0.0259163\n",
      "epoch 29,step 291000, training loss 0.0236825\n",
      "epoch 29,step 294000, training loss 0.0255783\n",
      "epoch 29,step 297000, training loss 0.0280687\n",
      "epoch 29,step 300000, training loss 0.0330152\n",
      "epoch 29,step 303000, training loss 0.0986243\n",
      "epoch 29,step 306000, training loss 0.0451875\n",
      "epoch 29,step 309000, training loss 0.0298373\n",
      "epoch 29,step 312000, training loss 0.0216294\n",
      "epoch 29,step 315000, training loss 0.0272908\n",
      "epoch 29,step 318000, training loss 0.0286198\n",
      "epoch 29,step 321000, training loss 0.0195218\n",
      "epoch 29,step 324000, training loss 0.0374811\n",
      "epoch 29,step 327000, training loss 0.0333682\n",
      "epoch 29,step 330000, training loss 0.031667\n",
      "epoch 29,step 333000, training loss 0.0890097\n",
      "epoch 29,step 336000, training loss 0.059787\n",
      "epoch 29,step 339000, training loss 0.0315674\n",
      "epoch 29,step 342000, training loss 0.0468359\n",
      "epoch 29,step 345000, training loss 0.0427431\n",
      "epoch 29,step 348000, training loss 0.0546853\n",
      "epoch 29,step 351000, training loss 0.0336054\n",
      "epoch 29,step 354000, training loss 0.0531857\n",
      "epoch 29,step 357000, training loss 0.0592421\n",
      "epoch 29,step 360000, training loss 0.0346167\n",
      "epoch 29,step 363000, training loss 0.0828462\n",
      "epoch 29,step 366000, training loss 0.0530128\n",
      "epoch 29,step 369000, training loss 0.0351931\n",
      "epoch 29,step 372000, training loss 0.031371\n",
      "epoch 29,step 375000, training loss 0.0343684\n",
      "epoch 29,step 378000, training loss 0.0241677\n",
      "epoch 29,step 381000, training loss 0.0327082\n",
      "epoch 29,step 384000, training loss 0.0282637\n",
      "epoch 29,step 387000, training loss 0.0220734\n",
      "epoch 29,step 390000, training loss 0.0185946\n",
      "epoch 29,step 393000, training loss 0.0773133\n",
      "epoch 29,step 396000, training loss 0.0651794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29,step 399000, training loss 0.0320543\n",
      "epoch 29,step 402000, training loss 0.0298976\n",
      "epoch 29,step 405000, training loss 0.0459361\n",
      "epoch 29,step 408000, training loss 0.0223734\n",
      "epoch 29,step 411000, training loss 0.0423075\n",
      "epoch 29,step 414000, training loss 0.0258585\n",
      "epoch 29,step 417000, training loss 0.0369868\n",
      "epoch 29,step 420000, training loss 0.0296797\n",
      "epoch 29,step 423000, training loss 0.125162\n",
      "epoch 29,step 426000, training loss 0.0762888\n",
      "epoch 29,step 429000, training loss 0.0442618\n",
      "epoch 29,step 432000, training loss 0.0301699\n",
      "epoch 29,step 435000, training loss 0.0351203\n",
      "epoch 29,step 438000, training loss 0.0302324\n",
      "epoch 29,step 441000, training loss 0.0383913\n",
      "epoch 29,step 444000, training loss 0.0270293\n",
      "epoch 29,step 447000, training loss 0.0286355\n",
      "epoch 29,step 450000, training loss 0.0277809\n",
      "epoch 29,step 453000, training loss 0.0776464\n",
      "epoch 29,step 456000, training loss 0.0624728\n",
      "epoch 29,step 459000, training loss 0.0424082\n",
      "epoch 29,step 462000, training loss 0.04064\n",
      "epoch 29,step 465000, training loss 0.0450417\n",
      "epoch 29,step 468000, training loss 0.0446779\n",
      "epoch 29,step 471000, training loss 0.0354414\n",
      "epoch 29,step 474000, training loss 0.0240614\n",
      "epoch 29,step 477000, training loss 0.0325771\n",
      "epoch 29,step 480000, training loss 0.0269218\n",
      "epoch 29,step 483000, training loss 0.0684019\n",
      "epoch 29,step 486000, training loss 0.0489575\n",
      "epoch 29,step 489000, training loss 0.0363166\n",
      "epoch 29,step 492000, training loss 0.0252927\n",
      "epoch 29,step 495000, training loss 0.0290154\n",
      "epoch 29,step 498000, training loss 0.0212761\n",
      "epoch 29,step 501000, training loss 0.0267635\n",
      "epoch 29,step 504000, training loss 0.0336261\n",
      "epoch 29,step 507000, training loss 0.0254099\n",
      "epoch 29,step 510000, training loss 0.028834\n",
      "epoch 29,step 513000, training loss 0.108714\n",
      "epoch 29,step 516000, training loss 0.050015\n",
      "epoch 29,step 519000, training loss 0.0380094\n",
      "epoch 29,step 522000, training loss 0.0518709\n",
      "epoch 29,step 525000, training loss 0.0386524\n",
      "epoch 29,step 528000, training loss 0.0296763\n",
      "epoch 29,step 531000, training loss 0.0418052\n",
      "epoch 29,step 534000, training loss 0.0302244\n",
      "epoch 29,step 537000, training loss 0.0298616\n",
      "epoch 29,step 540000, training loss 0.0290024\n",
      "epoch 29,step 543000, training loss 0.0755616\n",
      "epoch 29,step 546000, training loss 0.0499421\n",
      "epoch 29,step 549000, training loss 0.0317487\n",
      "epoch 29,step 552000, training loss 0.0381073\n",
      "epoch 29,step 555000, training loss 0.0316584\n",
      "epoch 29,step 558000, training loss 0.066103\n",
      "epoch 29,step 561000, training loss 0.0363845\n",
      "epoch 29,step 564000, training loss 0.0351345\n",
      "epoch 29,step 567000, training loss 0.0294936\n",
      "epoch 29,step 570000, training loss 0.030416\n",
      "epoch 29,step 573000, training loss 0.0716262\n",
      "epoch 29,step 576000, training loss 0.0390994\n",
      "epoch 29,step 579000, training loss 0.0228441\n",
      "epoch 29,step 582000, training loss 0.0239149\n",
      "epoch 29,step 585000, training loss 0.0286663\n",
      "epoch 29,step 588000, training loss 0.032494\n",
      "epoch 29,step 591000, training loss 0.0225279\n",
      "epoch 29,step 594000, training loss 0.0238268\n",
      "epoch 29,step 597000, training loss 0.025365\n",
      "epoch 29,step 600000, training loss 0.0246328\n",
      "epoch 29,step 603000, training loss 0.0728414\n",
      "epoch 29,step 606000, training loss 0.0708028\n",
      "epoch 29,step 609000, training loss 0.0344941\n",
      "epoch 29,step 612000, training loss 0.0225039\n",
      "epoch 29,step 615000, training loss 0.031294\n",
      "epoch 29,step 618000, training loss 0.0385364\n",
      "epoch 29,step 621000, training loss 0.0239775\n",
      "epoch 29,step 624000, training loss 0.0212171\n",
      "epoch 29,step 627000, training loss 0.0243415\n",
      "epoch 29,step 630000, training loss 0.0252541\n",
      "epoch 29,step 633000, training loss 0.089698\n",
      "epoch 29,step 636000, training loss 0.0440354\n",
      "epoch 29,step 639000, training loss 0.0323078\n",
      "epoch 29,step 642000, training loss 0.0306026\n",
      "epoch 29,step 645000, training loss 0.0273487\n",
      "epoch 29,step 648000, training loss 0.029845\n",
      "epoch 29,step 651000, training loss 0.0316075\n",
      "epoch 29,step 654000, training loss 0.0344071\n",
      "epoch 29,step 657000, training loss 0.0299313\n",
      "epoch 29,step 660000, training loss 0.0309657\n",
      "epoch 29,step 663000, training loss 0.0945516\n",
      "epoch 29,step 666000, training loss 0.0467015\n",
      "epoch 29,step 669000, training loss 0.0357061\n",
      "epoch 29,step 672000, training loss 0.0287559\n",
      "epoch 29,step 675000, training loss 0.0244767\n",
      "epoch 29,step 678000, training loss 0.0241339\n",
      "epoch 29,step 681000, training loss 0.0259436\n",
      "epoch 29,step 684000, training loss 0.0261524\n",
      "epoch 29,step 687000, training loss 0.0241377\n",
      "epoch 29,step 690000, training loss 0.0329767\n",
      "epoch 29,step 693000, training loss 0.0738071\n",
      "epoch 29,step 696000, training loss 0.0459031\n",
      "epoch 29,step 699000, training loss 0.0227164\n",
      "epoch 29,step 702000, training loss 0.0263239\n",
      "epoch 29,step 705000, training loss 0.0199607\n",
      "epoch 29,step 708000, training loss 0.0207731\n",
      "epoch 29,step 711000, training loss 0.0292672\n",
      "epoch 29,step 714000, training loss 0.025332\n",
      "epoch 29,step 717000, training loss 0.0251645\n",
      "epoch 29,step 720000, training loss 0.0261648\n",
      "epoch 29,step 723000, training loss 0.0928358\n",
      "epoch 29,step 726000, training loss 0.038916\n",
      "epoch 29,step 729000, training loss 0.0355252\n",
      "epoch 29,step 732000, training loss 0.0322303\n",
      "epoch 29,step 735000, training loss 0.028702\n",
      "epoch 29,step 738000, training loss 0.0390724\n",
      "epoch 29,step 741000, training loss 0.028439\n",
      "epoch 29,step 744000, training loss 0.0247092\n",
      "epoch 29,step 747000, training loss 0.0329488\n",
      "epoch 29,step 750000, training loss 0.026163\n",
      "epoch 29,step 753000, training loss 0.144888\n",
      "epoch 29,step 756000, training loss 0.0421869\n",
      "epoch 29,step 759000, training loss 0.022029\n",
      "epoch 29,step 762000, training loss 0.0288745\n",
      "epoch 29,step 765000, training loss 0.0300348\n",
      "epoch 29,step 768000, training loss 0.0183019\n",
      "epoch 29,step 771000, training loss 0.032917\n",
      "epoch 29,step 774000, training loss 0.0251531\n",
      "epoch 29,step 777000, training loss 0.0415409\n",
      "epoch 29,step 780000, training loss 0.0328546\n",
      "epoch 29,step 783000, training loss 0.1143\n",
      "epoch 29,step 786000, training loss 0.064722\n",
      "epoch 29,step 789000, training loss 0.0560645\n",
      "epoch 29,step 792000, training loss 0.0331475\n",
      "epoch 29,step 795000, training loss 0.0221453\n",
      "epoch 29,step 798000, training loss 0.0236263\n",
      "epoch 29,step 801000, training loss 0.0264473\n",
      "epoch 29,step 804000, training loss 0.0263561\n",
      "epoch 29,step 807000, training loss 0.0237965\n",
      "epoch 29,step 810000, training loss 0.0284406\n",
      "epoch 29,step 813000, training loss 0.0886777\n",
      "epoch 29,step 816000, training loss 0.0361911\n",
      "epoch 29,step 819000, training loss 0.0321383\n",
      "epoch 29,step 822000, training loss 0.0411854\n",
      "epoch 29,step 825000, training loss 0.0491034\n",
      "epoch 29,step 828000, training loss 0.0344492\n",
      "epoch 29,step 831000, training loss 0.0248679\n",
      "epoch 29,step 834000, training loss 0.0262986\n",
      "epoch 29,step 837000, training loss 0.0271995\n",
      "epoch 29,step 840000, training loss 0.024691\n",
      "epoch 29,step 843000, training loss 0.102397\n",
      "epoch 29,step 846000, training loss 0.0526007\n",
      "epoch 29,step 849000, training loss 0.0245427\n",
      "epoch 29,step 852000, training loss 0.0257414\n",
      "epoch 29,step 855000, training loss 0.0208563\n",
      "epoch 29,step 858000, training loss 0.024225\n",
      "epoch 29,step 861000, training loss 0.0310843\n",
      "epoch 29,step 864000, training loss 0.0218463\n",
      "epoch 29,step 867000, training loss 0.0278632\n",
      "epoch 29,step 870000, training loss 0.0298901\n",
      "epoch 29,step 873000, training loss 0.089436\n",
      "epoch 29,step 876000, training loss 0.0377635\n",
      "epoch 29,step 879000, training loss 0.0290859\n",
      "epoch 29,step 882000, training loss 0.0379719\n",
      "epoch 29,step 885000, training loss 0.0359949\n",
      "epoch 29,step 888000, training loss 0.0364117\n",
      "epoch 29,step 891000, training loss 0.0246143\n",
      "epoch 29,step 894000, training loss 0.0320713\n",
      "epoch 29,step 897000, training loss 0.0249513\n",
      "epoch 29,step 900000, training loss 0.0282647\n",
      "epoch 29,step 903000, training loss 0.0803579\n",
      "epoch 29,step 906000, training loss 0.0508643\n",
      "epoch 29,step 909000, training loss 0.0323736\n",
      "epoch 29,step 912000, training loss 0.0320866\n",
      "epoch 29,step 915000, training loss 0.0302644\n",
      "epoch 29,step 918000, training loss 0.0327358\n",
      "epoch 29,step 921000, training loss 0.0275583\n",
      "epoch 29,step 924000, training loss 0.043416\n",
      "epoch 29,step 927000, training loss 0.0223407\n",
      "epoch 29,step 930000, training loss 0.028965\n",
      "epoch 29,step 933000, training loss 0.0908732\n",
      "epoch 29,step 936000, training loss 0.0700532\n",
      "epoch 29,step 939000, training loss 0.0230054\n",
      "epoch 29,step 942000, training loss 0.0316476\n",
      "epoch 29,step 945000, training loss 0.0273675\n",
      "epoch 29,step 948000, training loss 0.0377972\n",
      "epoch 29,step 951000, training loss 0.0513956\n",
      "epoch 29,step 954000, training loss 0.0242089\n",
      "epoch 29,step 957000, training loss 0.047783\n",
      "epoch 29,step 960000, training loss 0.0383356\n",
      "epoch 29,step 963000, training loss 0.0860214\n",
      "epoch 29,step 966000, training loss 0.0621986\n",
      "epoch 29,step 969000, training loss 0.0345125\n",
      "epoch 29,step 972000, training loss 0.0352762\n",
      "epoch 29,step 975000, training loss 0.0423823\n",
      "epoch 29,step 978000, training loss 0.0256591\n",
      "epoch 29,step 981000, training loss 0.0280377\n",
      "epoch 29,step 984000, training loss 0.0246873\n",
      "epoch 29,step 987000, training loss 0.0353166\n",
      "epoch 29,step 990000, training loss 0.0264621\n",
      "epoch 29,step 993000, training loss 0.0748364\n",
      "epoch 29,step 996000, training loss 0.0712986\n",
      "epoch 29,step 999000, training loss 0.0266208\n",
      "epoch 29,step 1002000, training loss 0.040389\n",
      "epoch 29,step 1005000, training loss 0.02823\n",
      "epoch 29,step 1008000, training loss 0.0206319\n",
      "epoch 29,step 1011000, training loss 0.0268378\n",
      "epoch 29,step 1014000, training loss 0.0255374\n",
      "epoch 29,step 1017000, training loss 0.0263559\n",
      "epoch 29,step 1020000, training loss 0.0377967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29,step 1023000, training loss 0.0853145\n",
      "epoch 29,step 1026000, training loss 0.0535982\n",
      "epoch 29,step 1029000, training loss 0.0398395\n",
      "epoch 29,step 1032000, training loss 0.0376429\n",
      "epoch 29,step 1035000, training loss 0.0210251\n",
      "epoch 29,step 1038000, training loss 0.0250353\n",
      "epoch 29,step 1041000, training loss 0.028542\n",
      "epoch 29,step 1044000, training loss 0.0317488\n",
      "epoch 29,step 1047000, training loss 0.0322687\n",
      "epoch 29,step 1050000, training loss 0.0281279\n",
      "epoch 29,step 1053000, training loss 0.0799919\n",
      "epoch 29,step 1056000, training loss 0.0422386\n",
      "epoch 29,step 1059000, training loss 0.0374705\n",
      "epoch 29,step 1062000, training loss 0.0301092\n",
      "epoch 29,step 1065000, training loss 0.0352985\n",
      "epoch 29,step 1068000, training loss 0.0255533\n",
      "epoch 29,step 1071000, training loss 0.0254492\n",
      "epoch 29,step 1074000, training loss 0.0253635\n",
      "epoch 29,step 1077000, training loss 0.0234312\n",
      "epoch 29,step 1080000, training loss 0.0202076\n",
      "epoch 29,step 1083000, training loss 0.0858454\n",
      "epoch 29,step 1086000, training loss 0.0484685\n",
      "epoch 29,step 1089000, training loss 0.0344174\n",
      "epoch 29,step 1092000, training loss 0.026271\n",
      "epoch 29,step 1095000, training loss 0.0450127\n",
      "epoch 29,step 1098000, training loss 0.0353706\n",
      "epoch 29,step 1101000, training loss 0.0316959\n",
      "epoch 29,step 1104000, training loss 0.0347688\n",
      "epoch 29,step 1107000, training loss 0.0304851\n",
      "epoch 29,step 1110000, training loss 0.0389318\n",
      "epoch 29,step 1113000, training loss 0.0924027\n",
      "epoch 29,step 1116000, training loss 0.0503616\n",
      "epoch 29,step 1119000, training loss 0.0256922\n",
      "epoch 29,step 1122000, training loss 0.0459047\n",
      "epoch 29,step 1125000, training loss 0.0341928\n",
      "epoch 29,step 1128000, training loss 0.0311918\n",
      "epoch 29,step 1131000, training loss 0.0279002\n",
      "epoch 29,step 1134000, training loss 0.03258\n",
      "epoch 29,step 1137000, training loss 0.0230917\n",
      "epoch 29,step 1140000, training loss 0.028243\n",
      "epoch 29,step 1143000, training loss 0.0877836\n",
      "epoch 29,step 1146000, training loss 0.101998\n",
      "epoch 29,step 1149000, training loss 0.0452919\n",
      "epoch 29,step 1152000, training loss 0.0328582\n",
      "epoch 29,step 1155000, training loss 0.0287046\n",
      "epoch 29,step 1158000, training loss 0.0322093\n",
      "epoch 29,step 1161000, training loss 0.0296415\n",
      "epoch 29,step 1164000, training loss 0.0269782\n",
      "epoch 29,step 1167000, training loss 0.034618\n",
      "epoch 29,step 1170000, training loss 0.0258088\n",
      "epoch 29,step 1173000, training loss 0.079797\n",
      "epoch 29,step 1176000, training loss 0.0546719\n",
      "epoch 29,step 1179000, training loss 0.0229353\n",
      "epoch 29,step 1182000, training loss 0.0268377\n",
      "epoch 29,step 1185000, training loss 0.0319853\n",
      "epoch 29,step 1188000, training loss 0.0237398\n",
      "epoch 29,step 1191000, training loss 0.0234129\n",
      "epoch 29,step 1194000, training loss 0.0372154\n",
      "epoch 29,step 1197000, training loss 0.0272087\n",
      "epoch 29,step 1200000, training loss 0.0200241\n",
      "epoch 29,step 1203000, training loss 0.099631\n",
      "epoch 29,step 1206000, training loss 0.0950372\n",
      "epoch 29,step 1209000, training loss 0.0294169\n",
      "epoch 29,step 1212000, training loss 0.0271777\n",
      "epoch 29,step 1215000, training loss 0.0232577\n",
      "epoch 29,step 1218000, training loss 0.0260938\n",
      "epoch 29,step 1221000, training loss 0.0234409\n",
      "epoch 29,step 1224000, training loss 0.0304747\n",
      "epoch 29,step 1227000, training loss 0.0359486\n",
      "epoch 29,step 1230000, training loss 0.025823\n",
      "epoch 29,step 1233000, training loss 0.0783154\n",
      "epoch 29,step 1236000, training loss 0.0440089\n",
      "epoch 29,step 1239000, training loss 0.0316336\n",
      "epoch 29,step 1242000, training loss 0.0320189\n",
      "epoch 29,step 1245000, training loss 0.0258443\n",
      "epoch 29,step 1248000, training loss 0.0375583\n",
      "epoch 29,step 1251000, training loss 0.0329258\n",
      "epoch 29,step 1254000, training loss 0.0291052\n",
      "epoch 29,step 1257000, training loss 0.0197551\n",
      "epoch 29,step 1260000, training loss 0.0242314\n",
      "epoch 29,step 1263000, training loss 0.0853288\n",
      "epoch 29,step 1266000, training loss 0.0419105\n",
      "epoch 29,step 1269000, training loss 0.0159011\n",
      "epoch 29,step 1272000, training loss 0.0265617\n",
      "epoch 29,step 1275000, training loss 0.0363841\n",
      "epoch 29,step 1278000, training loss 0.0336386\n",
      "epoch 29,step 1281000, training loss 0.033983\n",
      "epoch 29,step 1284000, training loss 0.0388734\n",
      "epoch 29,step 1287000, training loss 0.0247895\n",
      "epoch 29,step 1290000, training loss 0.028347\n",
      "epoch 29,step 1293000, training loss 0.0843467\n",
      "epoch 29,step 1296000, training loss 0.0628878\n",
      "epoch 29,step 1299000, training loss 0.0375902\n",
      "epoch 29,step 1302000, training loss 0.0285007\n",
      "epoch 29,step 1305000, training loss 0.0359404\n",
      "epoch 29,step 1308000, training loss 0.025796\n",
      "epoch 29,step 1311000, training loss 0.0324991\n",
      "epoch 29,step 1314000, training loss 0.0318968\n",
      "epoch 29,step 1317000, training loss 0.028537\n",
      "epoch 29,step 1320000, training loss 0.0281865\n",
      "epoch 29,step 1323000, training loss 0.0930711\n",
      "epoch 29,step 1326000, training loss 0.0464099\n",
      "epoch 29,step 1329000, training loss 0.0348799\n",
      "epoch 29,step 1332000, training loss 0.0231047\n",
      "epoch 29,step 1335000, training loss 0.0205408\n",
      "epoch 29,step 1338000, training loss 0.0224564\n",
      "epoch 29,step 1341000, training loss 0.0311211\n",
      "epoch 29,step 1344000, training loss 0.0248445\n",
      "epoch 29,step 1347000, training loss 0.0278389\n",
      "epoch 29,step 1350000, training loss 0.0277524\n",
      "epoch 29,step 1353000, training loss 0.0802227\n",
      "epoch 29,step 1356000, training loss 0.0403296\n",
      "epoch 29,step 1359000, training loss 0.0361971\n",
      "epoch 29,step 1362000, training loss 0.0333781\n",
      "epoch 29,step 1365000, training loss 0.0246595\n",
      "epoch 29,step 1368000, training loss 0.0225498\n",
      "epoch 29,step 1371000, training loss 0.0278122\n",
      "epoch 29,step 1374000, training loss 0.0285175\n",
      "epoch 29,step 1377000, training loss 0.020521\n",
      "epoch 29,step 1380000, training loss 0.0264678\n",
      "epoch 29,step 1383000, training loss 0.0951786\n",
      "epoch 29,step 1386000, training loss 0.0552145\n",
      "epoch 29,step 1389000, training loss 0.0402456\n",
      "epoch 29,step 1392000, training loss 0.0353608\n",
      "epoch 29,step 1395000, training loss 0.0387872\n",
      "epoch 29,step 1398000, training loss 0.0285093\n",
      "epoch 29,step 1401000, training loss 0.0451024\n",
      "epoch 29,step 1404000, training loss 0.0322565\n",
      "epoch 29,step 1407000, training loss 0.0276692\n",
      "epoch 29,step 1410000, training loss 0.0301103\n",
      "epoch 29,step 1413000, training loss 0.0839072\n",
      "epoch 29,step 1416000, training loss 0.0856309\n",
      "epoch 29,step 1419000, training loss 0.0354536\n",
      "epoch 29,step 1422000, training loss 0.027585\n",
      "epoch 29,step 1425000, training loss 0.0228573\n",
      "epoch 29,step 1428000, training loss 0.0452099\n",
      "epoch 29,step 1431000, training loss 0.0319591\n",
      "epoch 29,step 1434000, training loss 0.0318728\n",
      "epoch 29,step 1437000, training loss 0.0266075\n",
      "epoch 29,step 1440000, training loss 0.0286995\n",
      "epoch 29,step 1443000, training loss 0.0683583\n",
      "epoch 29,step 1446000, training loss 0.0341177\n",
      "epoch 29,step 1449000, training loss 0.0351057\n",
      "epoch 29,step 1452000, training loss 0.0318332\n",
      "epoch 29,step 1455000, training loss 0.0255138\n",
      "epoch 29,step 1458000, training loss 0.0305865\n",
      "epoch 29,step 1461000, training loss 0.0260461\n",
      "epoch 29,step 1464000, training loss 0.0268448\n",
      "epoch 29,step 1467000, training loss 0.0271638\n",
      "epoch 29,step 1470000, training loss 0.0303975\n",
      "epoch 29,step 1473000, training loss 0.0925094\n",
      "epoch 29,step 1476000, training loss 0.0423626\n",
      "epoch 29,step 1479000, training loss 0.0272355\n",
      "epoch 29,step 1482000, training loss 0.0421012\n",
      "epoch 29,step 1485000, training loss 0.0347144\n",
      "epoch 29,step 1488000, training loss 0.0262507\n",
      "epoch 29,step 1491000, training loss 0.0281096\n",
      "epoch 29,step 1494000, training loss 0.0289864\n",
      "epoch 29,step 1497000, training loss 0.0548524\n",
      "epoch 29,step 1500000, training loss 0.0278306\n",
      "epoch 29,step 1503000, training loss 0.0959615\n",
      "epoch 29,step 1506000, training loss 0.05509\n",
      "epoch 29,step 1509000, training loss 0.0345058\n",
      "epoch 29,step 1512000, training loss 0.0350783\n",
      "epoch 29,step 1515000, training loss 0.0282134\n",
      "epoch 29,step 1518000, training loss 0.0269942\n",
      "epoch 29,step 1521000, training loss 0.0212953\n",
      "epoch 29,step 1524000, training loss 0.0279761\n",
      "epoch 29,step 1527000, training loss 0.0318757\n",
      "epoch 29,step 1530000, training loss 0.0245769\n",
      "epoch 29,step 1533000, training loss 0.0958554\n",
      "epoch 29,step 1536000, training loss 0.0988831\n",
      "epoch 29,step 1539000, training loss 0.0274062\n",
      "epoch 29,step 1542000, training loss 0.0284781\n",
      "epoch 29,step 1545000, training loss 0.0338413\n",
      "epoch 29,step 1548000, training loss 0.0349641\n",
      "epoch 29,step 1551000, training loss 0.0319985\n",
      "epoch 29,step 1554000, training loss 0.0272101\n",
      "epoch 29,step 1557000, training loss 0.0330239\n",
      "epoch 29,step 1560000, training loss 0.0214341\n",
      "epoch 29,step 1563000, training loss 0.0875811\n",
      "epoch 29,step 1566000, training loss 0.0581094\n",
      "epoch 29,step 1569000, training loss 0.0292368\n",
      "epoch 29,step 1572000, training loss 0.0274986\n",
      "epoch 29,step 1575000, training loss 0.0340695\n",
      "epoch 29,step 1578000, training loss 0.0252365\n",
      "epoch 29,step 1581000, training loss 0.0309541\n",
      "epoch 29,step 1584000, training loss 0.0318629\n",
      "epoch 29,step 1587000, training loss 0.0498998\n",
      "epoch 29,step 1590000, training loss 0.0385624\n",
      "epoch 29,step 1593000, training loss 0.117069\n",
      "epoch 29,step 1596000, training loss 0.0946016\n",
      "epoch 29,step 1599000, training loss 0.0447692\n",
      "epoch 29,step 1602000, training loss 0.0469148\n",
      "epoch 29,step 1605000, training loss 0.0378036\n",
      "epoch 29,step 1608000, training loss 0.0290937\n",
      "epoch 29,step 1611000, training loss 0.0276581\n",
      "epoch 29,step 1614000, training loss 0.0296844\n",
      "epoch 29,step 1617000, training loss 0.032042\n",
      "epoch 29,step 1620000, training loss 0.0292269\n",
      "epoch 29,step 1623000, training loss 0.0741045\n",
      "epoch 29,step 1626000, training loss 0.0754569\n",
      "epoch 29,step 1629000, training loss 0.0456708\n",
      "epoch 29,step 1632000, training loss 0.0509317\n",
      "epoch 29,step 1635000, training loss 0.0407304\n",
      "epoch 29,step 1638000, training loss 0.0283747\n",
      "epoch 29,step 1641000, training loss 0.0227008\n",
      "epoch 29,step 1644000, training loss 0.0311793\n",
      "epoch 29,step 1647000, training loss 0.0275365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29,step 1650000, training loss 0.0474365\n",
      "epoch 29,step 1653000, training loss 0.0871482\n",
      "epoch 29,step 1656000, training loss 0.0482731\n",
      "epoch 29,step 1659000, training loss 0.0324692\n",
      "epoch 29,step 1662000, training loss 0.0392547\n",
      "epoch 29,step 1665000, training loss 0.0345353\n",
      "epoch 29,step 1668000, training loss 0.0337623\n",
      "epoch 29,step 1671000, training loss 0.0282929\n",
      "epoch 29,step 1674000, training loss 0.037746\n",
      "epoch 29,step 1677000, training loss 0.0291544\n",
      "epoch 29,step 1680000, training loss 0.0393797\n",
      "epoch 29,step 1683000, training loss 0.0917751\n",
      "epoch 29,step 1686000, training loss 0.0704589\n",
      "epoch 29,step 1689000, training loss 0.0483927\n",
      "epoch 29,step 1692000, training loss 0.0351412\n",
      "epoch 29,step 1695000, training loss 0.0292798\n",
      "epoch 29,step 1698000, training loss 0.0278892\n",
      "epoch 29,step 1701000, training loss 0.0289053\n",
      "epoch 29,step 1704000, training loss 0.0285442\n",
      "epoch 29,step 1707000, training loss 0.0268628\n",
      "epoch 29,step 1710000, training loss 0.026694\n",
      "epoch 29,step 1713000, training loss 0.0903425\n",
      "epoch 29,step 1716000, training loss 0.0635422\n",
      "epoch 29,step 1719000, training loss 0.0422661\n",
      "epoch 29,step 1722000, training loss 0.0379838\n",
      "epoch 29,step 1725000, training loss 0.0483798\n",
      "epoch 29,step 1728000, training loss 0.0369637\n",
      "epoch 29,step 1731000, training loss 0.0331151\n",
      "epoch 29,step 1734000, training loss 0.0350163\n",
      "epoch 29,step 1737000, training loss 0.0273274\n",
      "epoch 29,step 1740000, training loss 0.021829\n",
      "epoch 29,step 1743000, training loss 0.0887735\n",
      "epoch 29,step 1746000, training loss 0.0483024\n",
      "epoch 29,step 1749000, training loss 0.0287059\n",
      "epoch 29,step 1752000, training loss 0.0324648\n",
      "epoch 29,step 1755000, training loss 0.0283537\n",
      "epoch 29,step 1758000, training loss 0.0313798\n",
      "epoch 29,step 1761000, training loss 0.0278257\n",
      "epoch 29,step 1764000, training loss 0.0373494\n",
      "epoch 29,step 1767000, training loss 0.0340212\n",
      "epoch 29,step 1770000, training loss 0.0232966\n",
      "epoch 29,step 1773000, training loss 0.0793395\n",
      "epoch 29,step 1776000, training loss 0.0435156\n",
      "epoch 29,step 1779000, training loss 0.0330101\n",
      "epoch 29,step 1782000, training loss 0.0378724\n",
      "epoch 29,step 1785000, training loss 0.0302516\n",
      "epoch 29,step 1788000, training loss 0.030365\n",
      "epoch 29,step 1791000, training loss 0.0230423\n",
      "epoch 29,step 1794000, training loss 0.0275137\n",
      "epoch 29,step 1797000, training loss 0.0273698\n",
      "epoch 29,step 1800000, training loss 0.0378249\n",
      "epoch 29,step 1803000, training loss 0.143096\n",
      "epoch 29,step 1806000, training loss 0.0514415\n",
      "epoch 29,step 1809000, training loss 0.0343253\n",
      "epoch 29,step 1812000, training loss 0.0476432\n",
      "epoch 29,step 1815000, training loss 0.0215918\n",
      "epoch 29,step 1818000, training loss 0.0329072\n",
      "epoch 29,step 1821000, training loss 0.0269945\n",
      "epoch 29,step 1824000, training loss 0.0242959\n",
      "epoch 29,step 1827000, training loss 0.0266573\n",
      "epoch 29,step 1830000, training loss 0.0219807\n",
      "epoch 29,step 1833000, training loss 0.109431\n",
      "epoch 29,step 1836000, training loss 0.0584031\n",
      "epoch 29,step 1839000, training loss 0.0345135\n",
      "epoch 29,step 1842000, training loss 0.0238238\n",
      "epoch 29,step 1845000, training loss 0.0244105\n",
      "epoch 29,step 1848000, training loss 0.0276417\n",
      "epoch 29,step 1851000, training loss 0.0232972\n",
      "epoch 29,step 1854000, training loss 0.0229179\n",
      "epoch 29,step 1857000, training loss 0.0250613\n",
      "epoch 29,step 1860000, training loss 0.0347213\n",
      "epoch 29,step 1863000, training loss 0.129797\n",
      "epoch 29,step 1866000, training loss 0.113934\n",
      "epoch 29,step 1869000, training loss 0.0418421\n",
      "epoch 29,step 1872000, training loss 0.0481489\n",
      "epoch 29,step 1875000, training loss 0.0350684\n",
      "epoch 29,step 1878000, training loss 0.0419989\n",
      "epoch 29,step 1881000, training loss 0.024558\n",
      "epoch 29,step 1884000, training loss 0.0298722\n",
      "epoch 29,step 1887000, training loss 0.0273257\n",
      "epoch 29,step 1890000, training loss 0.0315826\n",
      "epoch 29,step 1893000, training loss 0.0854525\n",
      "epoch 29,step 1896000, training loss 0.0494748\n",
      "epoch 29,step 1899000, training loss 0.0283086\n",
      "epoch 29,step 1902000, training loss 0.0325504\n",
      "epoch 29,step 1905000, training loss 0.0235652\n",
      "epoch 29,step 1908000, training loss 0.0222\n",
      "epoch 29,step 1911000, training loss 0.024472\n",
      "epoch 29,step 1914000, training loss 0.0353449\n",
      "epoch 29,step 1917000, training loss 0.0256873\n",
      "epoch 29,step 1920000, training loss 0.0309022\n",
      "epoch 29,step 1923000, training loss 0.131335\n",
      "epoch 29,step 1926000, training loss 0.0643622\n",
      "epoch 29,step 1929000, training loss 0.0307299\n",
      "epoch 29,step 1932000, training loss 0.0289278\n",
      "epoch 29,step 1935000, training loss 0.0358346\n",
      "epoch 29,step 1938000, training loss 0.0354908\n",
      "epoch 29,step 1941000, training loss 0.077152\n",
      "epoch 29,step 1944000, training loss 0.0260414\n",
      "epoch 29,step 1947000, training loss 0.0236528\n",
      "epoch 29,step 1950000, training loss 0.0323708\n",
      "epoch 29,step 1953000, training loss 0.0981056\n",
      "epoch 29,step 1956000, training loss 0.0391329\n",
      "epoch 29,step 1959000, training loss 0.0220886\n",
      "epoch 29,step 1962000, training loss 0.0281725\n",
      "epoch 29,step 1965000, training loss 0.02829\n",
      "epoch 29,step 1968000, training loss 0.0286458\n",
      "epoch 29,step 1971000, training loss 0.0217384\n",
      "epoch 29,step 1974000, training loss 0.0304437\n",
      "epoch 29,step 1977000, training loss 0.0429501\n",
      "epoch 29,step 1980000, training loss 0.0255391\n",
      "epoch 29,step 1983000, training loss 0.106026\n",
      "epoch 29,step 1986000, training loss 0.0540768\n",
      "epoch 29,step 1989000, training loss 0.0310305\n",
      "epoch 29,step 1992000, training loss 0.0319889\n",
      "epoch 29,step 1995000, training loss 0.0245175\n",
      "epoch 29,step 1998000, training loss 0.0283355\n",
      "epoch 29,step 2001000, training loss 0.0320229\n",
      "epoch 29,step 2004000, training loss 0.0372669\n",
      "epoch 29,step 2007000, training loss 0.0278683\n",
      "epoch 29,step 2010000, training loss 0.030271\n",
      "epoch 29,step 2013000, training loss 0.0983867\n",
      "epoch 29,step 2016000, training loss 0.0917964\n",
      "epoch 29,step 2019000, training loss 0.0347083\n",
      "epoch 29,step 2022000, training loss 0.0409126\n",
      "epoch 29,step 2025000, training loss 0.0229929\n",
      "epoch 29,step 2028000, training loss 0.0373447\n",
      "epoch 29,step 2031000, training loss 0.0396624\n",
      "epoch 29,step 2034000, training loss 0.0280071\n",
      "epoch 29,step 2037000, training loss 0.0256731\n",
      "epoch 29,step 2040000, training loss 0.0303656\n",
      "epoch 29,step 2043000, training loss 0.0969414\n",
      "epoch 29,step 2046000, training loss 0.0503278\n",
      "epoch 29,step 2049000, training loss 0.0372329\n",
      "epoch 29,step 2052000, training loss 0.0379547\n",
      "epoch 29,step 2055000, training loss 0.0267086\n",
      "epoch 29,step 2058000, training loss 0.0289042\n",
      "epoch 29,step 2061000, training loss 0.0308355\n",
      "epoch 29,step 2064000, training loss 0.0368874\n",
      "epoch 29,step 2067000, training loss 0.0321\n",
      "epoch 29,step 2070000, training loss 0.0292342\n",
      "epoch 29,step 2073000, training loss 0.091574\n",
      "epoch 29,step 2076000, training loss 0.0555279\n",
      "epoch 29,step 2079000, training loss 0.0276745\n",
      "epoch 29,step 2082000, training loss 0.0313035\n",
      "epoch 29,step 2085000, training loss 0.0313336\n",
      "epoch 29,step 2088000, training loss 0.0270143\n",
      "epoch 29,step 2091000, training loss 0.0279072\n",
      "epoch 29,step 2094000, training loss 0.0340036\n",
      "epoch 29,step 2097000, training loss 0.0319389\n",
      "epoch 29,step 2100000, training loss 0.0265646\n",
      "epoch 29,step 2103000, training loss 0.0878928\n",
      "epoch 29,step 2106000, training loss 0.0647483\n",
      "epoch 29,step 2109000, training loss 0.0330477\n",
      "epoch 29,step 2112000, training loss 0.0256979\n",
      "epoch 29,step 2115000, training loss 0.0266758\n",
      "epoch 29,step 2118000, training loss 0.0269928\n",
      "epoch 29,step 2121000, training loss 0.0312818\n",
      "epoch 29,step 2124000, training loss 0.0202929\n",
      "epoch 29,step 2127000, training loss 0.0227794\n",
      "epoch 29,step 2130000, training loss 0.0262554\n",
      "epoch 29,step 2133000, training loss 0.0879091\n",
      "epoch 29,step 2136000, training loss 0.0564052\n",
      "epoch 29,step 2139000, training loss 0.0315173\n",
      "epoch 29,step 2142000, training loss 0.0267947\n",
      "epoch 29,step 2145000, training loss 0.0309747\n",
      "epoch 29,step 2148000, training loss 0.0281635\n",
      "epoch 29,step 2151000, training loss 0.0391484\n",
      "epoch 29,step 2154000, training loss 0.0386162\n",
      "epoch 29,step 2157000, training loss 0.0261019\n",
      "epoch 29,step 2160000, training loss 0.0260467\n",
      "epoch 29,step 2163000, training loss 0.0658388\n",
      "epoch 29,step 2166000, training loss 0.0452617\n",
      "epoch 29,step 2169000, training loss 0.0425038\n",
      "epoch 29,step 2172000, training loss 0.034973\n",
      "epoch 29,step 2175000, training loss 0.0342321\n",
      "epoch 29,step 2178000, training loss 0.026254\n",
      "epoch 29,step 2181000, training loss 0.0402108\n",
      "epoch 29,step 2184000, training loss 0.0308863\n",
      "epoch 29,step 2187000, training loss 0.0274397\n",
      "epoch 29,step 2190000, training loss 0.0263033\n",
      "epoch 29,step 2193000, training loss 0.089399\n",
      "epoch 29,step 2196000, training loss 0.0482557\n",
      "epoch 29,step 2199000, training loss 0.0289899\n",
      "epoch 29,step 2202000, training loss 0.0425137\n",
      "epoch 29,step 2205000, training loss 0.0287931\n",
      "epoch 29,step 2208000, training loss 0.0293566\n",
      "epoch 29,step 2211000, training loss 0.0283812\n",
      "epoch 29,step 2214000, training loss 0.0378946\n",
      "epoch 29,step 2217000, training loss 0.0541909\n",
      "epoch 29,step 2220000, training loss 0.0384764\n",
      "epoch 29,step 2223000, training loss 0.0690713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29,step 2226000, training loss 0.0408455\n",
      "epoch 29,step 2229000, training loss 0.0251575\n",
      "epoch 29,step 2232000, training loss 0.022565\n",
      "epoch 29,step 2235000, training loss 0.0238492\n",
      "epoch 29,step 2238000, training loss 0.0277003\n",
      "epoch 29,step 2241000, training loss 0.0264388\n",
      "epoch 29,step 2244000, training loss 0.0245983\n",
      "epoch 29,step 2247000, training loss 0.0241945\n",
      "epoch 29,step 2250000, training loss 0.0579811\n",
      "epoch 29,step 2253000, training loss 0.0764897\n",
      "epoch 29,step 2256000, training loss 0.0391556\n",
      "epoch 29,step 2259000, training loss 0.027211\n",
      "epoch 29,step 2262000, training loss 0.025616\n",
      "epoch 29,step 2265000, training loss 0.0241692\n",
      "epoch 29,step 2268000, training loss 0.0278401\n",
      "epoch 29,step 2271000, training loss 0.0269886\n",
      "epoch 29,step 2274000, training loss 0.0247726\n",
      "epoch 29,step 2277000, training loss 0.017383\n",
      "epoch 29,step 2280000, training loss 0.0222517\n",
      "epoch 29,step 2283000, training loss 0.0785869\n",
      "epoch 29,step 2286000, training loss 0.0728413\n",
      "epoch 29,step 2289000, training loss 0.0296937\n",
      "epoch 29,step 2292000, training loss 0.0243111\n",
      "epoch 29,step 2295000, training loss 0.0365148\n",
      "epoch 29,step 2298000, training loss 0.0432612\n",
      "epoch 29,step 2301000, training loss 0.0245071\n",
      "epoch 29,step 2304000, training loss 0.0369399\n",
      "epoch 29,step 2307000, training loss 0.0284312\n",
      "epoch 29,step 2310000, training loss 0.0174863\n",
      "epoch 29,step 2313000, training loss 0.109551\n",
      "epoch 29,step 2316000, training loss 0.0766209\n",
      "epoch 29,step 2319000, training loss 0.0366189\n",
      "epoch 29,step 2322000, training loss 0.0348131\n",
      "epoch 29,step 2325000, training loss 0.0298904\n",
      "epoch 29,step 2328000, training loss 0.0222998\n",
      "epoch 29,step 2331000, training loss 0.0253266\n",
      "epoch 29,step 2334000, training loss 0.0326221\n",
      "epoch 29,step 2337000, training loss 0.0335099\n",
      "epoch 29,step 2340000, training loss 0.0219708\n",
      "epoch 29,step 2343000, training loss 0.0801967\n",
      "epoch 29,step 2346000, training loss 0.0645558\n",
      "epoch 29,step 2349000, training loss 0.0391725\n",
      "epoch 29,step 2352000, training loss 0.0279353\n",
      "epoch 29,step 2355000, training loss 0.0292039\n",
      "epoch 29,step 2358000, training loss 0.0238604\n",
      "epoch 29,step 2361000, training loss 0.0496449\n",
      "epoch 29,step 2364000, training loss 0.0357366\n",
      "epoch 29,step 2367000, training loss 0.0269412\n",
      "epoch 29,step 2370000, training loss 0.0313182\n",
      "epoch 29,step 2373000, training loss 0.111286\n",
      "epoch 29,step 2376000, training loss 0.0570537\n",
      "epoch 29,step 2379000, training loss 0.0469927\n",
      "epoch 29,step 2382000, training loss 0.0455128\n",
      "epoch 29,step 2385000, training loss 0.0302627\n",
      "epoch 29,step 2388000, training loss 0.0317838\n",
      "epoch 29,step 2391000, training loss 0.0225641\n",
      "epoch 29,step 2394000, training loss 0.0332811\n",
      "epoch 29,step 2397000, training loss 0.0250753\n",
      "epoch 29,step 2400000, training loss 0.0256122\n",
      "epoch 29,step 2403000, training loss 0.0973406\n",
      "epoch 29,step 2406000, training loss 0.0399567\n",
      "epoch 29,step 2409000, training loss 0.036779\n",
      "epoch 29,step 2412000, training loss 0.0411126\n",
      "epoch 29,step 2415000, training loss 0.0321072\n",
      "epoch 29,step 2418000, training loss 0.0379511\n",
      "epoch 29,step 2421000, training loss 0.0299425\n",
      "epoch 29,step 2424000, training loss 0.0377575\n",
      "epoch 29,step 2427000, training loss 0.0308144\n",
      "epoch 29,step 2430000, training loss 0.0252976\n",
      "epoch 29,step 2433000, training loss 0.0869132\n",
      "epoch 29,step 2436000, training loss 0.0670647\n",
      "epoch 29,step 2439000, training loss 0.0336689\n",
      "epoch 29,step 2442000, training loss 0.0364459\n",
      "epoch 29,step 2445000, training loss 0.0350726\n",
      "epoch 29,step 2448000, training loss 0.0278162\n",
      "epoch 29,step 2451000, training loss 0.0295014\n",
      "epoch 29,step 2454000, training loss 0.0335647\n",
      "epoch 29,step 2457000, training loss 0.0310052\n",
      "epoch 29,step 2460000, training loss 0.0336261\n",
      "epoch 29,step 2463000, training loss 0.0977753\n",
      "epoch 29,step 2466000, training loss 0.0401452\n",
      "epoch 29,step 2469000, training loss 0.02774\n",
      "epoch 29,step 2472000, training loss 0.0389194\n",
      "epoch 29,step 2475000, training loss 0.0456287\n",
      "epoch 29,step 2478000, training loss 0.0341168\n",
      "epoch 29,step 2481000, training loss 0.0292615\n",
      "epoch 29,step 2484000, training loss 0.0494391\n",
      "epoch 29,step 2487000, training loss 0.0422883\n",
      "epoch 29,step 2490000, training loss 0.034481\n",
      "epoch 29,step 2493000, training loss 0.0906285\n",
      "epoch 29,step 2496000, training loss 0.0609263\n",
      "epoch 29,step 2499000, training loss 0.0281569\n",
      "epoch 29,step 2502000, training loss 0.0326882\n",
      "epoch 29,step 2505000, training loss 0.0311155\n",
      "epoch 29,step 2508000, training loss 0.036759\n",
      "epoch 29,step 2511000, training loss 0.0267317\n",
      "epoch 29,step 2514000, training loss 0.0266588\n",
      "epoch 29,step 2517000, training loss 0.0362925\n",
      "epoch 29,step 2520000, training loss 0.0270272\n",
      "epoch 29,step 2523000, training loss 0.0989811\n",
      "epoch 29,step 2526000, training loss 0.0728494\n",
      "epoch 29,step 2529000, training loss 0.0367747\n",
      "epoch 29,step 2532000, training loss 0.0313789\n",
      "epoch 29,step 2535000, training loss 0.0354547\n",
      "epoch 29,step 2538000, training loss 0.0317176\n",
      "epoch 29,step 2541000, training loss 0.0459681\n",
      "epoch 29,step 2544000, training loss 0.0225765\n",
      "epoch 29,step 2547000, training loss 0.0287278\n",
      "epoch 29,step 2550000, training loss 0.0330902\n",
      "epoch 29,step 2553000, training loss 0.084343\n",
      "epoch 29,step 2556000, training loss 0.0501717\n",
      "epoch 29,step 2559000, training loss 0.030471\n",
      "epoch 29,step 2562000, training loss 0.0411425\n",
      "epoch 29,step 2565000, training loss 0.0394569\n",
      "epoch 29,step 2568000, training loss 0.0362532\n",
      "epoch 29,step 2571000, training loss 0.0290471\n",
      "epoch 29,step 2574000, training loss 0.0337208\n",
      "epoch 29,step 2577000, training loss 0.0332506\n",
      "epoch 29,step 2580000, training loss 0.0431382\n",
      "epoch 29,step 2583000, training loss 0.0778374\n",
      "epoch 29,step 2586000, training loss 0.0539045\n",
      "epoch 29,step 2589000, training loss 0.0280511\n",
      "epoch 29,step 2592000, training loss 0.0278489\n",
      "epoch 29,step 2595000, training loss 0.0284957\n",
      "epoch 29,step 2598000, training loss 0.0264725\n",
      "epoch 29,step 2601000, training loss 0.0291349\n",
      "epoch 29,step 2604000, training loss 0.0272834\n",
      "epoch 29,step 2607000, training loss 0.0331964\n",
      "epoch 29,step 2610000, training loss 0.0352338\n",
      "epoch 29,step 2613000, training loss 0.0848153\n",
      "epoch 29,step 2616000, training loss 0.046675\n",
      "epoch 29,step 2619000, training loss 0.026922\n",
      "epoch 29,step 2622000, training loss 0.0285898\n",
      "epoch 29,step 2625000, training loss 0.0273253\n",
      "epoch 29,step 2628000, training loss 0.0425066\n",
      "epoch 29,step 2631000, training loss 0.0409182\n",
      "epoch 29,step 2634000, training loss 0.0253577\n",
      "epoch 29,step 2637000, training loss 0.0197462\n",
      "epoch 29,step 2640000, training loss 0.0275319\n",
      "epoch 29,step 2643000, training loss 0.127836\n",
      "epoch 29,step 2646000, training loss 0.0775368\n",
      "epoch 29,step 2649000, training loss 0.0499142\n",
      "epoch 29,step 2652000, training loss 0.0282006\n",
      "epoch 29,step 2655000, training loss 0.0294793\n",
      "epoch 29,step 2658000, training loss 0.0256687\n",
      "epoch 29,step 2661000, training loss 0.026376\n",
      "epoch 29,step 2664000, training loss 0.0258313\n",
      "epoch 29,step 2667000, training loss 0.0521009\n",
      "epoch 29,step 2670000, training loss 0.0241632\n",
      "epoch 29,step 2673000, training loss 0.0957252\n",
      "epoch 29,step 2676000, training loss 0.0869451\n",
      "epoch 29,step 2679000, training loss 0.0411269\n",
      "epoch 29,step 2682000, training loss 0.0313152\n",
      "epoch 29,step 2685000, training loss 0.0382857\n",
      "epoch 29,step 2688000, training loss 0.0377166\n",
      "epoch 29,step 2691000, training loss 0.0341791\n",
      "epoch 29,step 2694000, training loss 0.0313999\n",
      "epoch 29,step 2697000, training loss 0.0295423\n",
      "epoch 29,step 2700000, training loss 0.0630818\n",
      "epoch 29,step 2703000, training loss 0.0791177\n",
      "epoch 29,step 2706000, training loss 0.0338135\n",
      "epoch 29,step 2709000, training loss 0.0252378\n",
      "epoch 29,step 2712000, training loss 0.0428328\n",
      "epoch 29,step 2715000, training loss 0.0277807\n",
      "epoch 29,step 2718000, training loss 0.0256858\n",
      "epoch 29,step 2721000, training loss 0.021735\n",
      "epoch 29,step 2724000, training loss 0.0238913\n",
      "epoch 29,step 2727000, training loss 0.0268388\n",
      "epoch 29,step 2730000, training loss 0.0266972\n",
      "epoch 29,step 2733000, training loss 0.0825152\n",
      "epoch 29,step 2736000, training loss 0.048336\n",
      "epoch 29,step 2739000, training loss 0.0305374\n",
      "epoch 29,step 2742000, training loss 0.0349391\n",
      "epoch 29,step 2745000, training loss 0.0349553\n",
      "epoch 29,step 2748000, training loss 0.0265534\n",
      "epoch 29,step 2751000, training loss 0.0279854\n",
      "epoch 29,step 2754000, training loss 0.0336851\n",
      "epoch 29,step 2757000, training loss 0.0323924\n",
      "epoch 29,step 2760000, training loss 0.0268171\n",
      "epoch 29,step 2763000, training loss 0.0727964\n",
      "epoch 29,step 2766000, training loss 0.0638046\n",
      "epoch 29,step 2769000, training loss 0.0301444\n",
      "epoch 29,step 2772000, training loss 0.0266086\n",
      "epoch 29,step 2775000, training loss 0.022377\n",
      "epoch 29,step 2778000, training loss 0.0301195\n",
      "epoch 29,step 2781000, training loss 0.0253119\n",
      "epoch 29,step 2784000, training loss 0.0216198\n",
      "epoch 29,step 2787000, training loss 0.0261005\n",
      "epoch 29,step 2790000, training loss 0.0219012\n",
      "epoch 29,step 2793000, training loss 0.0860717\n",
      "epoch 29,step 2796000, training loss 0.0449712\n",
      "epoch 29,step 2799000, training loss 0.0244919\n",
      "epoch 29,step 2802000, training loss 0.0309124\n",
      "epoch 29,step 2805000, training loss 0.0387747\n",
      "epoch 29,step 2808000, training loss 0.0271206\n",
      "epoch 29,step 2811000, training loss 0.0220158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29,step 2814000, training loss 0.0361565\n",
      "epoch 29,step 2817000, training loss 0.024591\n",
      "epoch 29,step 2820000, training loss 0.0237267\n",
      "epoch 29,step 2823000, training loss 0.0950214\n",
      "epoch 29,step 2826000, training loss 0.0561455\n",
      "epoch 29,step 2829000, training loss 0.0260589\n",
      "epoch 29,step 2832000, training loss 0.033772\n",
      "epoch 29,step 2835000, training loss 0.0293417\n",
      "epoch 29,step 2838000, training loss 0.0341448\n",
      "epoch 29,step 2841000, training loss 0.038253\n",
      "epoch 29,step 2844000, training loss 0.0232743\n",
      "epoch 29,step 2847000, training loss 0.0322461\n",
      "epoch 29,step 2850000, training loss 0.0205449\n",
      "epoch 29,step 2853000, training loss 0.090841\n",
      "epoch 29,step 2856000, training loss 0.0770574\n",
      "epoch 29,step 2859000, training loss 0.028943\n",
      "epoch 29,step 2862000, training loss 0.0224925\n",
      "epoch 29,step 2865000, training loss 0.0246716\n",
      "epoch 29,step 2868000, training loss 0.0318503\n",
      "epoch 29,step 2871000, training loss 0.030434\n",
      "epoch 29,step 2874000, training loss 0.0359345\n",
      "epoch 29,step 2877000, training loss 0.0265121\n",
      "epoch 29,step 2880000, training loss 0.0303666\n",
      "epoch 29,step 2883000, training loss 0.0929448\n",
      "epoch 29,step 2886000, training loss 0.0485429\n",
      "epoch 29,step 2889000, training loss 0.0201844\n",
      "epoch 29,step 2892000, training loss 0.020221\n",
      "epoch 29,step 2895000, training loss 0.0211237\n",
      "epoch 29,step 2898000, training loss 0.0265299\n",
      "epoch 29,step 2901000, training loss 0.0249027\n",
      "epoch 29,step 2904000, training loss 0.0235294\n",
      "epoch 29,step 2907000, training loss 0.0251281\n",
      "epoch 29,step 2910000, training loss 0.0279149\n",
      "epoch 29,step 2913000, training loss 0.101215\n",
      "epoch 29,step 2916000, training loss 0.0468401\n",
      "epoch 29,step 2919000, training loss 0.0247841\n",
      "epoch 29,step 2922000, training loss 0.0269502\n",
      "epoch 29,step 2925000, training loss 0.0335336\n",
      "epoch 29,step 2928000, training loss 0.0237855\n",
      "epoch 29,step 2931000, training loss 0.0243322\n",
      "epoch 29,step 2934000, training loss 0.0272307\n",
      "epoch 29,step 2937000, training loss 0.0242534\n",
      "epoch 29,step 2940000, training loss 0.027244\n",
      "epoch 29,step 2943000, training loss 0.087723\n",
      "epoch 29,step 2946000, training loss 0.0545923\n",
      "epoch 29,step 2949000, training loss 0.0382616\n",
      "epoch 29,step 2952000, training loss 0.0265616\n",
      "epoch 29,step 2955000, training loss 0.0220977\n",
      "epoch 29,step 2958000, training loss 0.0506834\n",
      "epoch 29,step 2961000, training loss 0.0264857\n",
      "epoch 29,step 2964000, training loss 0.0301541\n",
      "epoch 29,step 2967000, training loss 0.0246016\n",
      "epoch 29,step 2970000, training loss 0.0228343\n",
      "epoch 29,step 2973000, training loss 0.0880189\n",
      "epoch 29,step 2976000, training loss 0.0398384\n",
      "epoch 29,step 2979000, training loss 0.0298571\n",
      "epoch 29,step 2982000, training loss 0.0214121\n",
      "epoch 29,step 2985000, training loss 0.0229385\n",
      "epoch 29,step 2988000, training loss 0.025161\n",
      "epoch 29,step 2991000, training loss 0.0201109\n",
      "epoch 29,step 2994000, training loss 0.0252759\n",
      "epoch 29,step 2997000, training loss 0.0357747\n",
      "epoch 29,training loss 0.0357747 ,test loss 0.0399659\n",
      "epoch 30,step 15500, training loss 0.0337335\n",
      "epoch 30,step 31000, training loss 0.0407853\n",
      "epoch 30,step 46500, training loss 0.0357359\n",
      "epoch 30,step 62000, training loss 0.020888\n",
      "epoch 30,step 77500, training loss 0.0236958\n",
      "epoch 30,step 93000, training loss 0.0226978\n",
      "epoch 30,step 108500, training loss 0.0277239\n",
      "epoch 30,step 124000, training loss 0.0224303\n",
      "epoch 30,step 139500, training loss 0.02703\n",
      "epoch 30,step 155000, training loss 0.0246835\n",
      "epoch 30,step 170500, training loss 0.0244792\n",
      "epoch 30,step 186000, training loss 0.0331594\n",
      "epoch 30,step 201500, training loss 0.0524572\n",
      "epoch 30,step 217000, training loss 0.0266863\n",
      "epoch 30,step 232500, training loss 0.0422399\n",
      "epoch 30,step 248000, training loss 0.0275037\n",
      "epoch 30,step 263500, training loss 0.0266028\n",
      "epoch 30,step 279000, training loss 0.0231681\n",
      "epoch 30,step 294500, training loss 0.0206479\n",
      "epoch 30,step 310000, training loss 0.03189\n",
      "epoch 30,step 325500, training loss 0.0261036\n",
      "epoch 30,step 341000, training loss 0.0306314\n",
      "epoch 30,step 356500, training loss 0.0422037\n",
      "epoch 30,step 372000, training loss 0.0340954\n",
      "epoch 30,step 387500, training loss 0.0329595\n",
      "epoch 30,step 403000, training loss 0.0179272\n",
      "epoch 30,step 418500, training loss 0.0454373\n",
      "epoch 30,step 434000, training loss 0.0290961\n",
      "epoch 30,step 449500, training loss 0.034039\n",
      "epoch 30,step 465000, training loss 0.0272975\n",
      "epoch 30,step 480500, training loss 0.0427243\n",
      "epoch 30,step 496000, training loss 0.0259817\n",
      "epoch 30,step 511500, training loss 0.0280696\n",
      "epoch 30,step 527000, training loss 0.0279318\n",
      "epoch 30,step 542500, training loss 0.0365138\n",
      "epoch 30,step 558000, training loss 0.0278831\n",
      "epoch 30,step 573500, training loss 0.0313074\n",
      "epoch 30,step 589000, training loss 0.0301289\n",
      "epoch 30,step 604500, training loss 0.028008\n",
      "epoch 30,step 620000, training loss 0.0240788\n",
      "epoch 30,step 635500, training loss 0.0310803\n",
      "epoch 30,step 651000, training loss 0.0244136\n",
      "epoch 30,step 666500, training loss 0.0270146\n",
      "epoch 30,step 682000, training loss 0.0304268\n",
      "epoch 30,step 697500, training loss 0.0242269\n",
      "epoch 30,step 713000, training loss 0.0320321\n",
      "epoch 30,step 728500, training loss 0.0199148\n",
      "epoch 30,step 744000, training loss 0.0258826\n",
      "epoch 30,step 759500, training loss 0.0278924\n",
      "epoch 30,step 775000, training loss 0.0249951\n",
      "epoch 30,step 790500, training loss 0.0282162\n",
      "epoch 30,step 806000, training loss 0.0322125\n",
      "epoch 30,step 821500, training loss 0.021822\n",
      "epoch 30,step 837000, training loss 0.0275016\n",
      "epoch 30,step 852500, training loss 0.0472525\n",
      "epoch 30,step 868000, training loss 0.0246728\n",
      "epoch 30,step 883500, training loss 0.0210963\n",
      "epoch 30,step 899000, training loss 0.0289549\n",
      "epoch 30,step 914500, training loss 0.0361351\n",
      "epoch 30,step 930000, training loss 0.0277265\n",
      "epoch 30,step 945500, training loss 0.0302837\n",
      "epoch 30,step 961000, training loss 0.0295059\n",
      "epoch 30,step 976500, training loss 0.02831\n",
      "epoch 30,step 992000, training loss 0.0373012\n",
      "epoch 30,step 1007500, training loss 0.0424506\n",
      "epoch 30,step 1023000, training loss 0.0267002\n",
      "epoch 30,step 1038500, training loss 0.0278285\n",
      "epoch 30,step 1054000, training loss 0.0381654\n",
      "epoch 30,step 1069500, training loss 0.0213619\n",
      "epoch 30,step 1085000, training loss 0.0273126\n",
      "epoch 30,step 1100500, training loss 0.0350837\n",
      "epoch 30,step 1116000, training loss 0.0204533\n",
      "epoch 30,step 1131500, training loss 0.0439168\n",
      "epoch 30,step 1147000, training loss 0.0387992\n",
      "epoch 30,step 1162500, training loss 0.0346379\n",
      "epoch 30,step 1178000, training loss 0.027972\n",
      "epoch 30,step 1193500, training loss 0.0284822\n",
      "epoch 30,step 1209000, training loss 0.0253885\n",
      "epoch 30,step 1224500, training loss 0.0318726\n",
      "epoch 30,step 1240000, training loss 0.0200473\n",
      "epoch 30,step 1255500, training loss 0.0239192\n",
      "epoch 30,step 1271000, training loss 0.0256224\n",
      "epoch 30,step 1286500, training loss 0.0253844\n",
      "epoch 30,step 1302000, training loss 0.0238264\n",
      "epoch 30,step 1317500, training loss 0.0355273\n",
      "epoch 30,step 1333000, training loss 0.0276767\n",
      "epoch 30,step 1348500, training loss 0.0356089\n",
      "epoch 30,step 1364000, training loss 0.0279695\n",
      "epoch 30,step 1379500, training loss 0.0202951\n",
      "epoch 30,step 1395000, training loss 0.0275703\n",
      "epoch 30,step 1410500, training loss 0.0252024\n",
      "epoch 30,step 1426000, training loss 0.0266208\n",
      "epoch 30,step 1441500, training loss 0.0403759\n",
      "epoch 30,step 1457000, training loss 0.0300384\n",
      "epoch 30,step 1472500, training loss 0.0234601\n",
      "epoch 30,step 1488000, training loss 0.0280891\n",
      "epoch 30,step 1503500, training loss 0.0263495\n",
      "epoch 30,step 1519000, training loss 0.0312323\n",
      "epoch 30,step 1534500, training loss 0.0349178\n",
      "epoch 30,step 1550000, training loss 0.0272387\n",
      "epoch 30,step 1565500, training loss 0.0277766\n",
      "epoch 30,step 1581000, training loss 0.0245125\n",
      "epoch 30,step 1596500, training loss 0.0334783\n",
      "epoch 30,step 1612000, training loss 0.0206746\n",
      "epoch 30,step 1627500, training loss 0.0335933\n",
      "epoch 30,step 1643000, training loss 0.0380501\n",
      "epoch 30,step 1658500, training loss 0.0390685\n",
      "epoch 30,step 1674000, training loss 0.0298426\n",
      "epoch 30,step 1689500, training loss 0.0397969\n",
      "epoch 30,step 1705000, training loss 0.0470031\n",
      "epoch 30,step 1720500, training loss 0.0338347\n",
      "epoch 30,step 1736000, training loss 0.0376241\n",
      "epoch 30,step 1751500, training loss 0.028899\n",
      "epoch 30,step 1767000, training loss 0.0257324\n",
      "epoch 30,step 1782500, training loss 0.0479473\n",
      "epoch 30,step 1798000, training loss 0.0223557\n",
      "epoch 30,step 1813500, training loss 0.0275342\n",
      "epoch 30,step 1829000, training loss 0.0227558\n",
      "epoch 30,step 1844500, training loss 0.0307166\n",
      "epoch 30,step 1860000, training loss 0.0319009\n",
      "epoch 30,step 1875500, training loss 0.0212126\n",
      "epoch 30,step 1891000, training loss 0.0213087\n",
      "epoch 30,step 1906500, training loss 0.0250493\n",
      "epoch 30,step 1922000, training loss 0.0351311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30,step 1937500, training loss 0.03481\n",
      "epoch 30,step 1953000, training loss 0.0312934\n",
      "epoch 30,step 1968500, training loss 0.0237471\n",
      "epoch 30,step 1984000, training loss 0.0299115\n",
      "epoch 30,step 1999500, training loss 0.0355238\n",
      "epoch 30,step 2015000, training loss 0.0321258\n",
      "epoch 30,step 2030500, training loss 0.0294127\n",
      "epoch 30,step 2046000, training loss 0.0264784\n",
      "epoch 30,step 2061500, training loss 0.023543\n",
      "epoch 30,step 2077000, training loss 0.0303909\n",
      "epoch 30,step 2092500, training loss 0.0226233\n",
      "epoch 30,step 2108000, training loss 0.0309124\n",
      "epoch 30,step 2123500, training loss 0.0278556\n",
      "epoch 30,step 2139000, training loss 0.0282166\n",
      "epoch 30,step 2154500, training loss 0.0313394\n",
      "epoch 30,step 2170000, training loss 0.0263562\n",
      "epoch 30,step 2185500, training loss 0.0275903\n",
      "epoch 30,step 2201000, training loss 0.0259001\n",
      "epoch 30,step 2216500, training loss 0.0303465\n",
      "epoch 30,step 2232000, training loss 0.0257931\n",
      "epoch 30,step 2247500, training loss 0.0349432\n",
      "epoch 30,step 2263000, training loss 0.0259985\n",
      "epoch 30,step 2278500, training loss 0.0274901\n",
      "epoch 30,step 2294000, training loss 0.037102\n",
      "epoch 30,step 2309500, training loss 0.0237803\n",
      "epoch 30,step 2325000, training loss 0.0575907\n",
      "epoch 30,step 2340500, training loss 0.0237851\n",
      "epoch 30,step 2356000, training loss 0.0210817\n",
      "epoch 30,step 2371500, training loss 0.0356671\n",
      "epoch 30,step 2387000, training loss 0.0181482\n",
      "epoch 30,step 2402500, training loss 0.0294108\n",
      "epoch 30,step 2418000, training loss 0.0216994\n",
      "epoch 30,step 2433500, training loss 0.0282265\n",
      "epoch 30,step 2449000, training loss 0.0285974\n",
      "epoch 30,step 2464500, training loss 0.0298683\n",
      "epoch 30,step 2480000, training loss 0.025025\n",
      "epoch 30,step 2495500, training loss 0.0306059\n",
      "epoch 30,step 2511000, training loss 0.024461\n",
      "epoch 30,step 2526500, training loss 0.0335594\n",
      "epoch 30,step 2542000, training loss 0.0323527\n",
      "epoch 30,step 2557500, training loss 0.0448712\n",
      "epoch 30,step 2573000, training loss 0.0349221\n",
      "epoch 30,step 2588500, training loss 0.0302605\n",
      "epoch 30,step 2604000, training loss 0.0259071\n",
      "epoch 30,step 2619500, training loss 0.035641\n",
      "epoch 30,step 2635000, training loss 0.0336021\n",
      "epoch 30,step 2650500, training loss 0.0380769\n",
      "epoch 30,step 2666000, training loss 0.0417961\n",
      "epoch 30,step 2681500, training loss 0.0275439\n",
      "epoch 30,step 2697000, training loss 0.0350027\n",
      "epoch 30,step 2712500, training loss 0.0263892\n",
      "epoch 30,step 2728000, training loss 0.0264964\n",
      "epoch 30,step 2743500, training loss 0.0286077\n",
      "epoch 30,step 2759000, training loss 0.0239797\n",
      "epoch 30,step 2774500, training loss 0.0376169\n",
      "epoch 30,step 2790000, training loss 0.0617303\n",
      "epoch 30,step 2805500, training loss 0.027132\n",
      "epoch 30,step 2821000, training loss 0.026804\n",
      "epoch 30,step 2836500, training loss 0.03453\n",
      "epoch 30,step 2852000, training loss 0.0257425\n",
      "epoch 30,step 2867500, training loss 0.0223233\n",
      "epoch 30,step 2883000, training loss 0.0217904\n",
      "epoch 30,step 2898500, training loss 0.0372636\n",
      "epoch 30,step 2914000, training loss 0.023149\n",
      "epoch 30,step 2929500, training loss 0.0286079\n",
      "epoch 30,step 2945000, training loss 0.0203317\n",
      "epoch 30,step 2960500, training loss 0.024844\n",
      "epoch 30,step 2976000, training loss 0.0301671\n",
      "epoch 30,step 2991500, training loss 0.0209452\n",
      "epoch 30,step 3007000, training loss 0.0285369\n",
      "epoch 30,step 3022500, training loss 0.0323933\n",
      "epoch 30,step 3038000, training loss 0.0266688\n",
      "epoch 30,step 3053500, training loss 0.0214592\n",
      "epoch 30,step 3069000, training loss 0.0220155\n",
      "epoch 30,step 3084500, training loss 0.0225462\n",
      "epoch 30,training loss 0.0353104 ,test loss 0.0397883\n",
      "epoch 31,step 16000, training loss 0.0323942\n",
      "epoch 31,step 32000, training loss 0.0412945\n",
      "epoch 31,step 48000, training loss 0.036167\n",
      "epoch 31,step 64000, training loss 0.0210663\n",
      "epoch 31,step 80000, training loss 0.0232882\n",
      "epoch 31,step 96000, training loss 0.0221452\n",
      "epoch 31,step 112000, training loss 0.0265826\n",
      "epoch 31,step 128000, training loss 0.0220651\n",
      "epoch 31,step 144000, training loss 0.0262137\n",
      "epoch 31,step 160000, training loss 0.023851\n",
      "epoch 31,step 176000, training loss 0.0240171\n",
      "epoch 31,step 192000, training loss 0.033214\n",
      "epoch 31,step 208000, training loss 0.0525061\n",
      "epoch 31,step 224000, training loss 0.0260027\n",
      "epoch 31,step 240000, training loss 0.0407899\n",
      "epoch 31,step 256000, training loss 0.0272588\n",
      "epoch 31,step 272000, training loss 0.0256783\n",
      "epoch 31,step 288000, training loss 0.023001\n",
      "epoch 31,step 304000, training loss 0.0205825\n",
      "epoch 31,step 320000, training loss 0.0315399\n",
      "epoch 31,step 336000, training loss 0.0273624\n",
      "epoch 31,step 352000, training loss 0.0316934\n",
      "epoch 31,step 368000, training loss 0.042003\n",
      "epoch 31,step 384000, training loss 0.0345095\n",
      "epoch 31,step 400000, training loss 0.0332583\n",
      "epoch 31,step 416000, training loss 0.0177516\n",
      "epoch 31,step 432000, training loss 0.0439301\n",
      "epoch 31,step 448000, training loss 0.0285033\n",
      "epoch 31,step 464000, training loss 0.034395\n",
      "epoch 31,step 480000, training loss 0.0272825\n",
      "epoch 31,step 496000, training loss 0.0433668\n",
      "epoch 31,step 512000, training loss 0.0256562\n",
      "epoch 31,step 528000, training loss 0.0281131\n",
      "epoch 31,step 544000, training loss 0.0282706\n",
      "epoch 31,step 560000, training loss 0.0370326\n",
      "epoch 31,step 576000, training loss 0.0284179\n",
      "epoch 31,step 592000, training loss 0.0319976\n",
      "epoch 31,step 608000, training loss 0.0300643\n",
      "epoch 31,step 624000, training loss 0.0282275\n",
      "epoch 31,step 640000, training loss 0.0235806\n",
      "epoch 31,step 656000, training loss 0.0310263\n",
      "epoch 31,step 672000, training loss 0.0246428\n",
      "epoch 31,step 688000, training loss 0.0266618\n",
      "epoch 31,step 704000, training loss 0.0308428\n",
      "epoch 31,step 720000, training loss 0.0244632\n",
      "epoch 31,step 736000, training loss 0.0319023\n",
      "epoch 31,step 752000, training loss 0.0202308\n",
      "epoch 31,step 768000, training loss 0.0262446\n",
      "epoch 31,step 784000, training loss 0.0283223\n",
      "epoch 31,step 800000, training loss 0.025223\n",
      "epoch 31,step 816000, training loss 0.0288591\n",
      "epoch 31,step 832000, training loss 0.0317499\n",
      "epoch 31,step 848000, training loss 0.0218137\n",
      "epoch 31,step 864000, training loss 0.0272054\n",
      "epoch 31,step 880000, training loss 0.0464818\n",
      "epoch 31,step 896000, training loss 0.0242317\n",
      "epoch 31,step 912000, training loss 0.0211731\n",
      "epoch 31,step 928000, training loss 0.0289949\n",
      "epoch 31,step 944000, training loss 0.0358274\n",
      "epoch 31,step 960000, training loss 0.027408\n",
      "epoch 31,step 976000, training loss 0.0311827\n",
      "epoch 31,step 992000, training loss 0.0292536\n",
      "epoch 31,step 1008000, training loss 0.026774\n",
      "epoch 31,step 1024000, training loss 0.0370507\n",
      "epoch 31,step 1040000, training loss 0.0412732\n",
      "epoch 31,step 1056000, training loss 0.0259409\n",
      "epoch 31,step 1072000, training loss 0.0278258\n",
      "epoch 31,step 1088000, training loss 0.0380327\n",
      "epoch 31,step 1104000, training loss 0.0211549\n",
      "epoch 31,step 1120000, training loss 0.0273847\n",
      "epoch 31,step 1136000, training loss 0.0348447\n",
      "epoch 31,step 1152000, training loss 0.0197182\n",
      "epoch 31,step 1168000, training loss 0.0425556\n",
      "epoch 31,step 1184000, training loss 0.0386734\n",
      "epoch 31,step 1200000, training loss 0.0337173\n",
      "epoch 31,step 1216000, training loss 0.0281052\n",
      "epoch 31,step 1232000, training loss 0.0280542\n",
      "epoch 31,step 1248000, training loss 0.0254219\n",
      "epoch 31,step 1264000, training loss 0.0321702\n",
      "epoch 31,step 1280000, training loss 0.0197898\n",
      "epoch 31,step 1296000, training loss 0.0230413\n",
      "epoch 31,step 1312000, training loss 0.0256436\n",
      "epoch 31,step 1328000, training loss 0.0257942\n",
      "epoch 31,step 1344000, training loss 0.0245199\n",
      "epoch 31,step 1360000, training loss 0.0363536\n",
      "epoch 31,step 1376000, training loss 0.0278592\n",
      "epoch 31,step 1392000, training loss 0.0353761\n",
      "epoch 31,step 1408000, training loss 0.0277185\n",
      "epoch 31,step 1424000, training loss 0.019516\n",
      "epoch 31,step 1440000, training loss 0.0270691\n",
      "epoch 31,step 1456000, training loss 0.0239698\n",
      "epoch 31,step 1472000, training loss 0.0260258\n",
      "epoch 31,step 1488000, training loss 0.0385836\n",
      "epoch 31,step 1504000, training loss 0.0293767\n",
      "epoch 31,step 1520000, training loss 0.0230145\n",
      "epoch 31,step 1536000, training loss 0.0275026\n",
      "epoch 31,step 1552000, training loss 0.0264457\n",
      "epoch 31,step 1568000, training loss 0.0314695\n",
      "epoch 31,step 1584000, training loss 0.033984\n",
      "epoch 31,step 1600000, training loss 0.0266426\n",
      "epoch 31,step 1616000, training loss 0.0272704\n",
      "epoch 31,step 1632000, training loss 0.0235985\n",
      "epoch 31,step 1648000, training loss 0.0331256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31,step 1664000, training loss 0.0206235\n",
      "epoch 31,step 1680000, training loss 0.0326483\n",
      "epoch 31,step 1696000, training loss 0.0381243\n",
      "epoch 31,step 1712000, training loss 0.0369574\n",
      "epoch 31,step 1728000, training loss 0.0282038\n",
      "epoch 31,step 1744000, training loss 0.0387101\n",
      "epoch 31,step 1760000, training loss 0.0471984\n",
      "epoch 31,step 1776000, training loss 0.0340679\n",
      "epoch 31,step 1792000, training loss 0.0377428\n",
      "epoch 31,step 1808000, training loss 0.028388\n",
      "epoch 31,step 1824000, training loss 0.0248348\n",
      "epoch 31,step 1840000, training loss 0.0477718\n",
      "epoch 31,step 1856000, training loss 0.0219964\n",
      "epoch 31,step 1872000, training loss 0.0263421\n",
      "epoch 31,step 1888000, training loss 0.0226789\n",
      "epoch 31,step 1904000, training loss 0.0299626\n",
      "epoch 31,step 1920000, training loss 0.0324658\n",
      "epoch 31,step 1936000, training loss 0.0217019\n",
      "epoch 31,step 1952000, training loss 0.0216306\n",
      "epoch 31,step 1968000, training loss 0.0239274\n",
      "epoch 31,step 1984000, training loss 0.0346104\n",
      "epoch 31,step 2000000, training loss 0.0345901\n",
      "epoch 31,step 2016000, training loss 0.0298163\n",
      "epoch 31,step 2032000, training loss 0.0236181\n",
      "epoch 31,step 2048000, training loss 0.030552\n",
      "epoch 31,step 2064000, training loss 0.0351478\n",
      "epoch 31,step 2080000, training loss 0.031735\n",
      "epoch 31,step 2096000, training loss 0.0265266\n",
      "epoch 31,step 2112000, training loss 0.0248126\n",
      "epoch 31,step 2128000, training loss 0.0237241\n",
      "epoch 31,step 2144000, training loss 0.0300617\n",
      "epoch 31,step 2160000, training loss 0.0220292\n",
      "epoch 31,step 2176000, training loss 0.0297377\n",
      "epoch 31,step 2192000, training loss 0.026698\n",
      "epoch 31,step 2208000, training loss 0.029547\n",
      "epoch 31,step 2224000, training loss 0.0307993\n",
      "epoch 31,step 2240000, training loss 0.0254969\n",
      "epoch 31,step 2256000, training loss 0.0267489\n",
      "epoch 31,step 2272000, training loss 0.0255132\n",
      "epoch 31,step 2288000, training loss 0.030589\n",
      "epoch 31,step 2304000, training loss 0.0260022\n",
      "epoch 31,step 2320000, training loss 0.0338562\n",
      "epoch 31,step 2336000, training loss 0.0249406\n",
      "epoch 31,step 2352000, training loss 0.0284215\n",
      "epoch 31,step 2368000, training loss 0.0369966\n",
      "epoch 31,step 2384000, training loss 0.0231336\n",
      "epoch 31,step 2400000, training loss 0.0564114\n",
      "epoch 31,step 2416000, training loss 0.0242606\n",
      "epoch 31,step 2432000, training loss 0.0211764\n",
      "epoch 31,step 2448000, training loss 0.0350249\n",
      "epoch 31,step 2464000, training loss 0.0180465\n",
      "epoch 31,step 2480000, training loss 0.0293666\n",
      "epoch 31,step 2496000, training loss 0.0214395\n",
      "epoch 31,step 2512000, training loss 0.0272575\n",
      "epoch 31,step 2528000, training loss 0.0280689\n",
      "epoch 31,step 2544000, training loss 0.028964\n",
      "epoch 31,step 2560000, training loss 0.0246948\n",
      "epoch 31,step 2576000, training loss 0.0291009\n",
      "epoch 31,step 2592000, training loss 0.024954\n",
      "epoch 31,step 2608000, training loss 0.0340582\n",
      "epoch 31,step 2624000, training loss 0.0331531\n",
      "epoch 31,step 2640000, training loss 0.0434542\n",
      "epoch 31,step 2656000, training loss 0.0337658\n",
      "epoch 31,step 2672000, training loss 0.028339\n",
      "epoch 31,step 2688000, training loss 0.0258372\n",
      "epoch 31,step 2704000, training loss 0.0345942\n",
      "epoch 31,step 2720000, training loss 0.0323727\n",
      "epoch 31,step 2736000, training loss 0.0387695\n",
      "epoch 31,step 2752000, training loss 0.0412787\n",
      "epoch 31,step 2768000, training loss 0.0273273\n",
      "epoch 31,step 2784000, training loss 0.035\n",
      "epoch 31,step 2800000, training loss 0.0259429\n",
      "epoch 31,step 2816000, training loss 0.026137\n",
      "epoch 31,step 2832000, training loss 0.0281021\n",
      "epoch 31,step 2848000, training loss 0.0238\n",
      "epoch 31,step 2864000, training loss 0.0372819\n",
      "epoch 31,step 2880000, training loss 0.0605601\n",
      "epoch 31,step 2896000, training loss 0.0274237\n",
      "epoch 31,step 2912000, training loss 0.0263281\n",
      "epoch 31,step 2928000, training loss 0.034192\n",
      "epoch 31,step 2944000, training loss 0.026134\n",
      "epoch 31,step 2960000, training loss 0.0218165\n",
      "epoch 31,step 2976000, training loss 0.0213861\n",
      "epoch 31,step 2992000, training loss 0.0374548\n",
      "epoch 31,step 3008000, training loss 0.0235059\n",
      "epoch 31,step 3024000, training loss 0.0284125\n",
      "epoch 31,step 3040000, training loss 0.0202489\n",
      "epoch 31,step 3056000, training loss 0.023538\n",
      "epoch 31,step 3072000, training loss 0.0293899\n",
      "epoch 31,step 3088000, training loss 0.0207229\n",
      "epoch 31,step 3104000, training loss 0.0280775\n",
      "epoch 31,step 3120000, training loss 0.0322228\n",
      "epoch 31,step 3136000, training loss 0.0263948\n",
      "epoch 31,step 3152000, training loss 0.0224342\n",
      "epoch 31,step 3168000, training loss 0.0224277\n",
      "epoch 31,step 3184000, training loss 0.0227517\n",
      "epoch 31,training loss 0.035284 ,test loss 0.0392438\n",
      "epoch 32,step 16500, training loss 0.0332903\n",
      "epoch 32,step 33000, training loss 0.0405306\n",
      "epoch 32,step 49500, training loss 0.0334449\n",
      "epoch 32,step 66000, training loss 0.0203726\n",
      "epoch 32,step 82500, training loss 0.0229287\n",
      "epoch 32,step 99000, training loss 0.0218937\n",
      "epoch 32,step 115500, training loss 0.0259295\n",
      "epoch 32,step 132000, training loss 0.022068\n",
      "epoch 32,step 148500, training loss 0.0252998\n",
      "epoch 32,step 165000, training loss 0.0235164\n",
      "epoch 32,step 181500, training loss 0.0230656\n",
      "epoch 32,step 198000, training loss 0.0330562\n",
      "epoch 32,step 214500, training loss 0.0510597\n",
      "epoch 32,step 231000, training loss 0.0260931\n",
      "epoch 32,step 247500, training loss 0.0397625\n",
      "epoch 32,step 264000, training loss 0.0273138\n",
      "epoch 32,step 280500, training loss 0.0245582\n",
      "epoch 32,step 297000, training loss 0.0226528\n",
      "epoch 32,step 313500, training loss 0.0195056\n",
      "epoch 32,step 330000, training loss 0.0310629\n",
      "epoch 32,step 346500, training loss 0.0256966\n",
      "epoch 32,step 363000, training loss 0.0305993\n",
      "epoch 32,step 379500, training loss 0.0422606\n",
      "epoch 32,step 396000, training loss 0.0343775\n",
      "epoch 32,step 412500, training loss 0.0316871\n",
      "epoch 32,step 429000, training loss 0.0176837\n",
      "epoch 32,step 445500, training loss 0.0432717\n",
      "epoch 32,step 462000, training loss 0.0285359\n",
      "epoch 32,step 478500, training loss 0.0340765\n",
      "epoch 32,step 495000, training loss 0.0264306\n",
      "epoch 32,step 511500, training loss 0.0404909\n",
      "epoch 32,step 528000, training loss 0.0250561\n",
      "epoch 32,step 544500, training loss 0.0270259\n",
      "epoch 32,step 561000, training loss 0.0277278\n",
      "epoch 32,step 577500, training loss 0.0353299\n",
      "epoch 32,step 594000, training loss 0.0277929\n",
      "epoch 32,step 610500, training loss 0.0308136\n",
      "epoch 32,step 627000, training loss 0.0295495\n",
      "epoch 32,step 643500, training loss 0.0278517\n",
      "epoch 32,step 660000, training loss 0.0239321\n",
      "epoch 32,step 676500, training loss 0.0317354\n",
      "epoch 32,step 693000, training loss 0.0245882\n",
      "epoch 32,step 709500, training loss 0.026943\n",
      "epoch 32,step 726000, training loss 0.0300926\n",
      "epoch 32,step 742500, training loss 0.023908\n",
      "epoch 32,step 759000, training loss 0.0318378\n",
      "epoch 32,step 775500, training loss 0.0194344\n",
      "epoch 32,step 792000, training loss 0.0255391\n",
      "epoch 32,step 808500, training loss 0.0283443\n",
      "epoch 32,step 825000, training loss 0.0249511\n",
      "epoch 32,step 841500, training loss 0.0291264\n",
      "epoch 32,step 858000, training loss 0.031663\n",
      "epoch 32,step 874500, training loss 0.0217997\n",
      "epoch 32,step 891000, training loss 0.0269936\n",
      "epoch 32,step 907500, training loss 0.0457118\n",
      "epoch 32,step 924000, training loss 0.0236696\n",
      "epoch 32,step 940500, training loss 0.0212641\n",
      "epoch 32,step 957000, training loss 0.0277929\n",
      "epoch 32,step 973500, training loss 0.0358607\n",
      "epoch 32,step 990000, training loss 0.0270493\n",
      "epoch 32,step 1006500, training loss 0.0308528\n",
      "epoch 32,step 1023000, training loss 0.0285095\n",
      "epoch 32,step 1039500, training loss 0.0262213\n",
      "epoch 32,step 1056000, training loss 0.0367222\n",
      "epoch 32,step 1072500, training loss 0.0403409\n",
      "epoch 32,step 1089000, training loss 0.0256525\n",
      "epoch 32,step 1105500, training loss 0.0274158\n",
      "epoch 32,step 1122000, training loss 0.0377228\n",
      "epoch 32,step 1138500, training loss 0.0214692\n",
      "epoch 32,step 1155000, training loss 0.0269845\n",
      "epoch 32,step 1171500, training loss 0.0343346\n",
      "epoch 32,step 1188000, training loss 0.0201052\n",
      "epoch 32,step 1204500, training loss 0.0425993\n",
      "epoch 32,step 1221000, training loss 0.0385329\n",
      "epoch 32,step 1237500, training loss 0.033802\n",
      "epoch 32,step 1254000, training loss 0.0273071\n",
      "epoch 32,step 1270500, training loss 0.0278669\n",
      "epoch 32,step 1287000, training loss 0.0248279\n",
      "epoch 32,step 1303500, training loss 0.0313055\n",
      "epoch 32,step 1320000, training loss 0.0196159\n",
      "epoch 32,step 1336500, training loss 0.0235949\n",
      "epoch 32,step 1353000, training loss 0.0251442\n",
      "epoch 32,step 1369500, training loss 0.0257309\n",
      "epoch 32,step 1386000, training loss 0.0239819\n",
      "epoch 32,step 1402500, training loss 0.0364449\n",
      "epoch 32,step 1419000, training loss 0.0282394\n",
      "epoch 32,step 1435500, training loss 0.0350053\n",
      "epoch 32,step 1452000, training loss 0.027614\n",
      "epoch 32,step 1468500, training loss 0.0206145\n",
      "epoch 32,step 1485000, training loss 0.0281423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32,step 1501500, training loss 0.0249418\n",
      "epoch 32,step 1518000, training loss 0.0260055\n",
      "epoch 32,step 1534500, training loss 0.0388702\n",
      "epoch 32,step 1551000, training loss 0.0283276\n",
      "epoch 32,step 1567500, training loss 0.0229601\n",
      "epoch 32,step 1584000, training loss 0.0276092\n",
      "epoch 32,step 1600500, training loss 0.025617\n",
      "epoch 32,step 1617000, training loss 0.0308557\n",
      "epoch 32,step 1633500, training loss 0.0350021\n",
      "epoch 32,step 1650000, training loss 0.0270402\n",
      "epoch 32,step 1666500, training loss 0.0279582\n",
      "epoch 32,step 1683000, training loss 0.0238936\n",
      "epoch 32,step 1699500, training loss 0.0317359\n",
      "epoch 32,step 1716000, training loss 0.0204517\n",
      "epoch 32,step 1732500, training loss 0.0332504\n",
      "epoch 32,step 1749000, training loss 0.0378311\n",
      "epoch 32,step 1765500, training loss 0.0352991\n",
      "epoch 32,step 1782000, training loss 0.0269026\n",
      "epoch 32,step 1798500, training loss 0.0401968\n",
      "epoch 32,step 1815000, training loss 0.0465202\n",
      "epoch 32,step 1831500, training loss 0.0333849\n",
      "epoch 32,step 1848000, training loss 0.0376491\n",
      "epoch 32,step 1864500, training loss 0.0292084\n",
      "epoch 32,step 1881000, training loss 0.0244549\n",
      "epoch 32,step 1897500, training loss 0.0477946\n",
      "epoch 32,step 1914000, training loss 0.0223005\n",
      "epoch 32,step 1930500, training loss 0.0257249\n",
      "epoch 32,step 1947000, training loss 0.0220891\n",
      "epoch 32,step 1963500, training loss 0.0290805\n",
      "epoch 32,step 1980000, training loss 0.0338679\n",
      "epoch 32,step 1996500, training loss 0.0209039\n",
      "epoch 32,step 2013000, training loss 0.0219838\n",
      "epoch 32,step 2029500, training loss 0.0236845\n",
      "epoch 32,step 2046000, training loss 0.0334426\n",
      "epoch 32,step 2062500, training loss 0.0338809\n",
      "epoch 32,step 2079000, training loss 0.0351853\n",
      "epoch 32,step 2095500, training loss 0.0241714\n",
      "epoch 32,step 2112000, training loss 0.031104\n",
      "epoch 32,step 2128500, training loss 0.0355541\n",
      "epoch 32,step 2145000, training loss 0.0319119\n",
      "epoch 32,step 2161500, training loss 0.0282324\n",
      "epoch 32,step 2178000, training loss 0.0260252\n",
      "epoch 32,step 2194500, training loss 0.0254039\n",
      "epoch 32,step 2211000, training loss 0.0306188\n",
      "epoch 32,step 2227500, training loss 0.0226603\n",
      "epoch 32,step 2244000, training loss 0.030659\n",
      "epoch 32,step 2260500, training loss 0.0269654\n",
      "epoch 32,step 2277000, training loss 0.0300145\n",
      "epoch 32,step 2293500, training loss 0.030306\n",
      "epoch 32,step 2310000, training loss 0.0250346\n",
      "epoch 32,step 2326500, training loss 0.0255349\n",
      "epoch 32,step 2343000, training loss 0.0246099\n",
      "epoch 32,step 2359500, training loss 0.028503\n",
      "epoch 32,step 2376000, training loss 0.0253958\n",
      "epoch 32,step 2392500, training loss 0.0342748\n",
      "epoch 32,step 2409000, training loss 0.0249633\n",
      "epoch 32,step 2425500, training loss 0.0279117\n",
      "epoch 32,step 2442000, training loss 0.0365709\n",
      "epoch 32,step 2458500, training loss 0.0233309\n",
      "epoch 32,step 2475000, training loss 0.0567263\n",
      "epoch 32,step 2491500, training loss 0.0238338\n",
      "epoch 32,step 2508000, training loss 0.0210584\n",
      "epoch 32,step 2524500, training loss 0.0350455\n",
      "epoch 32,step 2541000, training loss 0.0181775\n",
      "epoch 32,step 2557500, training loss 0.0285737\n",
      "epoch 32,step 2574000, training loss 0.0205641\n",
      "epoch 32,step 2590500, training loss 0.027063\n",
      "epoch 32,step 2607000, training loss 0.0279116\n",
      "epoch 32,step 2623500, training loss 0.0294958\n",
      "epoch 32,step 2640000, training loss 0.0248828\n",
      "epoch 32,step 2656500, training loss 0.0298987\n",
      "epoch 32,step 2673000, training loss 0.0243461\n",
      "epoch 32,step 2689500, training loss 0.0331981\n",
      "epoch 32,step 2706000, training loss 0.0328597\n",
      "epoch 32,step 2722500, training loss 0.0426864\n",
      "epoch 32,step 2739000, training loss 0.0336943\n",
      "epoch 32,step 2755500, training loss 0.0297321\n",
      "epoch 32,step 2772000, training loss 0.0255845\n",
      "epoch 32,step 2788500, training loss 0.0345021\n",
      "epoch 32,step 2805000, training loss 0.0320298\n",
      "epoch 32,step 2821500, training loss 0.0377181\n",
      "epoch 32,step 2838000, training loss 0.0413347\n",
      "epoch 32,step 2854500, training loss 0.0285712\n",
      "epoch 32,step 2871000, training loss 0.0355964\n",
      "epoch 32,step 2887500, training loss 0.0260255\n",
      "epoch 32,step 2904000, training loss 0.0258109\n",
      "epoch 32,step 2920500, training loss 0.0294018\n",
      "epoch 32,step 2937000, training loss 0.0242558\n",
      "epoch 32,step 2953500, training loss 0.0368697\n",
      "epoch 32,step 2970000, training loss 0.0603719\n",
      "epoch 32,step 2986500, training loss 0.0269563\n",
      "epoch 32,step 3003000, training loss 0.0260511\n",
      "epoch 32,step 3019500, training loss 0.0342936\n",
      "epoch 32,step 3036000, training loss 0.0251461\n",
      "epoch 32,step 3052500, training loss 0.021752\n",
      "epoch 32,step 3069000, training loss 0.021618\n",
      "epoch 32,step 3085500, training loss 0.0375724\n",
      "epoch 32,step 3102000, training loss 0.023265\n",
      "epoch 32,step 3118500, training loss 0.0284494\n",
      "epoch 32,step 3135000, training loss 0.0190069\n",
      "epoch 32,step 3151500, training loss 0.023591\n",
      "epoch 32,step 3168000, training loss 0.0291029\n",
      "epoch 32,step 3184500, training loss 0.0207575\n",
      "epoch 32,step 3201000, training loss 0.0277217\n",
      "epoch 32,step 3217500, training loss 0.0319058\n",
      "epoch 32,step 3234000, training loss 0.0261214\n",
      "epoch 32,step 3250500, training loss 0.0209706\n",
      "epoch 32,step 3267000, training loss 0.0213865\n",
      "epoch 32,step 3283500, training loss 0.0222578\n",
      "epoch 32,training loss 0.0354103 ,test loss 0.0389321\n",
      "epoch 33,step 17000, training loss 0.0318179\n",
      "epoch 33,step 34000, training loss 0.0399679\n",
      "epoch 33,step 51000, training loss 0.033387\n",
      "epoch 33,step 68000, training loss 0.0200145\n",
      "epoch 33,step 85000, training loss 0.0225681\n",
      "epoch 33,step 102000, training loss 0.0217753\n",
      "epoch 33,step 119000, training loss 0.0263195\n",
      "epoch 33,step 136000, training loss 0.0222115\n",
      "epoch 33,step 153000, training loss 0.0253982\n",
      "epoch 33,step 170000, training loss 0.0233031\n",
      "epoch 33,step 187000, training loss 0.0238427\n",
      "epoch 33,step 204000, training loss 0.0327376\n",
      "epoch 33,step 221000, training loss 0.0507231\n",
      "epoch 33,step 238000, training loss 0.026075\n",
      "epoch 33,step 255000, training loss 0.0402168\n",
      "epoch 33,step 272000, training loss 0.0268084\n",
      "epoch 33,step 289000, training loss 0.0240423\n",
      "epoch 33,step 306000, training loss 0.0219622\n",
      "epoch 33,step 323000, training loss 0.018787\n",
      "epoch 33,step 340000, training loss 0.0305322\n",
      "epoch 33,step 357000, training loss 0.0261956\n",
      "epoch 33,step 374000, training loss 0.0308072\n",
      "epoch 33,step 391000, training loss 0.0415016\n",
      "epoch 33,step 408000, training loss 0.0337362\n",
      "epoch 33,step 425000, training loss 0.0327886\n",
      "epoch 33,step 442000, training loss 0.0182145\n",
      "epoch 33,step 459000, training loss 0.0436562\n",
      "epoch 33,step 476000, training loss 0.027703\n",
      "epoch 33,step 493000, training loss 0.0335474\n",
      "epoch 33,step 510000, training loss 0.0268607\n",
      "epoch 33,step 527000, training loss 0.0399018\n",
      "epoch 33,step 544000, training loss 0.024501\n",
      "epoch 33,step 561000, training loss 0.0270122\n",
      "epoch 33,step 578000, training loss 0.0266607\n",
      "epoch 33,step 595000, training loss 0.0349171\n",
      "epoch 33,step 612000, training loss 0.0271331\n",
      "epoch 33,step 629000, training loss 0.0303256\n",
      "epoch 33,step 646000, training loss 0.0305692\n",
      "epoch 33,step 663000, training loss 0.0271703\n",
      "epoch 33,step 680000, training loss 0.0239776\n",
      "epoch 33,step 697000, training loss 0.0314577\n",
      "epoch 33,step 714000, training loss 0.0245894\n",
      "epoch 33,step 731000, training loss 0.0266447\n",
      "epoch 33,step 748000, training loss 0.0301417\n",
      "epoch 33,step 765000, training loss 0.0242503\n",
      "epoch 33,step 782000, training loss 0.0321776\n",
      "epoch 33,step 799000, training loss 0.0202033\n",
      "epoch 33,step 816000, training loss 0.0250597\n",
      "epoch 33,step 833000, training loss 0.0278769\n",
      "epoch 33,step 850000, training loss 0.0254409\n",
      "epoch 33,step 867000, training loss 0.0284749\n",
      "epoch 33,step 884000, training loss 0.0313718\n",
      "epoch 33,step 901000, training loss 0.0215546\n",
      "epoch 33,step 918000, training loss 0.0266559\n",
      "epoch 33,step 935000, training loss 0.0445428\n",
      "epoch 33,step 952000, training loss 0.0238753\n",
      "epoch 33,step 969000, training loss 0.0202526\n",
      "epoch 33,step 986000, training loss 0.0274273\n",
      "epoch 33,step 1003000, training loss 0.0356246\n",
      "epoch 33,step 1020000, training loss 0.0271551\n",
      "epoch 33,step 1037000, training loss 0.0303591\n",
      "epoch 33,step 1054000, training loss 0.0289055\n",
      "epoch 33,step 1071000, training loss 0.027557\n",
      "epoch 33,step 1088000, training loss 0.0361189\n",
      "epoch 33,step 1105000, training loss 0.0406016\n",
      "epoch 33,step 1122000, training loss 0.025376\n",
      "epoch 33,step 1139000, training loss 0.0268763\n",
      "epoch 33,step 1156000, training loss 0.0365586\n",
      "epoch 33,step 1173000, training loss 0.0207594\n",
      "epoch 33,step 1190000, training loss 0.0266285\n",
      "epoch 33,step 1207000, training loss 0.0338655\n",
      "epoch 33,step 1224000, training loss 0.0191798\n",
      "epoch 33,step 1241000, training loss 0.0420939\n",
      "epoch 33,step 1258000, training loss 0.0368553\n",
      "epoch 33,step 1275000, training loss 0.03216\n",
      "epoch 33,step 1292000, training loss 0.0269911\n",
      "epoch 33,step 1309000, training loss 0.0280683\n",
      "epoch 33,step 1326000, training loss 0.0246124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33,step 1343000, training loss 0.0308808\n",
      "epoch 33,step 1360000, training loss 0.0194879\n",
      "epoch 33,step 1377000, training loss 0.0238012\n",
      "epoch 33,step 1394000, training loss 0.0262556\n",
      "epoch 33,step 1411000, training loss 0.0253101\n",
      "epoch 33,step 1428000, training loss 0.0237334\n",
      "epoch 33,step 1445000, training loss 0.0354417\n",
      "epoch 33,step 1462000, training loss 0.0277881\n",
      "epoch 33,step 1479000, training loss 0.0346608\n",
      "epoch 33,step 1496000, training loss 0.0276263\n",
      "epoch 33,step 1513000, training loss 0.0196449\n",
      "epoch 33,step 1530000, training loss 0.0269419\n",
      "epoch 33,step 1547000, training loss 0.0240653\n",
      "epoch 33,step 1564000, training loss 0.025619\n",
      "epoch 33,step 1581000, training loss 0.0396937\n",
      "epoch 33,step 1598000, training loss 0.0288241\n",
      "epoch 33,step 1615000, training loss 0.0225364\n",
      "epoch 33,step 1632000, training loss 0.0268392\n",
      "epoch 33,step 1649000, training loss 0.0253555\n",
      "epoch 33,step 1666000, training loss 0.0304236\n",
      "epoch 33,step 1683000, training loss 0.0344594\n",
      "epoch 33,step 1700000, training loss 0.0272262\n",
      "epoch 33,step 1717000, training loss 0.0271821\n",
      "epoch 33,step 1734000, training loss 0.0232926\n",
      "epoch 33,step 1751000, training loss 0.0317911\n",
      "epoch 33,step 1768000, training loss 0.0202851\n",
      "epoch 33,step 1785000, training loss 0.0317931\n",
      "epoch 33,step 1802000, training loss 0.0364017\n",
      "epoch 33,step 1819000, training loss 0.0370694\n",
      "epoch 33,step 1836000, training loss 0.0275089\n",
      "epoch 33,step 1853000, training loss 0.0380566\n",
      "epoch 33,step 1870000, training loss 0.0461363\n",
      "epoch 33,step 1887000, training loss 0.03329\n",
      "epoch 33,step 1904000, training loss 0.03557\n",
      "epoch 33,step 1921000, training loss 0.0289117\n",
      "epoch 33,step 1938000, training loss 0.0240311\n",
      "epoch 33,step 1955000, training loss 0.0462975\n",
      "epoch 33,step 1972000, training loss 0.0213931\n",
      "epoch 33,step 1989000, training loss 0.0267779\n",
      "epoch 33,step 2006000, training loss 0.022405\n",
      "epoch 33,step 2023000, training loss 0.0288688\n",
      "epoch 33,step 2040000, training loss 0.0314414\n",
      "epoch 33,step 2057000, training loss 0.0208174\n",
      "epoch 33,step 2074000, training loss 0.0208311\n",
      "epoch 33,step 2091000, training loss 0.0233723\n",
      "epoch 33,step 2108000, training loss 0.0341429\n",
      "epoch 33,step 2125000, training loss 0.0340691\n",
      "epoch 33,step 2142000, training loss 0.0291463\n",
      "epoch 33,step 2159000, training loss 0.0233021\n",
      "epoch 33,step 2176000, training loss 0.0288292\n",
      "epoch 33,step 2193000, training loss 0.0333652\n",
      "epoch 33,step 2210000, training loss 0.0297132\n",
      "epoch 33,step 2227000, training loss 0.0271616\n",
      "epoch 33,step 2244000, training loss 0.0251357\n",
      "epoch 33,step 2261000, training loss 0.0251306\n",
      "epoch 33,step 2278000, training loss 0.0302135\n",
      "epoch 33,step 2295000, training loss 0.0224024\n",
      "epoch 33,step 2312000, training loss 0.0298291\n",
      "epoch 33,step 2329000, training loss 0.0268965\n",
      "epoch 33,step 2346000, training loss 0.0307423\n",
      "epoch 33,step 2363000, training loss 0.0307145\n",
      "epoch 33,step 2380000, training loss 0.0247227\n",
      "epoch 33,step 2397000, training loss 0.0275965\n",
      "epoch 33,step 2414000, training loss 0.0242488\n",
      "epoch 33,step 2431000, training loss 0.0292812\n",
      "epoch 33,step 2448000, training loss 0.0257563\n",
      "epoch 33,step 2465000, training loss 0.0338738\n",
      "epoch 33,step 2482000, training loss 0.0238423\n",
      "epoch 33,step 2499000, training loss 0.0274544\n",
      "epoch 33,step 2516000, training loss 0.035692\n",
      "epoch 33,step 2533000, training loss 0.0234858\n",
      "epoch 33,step 2550000, training loss 0.0546559\n",
      "epoch 33,step 2567000, training loss 0.0238195\n",
      "epoch 33,step 2584000, training loss 0.0211687\n",
      "epoch 33,step 2601000, training loss 0.0345343\n",
      "epoch 33,step 2618000, training loss 0.0180066\n",
      "epoch 33,step 2635000, training loss 0.0285419\n",
      "epoch 33,step 2652000, training loss 0.0208666\n",
      "epoch 33,step 2669000, training loss 0.0265443\n",
      "epoch 33,step 2686000, training loss 0.0271855\n",
      "epoch 33,step 2703000, training loss 0.0293934\n",
      "epoch 33,step 2720000, training loss 0.0244039\n",
      "epoch 33,step 2737000, training loss 0.0300118\n",
      "epoch 33,step 2754000, training loss 0.0247818\n",
      "epoch 33,step 2771000, training loss 0.0329361\n",
      "epoch 33,step 2788000, training loss 0.032333\n",
      "epoch 33,step 2805000, training loss 0.0427234\n",
      "epoch 33,step 2822000, training loss 0.0334837\n",
      "epoch 33,step 2839000, training loss 0.0289486\n",
      "epoch 33,step 2856000, training loss 0.0254741\n",
      "epoch 33,step 2873000, training loss 0.0358362\n",
      "epoch 33,step 2890000, training loss 0.0323689\n",
      "epoch 33,step 2907000, training loss 0.03813\n",
      "epoch 33,step 2924000, training loss 0.0408449\n",
      "epoch 33,step 2941000, training loss 0.0288034\n",
      "epoch 33,step 2958000, training loss 0.0348819\n",
      "epoch 33,step 2975000, training loss 0.0257325\n",
      "epoch 33,step 2992000, training loss 0.0250406\n",
      "epoch 33,step 3009000, training loss 0.0287011\n",
      "epoch 33,step 3026000, training loss 0.0233717\n",
      "epoch 33,step 3043000, training loss 0.034844\n",
      "epoch 33,step 3060000, training loss 0.0591289\n",
      "epoch 33,step 3077000, training loss 0.0277259\n",
      "epoch 33,step 3094000, training loss 0.0263344\n",
      "epoch 33,step 3111000, training loss 0.0342651\n",
      "epoch 33,step 3128000, training loss 0.0252588\n",
      "epoch 33,step 3145000, training loss 0.0215955\n",
      "epoch 33,step 3162000, training loss 0.0211079\n",
      "epoch 33,step 3179000, training loss 0.036756\n",
      "epoch 33,step 3196000, training loss 0.0225104\n",
      "epoch 33,step 3213000, training loss 0.0282729\n",
      "epoch 33,step 3230000, training loss 0.0192094\n",
      "epoch 33,step 3247000, training loss 0.023276\n",
      "epoch 33,step 3264000, training loss 0.0290943\n",
      "epoch 33,step 3281000, training loss 0.0200201\n",
      "epoch 33,step 3298000, training loss 0.0270649\n",
      "epoch 33,step 3315000, training loss 0.0321504\n",
      "epoch 33,step 3332000, training loss 0.0253174\n",
      "epoch 33,step 3349000, training loss 0.0213777\n",
      "epoch 33,step 3366000, training loss 0.0214118\n",
      "epoch 33,step 3383000, training loss 0.0216638\n",
      "epoch 33,training loss 0.0341325 ,test loss 0.0384465\n",
      "epoch 34,step 3500, training loss 0.0949839\n",
      "epoch 34,step 7000, training loss 0.0631448\n",
      "epoch 34,step 10500, training loss 0.0307107\n",
      "epoch 34,step 14000, training loss 0.021437\n",
      "epoch 34,step 17500, training loss 0.0318317\n",
      "epoch 34,step 21000, training loss 0.021388\n",
      "epoch 34,step 24500, training loss 0.0202862\n",
      "epoch 34,step 28000, training loss 0.0211805\n",
      "epoch 34,step 31500, training loss 0.0275525\n",
      "epoch 34,step 35000, training loss 0.0391619\n",
      "epoch 34,step 38500, training loss 0.117037\n",
      "epoch 34,step 42000, training loss 0.0611393\n",
      "epoch 34,step 45500, training loss 0.0400483\n",
      "epoch 34,step 49000, training loss 0.0395628\n",
      "epoch 34,step 52500, training loss 0.0330213\n",
      "epoch 34,step 56000, training loss 0.024223\n",
      "epoch 34,step 59500, training loss 0.0355355\n",
      "epoch 34,step 63000, training loss 0.0274925\n",
      "epoch 34,step 66500, training loss 0.0462785\n",
      "epoch 34,step 70000, training loss 0.019778\n",
      "epoch 34,step 73500, training loss 0.0868236\n",
      "epoch 34,step 77000, training loss 0.0417869\n",
      "epoch 34,step 80500, training loss 0.0375411\n",
      "epoch 34,step 84000, training loss 0.0343789\n",
      "epoch 34,step 87500, training loss 0.0226004\n",
      "epoch 34,step 91000, training loss 0.027453\n",
      "epoch 34,step 94500, training loss 0.0240312\n",
      "epoch 34,step 98000, training loss 0.0222563\n",
      "epoch 34,step 101500, training loss 0.0259225\n",
      "epoch 34,step 105000, training loss 0.0215222\n",
      "epoch 34,step 108500, training loss 0.0709233\n",
      "epoch 34,step 112000, training loss 0.0378483\n",
      "epoch 34,step 115500, training loss 0.0299157\n",
      "epoch 34,step 119000, training loss 0.0270866\n",
      "epoch 34,step 122500, training loss 0.0261843\n",
      "epoch 34,step 126000, training loss 0.0412723\n",
      "epoch 34,step 129500, training loss 0.0246427\n",
      "epoch 34,step 133000, training loss 0.023822\n",
      "epoch 34,step 136500, training loss 0.020227\n",
      "epoch 34,step 140000, training loss 0.0218982\n",
      "epoch 34,step 143500, training loss 0.0722966\n",
      "epoch 34,step 147000, training loss 0.0339281\n",
      "epoch 34,step 150500, training loss 0.0221775\n",
      "epoch 34,step 154000, training loss 0.0359781\n",
      "epoch 34,step 157500, training loss 0.0249662\n",
      "epoch 34,step 161000, training loss 0.0240969\n",
      "epoch 34,step 164500, training loss 0.0320177\n",
      "epoch 34,step 168000, training loss 0.0232854\n",
      "epoch 34,step 171500, training loss 0.0228546\n",
      "epoch 34,step 175000, training loss 0.0225952\n",
      "epoch 34,step 178500, training loss 0.0796277\n",
      "epoch 34,step 182000, training loss 0.0602066\n",
      "epoch 34,step 185500, training loss 0.0311706\n",
      "epoch 34,step 189000, training loss 0.0259269\n",
      "epoch 34,step 192500, training loss 0.02325\n",
      "epoch 34,step 196000, training loss 0.023267\n",
      "epoch 34,step 199500, training loss 0.0209046\n",
      "epoch 34,step 203000, training loss 0.0325759\n",
      "epoch 34,step 206500, training loss 0.0367946\n",
      "epoch 34,step 210000, training loss 0.032185\n",
      "epoch 34,step 213500, training loss 0.0757608\n",
      "epoch 34,step 217000, training loss 0.0524147\n",
      "epoch 34,step 220500, training loss 0.0310199\n",
      "epoch 34,step 224000, training loss 0.0274876\n",
      "epoch 34,step 227500, training loss 0.0501738\n",
      "epoch 34,step 231000, training loss 0.0463459\n",
      "epoch 34,step 234500, training loss 0.0346706\n",
      "epoch 34,step 238000, training loss 0.0289769\n",
      "epoch 34,step 241500, training loss 0.0241032\n",
      "epoch 34,step 245000, training loss 0.025677\n",
      "epoch 34,step 248500, training loss 0.079065\n",
      "epoch 34,step 252000, training loss 0.0349745\n",
      "epoch 34,step 255500, training loss 0.0264213\n",
      "epoch 34,step 259000, training loss 0.0220509\n",
      "epoch 34,step 262500, training loss 0.039319\n",
      "epoch 34,step 266000, training loss 0.0320402\n",
      "epoch 34,step 269500, training loss 0.0311064\n",
      "epoch 34,step 273000, training loss 0.0222134\n",
      "epoch 34,step 276500, training loss 0.0266669\n",
      "epoch 34,step 280000, training loss 0.0265663\n",
      "epoch 34,step 283500, training loss 0.0882649\n",
      "epoch 34,step 287000, training loss 0.0408938\n",
      "epoch 34,step 290500, training loss 0.0652211\n",
      "epoch 34,step 294000, training loss 0.0250267\n",
      "epoch 34,step 297500, training loss 0.0239711\n",
      "epoch 34,step 301000, training loss 0.0238896\n",
      "epoch 34,step 304500, training loss 0.025339\n",
      "epoch 34,step 308000, training loss 0.0240711\n",
      "epoch 34,step 311500, training loss 0.0194828\n",
      "epoch 34,step 315000, training loss 0.0225988\n",
      "epoch 34,step 318500, training loss 0.0774724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34,step 322000, training loss 0.0603721\n",
      "epoch 34,step 325500, training loss 0.0304711\n",
      "epoch 34,step 329000, training loss 0.0269849\n",
      "epoch 34,step 332500, training loss 0.0187738\n",
      "epoch 34,step 336000, training loss 0.0246321\n",
      "epoch 34,step 339500, training loss 0.0229895\n",
      "epoch 34,step 343000, training loss 0.0246375\n",
      "epoch 34,step 346500, training loss 0.026176\n",
      "epoch 34,step 350000, training loss 0.0301667\n",
      "epoch 34,step 353500, training loss 0.0921849\n",
      "epoch 34,step 357000, training loss 0.0415073\n",
      "epoch 34,step 360500, training loss 0.0289098\n",
      "epoch 34,step 364000, training loss 0.0211151\n",
      "epoch 34,step 367500, training loss 0.025929\n",
      "epoch 34,step 371000, training loss 0.0261173\n",
      "epoch 34,step 374500, training loss 0.0186785\n",
      "epoch 34,step 378000, training loss 0.0358811\n",
      "epoch 34,step 381500, training loss 0.0318937\n",
      "epoch 34,step 385000, training loss 0.0302092\n",
      "epoch 34,step 388500, training loss 0.0822119\n",
      "epoch 34,step 392000, training loss 0.05492\n",
      "epoch 34,step 395500, training loss 0.0302326\n",
      "epoch 34,step 399000, training loss 0.0441289\n",
      "epoch 34,step 402500, training loss 0.0406331\n",
      "epoch 34,step 406000, training loss 0.0523981\n",
      "epoch 34,step 409500, training loss 0.0319218\n",
      "epoch 34,step 413000, training loss 0.0505272\n",
      "epoch 34,step 416500, training loss 0.0553811\n",
      "epoch 34,step 420000, training loss 0.0339172\n",
      "epoch 34,step 423500, training loss 0.0801817\n",
      "epoch 34,step 427000, training loss 0.0508768\n",
      "epoch 34,step 430500, training loss 0.0336926\n",
      "epoch 34,step 434000, training loss 0.0289408\n",
      "epoch 34,step 437500, training loss 0.0311835\n",
      "epoch 34,step 441000, training loss 0.0224198\n",
      "epoch 34,step 444500, training loss 0.0303419\n",
      "epoch 34,step 448000, training loss 0.026479\n",
      "epoch 34,step 451500, training loss 0.0211194\n",
      "epoch 34,step 455000, training loss 0.0180698\n",
      "epoch 34,step 458500, training loss 0.0705754\n",
      "epoch 34,step 462000, training loss 0.0615371\n",
      "epoch 34,step 465500, training loss 0.0291121\n",
      "epoch 34,step 469000, training loss 0.0273631\n",
      "epoch 34,step 472500, training loss 0.0425027\n",
      "epoch 34,step 476000, training loss 0.020664\n",
      "epoch 34,step 479500, training loss 0.0400219\n",
      "epoch 34,step 483000, training loss 0.0239435\n",
      "epoch 34,step 486500, training loss 0.0331097\n",
      "epoch 34,step 490000, training loss 0.0277351\n",
      "epoch 34,step 493500, training loss 0.11502\n",
      "epoch 34,step 497000, training loss 0.0713795\n",
      "epoch 34,step 500500, training loss 0.0429631\n",
      "epoch 34,step 504000, training loss 0.0281068\n",
      "epoch 34,step 507500, training loss 0.031892\n",
      "epoch 34,step 511000, training loss 0.0274286\n",
      "epoch 34,step 514500, training loss 0.0361702\n",
      "epoch 34,step 518000, training loss 0.0262734\n",
      "epoch 34,step 521500, training loss 0.0270518\n",
      "epoch 34,step 525000, training loss 0.0252906\n",
      "epoch 34,step 528500, training loss 0.0717731\n",
      "epoch 34,step 532000, training loss 0.0585805\n",
      "epoch 34,step 535500, training loss 0.0387551\n",
      "epoch 34,step 539000, training loss 0.036889\n",
      "epoch 34,step 542500, training loss 0.0411473\n",
      "epoch 34,step 546000, training loss 0.0407874\n",
      "epoch 34,step 549500, training loss 0.0317911\n",
      "epoch 34,step 553000, training loss 0.0214426\n",
      "epoch 34,step 556500, training loss 0.0299492\n",
      "epoch 34,step 560000, training loss 0.0245968\n",
      "epoch 34,step 563500, training loss 0.0669355\n",
      "epoch 34,step 567000, training loss 0.0448748\n",
      "epoch 34,step 570500, training loss 0.0336099\n",
      "epoch 34,step 574000, training loss 0.0241768\n",
      "epoch 34,step 577500, training loss 0.0286462\n",
      "epoch 34,step 581000, training loss 0.0203923\n",
      "epoch 34,step 584500, training loss 0.0254686\n",
      "epoch 34,step 588000, training loss 0.0318654\n",
      "epoch 34,step 591500, training loss 0.0234797\n",
      "epoch 34,step 595000, training loss 0.0266185\n",
      "epoch 34,step 598500, training loss 0.102639\n",
      "epoch 34,step 602000, training loss 0.0488452\n",
      "epoch 34,step 605500, training loss 0.0367294\n",
      "epoch 34,step 609000, training loss 0.0511011\n",
      "epoch 34,step 612500, training loss 0.0351177\n",
      "epoch 34,step 616000, training loss 0.0275909\n",
      "epoch 34,step 619500, training loss 0.0395693\n",
      "epoch 34,step 623000, training loss 0.029715\n",
      "epoch 34,step 626500, training loss 0.0292742\n",
      "epoch 34,step 630000, training loss 0.0283573\n",
      "epoch 34,step 633500, training loss 0.0724726\n",
      "epoch 34,step 637000, training loss 0.0468554\n",
      "epoch 34,step 640500, training loss 0.0311929\n",
      "epoch 34,step 644000, training loss 0.0370138\n",
      "epoch 34,step 647500, training loss 0.030337\n",
      "epoch 34,step 651000, training loss 0.0618424\n",
      "epoch 34,step 654500, training loss 0.0346754\n",
      "epoch 34,step 658000, training loss 0.0332162\n",
      "epoch 34,step 661500, training loss 0.0272256\n",
      "epoch 34,step 665000, training loss 0.0293274\n",
      "epoch 34,step 668500, training loss 0.0684334\n",
      "epoch 34,step 672000, training loss 0.0369674\n",
      "epoch 34,step 675500, training loss 0.022363\n",
      "epoch 34,step 679000, training loss 0.0234185\n",
      "epoch 34,step 682500, training loss 0.0278559\n",
      "epoch 34,step 686000, training loss 0.0312355\n",
      "epoch 34,step 689500, training loss 0.0212492\n",
      "epoch 34,step 693000, training loss 0.0226082\n",
      "epoch 34,step 696500, training loss 0.0240662\n",
      "epoch 34,step 700000, training loss 0.0232955\n",
      "epoch 34,step 703500, training loss 0.0813044\n",
      "epoch 34,step 707000, training loss 0.0687479\n",
      "epoch 34,step 710500, training loss 0.0340857\n",
      "epoch 34,step 714000, training loss 0.0231342\n",
      "epoch 34,step 717500, training loss 0.0310529\n",
      "epoch 34,step 721000, training loss 0.037608\n",
      "epoch 34,step 724500, training loss 0.0238768\n",
      "epoch 34,step 728000, training loss 0.0213126\n",
      "epoch 34,step 731500, training loss 0.0228556\n",
      "epoch 34,step 735000, training loss 0.0247276\n",
      "epoch 34,step 738500, training loss 0.098703\n",
      "epoch 34,step 742000, training loss 0.0412243\n",
      "epoch 34,step 745500, training loss 0.0303249\n",
      "epoch 34,step 749000, training loss 0.0295976\n",
      "epoch 34,step 752500, training loss 0.0265167\n",
      "epoch 34,step 756000, training loss 0.0288073\n",
      "epoch 34,step 759500, training loss 0.0299571\n",
      "epoch 34,step 763000, training loss 0.033766\n",
      "epoch 34,step 766500, training loss 0.0285786\n",
      "epoch 34,step 770000, training loss 0.0303856\n",
      "epoch 34,step 773500, training loss 0.0996499\n",
      "epoch 34,step 777000, training loss 0.0462638\n",
      "epoch 34,step 780500, training loss 0.0386849\n",
      "epoch 34,step 784000, training loss 0.0305214\n",
      "epoch 34,step 787500, training loss 0.0247197\n",
      "epoch 34,step 791000, training loss 0.0242755\n",
      "epoch 34,step 794500, training loss 0.0257605\n",
      "epoch 34,step 798000, training loss 0.0276661\n",
      "epoch 34,step 801500, training loss 0.0240128\n",
      "epoch 34,step 805000, training loss 0.0314616\n",
      "epoch 34,step 808500, training loss 0.0751491\n",
      "epoch 34,step 812000, training loss 0.0432749\n",
      "epoch 34,step 815500, training loss 0.0227793\n",
      "epoch 34,step 819000, training loss 0.0259834\n",
      "epoch 34,step 822500, training loss 0.0196356\n",
      "epoch 34,step 826000, training loss 0.0202191\n",
      "epoch 34,step 829500, training loss 0.0291836\n",
      "epoch 34,step 833000, training loss 0.0236577\n",
      "epoch 34,step 836500, training loss 0.0244221\n",
      "epoch 34,step 840000, training loss 0.0255675\n",
      "epoch 34,step 843500, training loss 0.0942653\n",
      "epoch 34,step 847000, training loss 0.0380895\n",
      "epoch 34,step 850500, training loss 0.0343342\n",
      "epoch 34,step 854000, training loss 0.0316197\n",
      "epoch 34,step 857500, training loss 0.0282833\n",
      "epoch 34,step 861000, training loss 0.0372714\n",
      "epoch 34,step 864500, training loss 0.0263425\n",
      "epoch 34,step 868000, training loss 0.0241686\n",
      "epoch 34,step 871500, training loss 0.0304814\n",
      "epoch 34,step 875000, training loss 0.0250317\n",
      "epoch 34,step 878500, training loss 0.138671\n",
      "epoch 34,step 882000, training loss 0.0420045\n",
      "epoch 34,step 885500, training loss 0.020789\n",
      "epoch 34,step 889000, training loss 0.0279706\n",
      "epoch 34,step 892500, training loss 0.029093\n",
      "epoch 34,step 896000, training loss 0.017779\n",
      "epoch 34,step 899500, training loss 0.0307806\n",
      "epoch 34,step 903000, training loss 0.0250707\n",
      "epoch 34,step 906500, training loss 0.0403535\n",
      "epoch 34,step 910000, training loss 0.0310416\n",
      "epoch 34,step 913500, training loss 0.107082\n",
      "epoch 34,step 917000, training loss 0.0612403\n",
      "epoch 34,step 920500, training loss 0.052839\n",
      "epoch 34,step 924000, training loss 0.0311803\n",
      "epoch 34,step 927500, training loss 0.020705\n",
      "epoch 34,step 931000, training loss 0.0226987\n",
      "epoch 34,step 934500, training loss 0.0247907\n",
      "epoch 34,step 938000, training loss 0.0236376\n",
      "epoch 34,step 941500, training loss 0.0214141\n",
      "epoch 34,step 945000, training loss 0.0260927\n",
      "epoch 34,step 948500, training loss 0.0891227\n",
      "epoch 34,step 952000, training loss 0.0346369\n",
      "epoch 34,step 955500, training loss 0.0311806\n",
      "epoch 34,step 959000, training loss 0.0395378\n",
      "epoch 34,step 962500, training loss 0.0444072\n",
      "epoch 34,step 966000, training loss 0.0310167\n",
      "epoch 34,step 969500, training loss 0.0225427\n",
      "epoch 34,step 973000, training loss 0.0247375\n",
      "epoch 34,step 976500, training loss 0.026184\n",
      "epoch 34,step 980000, training loss 0.0232505\n",
      "epoch 34,step 983500, training loss 0.0964307\n",
      "epoch 34,step 987000, training loss 0.0481631\n",
      "epoch 34,step 990500, training loss 0.0222845\n",
      "epoch 34,step 994000, training loss 0.0242722\n",
      "epoch 34,step 997500, training loss 0.0201059\n",
      "epoch 34,step 1001000, training loss 0.0233015\n",
      "epoch 34,step 1004500, training loss 0.0301813\n",
      "epoch 34,step 1008000, training loss 0.0210656\n",
      "epoch 34,step 1011500, training loss 0.026037\n",
      "epoch 34,step 1015000, training loss 0.0273317\n",
      "epoch 34,step 1018500, training loss 0.0811513\n",
      "epoch 34,step 1022000, training loss 0.0355025\n",
      "epoch 34,step 1025500, training loss 0.0281215\n",
      "epoch 34,step 1029000, training loss 0.0368838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34,step 1032500, training loss 0.0344242\n",
      "epoch 34,step 1036000, training loss 0.0338772\n",
      "epoch 34,step 1039500, training loss 0.0239126\n",
      "epoch 34,step 1043000, training loss 0.0319977\n",
      "epoch 34,step 1046500, training loss 0.0247752\n",
      "epoch 34,step 1050000, training loss 0.0254425\n",
      "epoch 34,step 1053500, training loss 0.0757454\n",
      "epoch 34,step 1057000, training loss 0.0501175\n",
      "epoch 34,step 1060500, training loss 0.03317\n",
      "epoch 34,step 1064000, training loss 0.032074\n",
      "epoch 34,step 1067500, training loss 0.0302527\n",
      "epoch 34,step 1071000, training loss 0.0309777\n",
      "epoch 34,step 1074500, training loss 0.0281058\n",
      "epoch 34,step 1078000, training loss 0.0425436\n",
      "epoch 34,step 1081500, training loss 0.0224412\n",
      "epoch 34,step 1085000, training loss 0.0277976\n",
      "epoch 34,step 1088500, training loss 0.0875948\n",
      "epoch 34,step 1092000, training loss 0.067623\n",
      "epoch 34,step 1095500, training loss 0.0231263\n",
      "epoch 34,step 1099000, training loss 0.0327714\n",
      "epoch 34,step 1102500, training loss 0.0259553\n",
      "epoch 34,step 1106000, training loss 0.0374594\n",
      "epoch 34,step 1109500, training loss 0.0501682\n",
      "epoch 34,step 1113000, training loss 0.0257082\n",
      "epoch 34,step 1116500, training loss 0.0440987\n",
      "epoch 34,step 1120000, training loss 0.0359183\n",
      "epoch 34,step 1123500, training loss 0.0836593\n",
      "epoch 34,step 1127000, training loss 0.0604589\n",
      "epoch 34,step 1130500, training loss 0.0342042\n",
      "epoch 34,step 1134000, training loss 0.0329384\n",
      "epoch 34,step 1137500, training loss 0.0399394\n",
      "epoch 34,step 1141000, training loss 0.0247743\n",
      "epoch 34,step 1144500, training loss 0.0288564\n",
      "epoch 34,step 1148000, training loss 0.0240064\n",
      "epoch 34,step 1151500, training loss 0.0348056\n",
      "epoch 34,step 1155000, training loss 0.0255663\n",
      "epoch 34,step 1158500, training loss 0.07347\n",
      "epoch 34,step 1162000, training loss 0.068846\n",
      "epoch 34,step 1165500, training loss 0.0266494\n",
      "epoch 34,step 1169000, training loss 0.0429068\n",
      "epoch 34,step 1172500, training loss 0.026235\n",
      "epoch 34,step 1176000, training loss 0.0201393\n",
      "epoch 34,step 1179500, training loss 0.0266227\n",
      "epoch 34,step 1183000, training loss 0.0253923\n",
      "epoch 34,step 1186500, training loss 0.0261411\n",
      "epoch 34,step 1190000, training loss 0.0381343\n",
      "epoch 34,step 1193500, training loss 0.0812407\n",
      "epoch 34,step 1197000, training loss 0.0507998\n",
      "epoch 34,step 1200500, training loss 0.0425064\n",
      "epoch 34,step 1204000, training loss 0.0362776\n",
      "epoch 34,step 1207500, training loss 0.0212559\n",
      "epoch 34,step 1211000, training loss 0.024567\n",
      "epoch 34,step 1214500, training loss 0.0265454\n",
      "epoch 34,step 1218000, training loss 0.0309249\n",
      "epoch 34,step 1221500, training loss 0.0318167\n",
      "epoch 34,step 1225000, training loss 0.0271662\n",
      "epoch 34,step 1228500, training loss 0.0751918\n",
      "epoch 34,step 1232000, training loss 0.0392291\n",
      "epoch 34,step 1235500, training loss 0.0364153\n",
      "epoch 34,step 1239000, training loss 0.0297444\n",
      "epoch 34,step 1242500, training loss 0.0340226\n",
      "epoch 34,step 1246000, training loss 0.0250902\n",
      "epoch 34,step 1249500, training loss 0.0239763\n",
      "epoch 34,step 1253000, training loss 0.0234374\n",
      "epoch 34,step 1256500, training loss 0.0226152\n",
      "epoch 34,step 1260000, training loss 0.0196805\n",
      "epoch 34,step 1263500, training loss 0.0822118\n",
      "epoch 34,step 1267000, training loss 0.0453354\n",
      "epoch 34,step 1270500, training loss 0.0320342\n",
      "epoch 34,step 1274000, training loss 0.0245791\n",
      "epoch 34,step 1277500, training loss 0.0418662\n",
      "epoch 34,step 1281000, training loss 0.034063\n",
      "epoch 34,step 1284500, training loss 0.0304342\n",
      "epoch 34,step 1288000, training loss 0.0339156\n",
      "epoch 34,step 1291500, training loss 0.0282696\n",
      "epoch 34,step 1295000, training loss 0.0377258\n",
      "epoch 34,step 1298500, training loss 0.0871812\n",
      "epoch 34,step 1302000, training loss 0.0474681\n",
      "epoch 34,step 1305500, training loss 0.024994\n",
      "epoch 34,step 1309000, training loss 0.0453431\n",
      "epoch 34,step 1312500, training loss 0.0326734\n",
      "epoch 34,step 1316000, training loss 0.0288115\n",
      "epoch 34,step 1319500, training loss 0.0270464\n",
      "epoch 34,step 1323000, training loss 0.0316648\n",
      "epoch 34,step 1326500, training loss 0.0219785\n",
      "epoch 34,step 1330000, training loss 0.0263298\n",
      "epoch 34,step 1333500, training loss 0.0819043\n",
      "epoch 34,step 1337000, training loss 0.0940807\n",
      "epoch 34,step 1340500, training loss 0.0431552\n",
      "epoch 34,step 1344000, training loss 0.031906\n",
      "epoch 34,step 1347500, training loss 0.0274173\n",
      "epoch 34,step 1351000, training loss 0.030039\n",
      "epoch 34,step 1354500, training loss 0.0280669\n",
      "epoch 34,step 1358000, training loss 0.0264099\n",
      "epoch 34,step 1361500, training loss 0.0322974\n",
      "epoch 34,step 1365000, training loss 0.0241095\n",
      "epoch 34,step 1368500, training loss 0.0733653\n",
      "epoch 34,step 1372000, training loss 0.0519279\n",
      "epoch 34,step 1375500, training loss 0.0227203\n",
      "epoch 34,step 1379000, training loss 0.0279577\n",
      "epoch 34,step 1382500, training loss 0.031043\n",
      "epoch 34,step 1386000, training loss 0.0223429\n",
      "epoch 34,step 1389500, training loss 0.0225242\n",
      "epoch 34,step 1393000, training loss 0.0344215\n",
      "epoch 34,step 1396500, training loss 0.026637\n",
      "epoch 34,step 1400000, training loss 0.0192139\n",
      "epoch 34,step 1403500, training loss 0.0936386\n",
      "epoch 34,step 1407000, training loss 0.0915546\n",
      "epoch 34,step 1410500, training loss 0.0273044\n",
      "epoch 34,step 1414000, training loss 0.0266554\n",
      "epoch 34,step 1417500, training loss 0.0225434\n",
      "epoch 34,step 1421000, training loss 0.0247671\n",
      "epoch 34,step 1424500, training loss 0.0212651\n",
      "epoch 34,step 1428000, training loss 0.0295329\n",
      "epoch 34,step 1431500, training loss 0.0351323\n",
      "epoch 34,step 1435000, training loss 0.0246619\n",
      "epoch 34,step 1438500, training loss 0.0720628\n",
      "epoch 34,step 1442000, training loss 0.0413452\n",
      "epoch 34,step 1445500, training loss 0.0303893\n",
      "epoch 34,step 1449000, training loss 0.0319016\n",
      "epoch 34,step 1452500, training loss 0.0251149\n",
      "epoch 34,step 1456000, training loss 0.0357716\n",
      "epoch 34,step 1459500, training loss 0.0311345\n",
      "epoch 34,step 1463000, training loss 0.0266437\n",
      "epoch 34,step 1466500, training loss 0.0200131\n",
      "epoch 34,step 1470000, training loss 0.0234748\n",
      "epoch 34,step 1473500, training loss 0.0782221\n",
      "epoch 34,step 1477000, training loss 0.0394719\n",
      "epoch 34,step 1480500, training loss 0.0147547\n",
      "epoch 34,step 1484000, training loss 0.0252402\n",
      "epoch 34,step 1487500, training loss 0.0354782\n",
      "epoch 34,step 1491000, training loss 0.031697\n",
      "epoch 34,step 1494500, training loss 0.0328297\n",
      "epoch 34,step 1498000, training loss 0.0379905\n",
      "epoch 34,step 1501500, training loss 0.0236264\n",
      "epoch 34,step 1505000, training loss 0.0270189\n",
      "epoch 34,step 1508500, training loss 0.0792868\n",
      "epoch 34,step 1512000, training loss 0.0609684\n",
      "epoch 34,step 1515500, training loss 0.0347849\n",
      "epoch 34,step 1519000, training loss 0.0275979\n",
      "epoch 34,step 1522500, training loss 0.0350072\n",
      "epoch 34,step 1526000, training loss 0.0251163\n",
      "epoch 34,step 1529500, training loss 0.0301147\n",
      "epoch 34,step 1533000, training loss 0.0297577\n",
      "epoch 34,step 1536500, training loss 0.02747\n",
      "epoch 34,step 1540000, training loss 0.0280922\n",
      "epoch 34,step 1543500, training loss 0.0884514\n",
      "epoch 34,step 1547000, training loss 0.0446412\n",
      "epoch 34,step 1550500, training loss 0.0358221\n",
      "epoch 34,step 1554000, training loss 0.0239304\n",
      "epoch 34,step 1557500, training loss 0.0197337\n",
      "epoch 34,step 1561000, training loss 0.0207396\n",
      "epoch 34,step 1564500, training loss 0.0305153\n",
      "epoch 34,step 1568000, training loss 0.0256915\n",
      "epoch 34,step 1571500, training loss 0.0275921\n",
      "epoch 34,step 1575000, training loss 0.0262732\n",
      "epoch 34,step 1578500, training loss 0.0768088\n",
      "epoch 34,step 1582000, training loss 0.0370095\n",
      "epoch 34,step 1585500, training loss 0.0356785\n",
      "epoch 34,step 1589000, training loss 0.0330424\n",
      "epoch 34,step 1592500, training loss 0.0235726\n",
      "epoch 34,step 1596000, training loss 0.0210416\n",
      "epoch 34,step 1599500, training loss 0.026349\n",
      "epoch 34,step 1603000, training loss 0.0272323\n",
      "epoch 34,step 1606500, training loss 0.019645\n",
      "epoch 34,step 1610000, training loss 0.0253669\n",
      "epoch 34,step 1613500, training loss 0.0913597\n",
      "epoch 34,step 1617000, training loss 0.0560285\n",
      "epoch 34,step 1620500, training loss 0.0369923\n",
      "epoch 34,step 1624000, training loss 0.0339618\n",
      "epoch 34,step 1627500, training loss 0.0365777\n",
      "epoch 34,step 1631000, training loss 0.0263407\n",
      "epoch 34,step 1634500, training loss 0.0437112\n",
      "epoch 34,step 1638000, training loss 0.0320061\n",
      "epoch 34,step 1641500, training loss 0.027044\n",
      "epoch 34,step 1645000, training loss 0.0282422\n",
      "epoch 34,step 1648500, training loss 0.0829937\n",
      "epoch 34,step 1652000, training loss 0.0841733\n",
      "epoch 34,step 1655500, training loss 0.0359505\n",
      "epoch 34,step 1659000, training loss 0.0289233\n",
      "epoch 34,step 1662500, training loss 0.0223296\n",
      "epoch 34,step 1666000, training loss 0.0418715\n",
      "epoch 34,step 1669500, training loss 0.0310226\n",
      "epoch 34,step 1673000, training loss 0.0315816\n",
      "epoch 34,step 1676500, training loss 0.0261165\n",
      "epoch 34,step 1680000, training loss 0.0261859\n",
      "epoch 34,step 1683500, training loss 0.0716852\n",
      "epoch 34,step 1687000, training loss 0.032289\n",
      "epoch 34,step 1690500, training loss 0.0342478\n",
      "epoch 34,step 1694000, training loss 0.0306027\n",
      "epoch 34,step 1697500, training loss 0.0249108\n",
      "epoch 34,step 1701000, training loss 0.029298\n",
      "epoch 34,step 1704500, training loss 0.0246532\n",
      "epoch 34,step 1708000, training loss 0.0255608\n",
      "epoch 34,step 1711500, training loss 0.0261501\n",
      "epoch 34,step 1715000, training loss 0.0297127\n",
      "epoch 34,step 1718500, training loss 0.0898543\n",
      "epoch 34,step 1722000, training loss 0.0389502\n",
      "epoch 34,step 1725500, training loss 0.0262414\n",
      "epoch 34,step 1729000, training loss 0.0430981\n",
      "epoch 34,step 1732500, training loss 0.0342462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34,step 1736000, training loss 0.0249986\n",
      "epoch 34,step 1739500, training loss 0.0274004\n",
      "epoch 34,step 1743000, training loss 0.0289781\n",
      "epoch 34,step 1746500, training loss 0.0489607\n",
      "epoch 34,step 1750000, training loss 0.0270604\n",
      "epoch 34,step 1753500, training loss 0.0938646\n",
      "epoch 34,step 1757000, training loss 0.0527981\n",
      "epoch 34,step 1760500, training loss 0.0334947\n",
      "epoch 34,step 1764000, training loss 0.0345325\n",
      "epoch 34,step 1767500, training loss 0.0274653\n",
      "epoch 34,step 1771000, training loss 0.0259085\n",
      "epoch 34,step 1774500, training loss 0.020198\n",
      "epoch 34,step 1778000, training loss 0.0255574\n",
      "epoch 34,step 1781500, training loss 0.031329\n",
      "epoch 34,step 1785000, training loss 0.0237582\n",
      "epoch 34,step 1788500, training loss 0.0945046\n",
      "epoch 34,step 1792000, training loss 0.0946707\n",
      "epoch 34,step 1795500, training loss 0.0274711\n",
      "epoch 34,step 1799000, training loss 0.0275928\n",
      "epoch 34,step 1802500, training loss 0.0325212\n",
      "epoch 34,step 1806000, training loss 0.0333002\n",
      "epoch 34,step 1809500, training loss 0.0319337\n",
      "epoch 34,step 1813000, training loss 0.0261576\n",
      "epoch 34,step 1816500, training loss 0.0314191\n",
      "epoch 34,step 1820000, training loss 0.0203116\n",
      "epoch 34,step 1823500, training loss 0.0821445\n",
      "epoch 34,step 1827000, training loss 0.0573776\n",
      "epoch 34,step 1830500, training loss 0.0287153\n",
      "epoch 34,step 1834000, training loss 0.0277266\n",
      "epoch 34,step 1837500, training loss 0.0323188\n",
      "epoch 34,step 1841000, training loss 0.0223086\n",
      "epoch 34,step 1844500, training loss 0.0293975\n",
      "epoch 34,step 1848000, training loss 0.0310696\n",
      "epoch 34,step 1851500, training loss 0.0480527\n",
      "epoch 34,step 1855000, training loss 0.0371075\n",
      "epoch 34,step 1858500, training loss 0.110488\n",
      "epoch 34,step 1862000, training loss 0.093304\n",
      "epoch 34,step 1865500, training loss 0.0401535\n",
      "epoch 34,step 1869000, training loss 0.0437294\n",
      "epoch 34,step 1872500, training loss 0.036909\n",
      "epoch 34,step 1876000, training loss 0.0278279\n",
      "epoch 34,step 1879500, training loss 0.0261641\n",
      "epoch 34,step 1883000, training loss 0.0277725\n",
      "epoch 34,step 1886500, training loss 0.0297424\n",
      "epoch 34,step 1890000, training loss 0.0277729\n",
      "epoch 34,step 1893500, training loss 0.0697804\n",
      "epoch 34,step 1897000, training loss 0.0710073\n",
      "epoch 34,step 1900500, training loss 0.0436247\n",
      "epoch 34,step 1904000, training loss 0.0470791\n",
      "epoch 34,step 1907500, training loss 0.0384053\n",
      "epoch 34,step 1911000, training loss 0.0260909\n",
      "epoch 34,step 1914500, training loss 0.0213219\n",
      "epoch 34,step 1918000, training loss 0.0295023\n",
      "epoch 34,step 1921500, training loss 0.0267157\n",
      "epoch 34,step 1925000, training loss 0.046964\n",
      "epoch 34,step 1928500, training loss 0.0784446\n",
      "epoch 34,step 1932000, training loss 0.0446288\n",
      "epoch 34,step 1935500, training loss 0.0316504\n",
      "epoch 34,step 1939000, training loss 0.0372638\n",
      "epoch 34,step 1942500, training loss 0.0333095\n",
      "epoch 34,step 1946000, training loss 0.0310196\n",
      "epoch 34,step 1949500, training loss 0.0268175\n",
      "epoch 34,step 1953000, training loss 0.0388959\n",
      "epoch 34,step 1956500, training loss 0.0275224\n",
      "epoch 34,step 1960000, training loss 0.0354422\n",
      "epoch 34,step 1963500, training loss 0.0869406\n",
      "epoch 34,step 1967000, training loss 0.0716631\n",
      "epoch 34,step 1970500, training loss 0.0455206\n",
      "epoch 34,step 1974000, training loss 0.0345574\n",
      "epoch 34,step 1977500, training loss 0.0273211\n",
      "epoch 34,step 1981000, training loss 0.0257974\n",
      "epoch 34,step 1984500, training loss 0.0286114\n",
      "epoch 34,step 1988000, training loss 0.0280046\n",
      "epoch 34,step 1991500, training loss 0.026361\n",
      "epoch 34,step 1995000, training loss 0.0238266\n",
      "epoch 34,step 1998500, training loss 0.0851987\n",
      "epoch 34,step 2002000, training loss 0.0618669\n",
      "epoch 34,step 2005500, training loss 0.0419535\n",
      "epoch 34,step 2009000, training loss 0.0369762\n",
      "epoch 34,step 2012500, training loss 0.0463853\n",
      "epoch 34,step 2016000, training loss 0.0364952\n",
      "epoch 34,step 2019500, training loss 0.0312239\n",
      "epoch 34,step 2023000, training loss 0.033106\n",
      "epoch 34,step 2026500, training loss 0.0266468\n",
      "epoch 34,step 2030000, training loss 0.020948\n",
      "epoch 34,step 2033500, training loss 0.0834019\n",
      "epoch 34,step 2037000, training loss 0.0471646\n",
      "epoch 34,step 2040500, training loss 0.0262925\n",
      "epoch 34,step 2044000, training loss 0.0295879\n",
      "epoch 34,step 2047500, training loss 0.0268977\n",
      "epoch 34,step 2051000, training loss 0.0301473\n",
      "epoch 34,step 2054500, training loss 0.0250525\n",
      "epoch 34,step 2058000, training loss 0.0353392\n",
      "epoch 34,step 2061500, training loss 0.0310459\n",
      "epoch 34,step 2065000, training loss 0.0218259\n",
      "epoch 34,step 2068500, training loss 0.076561\n",
      "epoch 34,step 2072000, training loss 0.0411777\n",
      "epoch 34,step 2075500, training loss 0.0300339\n",
      "epoch 34,step 2079000, training loss 0.0344717\n",
      "epoch 34,step 2082500, training loss 0.0285882\n",
      "epoch 34,step 2086000, training loss 0.030241\n",
      "epoch 34,step 2089500, training loss 0.0215837\n",
      "epoch 34,step 2093000, training loss 0.025757\n",
      "epoch 34,step 2096500, training loss 0.0265955\n",
      "epoch 34,step 2100000, training loss 0.0329377\n",
      "epoch 34,step 2103500, training loss 0.13718\n",
      "epoch 34,step 2107000, training loss 0.0474663\n",
      "epoch 34,step 2110500, training loss 0.0336211\n",
      "epoch 34,step 2114000, training loss 0.0472304\n",
      "epoch 34,step 2117500, training loss 0.0205571\n",
      "epoch 34,step 2121000, training loss 0.0303327\n",
      "epoch 34,step 2124500, training loss 0.0256233\n",
      "epoch 34,step 2128000, training loss 0.0237114\n",
      "epoch 34,step 2131500, training loss 0.0267879\n",
      "epoch 34,step 2135000, training loss 0.021326\n",
      "epoch 34,step 2138500, training loss 0.105102\n",
      "epoch 34,step 2142000, training loss 0.0565984\n",
      "epoch 34,step 2145500, training loss 0.0313587\n",
      "epoch 34,step 2149000, training loss 0.0227673\n",
      "epoch 34,step 2152500, training loss 0.0235616\n",
      "epoch 34,step 2156000, training loss 0.0253935\n",
      "epoch 34,step 2159500, training loss 0.0225104\n",
      "epoch 34,step 2163000, training loss 0.0214445\n",
      "epoch 34,step 2166500, training loss 0.0224636\n",
      "epoch 34,step 2170000, training loss 0.032991\n",
      "epoch 34,step 2173500, training loss 0.120675\n",
      "epoch 34,step 2177000, training loss 0.114388\n",
      "epoch 34,step 2180500, training loss 0.039347\n",
      "epoch 34,step 2184000, training loss 0.0459471\n",
      "epoch 34,step 2187500, training loss 0.0344891\n",
      "epoch 34,step 2191000, training loss 0.0399488\n",
      "epoch 34,step 2194500, training loss 0.0240409\n",
      "epoch 34,step 2198000, training loss 0.027434\n",
      "epoch 34,step 2201500, training loss 0.0253437\n",
      "epoch 34,step 2205000, training loss 0.0295039\n",
      "epoch 34,step 2208500, training loss 0.0808537\n",
      "epoch 34,step 2212000, training loss 0.0466349\n",
      "epoch 34,step 2215500, training loss 0.0266112\n",
      "epoch 34,step 2219000, training loss 0.031503\n",
      "epoch 34,step 2222500, training loss 0.0230242\n",
      "epoch 34,step 2226000, training loss 0.0204565\n",
      "epoch 34,step 2229500, training loss 0.0223661\n",
      "epoch 34,step 2233000, training loss 0.0327222\n",
      "epoch 34,step 2236500, training loss 0.0245687\n",
      "epoch 34,step 2240000, training loss 0.0290011\n",
      "epoch 34,step 2243500, training loss 0.120187\n",
      "epoch 34,step 2247000, training loss 0.061211\n",
      "epoch 34,step 2250500, training loss 0.0282669\n",
      "epoch 34,step 2254000, training loss 0.0260109\n",
      "epoch 34,step 2257500, training loss 0.035169\n",
      "epoch 34,step 2261000, training loss 0.0338463\n",
      "epoch 34,step 2264500, training loss 0.073385\n",
      "epoch 34,step 2268000, training loss 0.0228714\n",
      "epoch 34,step 2271500, training loss 0.0222994\n",
      "epoch 34,step 2275000, training loss 0.032151\n",
      "epoch 34,step 2278500, training loss 0.0882523\n",
      "epoch 34,step 2282000, training loss 0.0353206\n",
      "epoch 34,step 2285500, training loss 0.0207262\n",
      "epoch 34,step 2289000, training loss 0.026074\n",
      "epoch 34,step 2292500, training loss 0.0277521\n",
      "epoch 34,step 2296000, training loss 0.0279016\n",
      "epoch 34,step 2299500, training loss 0.0219633\n",
      "epoch 34,step 2303000, training loss 0.0273532\n",
      "epoch 34,step 2306500, training loss 0.0425569\n",
      "epoch 34,step 2310000, training loss 0.0256152\n",
      "epoch 34,step 2313500, training loss 0.107681\n",
      "epoch 34,step 2317000, training loss 0.0542987\n",
      "epoch 34,step 2320500, training loss 0.0263894\n",
      "epoch 34,step 2324000, training loss 0.0319165\n",
      "epoch 34,step 2327500, training loss 0.0251646\n",
      "epoch 34,step 2331000, training loss 0.0260345\n",
      "epoch 34,step 2334500, training loss 0.0282514\n",
      "epoch 34,step 2338000, training loss 0.0363706\n",
      "epoch 34,step 2341500, training loss 0.0269924\n",
      "epoch 34,step 2345000, training loss 0.029567\n",
      "epoch 34,step 2348500, training loss 0.102443\n",
      "epoch 34,step 2352000, training loss 0.0868394\n",
      "epoch 34,step 2355500, training loss 0.0323941\n",
      "epoch 34,step 2359000, training loss 0.0389068\n",
      "epoch 34,step 2362500, training loss 0.0218346\n",
      "epoch 34,step 2366000, training loss 0.035567\n",
      "epoch 34,step 2369500, training loss 0.0373835\n",
      "epoch 34,step 2373000, training loss 0.0274144\n",
      "epoch 34,step 2376500, training loss 0.0243521\n",
      "epoch 34,step 2380000, training loss 0.0294968\n",
      "epoch 34,step 2383500, training loss 0.0969538\n",
      "epoch 34,step 2387000, training loss 0.0469717\n",
      "epoch 34,step 2390500, training loss 0.0352017\n",
      "epoch 34,step 2394000, training loss 0.036275\n",
      "epoch 34,step 2397500, training loss 0.0266324\n",
      "epoch 34,step 2401000, training loss 0.0277073\n",
      "epoch 34,step 2404500, training loss 0.0276136\n",
      "epoch 34,step 2408000, training loss 0.0348952\n",
      "epoch 34,step 2411500, training loss 0.0300305\n",
      "epoch 34,step 2415000, training loss 0.0277321\n",
      "epoch 34,step 2418500, training loss 0.0901106\n",
      "epoch 34,step 2422000, training loss 0.0536468\n",
      "epoch 34,step 2425500, training loss 0.0243674\n",
      "epoch 34,step 2429000, training loss 0.0275125\n",
      "epoch 34,step 2432500, training loss 0.0297812\n",
      "epoch 34,step 2436000, training loss 0.0261764\n",
      "epoch 34,step 2439500, training loss 0.0254927\n",
      "epoch 34,step 2443000, training loss 0.0309627\n",
      "epoch 34,step 2446500, training loss 0.0302629\n",
      "epoch 34,step 2450000, training loss 0.024585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34,step 2453500, training loss 0.0818137\n",
      "epoch 34,step 2457000, training loss 0.0617911\n",
      "epoch 34,step 2460500, training loss 0.0302105\n",
      "epoch 34,step 2464000, training loss 0.0255752\n",
      "epoch 34,step 2467500, training loss 0.0260303\n",
      "epoch 34,step 2471000, training loss 0.0251796\n",
      "epoch 34,step 2474500, training loss 0.029476\n",
      "epoch 34,step 2478000, training loss 0.0199058\n",
      "epoch 34,step 2481500, training loss 0.0228899\n",
      "epoch 34,step 2485000, training loss 0.0245304\n",
      "epoch 34,step 2488500, training loss 0.0816245\n",
      "epoch 34,step 2492000, training loss 0.0542964\n",
      "epoch 34,step 2495500, training loss 0.0297626\n",
      "epoch 34,step 2499000, training loss 0.0254163\n",
      "epoch 34,step 2502500, training loss 0.0289377\n",
      "epoch 34,step 2506000, training loss 0.0269533\n",
      "epoch 34,step 2509500, training loss 0.0351224\n",
      "epoch 34,step 2513000, training loss 0.0351358\n",
      "epoch 34,step 2516500, training loss 0.0243204\n",
      "epoch 34,step 2520000, training loss 0.0258493\n",
      "epoch 34,step 2523500, training loss 0.0629045\n",
      "epoch 34,step 2527000, training loss 0.0434056\n",
      "epoch 34,step 2530500, training loss 0.0399634\n",
      "epoch 34,step 2534000, training loss 0.0321584\n",
      "epoch 34,step 2537500, training loss 0.0350426\n",
      "epoch 34,step 2541000, training loss 0.0243627\n",
      "epoch 34,step 2544500, training loss 0.0377008\n",
      "epoch 34,step 2548000, training loss 0.0282812\n",
      "epoch 34,step 2551500, training loss 0.0268286\n",
      "epoch 34,step 2555000, training loss 0.0249167\n",
      "epoch 34,step 2558500, training loss 0.0864877\n",
      "epoch 34,step 2562000, training loss 0.0441007\n",
      "epoch 34,step 2565500, training loss 0.0273964\n",
      "epoch 34,step 2569000, training loss 0.0412232\n",
      "epoch 34,step 2572500, training loss 0.0274644\n",
      "epoch 34,step 2576000, training loss 0.0262944\n",
      "epoch 34,step 2579500, training loss 0.0270202\n",
      "epoch 34,step 2583000, training loss 0.036276\n",
      "epoch 34,step 2586500, training loss 0.0527618\n",
      "epoch 34,step 2590000, training loss 0.0354256\n",
      "epoch 34,step 2593500, training loss 0.0651366\n",
      "epoch 34,step 2597000, training loss 0.0418715\n",
      "epoch 34,step 2600500, training loss 0.0242016\n",
      "epoch 34,step 2604000, training loss 0.0223359\n",
      "epoch 34,step 2607500, training loss 0.0238005\n",
      "epoch 34,step 2611000, training loss 0.0264674\n",
      "epoch 34,step 2614500, training loss 0.0247884\n",
      "epoch 34,step 2618000, training loss 0.023431\n",
      "epoch 34,step 2621500, training loss 0.023734\n",
      "epoch 34,step 2625000, training loss 0.054948\n",
      "epoch 34,step 2628500, training loss 0.0708394\n",
      "epoch 34,step 2632000, training loss 0.0370173\n",
      "epoch 34,step 2635500, training loss 0.026031\n",
      "epoch 34,step 2639000, training loss 0.0243315\n",
      "epoch 34,step 2642500, training loss 0.022835\n",
      "epoch 34,step 2646000, training loss 0.0255145\n",
      "epoch 34,step 2649500, training loss 0.0254348\n",
      "epoch 34,step 2653000, training loss 0.0235884\n",
      "epoch 34,step 2656500, training loss 0.0172101\n",
      "epoch 34,step 2660000, training loss 0.0212156\n",
      "epoch 34,step 2663500, training loss 0.0766073\n",
      "epoch 34,step 2667000, training loss 0.0706049\n",
      "epoch 34,step 2670500, training loss 0.0270946\n",
      "epoch 34,step 2674000, training loss 0.0222922\n",
      "epoch 34,step 2677500, training loss 0.0354896\n",
      "epoch 34,step 2681000, training loss 0.0418228\n",
      "epoch 34,step 2684500, training loss 0.0222826\n",
      "epoch 34,step 2688000, training loss 0.0340182\n",
      "epoch 34,step 2691500, training loss 0.0264386\n",
      "epoch 34,step 2695000, training loss 0.0184169\n",
      "epoch 34,step 2698500, training loss 0.106149\n",
      "epoch 34,step 2702000, training loss 0.0755259\n",
      "epoch 34,step 2705500, training loss 0.0340801\n",
      "epoch 34,step 2709000, training loss 0.0322101\n",
      "epoch 34,step 2712500, training loss 0.027915\n",
      "epoch 34,step 2716000, training loss 0.0212482\n",
      "epoch 34,step 2719500, training loss 0.0238805\n",
      "epoch 34,step 2723000, training loss 0.032199\n",
      "epoch 34,step 2726500, training loss 0.0326804\n",
      "epoch 34,step 2730000, training loss 0.0203443\n",
      "epoch 34,step 2733500, training loss 0.0736719\n",
      "epoch 34,step 2737000, training loss 0.061076\n",
      "epoch 34,step 2740500, training loss 0.0362622\n",
      "epoch 34,step 2744000, training loss 0.0259199\n",
      "epoch 34,step 2747500, training loss 0.0269504\n",
      "epoch 34,step 2751000, training loss 0.0222611\n",
      "epoch 34,step 2754500, training loss 0.0464181\n",
      "epoch 34,step 2758000, training loss 0.0322165\n",
      "epoch 34,step 2761500, training loss 0.0255713\n",
      "epoch 34,step 2765000, training loss 0.0263594\n",
      "epoch 34,step 2768500, training loss 0.105797\n",
      "epoch 34,step 2772000, training loss 0.0538743\n",
      "epoch 34,step 2775500, training loss 0.0436781\n",
      "epoch 34,step 2779000, training loss 0.041853\n",
      "epoch 34,step 2782500, training loss 0.0294099\n",
      "epoch 34,step 2786000, training loss 0.0304327\n",
      "epoch 34,step 2789500, training loss 0.0207054\n",
      "epoch 34,step 2793000, training loss 0.028836\n",
      "epoch 34,step 2796500, training loss 0.022817\n",
      "epoch 34,step 2800000, training loss 0.0252374\n",
      "epoch 34,step 2803500, training loss 0.0985544\n",
      "epoch 34,step 2807000, training loss 0.0378659\n",
      "epoch 34,step 2810500, training loss 0.0373425\n",
      "epoch 34,step 2814000, training loss 0.0384751\n",
      "epoch 34,step 2817500, training loss 0.0294563\n",
      "epoch 34,step 2821000, training loss 0.0357454\n",
      "epoch 34,step 2824500, training loss 0.0295332\n",
      "epoch 34,step 2828000, training loss 0.0377115\n",
      "epoch 34,step 2831500, training loss 0.0303947\n",
      "epoch 34,step 2835000, training loss 0.0237389\n",
      "epoch 34,step 2838500, training loss 0.0868111\n",
      "epoch 34,step 2842000, training loss 0.0635877\n",
      "epoch 34,step 2845500, training loss 0.0315213\n",
      "epoch 34,step 2849000, training loss 0.0350299\n",
      "epoch 34,step 2852500, training loss 0.0328033\n",
      "epoch 34,step 2856000, training loss 0.0256183\n",
      "epoch 34,step 2859500, training loss 0.0274963\n",
      "epoch 34,step 2863000, training loss 0.0321642\n",
      "epoch 34,step 2866500, training loss 0.029043\n",
      "epoch 34,step 2870000, training loss 0.0313538\n",
      "epoch 34,step 2873500, training loss 0.0981709\n",
      "epoch 34,step 2877000, training loss 0.0387688\n",
      "epoch 34,step 2880500, training loss 0.0292371\n",
      "epoch 34,step 2884000, training loss 0.0409154\n",
      "epoch 34,step 2887500, training loss 0.0443823\n",
      "epoch 34,step 2891000, training loss 0.0330115\n",
      "epoch 34,step 2894500, training loss 0.031797\n",
      "epoch 34,step 2898000, training loss 0.0484752\n",
      "epoch 34,step 2901500, training loss 0.0411395\n",
      "epoch 34,step 2905000, training loss 0.0341219\n",
      "epoch 34,step 2908500, training loss 0.0911113\n",
      "epoch 34,step 2912000, training loss 0.060327\n",
      "epoch 34,step 2915500, training loss 0.0271321\n",
      "epoch 34,step 2919000, training loss 0.03107\n",
      "epoch 34,step 2922500, training loss 0.0302142\n",
      "epoch 34,step 2926000, training loss 0.0356176\n",
      "epoch 34,step 2929500, training loss 0.0251189\n",
      "epoch 34,step 2933000, training loss 0.0251842\n",
      "epoch 34,step 2936500, training loss 0.0360984\n",
      "epoch 34,step 2940000, training loss 0.025678\n",
      "epoch 34,step 2943500, training loss 0.097732\n",
      "epoch 34,step 2947000, training loss 0.0668531\n",
      "epoch 34,step 2950500, training loss 0.0350365\n",
      "epoch 34,step 2954000, training loss 0.0289492\n",
      "epoch 34,step 2957500, training loss 0.0349007\n",
      "epoch 34,step 2961000, training loss 0.0295684\n",
      "epoch 34,step 2964500, training loss 0.0434864\n",
      "epoch 34,step 2968000, training loss 0.0208093\n",
      "epoch 34,step 2971500, training loss 0.0278829\n",
      "epoch 34,step 2975000, training loss 0.0322669\n",
      "epoch 34,step 2978500, training loss 0.0795374\n",
      "epoch 34,step 2982000, training loss 0.0479914\n",
      "epoch 34,step 2985500, training loss 0.0286082\n",
      "epoch 34,step 2989000, training loss 0.0385413\n",
      "epoch 34,step 2992500, training loss 0.0376563\n",
      "epoch 34,step 2996000, training loss 0.0332003\n",
      "epoch 34,step 2999500, training loss 0.0278292\n",
      "epoch 34,step 3003000, training loss 0.0324387\n",
      "epoch 34,step 3006500, training loss 0.0319672\n",
      "epoch 34,step 3010000, training loss 0.0402201\n",
      "epoch 34,step 3013500, training loss 0.0749962\n",
      "epoch 34,step 3017000, training loss 0.0495378\n",
      "epoch 34,step 3020500, training loss 0.0252354\n",
      "epoch 34,step 3024000, training loss 0.0274706\n",
      "epoch 34,step 3027500, training loss 0.0277236\n",
      "epoch 34,step 3031000, training loss 0.0242521\n",
      "epoch 34,step 3034500, training loss 0.0264136\n",
      "epoch 34,step 3038000, training loss 0.0260147\n",
      "epoch 34,step 3041500, training loss 0.03337\n",
      "epoch 34,step 3045000, training loss 0.0349357\n",
      "epoch 34,step 3048500, training loss 0.0800217\n",
      "epoch 34,step 3052000, training loss 0.0430282\n",
      "epoch 34,step 3055500, training loss 0.0258926\n",
      "epoch 34,step 3059000, training loss 0.0265583\n",
      "epoch 34,step 3062500, training loss 0.0246806\n",
      "epoch 34,step 3066000, training loss 0.0396619\n",
      "epoch 34,step 3069500, training loss 0.0380943\n",
      "epoch 34,step 3073000, training loss 0.0246115\n",
      "epoch 34,step 3076500, training loss 0.0185446\n",
      "epoch 34,step 3080000, training loss 0.0246734\n",
      "epoch 34,step 3083500, training loss 0.117669\n",
      "epoch 34,step 3087000, training loss 0.0731603\n",
      "epoch 34,step 3090500, training loss 0.0445324\n",
      "epoch 34,step 3094000, training loss 0.0272028\n",
      "epoch 34,step 3097500, training loss 0.0283835\n",
      "epoch 34,step 3101000, training loss 0.0234556\n",
      "epoch 34,step 3104500, training loss 0.0235522\n",
      "epoch 34,step 3108000, training loss 0.023455\n",
      "epoch 34,step 3111500, training loss 0.0490636\n",
      "epoch 34,step 3115000, training loss 0.0233858\n",
      "epoch 34,step 3118500, training loss 0.0898699\n",
      "epoch 34,step 3122000, training loss 0.0822053\n",
      "epoch 34,step 3125500, training loss 0.0378333\n",
      "epoch 34,step 3129000, training loss 0.0278114\n",
      "epoch 34,step 3132500, training loss 0.0352396\n",
      "epoch 34,step 3136000, training loss 0.0348778\n",
      "epoch 34,step 3139500, training loss 0.0315941\n",
      "epoch 34,step 3143000, training loss 0.0283308\n",
      "epoch 34,step 3146500, training loss 0.0269519\n",
      "epoch 34,step 3150000, training loss 0.0587003\n",
      "epoch 34,step 3153500, training loss 0.0736919\n",
      "epoch 34,step 3157000, training loss 0.030097\n",
      "epoch 34,step 3160500, training loss 0.0234775\n",
      "epoch 34,step 3164000, training loss 0.0388132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34,step 3167500, training loss 0.026575\n",
      "epoch 34,step 3171000, training loss 0.0246359\n",
      "epoch 34,step 3174500, training loss 0.020333\n",
      "epoch 34,step 3178000, training loss 0.021472\n",
      "epoch 34,step 3181500, training loss 0.0251435\n",
      "epoch 34,step 3185000, training loss 0.0250981\n",
      "epoch 34,step 3188500, training loss 0.0786462\n",
      "epoch 34,step 3192000, training loss 0.0471663\n",
      "epoch 34,step 3195500, training loss 0.0293949\n",
      "epoch 34,step 3199000, training loss 0.0320904\n",
      "epoch 34,step 3202500, training loss 0.0324123\n",
      "epoch 34,step 3206000, training loss 0.0261268\n",
      "epoch 34,step 3209500, training loss 0.0256241\n",
      "epoch 34,step 3213000, training loss 0.0316447\n",
      "epoch 34,step 3216500, training loss 0.031111\n",
      "epoch 34,step 3220000, training loss 0.0253728\n",
      "epoch 34,step 3223500, training loss 0.0688573\n",
      "epoch 34,step 3227000, training loss 0.0613567\n",
      "epoch 34,step 3230500, training loss 0.0281614\n",
      "epoch 34,step 3234000, training loss 0.0245844\n",
      "epoch 34,step 3237500, training loss 0.0208083\n",
      "epoch 34,step 3241000, training loss 0.0284927\n",
      "epoch 34,step 3244500, training loss 0.0243845\n",
      "epoch 34,step 3248000, training loss 0.0197112\n",
      "epoch 34,step 3251500, training loss 0.0238837\n",
      "epoch 34,step 3255000, training loss 0.020611\n",
      "epoch 34,step 3258500, training loss 0.0800684\n",
      "epoch 34,step 3262000, training loss 0.0431207\n",
      "epoch 34,step 3265500, training loss 0.0222608\n",
      "epoch 34,step 3269000, training loss 0.0281436\n",
      "epoch 34,step 3272500, training loss 0.0354483\n",
      "epoch 34,step 3276000, training loss 0.0246197\n",
      "epoch 34,step 3279500, training loss 0.0197291\n",
      "epoch 34,step 3283000, training loss 0.0345191\n",
      "epoch 34,step 3286500, training loss 0.0227931\n",
      "epoch 34,step 3290000, training loss 0.0223193\n",
      "epoch 34,step 3293500, training loss 0.0883349\n",
      "epoch 34,step 3297000, training loss 0.0535183\n",
      "epoch 34,step 3300500, training loss 0.023198\n",
      "epoch 34,step 3304000, training loss 0.0315355\n",
      "epoch 34,step 3307500, training loss 0.0275602\n",
      "epoch 34,step 3311000, training loss 0.0319628\n",
      "epoch 34,step 3314500, training loss 0.0352079\n",
      "epoch 34,step 3318000, training loss 0.0218519\n",
      "epoch 34,step 3321500, training loss 0.0293323\n",
      "epoch 34,step 3325000, training loss 0.0186495\n",
      "epoch 34,step 3328500, training loss 0.0865995\n",
      "epoch 34,step 3332000, training loss 0.0723491\n",
      "epoch 34,step 3335500, training loss 0.0275267\n",
      "epoch 34,step 3339000, training loss 0.0223038\n",
      "epoch 34,step 3342500, training loss 0.0243848\n",
      "epoch 34,step 3346000, training loss 0.0311339\n",
      "epoch 34,step 3349500, training loss 0.0285797\n",
      "epoch 34,step 3353000, training loss 0.0343087\n",
      "epoch 34,step 3356500, training loss 0.0263741\n",
      "epoch 34,step 3360000, training loss 0.0297075\n",
      "epoch 34,step 3363500, training loss 0.0850584\n",
      "epoch 34,step 3367000, training loss 0.0451586\n",
      "epoch 34,step 3370500, training loss 0.0188658\n",
      "epoch 34,step 3374000, training loss 0.0190154\n",
      "epoch 34,step 3377500, training loss 0.0200305\n",
      "epoch 34,step 3381000, training loss 0.0243403\n",
      "epoch 34,step 3384500, training loss 0.0235006\n",
      "epoch 34,step 3388000, training loss 0.0225342\n",
      "epoch 34,step 3391500, training loss 0.0240708\n",
      "epoch 34,step 3395000, training loss 0.027174\n",
      "epoch 34,step 3398500, training loss 0.0914426\n",
      "epoch 34,step 3402000, training loss 0.0452106\n",
      "epoch 34,step 3405500, training loss 0.023792\n",
      "epoch 34,step 3409000, training loss 0.0243878\n",
      "epoch 34,step 3412500, training loss 0.031448\n",
      "epoch 34,step 3416000, training loss 0.0222521\n",
      "epoch 34,step 3419500, training loss 0.0240485\n",
      "epoch 34,step 3423000, training loss 0.0256181\n",
      "epoch 34,step 3426500, training loss 0.0216497\n",
      "epoch 34,step 3430000, training loss 0.024584\n",
      "epoch 34,step 3433500, training loss 0.0782517\n",
      "epoch 34,step 3437000, training loss 0.0530884\n",
      "epoch 34,step 3440500, training loss 0.0373035\n",
      "epoch 34,step 3444000, training loss 0.0259384\n",
      "epoch 34,step 3447500, training loss 0.0206896\n",
      "epoch 34,step 3451000, training loss 0.0490827\n",
      "epoch 34,step 3454500, training loss 0.0245332\n",
      "epoch 34,step 3458000, training loss 0.0282489\n",
      "epoch 34,step 3461500, training loss 0.0229964\n",
      "epoch 34,step 3465000, training loss 0.0212782\n",
      "epoch 34,step 3468500, training loss 0.086394\n",
      "epoch 34,step 3472000, training loss 0.0380603\n",
      "epoch 34,step 3475500, training loss 0.0283894\n",
      "epoch 34,step 3479000, training loss 0.0203918\n",
      "epoch 34,step 3482500, training loss 0.0215253\n",
      "epoch 34,step 3486000, training loss 0.0235451\n",
      "epoch 34,step 3489500, training loss 0.018713\n",
      "epoch 34,step 3493000, training loss 0.0241565\n",
      "epoch 34,step 3496500, training loss 0.0342616\n",
      "epoch 34,training loss 0.0342616 ,test loss 0.037869\n",
      "epoch 35,step 18000, training loss 0.0318005\n",
      "epoch 35,step 36000, training loss 0.0392365\n",
      "epoch 35,step 54000, training loss 0.032705\n",
      "epoch 35,step 72000, training loss 0.019004\n",
      "epoch 35,step 90000, training loss 0.022226\n",
      "epoch 35,step 108000, training loss 0.0214328\n",
      "epoch 35,step 126000, training loss 0.0263575\n",
      "epoch 35,step 144000, training loss 0.021467\n",
      "epoch 35,step 162000, training loss 0.0251387\n",
      "epoch 35,step 180000, training loss 0.0225862\n",
      "epoch 35,step 198000, training loss 0.023106\n",
      "epoch 35,step 216000, training loss 0.0319275\n",
      "epoch 35,step 234000, training loss 0.0500341\n",
      "epoch 35,step 252000, training loss 0.025413\n",
      "epoch 35,step 270000, training loss 0.0386721\n",
      "epoch 35,step 288000, training loss 0.0267555\n",
      "epoch 35,step 306000, training loss 0.0234623\n",
      "epoch 35,step 324000, training loss 0.0228246\n",
      "epoch 35,step 342000, training loss 0.0189796\n",
      "epoch 35,step 360000, training loss 0.0300661\n",
      "epoch 35,step 378000, training loss 0.0256691\n",
      "epoch 35,step 396000, training loss 0.0309145\n",
      "epoch 35,step 414000, training loss 0.0408747\n",
      "epoch 35,step 432000, training loss 0.0324933\n",
      "epoch 35,step 450000, training loss 0.0302011\n",
      "epoch 35,step 468000, training loss 0.0173484\n",
      "epoch 35,step 486000, training loss 0.0418309\n",
      "epoch 35,step 504000, training loss 0.0273186\n",
      "epoch 35,step 522000, training loss 0.0320495\n",
      "epoch 35,step 540000, training loss 0.0250371\n",
      "epoch 35,step 558000, training loss 0.0402758\n",
      "epoch 35,step 576000, training loss 0.0240444\n",
      "epoch 35,step 594000, training loss 0.027068\n",
      "epoch 35,step 612000, training loss 0.0261245\n",
      "epoch 35,step 630000, training loss 0.0332241\n",
      "epoch 35,step 648000, training loss 0.0271739\n",
      "epoch 35,step 666000, training loss 0.030309\n",
      "epoch 35,step 684000, training loss 0.0296341\n",
      "epoch 35,step 702000, training loss 0.0268453\n",
      "epoch 35,step 720000, training loss 0.022836\n",
      "epoch 35,step 738000, training loss 0.029887\n",
      "epoch 35,step 756000, training loss 0.023651\n",
      "epoch 35,step 774000, training loss 0.0259164\n",
      "epoch 35,step 792000, training loss 0.0289186\n",
      "epoch 35,step 810000, training loss 0.0235448\n",
      "epoch 35,step 828000, training loss 0.0307726\n",
      "epoch 35,step 846000, training loss 0.0197344\n",
      "epoch 35,step 864000, training loss 0.0255135\n",
      "epoch 35,step 882000, training loss 0.0279231\n",
      "epoch 35,step 900000, training loss 0.0253401\n",
      "epoch 35,step 918000, training loss 0.0286554\n",
      "epoch 35,step 936000, training loss 0.030034\n",
      "epoch 35,step 954000, training loss 0.0198776\n",
      "epoch 35,step 972000, training loss 0.0249475\n",
      "epoch 35,step 990000, training loss 0.0430399\n",
      "epoch 35,step 1008000, training loss 0.0228361\n",
      "epoch 35,step 1026000, training loss 0.0209094\n",
      "epoch 35,step 1044000, training loss 0.0270434\n",
      "epoch 35,step 1062000, training loss 0.0332726\n",
      "epoch 35,step 1080000, training loss 0.0248438\n",
      "epoch 35,step 1098000, training loss 0.0302388\n",
      "epoch 35,step 1116000, training loss 0.027779\n",
      "epoch 35,step 1134000, training loss 0.0253624\n",
      "epoch 35,step 1152000, training loss 0.0353604\n",
      "epoch 35,step 1170000, training loss 0.0393274\n",
      "epoch 35,step 1188000, training loss 0.0246967\n",
      "epoch 35,step 1206000, training loss 0.0260844\n",
      "epoch 35,step 1224000, training loss 0.0366649\n",
      "epoch 35,step 1242000, training loss 0.0212211\n",
      "epoch 35,step 1260000, training loss 0.0265797\n",
      "epoch 35,step 1278000, training loss 0.0332533\n",
      "epoch 35,step 1296000, training loss 0.0188174\n",
      "epoch 35,step 1314000, training loss 0.0413821\n",
      "epoch 35,step 1332000, training loss 0.0364723\n",
      "epoch 35,step 1350000, training loss 0.0317733\n",
      "epoch 35,step 1368000, training loss 0.0265269\n",
      "epoch 35,step 1386000, training loss 0.0258695\n",
      "epoch 35,step 1404000, training loss 0.0242625\n",
      "epoch 35,step 1422000, training loss 0.0310064\n",
      "epoch 35,step 1440000, training loss 0.0186901\n",
      "epoch 35,step 1458000, training loss 0.0221553\n",
      "epoch 35,step 1476000, training loss 0.0243767\n",
      "epoch 35,step 1494000, training loss 0.0253374\n",
      "epoch 35,step 1512000, training loss 0.0241092\n",
      "epoch 35,step 1530000, training loss 0.0353095\n",
      "epoch 35,step 1548000, training loss 0.0265629\n",
      "epoch 35,step 1566000, training loss 0.0340201\n",
      "epoch 35,step 1584000, training loss 0.027477\n",
      "epoch 35,step 1602000, training loss 0.0198102\n",
      "epoch 35,step 1620000, training loss 0.0266025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35,step 1638000, training loss 0.0232664\n",
      "epoch 35,step 1656000, training loss 0.025144\n",
      "epoch 35,step 1674000, training loss 0.0378591\n",
      "epoch 35,step 1692000, training loss 0.0286354\n",
      "epoch 35,step 1710000, training loss 0.0220042\n",
      "epoch 35,step 1728000, training loss 0.0262284\n",
      "epoch 35,step 1746000, training loss 0.0242677\n",
      "epoch 35,step 1764000, training loss 0.0309127\n",
      "epoch 35,step 1782000, training loss 0.0333666\n",
      "epoch 35,step 1800000, training loss 0.0261072\n",
      "epoch 35,step 1818000, training loss 0.0269637\n",
      "epoch 35,step 1836000, training loss 0.0241839\n",
      "epoch 35,step 1854000, training loss 0.03223\n",
      "epoch 35,step 1872000, training loss 0.0201625\n",
      "epoch 35,step 1890000, training loss 0.0310466\n",
      "epoch 35,step 1908000, training loss 0.0370553\n",
      "epoch 35,step 1926000, training loss 0.0367873\n",
      "epoch 35,step 1944000, training loss 0.0281637\n",
      "epoch 35,step 1962000, training loss 0.0369455\n",
      "epoch 35,step 1980000, training loss 0.0457468\n",
      "epoch 35,step 1998000, training loss 0.0324607\n",
      "epoch 35,step 2016000, training loss 0.0355974\n",
      "epoch 35,step 2034000, training loss 0.0276582\n",
      "epoch 35,step 2052000, training loss 0.0224627\n",
      "epoch 35,step 2070000, training loss 0.0449121\n",
      "epoch 35,step 2088000, training loss 0.0208582\n",
      "epoch 35,step 2106000, training loss 0.0265035\n",
      "epoch 35,step 2124000, training loss 0.0218486\n",
      "epoch 35,step 2142000, training loss 0.0281051\n",
      "epoch 35,step 2160000, training loss 0.0336719\n",
      "epoch 35,step 2178000, training loss 0.0209504\n",
      "epoch 35,step 2196000, training loss 0.021182\n",
      "epoch 35,step 2214000, training loss 0.0229413\n",
      "epoch 35,step 2232000, training loss 0.0325584\n",
      "epoch 35,step 2250000, training loss 0.0339968\n",
      "epoch 35,step 2268000, training loss 0.0302176\n",
      "epoch 35,step 2286000, training loss 0.0229891\n",
      "epoch 35,step 2304000, training loss 0.0297343\n",
      "epoch 35,step 2322000, training loss 0.0330484\n",
      "epoch 35,step 2340000, training loss 0.0301297\n",
      "epoch 35,step 2358000, training loss 0.0244926\n",
      "epoch 35,step 2376000, training loss 0.024936\n",
      "epoch 35,step 2394000, training loss 0.0250896\n",
      "epoch 35,step 2412000, training loss 0.0301216\n",
      "epoch 35,step 2430000, training loss 0.022419\n",
      "epoch 35,step 2448000, training loss 0.0287521\n",
      "epoch 35,step 2466000, training loss 0.0256482\n",
      "epoch 35,step 2484000, training loss 0.0286115\n",
      "epoch 35,step 2502000, training loss 0.0300817\n",
      "epoch 35,step 2520000, training loss 0.0245074\n",
      "epoch 35,step 2538000, training loss 0.0264193\n",
      "epoch 35,step 2556000, training loss 0.0248819\n",
      "epoch 35,step 2574000, training loss 0.0303148\n",
      "epoch 35,step 2592000, training loss 0.0253495\n",
      "epoch 35,step 2610000, training loss 0.03315\n",
      "epoch 35,step 2628000, training loss 0.0234061\n",
      "epoch 35,step 2646000, training loss 0.0264408\n",
      "epoch 35,step 2664000, training loss 0.0344761\n",
      "epoch 35,step 2682000, training loss 0.0235486\n",
      "epoch 35,step 2700000, training loss 0.0546025\n",
      "epoch 35,step 2718000, training loss 0.023083\n",
      "epoch 35,step 2736000, training loss 0.021187\n",
      "epoch 35,step 2754000, training loss 0.0347792\n",
      "epoch 35,step 2772000, training loss 0.017966\n",
      "epoch 35,step 2790000, training loss 0.0278892\n",
      "epoch 35,step 2808000, training loss 0.0204832\n",
      "epoch 35,step 2826000, training loss 0.027304\n",
      "epoch 35,step 2844000, training loss 0.0267189\n",
      "epoch 35,step 2862000, training loss 0.0280582\n",
      "epoch 35,step 2880000, training loss 0.0244472\n",
      "epoch 35,step 2898000, training loss 0.0294569\n",
      "epoch 35,step 2916000, training loss 0.0239573\n",
      "epoch 35,step 2934000, training loss 0.0313982\n",
      "epoch 35,step 2952000, training loss 0.0314029\n",
      "epoch 35,step 2970000, training loss 0.0412424\n",
      "epoch 35,step 2988000, training loss 0.0310643\n",
      "epoch 35,step 3006000, training loss 0.0289582\n",
      "epoch 35,step 3024000, training loss 0.0240808\n",
      "epoch 35,step 3042000, training loss 0.033026\n",
      "epoch 35,step 3060000, training loss 0.0320814\n",
      "epoch 35,step 3078000, training loss 0.0376737\n",
      "epoch 35,step 3096000, training loss 0.0391509\n",
      "epoch 35,step 3114000, training loss 0.0274295\n",
      "epoch 35,step 3132000, training loss 0.0346425\n",
      "epoch 35,step 3150000, training loss 0.0252525\n",
      "epoch 35,step 3168000, training loss 0.024944\n",
      "epoch 35,step 3186000, training loss 0.0281901\n",
      "epoch 35,step 3204000, training loss 0.0233153\n",
      "epoch 35,step 3222000, training loss 0.0349686\n",
      "epoch 35,step 3240000, training loss 0.0579603\n",
      "epoch 35,step 3258000, training loss 0.0261351\n",
      "epoch 35,step 3276000, training loss 0.0251002\n",
      "epoch 35,step 3294000, training loss 0.0328676\n",
      "epoch 35,step 3312000, training loss 0.0252073\n",
      "epoch 35,step 3330000, training loss 0.0203289\n",
      "epoch 35,step 3348000, training loss 0.0195027\n",
      "epoch 35,step 3366000, training loss 0.0340993\n",
      "epoch 35,step 3384000, training loss 0.022813\n",
      "epoch 35,step 3402000, training loss 0.0270373\n",
      "epoch 35,step 3420000, training loss 0.0181607\n",
      "epoch 35,step 3438000, training loss 0.0237756\n",
      "epoch 35,step 3456000, training loss 0.0285678\n",
      "epoch 35,step 3474000, training loss 0.0205197\n",
      "epoch 35,step 3492000, training loss 0.0268394\n",
      "epoch 35,step 3510000, training loss 0.0309837\n",
      "epoch 35,step 3528000, training loss 0.0249258\n",
      "epoch 35,step 3546000, training loss 0.020731\n",
      "epoch 35,step 3564000, training loss 0.0214432\n",
      "epoch 35,step 3582000, training loss 0.0201857\n",
      "epoch 35,training loss 0.0341007 ,test loss 0.0369345\n",
      "epoch 36,step 18500, training loss 0.0306472\n",
      "epoch 36,step 37000, training loss 0.0387496\n",
      "epoch 36,step 55500, training loss 0.0324511\n",
      "epoch 36,step 74000, training loss 0.0188781\n",
      "epoch 36,step 92500, training loss 0.0223936\n",
      "epoch 36,step 111000, training loss 0.0215399\n",
      "epoch 36,step 129500, training loss 0.0272075\n",
      "epoch 36,step 148000, training loss 0.0215049\n",
      "epoch 36,step 166500, training loss 0.0252166\n",
      "epoch 36,step 185000, training loss 0.0233809\n",
      "epoch 36,step 203500, training loss 0.0233366\n",
      "epoch 36,step 222000, training loss 0.0316823\n",
      "epoch 36,step 240500, training loss 0.0497524\n",
      "epoch 36,step 259000, training loss 0.0255836\n",
      "epoch 36,step 277500, training loss 0.0387608\n",
      "epoch 36,step 296000, training loss 0.0267549\n",
      "epoch 36,step 314500, training loss 0.0224977\n",
      "epoch 36,step 333000, training loss 0.0217031\n",
      "epoch 36,step 351500, training loss 0.0181575\n",
      "epoch 36,step 370000, training loss 0.0293951\n",
      "epoch 36,step 388500, training loss 0.0251542\n",
      "epoch 36,step 407000, training loss 0.029809\n",
      "epoch 36,step 425500, training loss 0.0402701\n",
      "epoch 36,step 444000, training loss 0.0333945\n",
      "epoch 36,step 462500, training loss 0.0305125\n",
      "epoch 36,step 481000, training loss 0.0172023\n",
      "epoch 36,step 499500, training loss 0.0429352\n",
      "epoch 36,step 518000, training loss 0.0274636\n",
      "epoch 36,step 536500, training loss 0.0312684\n",
      "epoch 36,step 555000, training loss 0.0249116\n",
      "epoch 36,step 573500, training loss 0.0402941\n",
      "epoch 36,step 592000, training loss 0.023268\n",
      "epoch 36,step 610500, training loss 0.0271725\n",
      "epoch 36,step 629000, training loss 0.0265896\n",
      "epoch 36,step 647500, training loss 0.0329215\n",
      "epoch 36,step 666000, training loss 0.026206\n",
      "epoch 36,step 684500, training loss 0.0308881\n",
      "epoch 36,step 703000, training loss 0.0306268\n",
      "epoch 36,step 721500, training loss 0.0268741\n",
      "epoch 36,step 740000, training loss 0.0232913\n",
      "epoch 36,step 758500, training loss 0.0288574\n",
      "epoch 36,step 777000, training loss 0.0231999\n",
      "epoch 36,step 795500, training loss 0.0251785\n",
      "epoch 36,step 814000, training loss 0.0280666\n",
      "epoch 36,step 832500, training loss 0.0230523\n",
      "epoch 36,step 851000, training loss 0.0302804\n",
      "epoch 36,step 869500, training loss 0.0190933\n",
      "epoch 36,step 888000, training loss 0.0245995\n",
      "epoch 36,step 906500, training loss 0.0277717\n",
      "epoch 36,step 925000, training loss 0.0249788\n",
      "epoch 36,step 943500, training loss 0.0280752\n",
      "epoch 36,step 962000, training loss 0.0299896\n",
      "epoch 36,step 980500, training loss 0.0198301\n",
      "epoch 36,step 999000, training loss 0.0256168\n",
      "epoch 36,step 1017500, training loss 0.0434081\n",
      "epoch 36,step 1036000, training loss 0.0232992\n",
      "epoch 36,step 1054500, training loss 0.0200795\n",
      "epoch 36,step 1073000, training loss 0.0267329\n",
      "epoch 36,step 1091500, training loss 0.0352082\n",
      "epoch 36,step 1110000, training loss 0.025449\n",
      "epoch 36,step 1128500, training loss 0.0299664\n",
      "epoch 36,step 1147000, training loss 0.0276859\n",
      "epoch 36,step 1165500, training loss 0.025744\n",
      "epoch 36,step 1184000, training loss 0.0352887\n",
      "epoch 36,step 1202500, training loss 0.0395962\n",
      "epoch 36,step 1221000, training loss 0.0250282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36,step 1239500, training loss 0.0265783\n",
      "epoch 36,step 1258000, training loss 0.0358858\n",
      "epoch 36,step 1276500, training loss 0.0214767\n",
      "epoch 36,step 1295000, training loss 0.0265705\n",
      "epoch 36,step 1313500, training loss 0.032676\n",
      "epoch 36,step 1332000, training loss 0.0179855\n",
      "epoch 36,step 1350500, training loss 0.0419446\n",
      "epoch 36,step 1369000, training loss 0.0363512\n",
      "epoch 36,step 1387500, training loss 0.0324992\n",
      "epoch 36,step 1406000, training loss 0.0268083\n",
      "epoch 36,step 1424500, training loss 0.0260364\n",
      "epoch 36,step 1443000, training loss 0.0240452\n",
      "epoch 36,step 1461500, training loss 0.0307566\n",
      "epoch 36,step 1480000, training loss 0.0187831\n",
      "epoch 36,step 1498500, training loss 0.0220901\n",
      "epoch 36,step 1517000, training loss 0.0246997\n",
      "epoch 36,step 1535500, training loss 0.0240295\n",
      "epoch 36,step 1554000, training loss 0.02281\n",
      "epoch 36,step 1572500, training loss 0.0351102\n",
      "epoch 36,step 1591000, training loss 0.0265959\n",
      "epoch 36,step 1609500, training loss 0.0335952\n",
      "epoch 36,step 1628000, training loss 0.0272864\n",
      "epoch 36,step 1646500, training loss 0.0193466\n",
      "epoch 36,step 1665000, training loss 0.0262623\n",
      "epoch 36,step 1683500, training loss 0.0233323\n",
      "epoch 36,step 1702000, training loss 0.0247143\n",
      "epoch 36,step 1720500, training loss 0.037459\n",
      "epoch 36,step 1739000, training loss 0.0278956\n",
      "epoch 36,step 1757500, training loss 0.0217078\n",
      "epoch 36,step 1776000, training loss 0.0257024\n",
      "epoch 36,step 1794500, training loss 0.0240453\n",
      "epoch 36,step 1813000, training loss 0.0299415\n",
      "epoch 36,step 1831500, training loss 0.0333963\n",
      "epoch 36,step 1850000, training loss 0.0265558\n",
      "epoch 36,step 1868500, training loss 0.0268193\n",
      "epoch 36,step 1887000, training loss 0.0243109\n",
      "epoch 36,step 1905500, training loss 0.0333616\n",
      "epoch 36,step 1924000, training loss 0.0204015\n",
      "epoch 36,step 1942500, training loss 0.0313017\n",
      "epoch 36,step 1961000, training loss 0.0358142\n",
      "epoch 36,step 1979500, training loss 0.0371592\n",
      "epoch 36,step 1998000, training loss 0.0276461\n",
      "epoch 36,step 2016500, training loss 0.0370512\n",
      "epoch 36,step 2035000, training loss 0.0449864\n",
      "epoch 36,step 2053500, training loss 0.0326303\n",
      "epoch 36,step 2072000, training loss 0.0358163\n",
      "epoch 36,step 2090500, training loss 0.0268969\n",
      "epoch 36,step 2109000, training loss 0.022486\n",
      "epoch 36,step 2127500, training loss 0.0444839\n",
      "epoch 36,step 2146000, training loss 0.020344\n",
      "epoch 36,step 2164500, training loss 0.025972\n",
      "epoch 36,step 2183000, training loss 0.0218666\n",
      "epoch 36,step 2201500, training loss 0.0275498\n",
      "epoch 36,step 2220000, training loss 0.0299083\n",
      "epoch 36,step 2238500, training loss 0.0211082\n",
      "epoch 36,step 2257000, training loss 0.0207312\n",
      "epoch 36,step 2275500, training loss 0.0225622\n",
      "epoch 36,step 2294000, training loss 0.0320941\n",
      "epoch 36,step 2312500, training loss 0.0344041\n",
      "epoch 36,step 2331000, training loss 0.0299533\n",
      "epoch 36,step 2349500, training loss 0.0232862\n",
      "epoch 36,step 2368000, training loss 0.0282524\n",
      "epoch 36,step 2386500, training loss 0.033847\n",
      "epoch 36,step 2405000, training loss 0.0304781\n",
      "epoch 36,step 2423500, training loss 0.0251536\n",
      "epoch 36,step 2442000, training loss 0.0246846\n",
      "epoch 36,step 2460500, training loss 0.0248691\n",
      "epoch 36,step 2479000, training loss 0.0299034\n",
      "epoch 36,step 2497500, training loss 0.0227881\n",
      "epoch 36,step 2516000, training loss 0.028537\n",
      "epoch 36,step 2534500, training loss 0.0269735\n",
      "epoch 36,step 2553000, training loss 0.0298164\n",
      "epoch 36,step 2571500, training loss 0.028938\n",
      "epoch 36,step 2590000, training loss 0.0247566\n",
      "epoch 36,step 2608500, training loss 0.0263553\n",
      "epoch 36,step 2627000, training loss 0.0241932\n",
      "epoch 36,step 2645500, training loss 0.0288996\n",
      "epoch 36,step 2664000, training loss 0.0250626\n",
      "epoch 36,step 2682500, training loss 0.034141\n",
      "epoch 36,step 2701000, training loss 0.0234286\n",
      "epoch 36,step 2719500, training loss 0.0265997\n",
      "epoch 36,step 2738000, training loss 0.0347074\n",
      "epoch 36,step 2756500, training loss 0.0233629\n",
      "epoch 36,step 2775000, training loss 0.0535439\n",
      "epoch 36,step 2793500, training loss 0.0221877\n",
      "epoch 36,step 2812000, training loss 0.0211825\n",
      "epoch 36,step 2830500, training loss 0.0343106\n",
      "epoch 36,step 2849000, training loss 0.0177667\n",
      "epoch 36,step 2867500, training loss 0.0280365\n",
      "epoch 36,step 2886000, training loss 0.0200791\n",
      "epoch 36,step 2904500, training loss 0.0268393\n",
      "epoch 36,step 2923000, training loss 0.0263424\n",
      "epoch 36,step 2941500, training loss 0.0288001\n",
      "epoch 36,step 2960000, training loss 0.0245883\n",
      "epoch 36,step 2978500, training loss 0.0287677\n",
      "epoch 36,step 2997000, training loss 0.0243542\n",
      "epoch 36,step 3015500, training loss 0.0317854\n",
      "epoch 36,step 3034000, training loss 0.0312848\n",
      "epoch 36,step 3052500, training loss 0.0422586\n",
      "epoch 36,step 3071000, training loss 0.0312207\n",
      "epoch 36,step 3089500, training loss 0.0283226\n",
      "epoch 36,step 3108000, training loss 0.0245802\n",
      "epoch 36,step 3126500, training loss 0.0349554\n",
      "epoch 36,step 3145000, training loss 0.0323794\n",
      "epoch 36,step 3163500, training loss 0.0374767\n",
      "epoch 36,step 3182000, training loss 0.0388573\n",
      "epoch 36,step 3200500, training loss 0.0274677\n",
      "epoch 36,step 3219000, training loss 0.0338405\n",
      "epoch 36,step 3237500, training loss 0.0244907\n",
      "epoch 36,step 3256000, training loss 0.0244699\n",
      "epoch 36,step 3274500, training loss 0.0290471\n",
      "epoch 36,step 3293000, training loss 0.0234583\n",
      "epoch 36,step 3311500, training loss 0.0370121\n",
      "epoch 36,step 3330000, training loss 0.0581145\n",
      "epoch 36,step 3348500, training loss 0.0265709\n",
      "epoch 36,step 3367000, training loss 0.0255143\n",
      "epoch 36,step 3385500, training loss 0.0317943\n",
      "epoch 36,step 3404000, training loss 0.0249856\n",
      "epoch 36,step 3422500, training loss 0.0204617\n",
      "epoch 36,step 3441000, training loss 0.0204092\n",
      "epoch 36,step 3459500, training loss 0.0348742\n",
      "epoch 36,step 3478000, training loss 0.0227238\n",
      "epoch 36,step 3496500, training loss 0.0278506\n",
      "epoch 36,step 3515000, training loss 0.018478\n",
      "epoch 36,step 3533500, training loss 0.0233463\n",
      "epoch 36,step 3552000, training loss 0.0281545\n",
      "epoch 36,step 3570500, training loss 0.0200317\n",
      "epoch 36,step 3589000, training loss 0.026679\n",
      "epoch 36,step 3607500, training loss 0.0303058\n",
      "epoch 36,step 3626000, training loss 0.0248406\n",
      "epoch 36,step 3644500, training loss 0.0197449\n",
      "epoch 36,step 3663000, training loss 0.0203144\n",
      "epoch 36,step 3681500, training loss 0.0210271\n",
      "epoch 36,training loss 0.0346129 ,test loss 0.0371629\n",
      "epoch 37,step 19000, training loss 0.0309315\n",
      "epoch 37,step 38000, training loss 0.0395044\n",
      "epoch 37,step 57000, training loss 0.0325777\n",
      "epoch 37,step 76000, training loss 0.0186045\n",
      "epoch 37,step 95000, training loss 0.0225197\n",
      "epoch 37,step 114000, training loss 0.021445\n",
      "epoch 37,step 133000, training loss 0.0259038\n",
      "epoch 37,step 152000, training loss 0.0213956\n",
      "epoch 37,step 171000, training loss 0.0244396\n",
      "epoch 37,step 190000, training loss 0.0227469\n",
      "epoch 37,step 209000, training loss 0.0233059\n",
      "epoch 37,step 228000, training loss 0.0313182\n",
      "epoch 37,step 247000, training loss 0.049752\n",
      "epoch 37,step 266000, training loss 0.0250446\n",
      "epoch 37,step 285000, training loss 0.0382549\n",
      "epoch 37,step 304000, training loss 0.0261182\n",
      "epoch 37,step 323000, training loss 0.0228366\n",
      "epoch 37,step 342000, training loss 0.0211256\n",
      "epoch 37,step 361000, training loss 0.0184082\n",
      "epoch 37,step 380000, training loss 0.0293383\n",
      "epoch 37,step 399000, training loss 0.0250038\n",
      "epoch 37,step 418000, training loss 0.028924\n",
      "epoch 37,step 437000, training loss 0.0407422\n",
      "epoch 37,step 456000, training loss 0.0333539\n",
      "epoch 37,step 475000, training loss 0.0303672\n",
      "epoch 37,step 494000, training loss 0.0170207\n",
      "epoch 37,step 513000, training loss 0.0421324\n",
      "epoch 37,step 532000, training loss 0.0260676\n",
      "epoch 37,step 551000, training loss 0.0309311\n",
      "epoch 37,step 570000, training loss 0.0249286\n",
      "epoch 37,step 589000, training loss 0.0394684\n",
      "epoch 37,step 608000, training loss 0.0232035\n",
      "epoch 37,step 627000, training loss 0.0275143\n",
      "epoch 37,step 646000, training loss 0.0255914\n",
      "epoch 37,step 665000, training loss 0.0328539\n",
      "epoch 37,step 684000, training loss 0.0265668\n",
      "epoch 37,step 703000, training loss 0.0305998\n",
      "epoch 37,step 722000, training loss 0.0290993\n",
      "epoch 37,step 741000, training loss 0.0272395\n",
      "epoch 37,step 760000, training loss 0.0233098\n",
      "epoch 37,step 779000, training loss 0.029326\n",
      "epoch 37,step 798000, training loss 0.0237298\n",
      "epoch 37,step 817000, training loss 0.0257417\n",
      "epoch 37,step 836000, training loss 0.028339\n",
      "epoch 37,step 855000, training loss 0.0234613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37,step 874000, training loss 0.0310695\n",
      "epoch 37,step 893000, training loss 0.019921\n",
      "epoch 37,step 912000, training loss 0.0244898\n",
      "epoch 37,step 931000, training loss 0.0274363\n",
      "epoch 37,step 950000, training loss 0.0244638\n",
      "epoch 37,step 969000, training loss 0.0287608\n",
      "epoch 37,step 988000, training loss 0.0305261\n",
      "epoch 37,step 1007000, training loss 0.0202527\n",
      "epoch 37,step 1026000, training loss 0.0256266\n",
      "epoch 37,step 1045000, training loss 0.0425574\n",
      "epoch 37,step 1064000, training loss 0.0224679\n",
      "epoch 37,step 1083000, training loss 0.0197149\n",
      "epoch 37,step 1102000, training loss 0.0270079\n",
      "epoch 37,step 1121000, training loss 0.0334999\n",
      "epoch 37,step 1140000, training loss 0.0243982\n",
      "epoch 37,step 1159000, training loss 0.0287262\n",
      "epoch 37,step 1178000, training loss 0.0270927\n",
      "epoch 37,step 1197000, training loss 0.0261736\n",
      "epoch 37,step 1216000, training loss 0.0343721\n",
      "epoch 37,step 1235000, training loss 0.039388\n",
      "epoch 37,step 1254000, training loss 0.0248711\n",
      "epoch 37,step 1273000, training loss 0.0259586\n",
      "epoch 37,step 1292000, training loss 0.0362399\n",
      "epoch 37,step 1311000, training loss 0.0213268\n",
      "epoch 37,step 1330000, training loss 0.0267943\n",
      "epoch 37,step 1349000, training loss 0.032733\n",
      "epoch 37,step 1368000, training loss 0.0185825\n",
      "epoch 37,step 1387000, training loss 0.0415513\n",
      "epoch 37,step 1406000, training loss 0.035753\n",
      "epoch 37,step 1425000, training loss 0.0319814\n",
      "epoch 37,step 1444000, training loss 0.0261151\n",
      "epoch 37,step 1463000, training loss 0.0259253\n",
      "epoch 37,step 1482000, training loss 0.0234513\n",
      "epoch 37,step 1501000, training loss 0.0315032\n",
      "epoch 37,step 1520000, training loss 0.0196495\n",
      "epoch 37,step 1539000, training loss 0.0222977\n",
      "epoch 37,step 1558000, training loss 0.0241026\n",
      "epoch 37,step 1577000, training loss 0.0242248\n",
      "epoch 37,step 1596000, training loss 0.0228768\n",
      "epoch 37,step 1615000, training loss 0.034756\n",
      "epoch 37,step 1634000, training loss 0.0258822\n",
      "epoch 37,step 1653000, training loss 0.033074\n",
      "epoch 37,step 1672000, training loss 0.0274912\n",
      "epoch 37,step 1691000, training loss 0.0190941\n",
      "epoch 37,step 1710000, training loss 0.0260823\n",
      "epoch 37,step 1729000, training loss 0.02294\n",
      "epoch 37,step 1748000, training loss 0.0243224\n",
      "epoch 37,step 1767000, training loss 0.0362452\n",
      "epoch 37,step 1786000, training loss 0.0278995\n",
      "epoch 37,step 1805000, training loss 0.0215874\n",
      "epoch 37,step 1824000, training loss 0.0256982\n",
      "epoch 37,step 1843000, training loss 0.0240405\n",
      "epoch 37,step 1862000, training loss 0.0293509\n",
      "epoch 37,step 1881000, training loss 0.0328427\n",
      "epoch 37,step 1900000, training loss 0.0252001\n",
      "epoch 37,step 1919000, training loss 0.0268545\n",
      "epoch 37,step 1938000, training loss 0.023539\n",
      "epoch 37,step 1957000, training loss 0.031889\n",
      "epoch 37,step 1976000, training loss 0.0195857\n",
      "epoch 37,step 1995000, training loss 0.0315166\n",
      "epoch 37,step 2014000, training loss 0.0359655\n",
      "epoch 37,step 2033000, training loss 0.0352599\n",
      "epoch 37,step 2052000, training loss 0.0261229\n",
      "epoch 37,step 2071000, training loss 0.0370247\n",
      "epoch 37,step 2090000, training loss 0.0446475\n",
      "epoch 37,step 2109000, training loss 0.0323547\n",
      "epoch 37,step 2128000, training loss 0.0350896\n",
      "epoch 37,step 2147000, training loss 0.0260152\n",
      "epoch 37,step 2166000, training loss 0.0225642\n",
      "epoch 37,step 2185000, training loss 0.042874\n",
      "epoch 37,step 2204000, training loss 0.0197587\n",
      "epoch 37,step 2223000, training loss 0.0258327\n",
      "epoch 37,step 2242000, training loss 0.0213172\n",
      "epoch 37,step 2261000, training loss 0.0278168\n",
      "epoch 37,step 2280000, training loss 0.0335753\n",
      "epoch 37,step 2299000, training loss 0.0210358\n",
      "epoch 37,step 2318000, training loss 0.0211306\n",
      "epoch 37,step 2337000, training loss 0.0227076\n",
      "epoch 37,step 2356000, training loss 0.0321355\n",
      "epoch 37,step 2375000, training loss 0.0336097\n",
      "epoch 37,step 2394000, training loss 0.0291108\n",
      "epoch 37,step 2413000, training loss 0.0222617\n",
      "epoch 37,step 2432000, training loss 0.0280861\n",
      "epoch 37,step 2451000, training loss 0.0334397\n",
      "epoch 37,step 2470000, training loss 0.0294281\n",
      "epoch 37,step 2489000, training loss 0.025715\n",
      "epoch 37,step 2508000, training loss 0.0257024\n",
      "epoch 37,step 2527000, training loss 0.0250675\n",
      "epoch 37,step 2546000, training loss 0.0295982\n",
      "epoch 37,step 2565000, training loss 0.0227357\n",
      "epoch 37,step 2584000, training loss 0.0287203\n",
      "epoch 37,step 2603000, training loss 0.0255645\n",
      "epoch 37,step 2622000, training loss 0.0298401\n",
      "epoch 37,step 2641000, training loss 0.0288942\n",
      "epoch 37,step 2660000, training loss 0.0236894\n",
      "epoch 37,step 2679000, training loss 0.0261453\n",
      "epoch 37,step 2698000, training loss 0.0244408\n",
      "epoch 37,step 2717000, training loss 0.028992\n",
      "epoch 37,step 2736000, training loss 0.0252383\n",
      "epoch 37,step 2755000, training loss 0.0330444\n",
      "epoch 37,step 2774000, training loss 0.022866\n",
      "epoch 37,step 2793000, training loss 0.0259381\n",
      "epoch 37,step 2812000, training loss 0.0343192\n",
      "epoch 37,step 2831000, training loss 0.0227366\n",
      "epoch 37,step 2850000, training loss 0.0551878\n",
      "epoch 37,step 2869000, training loss 0.0223937\n",
      "epoch 37,step 2888000, training loss 0.020712\n",
      "epoch 37,step 2907000, training loss 0.0351733\n",
      "epoch 37,step 2926000, training loss 0.0180706\n",
      "epoch 37,step 2945000, training loss 0.0276525\n",
      "epoch 37,step 2964000, training loss 0.0203466\n",
      "epoch 37,step 2983000, training loss 0.0265\n",
      "epoch 37,step 3002000, training loss 0.0262576\n",
      "epoch 37,step 3021000, training loss 0.0291105\n",
      "epoch 37,step 3040000, training loss 0.0246313\n",
      "epoch 37,step 3059000, training loss 0.0279472\n",
      "epoch 37,step 3078000, training loss 0.0235589\n",
      "epoch 37,step 3097000, training loss 0.0312271\n",
      "epoch 37,step 3116000, training loss 0.0301582\n",
      "epoch 37,step 3135000, training loss 0.0398825\n",
      "epoch 37,step 3154000, training loss 0.0301212\n",
      "epoch 37,step 3173000, training loss 0.0285197\n",
      "epoch 37,step 3192000, training loss 0.0240207\n",
      "epoch 37,step 3211000, training loss 0.0336205\n",
      "epoch 37,step 3230000, training loss 0.0315894\n",
      "epoch 37,step 3249000, training loss 0.0370562\n",
      "epoch 37,step 3268000, training loss 0.0380088\n",
      "epoch 37,step 3287000, training loss 0.0279922\n",
      "epoch 37,step 3306000, training loss 0.0338442\n",
      "epoch 37,step 3325000, training loss 0.0241935\n",
      "epoch 37,step 3344000, training loss 0.0244225\n",
      "epoch 37,step 3363000, training loss 0.0278753\n",
      "epoch 37,step 3382000, training loss 0.0221605\n",
      "epoch 37,step 3401000, training loss 0.0348285\n",
      "epoch 37,step 3420000, training loss 0.0572309\n",
      "epoch 37,step 3439000, training loss 0.0263866\n",
      "epoch 37,step 3458000, training loss 0.0252081\n",
      "epoch 37,step 3477000, training loss 0.0320285\n",
      "epoch 37,step 3496000, training loss 0.024632\n",
      "epoch 37,step 3515000, training loss 0.0204899\n",
      "epoch 37,step 3534000, training loss 0.0202453\n",
      "epoch 37,step 3553000, training loss 0.0337562\n",
      "epoch 37,step 3572000, training loss 0.0226282\n",
      "epoch 37,step 3591000, training loss 0.0262929\n",
      "epoch 37,step 3610000, training loss 0.0180631\n",
      "epoch 37,step 3629000, training loss 0.02311\n",
      "epoch 37,step 3648000, training loss 0.0275401\n",
      "epoch 37,step 3667000, training loss 0.0198846\n",
      "epoch 37,step 3686000, training loss 0.0263808\n",
      "epoch 37,step 3705000, training loss 0.0304783\n",
      "epoch 37,step 3724000, training loss 0.0240878\n",
      "epoch 37,step 3743000, training loss 0.0202167\n",
      "epoch 37,step 3762000, training loss 0.0205427\n",
      "epoch 37,step 3781000, training loss 0.0205792\n",
      "epoch 37,training loss 0.0341932 ,test loss 0.0371233\n",
      "epoch 38,step 19500, training loss 0.0302785\n",
      "epoch 38,step 39000, training loss 0.0386819\n",
      "epoch 38,step 58500, training loss 0.0317762\n",
      "epoch 38,step 78000, training loss 0.0188259\n",
      "epoch 38,step 97500, training loss 0.0218601\n",
      "epoch 38,step 117000, training loss 0.0214097\n",
      "epoch 38,step 136500, training loss 0.0259275\n",
      "epoch 38,step 156000, training loss 0.0213503\n",
      "epoch 38,step 175500, training loss 0.0244463\n",
      "epoch 38,step 195000, training loss 0.02223\n",
      "epoch 38,step 214500, training loss 0.0231497\n",
      "epoch 38,step 234000, training loss 0.0305646\n",
      "epoch 38,step 253500, training loss 0.04895\n",
      "epoch 38,step 273000, training loss 0.0248698\n",
      "epoch 38,step 292500, training loss 0.037754\n",
      "epoch 38,step 312000, training loss 0.0269511\n",
      "epoch 38,step 331500, training loss 0.0230581\n",
      "epoch 38,step 351000, training loss 0.021676\n",
      "epoch 38,step 370500, training loss 0.0190195\n",
      "epoch 38,step 390000, training loss 0.029027\n",
      "epoch 38,step 409500, training loss 0.0257149\n",
      "epoch 38,step 429000, training loss 0.0294712\n",
      "epoch 38,step 448500, training loss 0.0415855\n",
      "epoch 38,step 468000, training loss 0.0334933\n",
      "epoch 38,step 487500, training loss 0.0297409\n",
      "epoch 38,step 507000, training loss 0.0170363\n",
      "epoch 38,step 526500, training loss 0.0416639\n",
      "epoch 38,step 546000, training loss 0.0268227\n",
      "epoch 38,step 565500, training loss 0.0305718\n",
      "epoch 38,step 585000, training loss 0.0239346\n",
      "epoch 38,step 604500, training loss 0.0389586\n",
      "epoch 38,step 624000, training loss 0.0231057\n",
      "epoch 38,step 643500, training loss 0.0266569\n",
      "epoch 38,step 663000, training loss 0.0256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38,step 682500, training loss 0.0320315\n",
      "epoch 38,step 702000, training loss 0.025937\n",
      "epoch 38,step 721500, training loss 0.0294761\n",
      "epoch 38,step 741000, training loss 0.0295795\n",
      "epoch 38,step 760500, training loss 0.0270544\n",
      "epoch 38,step 780000, training loss 0.0230546\n",
      "epoch 38,step 799500, training loss 0.0291177\n",
      "epoch 38,step 819000, training loss 0.0227283\n",
      "epoch 38,step 838500, training loss 0.0259572\n",
      "epoch 38,step 858000, training loss 0.0277586\n",
      "epoch 38,step 877500, training loss 0.0224004\n",
      "epoch 38,step 897000, training loss 0.030117\n",
      "epoch 38,step 916500, training loss 0.0196851\n",
      "epoch 38,step 936000, training loss 0.0241279\n",
      "epoch 38,step 955500, training loss 0.0278843\n",
      "epoch 38,step 975000, training loss 0.0244958\n",
      "epoch 38,step 994500, training loss 0.0279306\n",
      "epoch 38,step 1014000, training loss 0.029421\n",
      "epoch 38,step 1033500, training loss 0.0199575\n",
      "epoch 38,step 1053000, training loss 0.0248432\n",
      "epoch 38,step 1072500, training loss 0.0426772\n",
      "epoch 38,step 1092000, training loss 0.0226986\n",
      "epoch 38,step 1111500, training loss 0.019941\n",
      "epoch 38,step 1131000, training loss 0.0263567\n",
      "epoch 38,step 1150500, training loss 0.0324656\n",
      "epoch 38,step 1170000, training loss 0.0241395\n",
      "epoch 38,step 1189500, training loss 0.0292728\n",
      "epoch 38,step 1209000, training loss 0.0275354\n",
      "epoch 38,step 1228500, training loss 0.0256463\n",
      "epoch 38,step 1248000, training loss 0.0349923\n",
      "epoch 38,step 1267500, training loss 0.0387839\n",
      "epoch 38,step 1287000, training loss 0.0247353\n",
      "epoch 38,step 1306500, training loss 0.0259757\n",
      "epoch 38,step 1326000, training loss 0.0358688\n",
      "epoch 38,step 1345500, training loss 0.0210481\n",
      "epoch 38,step 1365000, training loss 0.0256113\n",
      "epoch 38,step 1384500, training loss 0.0322463\n",
      "epoch 38,step 1404000, training loss 0.0186524\n",
      "epoch 38,step 1423500, training loss 0.0407833\n",
      "epoch 38,step 1443000, training loss 0.0364505\n",
      "epoch 38,step 1462500, training loss 0.0309579\n",
      "epoch 38,step 1482000, training loss 0.0257058\n",
      "epoch 38,step 1501500, training loss 0.0244994\n",
      "epoch 38,step 1521000, training loss 0.0231064\n",
      "epoch 38,step 1540500, training loss 0.0298536\n",
      "epoch 38,step 1560000, training loss 0.0188181\n",
      "epoch 38,step 1579500, training loss 0.0218019\n",
      "epoch 38,step 1599000, training loss 0.0236847\n",
      "epoch 38,step 1618500, training loss 0.0241436\n",
      "epoch 38,step 1638000, training loss 0.0234792\n",
      "epoch 38,step 1657500, training loss 0.0350969\n",
      "epoch 38,step 1677000, training loss 0.0261629\n",
      "epoch 38,step 1696500, training loss 0.0338545\n",
      "epoch 38,step 1716000, training loss 0.0276868\n",
      "epoch 38,step 1735500, training loss 0.0186654\n",
      "epoch 38,step 1755000, training loss 0.0248788\n",
      "epoch 38,step 1774500, training loss 0.0230166\n",
      "epoch 38,step 1794000, training loss 0.0242688\n",
      "epoch 38,step 1813500, training loss 0.0362025\n",
      "epoch 38,step 1833000, training loss 0.0269711\n",
      "epoch 38,step 1852500, training loss 0.0213196\n",
      "epoch 38,step 1872000, training loss 0.0252076\n",
      "epoch 38,step 1891500, training loss 0.0238709\n",
      "epoch 38,step 1911000, training loss 0.0283403\n",
      "epoch 38,step 1930500, training loss 0.0332143\n",
      "epoch 38,step 1950000, training loss 0.0255336\n",
      "epoch 38,step 1969500, training loss 0.025486\n",
      "epoch 38,step 1989000, training loss 0.0225628\n",
      "epoch 38,step 2008500, training loss 0.0329793\n",
      "epoch 38,step 2028000, training loss 0.0198818\n",
      "epoch 38,step 2047500, training loss 0.0311217\n",
      "epoch 38,step 2067000, training loss 0.0358395\n",
      "epoch 38,step 2086500, training loss 0.0351382\n",
      "epoch 38,step 2106000, training loss 0.0269248\n",
      "epoch 38,step 2125500, training loss 0.0369774\n",
      "epoch 38,step 2145000, training loss 0.0455264\n",
      "epoch 38,step 2164500, training loss 0.0330348\n",
      "epoch 38,step 2184000, training loss 0.0357358\n",
      "epoch 38,step 2203500, training loss 0.0270471\n",
      "epoch 38,step 2223000, training loss 0.0224399\n",
      "epoch 38,step 2242500, training loss 0.0429729\n",
      "epoch 38,step 2262000, training loss 0.0195936\n",
      "epoch 38,step 2281500, training loss 0.0264767\n",
      "epoch 38,step 2301000, training loss 0.0218795\n",
      "epoch 38,step 2320500, training loss 0.0276007\n",
      "epoch 38,step 2340000, training loss 0.0312146\n",
      "epoch 38,step 2359500, training loss 0.0212152\n",
      "epoch 38,step 2379000, training loss 0.0210383\n",
      "epoch 38,step 2398500, training loss 0.0229914\n",
      "epoch 38,step 2418000, training loss 0.0325573\n",
      "epoch 38,step 2437500, training loss 0.0325477\n",
      "epoch 38,step 2457000, training loss 0.0289379\n",
      "epoch 38,step 2476500, training loss 0.0217893\n",
      "epoch 38,step 2496000, training loss 0.0279789\n",
      "epoch 38,step 2515500, training loss 0.0319913\n",
      "epoch 38,step 2535000, training loss 0.0290851\n",
      "epoch 38,step 2554500, training loss 0.0251982\n",
      "epoch 38,step 2574000, training loss 0.0243147\n",
      "epoch 38,step 2593500, training loss 0.0230462\n",
      "epoch 38,step 2613000, training loss 0.0293911\n",
      "epoch 38,step 2632500, training loss 0.0219603\n",
      "epoch 38,step 2652000, training loss 0.0283877\n",
      "epoch 38,step 2671500, training loss 0.0251621\n",
      "epoch 38,step 2691000, training loss 0.028157\n",
      "epoch 38,step 2710500, training loss 0.0290029\n",
      "epoch 38,step 2730000, training loss 0.0230959\n",
      "epoch 38,step 2749500, training loss 0.0261654\n",
      "epoch 38,step 2769000, training loss 0.0239957\n",
      "epoch 38,step 2788500, training loss 0.0277806\n",
      "epoch 38,step 2808000, training loss 0.0243715\n",
      "epoch 38,step 2827500, training loss 0.0335467\n",
      "epoch 38,step 2847000, training loss 0.0227496\n",
      "epoch 38,step 2866500, training loss 0.0259095\n",
      "epoch 38,step 2886000, training loss 0.0331263\n",
      "epoch 38,step 2905500, training loss 0.0230513\n",
      "epoch 38,step 2925000, training loss 0.0537737\n",
      "epoch 38,step 2944500, training loss 0.0223955\n",
      "epoch 38,step 2964000, training loss 0.0208451\n",
      "epoch 38,step 2983500, training loss 0.0346375\n",
      "epoch 38,step 3003000, training loss 0.0184422\n",
      "epoch 38,step 3022500, training loss 0.0278516\n",
      "epoch 38,step 3042000, training loss 0.0202524\n",
      "epoch 38,step 3061500, training loss 0.0260094\n",
      "epoch 38,step 3081000, training loss 0.026253\n",
      "epoch 38,step 3100500, training loss 0.0290919\n",
      "epoch 38,step 3120000, training loss 0.0250826\n",
      "epoch 38,step 3139500, training loss 0.0266936\n",
      "epoch 38,step 3159000, training loss 0.022477\n",
      "epoch 38,step 3178500, training loss 0.0303347\n",
      "epoch 38,step 3198000, training loss 0.0309114\n",
      "epoch 38,step 3217500, training loss 0.0407746\n",
      "epoch 38,step 3237000, training loss 0.030746\n",
      "epoch 38,step 3256500, training loss 0.0285947\n",
      "epoch 38,step 3276000, training loss 0.0242847\n",
      "epoch 38,step 3295500, training loss 0.0348533\n",
      "epoch 38,step 3315000, training loss 0.0317327\n",
      "epoch 38,step 3334500, training loss 0.0376158\n",
      "epoch 38,step 3354000, training loss 0.0379805\n",
      "epoch 38,step 3373500, training loss 0.0284969\n",
      "epoch 38,step 3393000, training loss 0.035214\n",
      "epoch 38,step 3412500, training loss 0.0237584\n",
      "epoch 38,step 3432000, training loss 0.0238123\n",
      "epoch 38,step 3451500, training loss 0.0267348\n",
      "epoch 38,step 3471000, training loss 0.0216084\n",
      "epoch 38,step 3490500, training loss 0.0335166\n",
      "epoch 38,step 3510000, training loss 0.0556676\n",
      "epoch 38,step 3529500, training loss 0.0263498\n",
      "epoch 38,step 3549000, training loss 0.0248565\n",
      "epoch 38,step 3568500, training loss 0.0320423\n",
      "epoch 38,step 3588000, training loss 0.0238944\n",
      "epoch 38,step 3607500, training loss 0.0200525\n",
      "epoch 38,step 3627000, training loss 0.0200817\n",
      "epoch 38,step 3646500, training loss 0.0326688\n",
      "epoch 38,step 3666000, training loss 0.0231889\n",
      "epoch 38,step 3685500, training loss 0.0265394\n",
      "epoch 38,step 3705000, training loss 0.0177838\n",
      "epoch 38,step 3724500, training loss 0.0232068\n",
      "epoch 38,step 3744000, training loss 0.0283195\n",
      "epoch 38,step 3763500, training loss 0.0197105\n",
      "epoch 38,step 3783000, training loss 0.0268161\n",
      "epoch 38,step 3802500, training loss 0.030644\n",
      "epoch 38,step 3822000, training loss 0.0241301\n",
      "epoch 38,step 3841500, training loss 0.0202646\n",
      "epoch 38,step 3861000, training loss 0.0205929\n",
      "epoch 38,step 3880500, training loss 0.0199694\n",
      "epoch 38,training loss 0.0341763 ,test loss 0.0370816\n",
      "epoch 39,step 4000, training loss 0.0938814\n",
      "epoch 39,step 8000, training loss 0.0594701\n",
      "epoch 39,step 12000, training loss 0.029441\n",
      "epoch 39,step 16000, training loss 0.021651\n",
      "epoch 39,step 20000, training loss 0.0304811\n",
      "epoch 39,step 24000, training loss 0.0204977\n",
      "epoch 39,step 28000, training loss 0.0199525\n",
      "epoch 39,step 32000, training loss 0.0199589\n",
      "epoch 39,step 36000, training loss 0.0268117\n",
      "epoch 39,step 40000, training loss 0.0386435\n",
      "epoch 39,step 44000, training loss 0.11061\n",
      "epoch 39,step 48000, training loss 0.0582039\n",
      "epoch 39,step 52000, training loss 0.0373745\n",
      "epoch 39,step 56000, training loss 0.0377602\n",
      "epoch 39,step 60000, training loss 0.0319312\n",
      "epoch 39,step 64000, training loss 0.0228467\n",
      "epoch 39,step 68000, training loss 0.034593\n",
      "epoch 39,step 72000, training loss 0.0257719\n",
      "epoch 39,step 76000, training loss 0.0442752\n",
      "epoch 39,step 80000, training loss 0.0185378\n",
      "epoch 39,step 84000, training loss 0.08752\n",
      "epoch 39,step 88000, training loss 0.0394974\n",
      "epoch 39,step 92000, training loss 0.0372682\n",
      "epoch 39,step 96000, training loss 0.0338035\n",
      "epoch 39,step 100000, training loss 0.0221629\n",
      "epoch 39,step 104000, training loss 0.0264235\n",
      "epoch 39,step 108000, training loss 0.0218918\n",
      "epoch 39,step 112000, training loss 0.0205949\n",
      "epoch 39,step 116000, training loss 0.0259071\n",
      "epoch 39,step 120000, training loss 0.021738\n",
      "epoch 39,step 124000, training loss 0.0713172\n",
      "epoch 39,step 128000, training loss 0.037104\n",
      "epoch 39,step 132000, training loss 0.0299432\n",
      "epoch 39,step 136000, training loss 0.0262426\n",
      "epoch 39,step 140000, training loss 0.0263836\n",
      "epoch 39,step 144000, training loss 0.0402255\n",
      "epoch 39,step 148000, training loss 0.024537\n",
      "epoch 39,step 152000, training loss 0.0236478\n",
      "epoch 39,step 156000, training loss 0.0203411\n",
      "epoch 39,step 160000, training loss 0.0212643\n",
      "epoch 39,step 164000, training loss 0.0661995\n",
      "epoch 39,step 168000, training loss 0.0334132\n",
      "epoch 39,step 172000, training loss 0.0220222\n",
      "epoch 39,step 176000, training loss 0.0343602\n",
      "epoch 39,step 180000, training loss 0.0242291\n",
      "epoch 39,step 184000, training loss 0.0240615\n",
      "epoch 39,step 188000, training loss 0.0310427\n",
      "epoch 39,step 192000, training loss 0.0229906\n",
      "epoch 39,step 196000, training loss 0.0219224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39,step 200000, training loss 0.0226955\n",
      "epoch 39,step 204000, training loss 0.0764415\n",
      "epoch 39,step 208000, training loss 0.0603883\n",
      "epoch 39,step 212000, training loss 0.0296295\n",
      "epoch 39,step 216000, training loss 0.0256676\n",
      "epoch 39,step 220000, training loss 0.0230724\n",
      "epoch 39,step 224000, training loss 0.0233625\n",
      "epoch 39,step 228000, training loss 0.0198103\n",
      "epoch 39,step 232000, training loss 0.030976\n",
      "epoch 39,step 236000, training loss 0.0360883\n",
      "epoch 39,step 240000, training loss 0.0306969\n",
      "epoch 39,step 244000, training loss 0.0726839\n",
      "epoch 39,step 248000, training loss 0.0496192\n",
      "epoch 39,step 252000, training loss 0.030284\n",
      "epoch 39,step 256000, training loss 0.0270864\n",
      "epoch 39,step 260000, training loss 0.0488741\n",
      "epoch 39,step 264000, training loss 0.0444993\n",
      "epoch 39,step 268000, training loss 0.0350259\n",
      "epoch 39,step 272000, training loss 0.0288844\n",
      "epoch 39,step 276000, training loss 0.0234493\n",
      "epoch 39,step 280000, training loss 0.0249633\n",
      "epoch 39,step 284000, training loss 0.0754149\n",
      "epoch 39,step 288000, training loss 0.0333805\n",
      "epoch 39,step 292000, training loss 0.0269677\n",
      "epoch 39,step 296000, training loss 0.0222895\n",
      "epoch 39,step 300000, training loss 0.0379201\n",
      "epoch 39,step 304000, training loss 0.0319733\n",
      "epoch 39,step 308000, training loss 0.0313466\n",
      "epoch 39,step 312000, training loss 0.022377\n",
      "epoch 39,step 316000, training loss 0.0258958\n",
      "epoch 39,step 320000, training loss 0.0266053\n",
      "epoch 39,step 324000, training loss 0.0893096\n",
      "epoch 39,step 328000, training loss 0.0400034\n",
      "epoch 39,step 332000, training loss 0.0633265\n",
      "epoch 39,step 336000, training loss 0.0246361\n",
      "epoch 39,step 340000, training loss 0.0225523\n",
      "epoch 39,step 344000, training loss 0.0221814\n",
      "epoch 39,step 348000, training loss 0.0250298\n",
      "epoch 39,step 352000, training loss 0.0234606\n",
      "epoch 39,step 356000, training loss 0.0188016\n",
      "epoch 39,step 360000, training loss 0.0219791\n",
      "epoch 39,step 364000, training loss 0.0771845\n",
      "epoch 39,step 368000, training loss 0.0588455\n",
      "epoch 39,step 372000, training loss 0.0309191\n",
      "epoch 39,step 376000, training loss 0.0291787\n",
      "epoch 39,step 380000, training loss 0.0190223\n",
      "epoch 39,step 384000, training loss 0.0244349\n",
      "epoch 39,step 388000, training loss 0.0231322\n",
      "epoch 39,step 392000, training loss 0.0251289\n",
      "epoch 39,step 396000, training loss 0.0258794\n",
      "epoch 39,step 400000, training loss 0.0291544\n",
      "epoch 39,step 404000, training loss 0.0876079\n",
      "epoch 39,step 408000, training loss 0.0406754\n",
      "epoch 39,step 412000, training loss 0.0284449\n",
      "epoch 39,step 416000, training loss 0.0214134\n",
      "epoch 39,step 420000, training loss 0.0258965\n",
      "epoch 39,step 424000, training loss 0.0245901\n",
      "epoch 39,step 428000, training loss 0.0178622\n",
      "epoch 39,step 432000, training loss 0.0346857\n",
      "epoch 39,step 436000, training loss 0.0315601\n",
      "epoch 39,step 440000, training loss 0.0297593\n",
      "epoch 39,step 444000, training loss 0.0799987\n",
      "epoch 39,step 448000, training loss 0.0520375\n",
      "epoch 39,step 452000, training loss 0.0294687\n",
      "epoch 39,step 456000, training loss 0.0442948\n",
      "epoch 39,step 460000, training loss 0.0407243\n",
      "epoch 39,step 464000, training loss 0.0502097\n",
      "epoch 39,step 468000, training loss 0.0307486\n",
      "epoch 39,step 472000, training loss 0.0490083\n",
      "epoch 39,step 476000, training loss 0.0541397\n",
      "epoch 39,step 480000, training loss 0.0324248\n",
      "epoch 39,step 484000, training loss 0.0722346\n",
      "epoch 39,step 488000, training loss 0.046638\n",
      "epoch 39,step 492000, training loss 0.0323408\n",
      "epoch 39,step 496000, training loss 0.0278537\n",
      "epoch 39,step 500000, training loss 0.0296974\n",
      "epoch 39,step 504000, training loss 0.0203174\n",
      "epoch 39,step 508000, training loss 0.0279707\n",
      "epoch 39,step 512000, training loss 0.0258638\n",
      "epoch 39,step 516000, training loss 0.0202735\n",
      "epoch 39,step 520000, training loss 0.0171071\n",
      "epoch 39,step 524000, training loss 0.0677317\n",
      "epoch 39,step 528000, training loss 0.0574499\n",
      "epoch 39,step 532000, training loss 0.0266301\n",
      "epoch 39,step 536000, training loss 0.0262203\n",
      "epoch 39,step 540000, training loss 0.0413795\n",
      "epoch 39,step 544000, training loss 0.0195928\n",
      "epoch 39,step 548000, training loss 0.0440021\n",
      "epoch 39,step 552000, training loss 0.0227347\n",
      "epoch 39,step 556000, training loss 0.0317144\n",
      "epoch 39,step 560000, training loss 0.0264557\n",
      "epoch 39,step 564000, training loss 0.10885\n",
      "epoch 39,step 568000, training loss 0.0693389\n",
      "epoch 39,step 572000, training loss 0.0406642\n",
      "epoch 39,step 576000, training loss 0.026796\n",
      "epoch 39,step 580000, training loss 0.0315109\n",
      "epoch 39,step 584000, training loss 0.0242537\n",
      "epoch 39,step 588000, training loss 0.034751\n",
      "epoch 39,step 592000, training loss 0.0242941\n",
      "epoch 39,step 596000, training loss 0.0261185\n",
      "epoch 39,step 600000, training loss 0.0244165\n",
      "epoch 39,step 604000, training loss 0.069036\n",
      "epoch 39,step 608000, training loss 0.0564857\n",
      "epoch 39,step 612000, training loss 0.0349596\n",
      "epoch 39,step 616000, training loss 0.0330526\n",
      "epoch 39,step 620000, training loss 0.0381333\n",
      "epoch 39,step 624000, training loss 0.0367483\n",
      "epoch 39,step 628000, training loss 0.0301653\n",
      "epoch 39,step 632000, training loss 0.0187529\n",
      "epoch 39,step 636000, training loss 0.0269059\n",
      "epoch 39,step 640000, training loss 0.0224111\n",
      "epoch 39,step 644000, training loss 0.063141\n",
      "epoch 39,step 648000, training loss 0.0412687\n",
      "epoch 39,step 652000, training loss 0.0282988\n",
      "epoch 39,step 656000, training loss 0.0207227\n",
      "epoch 39,step 660000, training loss 0.0266487\n",
      "epoch 39,step 664000, training loss 0.0192729\n",
      "epoch 39,step 668000, training loss 0.0222169\n",
      "epoch 39,step 672000, training loss 0.029038\n",
      "epoch 39,step 676000, training loss 0.0212483\n",
      "epoch 39,step 680000, training loss 0.0248308\n",
      "epoch 39,step 684000, training loss 0.0999504\n",
      "epoch 39,step 688000, training loss 0.0454228\n",
      "epoch 39,step 692000, training loss 0.034402\n",
      "epoch 39,step 696000, training loss 0.0485057\n",
      "epoch 39,step 700000, training loss 0.0315825\n",
      "epoch 39,step 704000, training loss 0.0259815\n",
      "epoch 39,step 708000, training loss 0.0374656\n",
      "epoch 39,step 712000, training loss 0.0280898\n",
      "epoch 39,step 716000, training loss 0.0282419\n",
      "epoch 39,step 720000, training loss 0.0256967\n",
      "epoch 39,step 724000, training loss 0.0699132\n",
      "epoch 39,step 728000, training loss 0.0453182\n",
      "epoch 39,step 732000, training loss 0.0310831\n",
      "epoch 39,step 736000, training loss 0.0359567\n",
      "epoch 39,step 740000, training loss 0.0302931\n",
      "epoch 39,step 744000, training loss 0.0591624\n",
      "epoch 39,step 748000, training loss 0.0327493\n",
      "epoch 39,step 752000, training loss 0.0320883\n",
      "epoch 39,step 756000, training loss 0.0279433\n",
      "epoch 39,step 760000, training loss 0.0296536\n",
      "epoch 39,step 764000, training loss 0.0689946\n",
      "epoch 39,step 768000, training loss 0.0336987\n",
      "epoch 39,step 772000, training loss 0.0204894\n",
      "epoch 39,step 776000, training loss 0.0229102\n",
      "epoch 39,step 780000, training loss 0.0268142\n",
      "epoch 39,step 784000, training loss 0.030566\n",
      "epoch 39,step 788000, training loss 0.0198453\n",
      "epoch 39,step 792000, training loss 0.0212082\n",
      "epoch 39,step 796000, training loss 0.0225045\n",
      "epoch 39,step 800000, training loss 0.0231408\n",
      "epoch 39,step 804000, training loss 0.0688208\n",
      "epoch 39,step 808000, training loss 0.064366\n",
      "epoch 39,step 812000, training loss 0.0320927\n",
      "epoch 39,step 816000, training loss 0.0223831\n",
      "epoch 39,step 820000, training loss 0.0291634\n",
      "epoch 39,step 824000, training loss 0.036404\n",
      "epoch 39,step 828000, training loss 0.0225365\n",
      "epoch 39,step 832000, training loss 0.020351\n",
      "epoch 39,step 836000, training loss 0.0231509\n",
      "epoch 39,step 840000, training loss 0.0239116\n",
      "epoch 39,step 844000, training loss 0.0875907\n",
      "epoch 39,step 848000, training loss 0.0393027\n",
      "epoch 39,step 852000, training loss 0.0283621\n",
      "epoch 39,step 856000, training loss 0.0277464\n",
      "epoch 39,step 860000, training loss 0.0262368\n",
      "epoch 39,step 864000, training loss 0.0282546\n",
      "epoch 39,step 868000, training loss 0.0271701\n",
      "epoch 39,step 872000, training loss 0.030107\n",
      "epoch 39,step 876000, training loss 0.02682\n",
      "epoch 39,step 880000, training loss 0.0277459\n",
      "epoch 39,step 884000, training loss 0.0952287\n",
      "epoch 39,step 888000, training loss 0.0438913\n",
      "epoch 39,step 892000, training loss 0.0382268\n",
      "epoch 39,step 896000, training loss 0.0311684\n",
      "epoch 39,step 900000, training loss 0.0232129\n",
      "epoch 39,step 904000, training loss 0.0221738\n",
      "epoch 39,step 908000, training loss 0.02556\n",
      "epoch 39,step 912000, training loss 0.0275193\n",
      "epoch 39,step 916000, training loss 0.0238297\n",
      "epoch 39,step 920000, training loss 0.0309038\n",
      "epoch 39,step 924000, training loss 0.0714184\n",
      "epoch 39,step 928000, training loss 0.0413739\n",
      "epoch 39,step 932000, training loss 0.0225966\n",
      "epoch 39,step 936000, training loss 0.026488\n",
      "epoch 39,step 940000, training loss 0.0197447\n",
      "epoch 39,step 944000, training loss 0.020257\n",
      "epoch 39,step 948000, training loss 0.0270274\n",
      "epoch 39,step 952000, training loss 0.0224181\n",
      "epoch 39,step 956000, training loss 0.0248988\n",
      "epoch 39,step 960000, training loss 0.0245441\n",
      "epoch 39,step 964000, training loss 0.0932064\n",
      "epoch 39,step 968000, training loss 0.0361702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39,step 972000, training loss 0.0319717\n",
      "epoch 39,step 976000, training loss 0.0314047\n",
      "epoch 39,step 980000, training loss 0.0283577\n",
      "epoch 39,step 984000, training loss 0.0344637\n",
      "epoch 39,step 988000, training loss 0.0247485\n",
      "epoch 39,step 992000, training loss 0.0234012\n",
      "epoch 39,step 996000, training loss 0.0290219\n",
      "epoch 39,step 1000000, training loss 0.0246038\n",
      "epoch 39,step 1004000, training loss 0.139532\n",
      "epoch 39,step 1008000, training loss 0.0416034\n",
      "epoch 39,step 1012000, training loss 0.0206587\n",
      "epoch 39,step 1016000, training loss 0.0273261\n",
      "epoch 39,step 1020000, training loss 0.028393\n",
      "epoch 39,step 1024000, training loss 0.0175351\n",
      "epoch 39,step 1028000, training loss 0.0294136\n",
      "epoch 39,step 1032000, training loss 0.024343\n",
      "epoch 39,step 1036000, training loss 0.0392595\n",
      "epoch 39,step 1040000, training loss 0.0300843\n",
      "epoch 39,step 1044000, training loss 0.101887\n",
      "epoch 39,step 1048000, training loss 0.0602354\n",
      "epoch 39,step 1052000, training loss 0.0504243\n",
      "epoch 39,step 1056000, training loss 0.0296128\n",
      "epoch 39,step 1060000, training loss 0.0199703\n",
      "epoch 39,step 1064000, training loss 0.0217041\n",
      "epoch 39,step 1068000, training loss 0.0234653\n",
      "epoch 39,step 1072000, training loss 0.0226034\n",
      "epoch 39,step 1076000, training loss 0.0204386\n",
      "epoch 39,step 1080000, training loss 0.0250697\n",
      "epoch 39,step 1084000, training loss 0.0821798\n",
      "epoch 39,step 1088000, training loss 0.0336606\n",
      "epoch 39,step 1092000, training loss 0.0288697\n",
      "epoch 39,step 1096000, training loss 0.0366675\n",
      "epoch 39,step 1100000, training loss 0.0414641\n",
      "epoch 39,step 1104000, training loss 0.0290548\n",
      "epoch 39,step 1108000, training loss 0.0217184\n",
      "epoch 39,step 1112000, training loss 0.0233495\n",
      "epoch 39,step 1116000, training loss 0.0244885\n",
      "epoch 39,step 1120000, training loss 0.02242\n",
      "epoch 39,step 1124000, training loss 0.0916218\n",
      "epoch 39,step 1128000, training loss 0.0459105\n",
      "epoch 39,step 1132000, training loss 0.0205884\n",
      "epoch 39,step 1136000, training loss 0.0234403\n",
      "epoch 39,step 1140000, training loss 0.019274\n",
      "epoch 39,step 1144000, training loss 0.0221082\n",
      "epoch 39,step 1148000, training loss 0.0288708\n",
      "epoch 39,step 1152000, training loss 0.0203712\n",
      "epoch 39,step 1156000, training loss 0.0248305\n",
      "epoch 39,step 1160000, training loss 0.0258677\n",
      "epoch 39,step 1164000, training loss 0.0743295\n",
      "epoch 39,step 1168000, training loss 0.0339103\n",
      "epoch 39,step 1172000, training loss 0.0276375\n",
      "epoch 39,step 1176000, training loss 0.0366831\n",
      "epoch 39,step 1180000, training loss 0.0316429\n",
      "epoch 39,step 1184000, training loss 0.0319282\n",
      "epoch 39,step 1188000, training loss 0.0236764\n",
      "epoch 39,step 1192000, training loss 0.0306736\n",
      "epoch 39,step 1196000, training loss 0.023837\n",
      "epoch 39,step 1200000, training loss 0.0236623\n",
      "epoch 39,step 1204000, training loss 0.0726191\n",
      "epoch 39,step 1208000, training loss 0.0476091\n",
      "epoch 39,step 1212000, training loss 0.0308506\n",
      "epoch 39,step 1216000, training loss 0.0297889\n",
      "epoch 39,step 1220000, training loss 0.0287274\n",
      "epoch 39,step 1224000, training loss 0.0298035\n",
      "epoch 39,step 1228000, training loss 0.0252034\n",
      "epoch 39,step 1232000, training loss 0.0402831\n",
      "epoch 39,step 1236000, training loss 0.0207983\n",
      "epoch 39,step 1240000, training loss 0.0265404\n",
      "epoch 39,step 1244000, training loss 0.0848742\n",
      "epoch 39,step 1248000, training loss 0.0641915\n",
      "epoch 39,step 1252000, training loss 0.0220076\n",
      "epoch 39,step 1256000, training loss 0.0321861\n",
      "epoch 39,step 1260000, training loss 0.0258669\n",
      "epoch 39,step 1264000, training loss 0.036576\n",
      "epoch 39,step 1268000, training loss 0.0477579\n",
      "epoch 39,step 1272000, training loss 0.0255139\n",
      "epoch 39,step 1276000, training loss 0.0417415\n",
      "epoch 39,step 1280000, training loss 0.0345705\n",
      "epoch 39,step 1284000, training loss 0.0824715\n",
      "epoch 39,step 1288000, training loss 0.0586936\n",
      "epoch 39,step 1292000, training loss 0.0341204\n",
      "epoch 39,step 1296000, training loss 0.0325077\n",
      "epoch 39,step 1300000, training loss 0.0388447\n",
      "epoch 39,step 1304000, training loss 0.0241923\n",
      "epoch 39,step 1308000, training loss 0.0278966\n",
      "epoch 39,step 1312000, training loss 0.0244047\n",
      "epoch 39,step 1316000, training loss 0.0332635\n",
      "epoch 39,step 1320000, training loss 0.0237992\n",
      "epoch 39,step 1324000, training loss 0.0686879\n",
      "epoch 39,step 1328000, training loss 0.0668806\n",
      "epoch 39,step 1332000, training loss 0.0256268\n",
      "epoch 39,step 1336000, training loss 0.0416031\n",
      "epoch 39,step 1340000, training loss 0.0249211\n",
      "epoch 39,step 1344000, training loss 0.0186624\n",
      "epoch 39,step 1348000, training loss 0.0261094\n",
      "epoch 39,step 1352000, training loss 0.0252412\n",
      "epoch 39,step 1356000, training loss 0.0261778\n",
      "epoch 39,step 1360000, training loss 0.0351335\n",
      "epoch 39,step 1364000, training loss 0.0807125\n",
      "epoch 39,step 1368000, training loss 0.0476274\n",
      "epoch 39,step 1372000, training loss 0.0411441\n",
      "epoch 39,step 1376000, training loss 0.0371762\n",
      "epoch 39,step 1380000, training loss 0.0217925\n",
      "epoch 39,step 1384000, training loss 0.023638\n",
      "epoch 39,step 1388000, training loss 0.0252188\n",
      "epoch 39,step 1392000, training loss 0.0300274\n",
      "epoch 39,step 1396000, training loss 0.0308915\n",
      "epoch 39,step 1400000, training loss 0.026549\n",
      "epoch 39,step 1404000, training loss 0.0701689\n",
      "epoch 39,step 1408000, training loss 0.0375638\n",
      "epoch 39,step 1412000, training loss 0.0362152\n",
      "epoch 39,step 1416000, training loss 0.0287237\n",
      "epoch 39,step 1420000, training loss 0.0326696\n",
      "epoch 39,step 1424000, training loss 0.0245937\n",
      "epoch 39,step 1428000, training loss 0.023104\n",
      "epoch 39,step 1432000, training loss 0.0225958\n",
      "epoch 39,step 1436000, training loss 0.0224623\n",
      "epoch 39,step 1440000, training loss 0.01908\n",
      "epoch 39,step 1444000, training loss 0.079623\n",
      "epoch 39,step 1448000, training loss 0.0431328\n",
      "epoch 39,step 1452000, training loss 0.0309307\n",
      "epoch 39,step 1456000, training loss 0.024083\n",
      "epoch 39,step 1460000, training loss 0.0409062\n",
      "epoch 39,step 1464000, training loss 0.0329314\n",
      "epoch 39,step 1468000, training loss 0.0296352\n",
      "epoch 39,step 1472000, training loss 0.0318631\n",
      "epoch 39,step 1476000, training loss 0.0261854\n",
      "epoch 39,step 1480000, training loss 0.0343458\n",
      "epoch 39,step 1484000, training loss 0.0829181\n",
      "epoch 39,step 1488000, training loss 0.0459645\n",
      "epoch 39,step 1492000, training loss 0.0247901\n",
      "epoch 39,step 1496000, training loss 0.0427873\n",
      "epoch 39,step 1500000, training loss 0.0310577\n",
      "epoch 39,step 1504000, training loss 0.0274273\n",
      "epoch 39,step 1508000, training loss 0.0252004\n",
      "epoch 39,step 1512000, training loss 0.0351171\n",
      "epoch 39,step 1516000, training loss 0.021912\n",
      "epoch 39,step 1520000, training loss 0.0255739\n",
      "epoch 39,step 1524000, training loss 0.0777564\n",
      "epoch 39,step 1528000, training loss 0.0932873\n",
      "epoch 39,step 1532000, training loss 0.041254\n",
      "epoch 39,step 1536000, training loss 0.0294845\n",
      "epoch 39,step 1540000, training loss 0.0255603\n",
      "epoch 39,step 1544000, training loss 0.027908\n",
      "epoch 39,step 1548000, training loss 0.0275859\n",
      "epoch 39,step 1552000, training loss 0.0256308\n",
      "epoch 39,step 1556000, training loss 0.0307151\n",
      "epoch 39,step 1560000, training loss 0.023326\n",
      "epoch 39,step 1564000, training loss 0.069379\n",
      "epoch 39,step 1568000, training loss 0.0515133\n",
      "epoch 39,step 1572000, training loss 0.0217721\n",
      "epoch 39,step 1576000, training loss 0.0268453\n",
      "epoch 39,step 1580000, training loss 0.0301739\n",
      "epoch 39,step 1584000, training loss 0.0212296\n",
      "epoch 39,step 1588000, training loss 0.0213792\n",
      "epoch 39,step 1592000, training loss 0.0325194\n",
      "epoch 39,step 1596000, training loss 0.0251024\n",
      "epoch 39,step 1600000, training loss 0.0184346\n",
      "epoch 39,step 1604000, training loss 0.087775\n",
      "epoch 39,step 1608000, training loss 0.119311\n",
      "epoch 39,step 1612000, training loss 0.0264254\n",
      "epoch 39,step 1616000, training loss 0.0243933\n",
      "epoch 39,step 1620000, training loss 0.0209477\n",
      "epoch 39,step 1624000, training loss 0.0235814\n",
      "epoch 39,step 1628000, training loss 0.0205185\n",
      "epoch 39,step 1632000, training loss 0.0274222\n",
      "epoch 39,step 1636000, training loss 0.0325139\n",
      "epoch 39,step 1640000, training loss 0.0238115\n",
      "epoch 39,step 1644000, training loss 0.0724616\n",
      "epoch 39,step 1648000, training loss 0.040126\n",
      "epoch 39,step 1652000, training loss 0.0299963\n",
      "epoch 39,step 1656000, training loss 0.0315079\n",
      "epoch 39,step 1660000, training loss 0.0235682\n",
      "epoch 39,step 1664000, training loss 0.0333695\n",
      "epoch 39,step 1668000, training loss 0.0305014\n",
      "epoch 39,step 1672000, training loss 0.0259175\n",
      "epoch 39,step 1676000, training loss 0.0200485\n",
      "epoch 39,step 1680000, training loss 0.0227758\n",
      "epoch 39,step 1684000, training loss 0.0793885\n",
      "epoch 39,step 1688000, training loss 0.0385686\n",
      "epoch 39,step 1692000, training loss 0.0155869\n",
      "epoch 39,step 1696000, training loss 0.0255959\n",
      "epoch 39,step 1700000, training loss 0.0344166\n",
      "epoch 39,step 1704000, training loss 0.0298606\n",
      "epoch 39,step 1708000, training loss 0.032036\n",
      "epoch 39,step 1712000, training loss 0.0368987\n",
      "epoch 39,step 1716000, training loss 0.0241847\n",
      "epoch 39,step 1720000, training loss 0.0252527\n",
      "epoch 39,step 1724000, training loss 0.079942\n",
      "epoch 39,step 1728000, training loss 0.0582566\n",
      "epoch 39,step 1732000, training loss 0.0332193\n",
      "epoch 39,step 1736000, training loss 0.0268356\n",
      "epoch 39,step 1740000, training loss 0.0333037\n",
      "epoch 39,step 1744000, training loss 0.0238618\n",
      "epoch 39,step 1748000, training loss 0.0272302\n",
      "epoch 39,step 1752000, training loss 0.0272594\n",
      "epoch 39,step 1756000, training loss 0.0275789\n",
      "epoch 39,step 1760000, training loss 0.0277314\n",
      "epoch 39,step 1764000, training loss 0.0843651\n",
      "epoch 39,step 1768000, training loss 0.042168\n",
      "epoch 39,step 1772000, training loss 0.0341397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39,step 1776000, training loss 0.0237571\n",
      "epoch 39,step 1780000, training loss 0.0188576\n",
      "epoch 39,step 1784000, training loss 0.0195718\n",
      "epoch 39,step 1788000, training loss 0.028708\n",
      "epoch 39,step 1792000, training loss 0.0248538\n",
      "epoch 39,step 1796000, training loss 0.0272214\n",
      "epoch 39,step 1800000, training loss 0.0259757\n",
      "epoch 39,step 1804000, training loss 0.0770427\n",
      "epoch 39,step 1808000, training loss 0.0358533\n",
      "epoch 39,step 1812000, training loss 0.0330326\n",
      "epoch 39,step 1816000, training loss 0.0315844\n",
      "epoch 39,step 1820000, training loss 0.0229475\n",
      "epoch 39,step 1824000, training loss 0.0200323\n",
      "epoch 39,step 1828000, training loss 0.0251571\n",
      "epoch 39,step 1832000, training loss 0.0258567\n",
      "epoch 39,step 1836000, training loss 0.0188433\n",
      "epoch 39,step 1840000, training loss 0.0243173\n",
      "epoch 39,step 1844000, training loss 0.0876869\n",
      "epoch 39,step 1848000, training loss 0.0537765\n",
      "epoch 39,step 1852000, training loss 0.0351253\n",
      "epoch 39,step 1856000, training loss 0.0334148\n",
      "epoch 39,step 1860000, training loss 0.036117\n",
      "epoch 39,step 1864000, training loss 0.0263276\n",
      "epoch 39,step 1868000, training loss 0.0414856\n",
      "epoch 39,step 1872000, training loss 0.0312218\n",
      "epoch 39,step 1876000, training loss 0.0267944\n",
      "epoch 39,step 1880000, training loss 0.0271675\n",
      "epoch 39,step 1884000, training loss 0.0793603\n",
      "epoch 39,step 1888000, training loss 0.0823657\n",
      "epoch 39,step 1892000, training loss 0.034925\n",
      "epoch 39,step 1896000, training loss 0.0288212\n",
      "epoch 39,step 1900000, training loss 0.0211419\n",
      "epoch 39,step 1904000, training loss 0.0395131\n",
      "epoch 39,step 1908000, training loss 0.0298032\n",
      "epoch 39,step 1912000, training loss 0.0306082\n",
      "epoch 39,step 1916000, training loss 0.0246942\n",
      "epoch 39,step 1920000, training loss 0.0260809\n",
      "epoch 39,step 1924000, training loss 0.0659516\n",
      "epoch 39,step 1928000, training loss 0.0304673\n",
      "epoch 39,step 1932000, training loss 0.0334038\n",
      "epoch 39,step 1936000, training loss 0.0289037\n",
      "epoch 39,step 1940000, training loss 0.0236172\n",
      "epoch 39,step 1944000, training loss 0.028536\n",
      "epoch 39,step 1948000, training loss 0.0235048\n",
      "epoch 39,step 1952000, training loss 0.0248617\n",
      "epoch 39,step 1956000, training loss 0.0251591\n",
      "epoch 39,step 1960000, training loss 0.0290899\n",
      "epoch 39,step 1964000, training loss 0.0833617\n",
      "epoch 39,step 1968000, training loss 0.0366264\n",
      "epoch 39,step 1972000, training loss 0.0241952\n",
      "epoch 39,step 1976000, training loss 0.0401978\n",
      "epoch 39,step 1980000, training loss 0.0329752\n",
      "epoch 39,step 1984000, training loss 0.023907\n",
      "epoch 39,step 1988000, training loss 0.0255198\n",
      "epoch 39,step 1992000, training loss 0.0277376\n",
      "epoch 39,step 1996000, training loss 0.0481617\n",
      "epoch 39,step 2000000, training loss 0.0258833\n",
      "epoch 39,step 2004000, training loss 0.0862127\n",
      "epoch 39,step 2008000, training loss 0.0490798\n",
      "epoch 39,step 2012000, training loss 0.0323099\n",
      "epoch 39,step 2016000, training loss 0.0330557\n",
      "epoch 39,step 2020000, training loss 0.0260626\n",
      "epoch 39,step 2024000, training loss 0.0242889\n",
      "epoch 39,step 2028000, training loss 0.019288\n",
      "epoch 39,step 2032000, training loss 0.0248476\n",
      "epoch 39,step 2036000, training loss 0.0303508\n",
      "epoch 39,step 2040000, training loss 0.0231421\n",
      "epoch 39,step 2044000, training loss 0.0921135\n",
      "epoch 39,step 2048000, training loss 0.0904396\n",
      "epoch 39,step 2052000, training loss 0.026935\n",
      "epoch 39,step 2056000, training loss 0.028172\n",
      "epoch 39,step 2060000, training loss 0.032511\n",
      "epoch 39,step 2064000, training loss 0.0321919\n",
      "epoch 39,step 2068000, training loss 0.0296165\n",
      "epoch 39,step 2072000, training loss 0.0251933\n",
      "epoch 39,step 2076000, training loss 0.0311453\n",
      "epoch 39,step 2080000, training loss 0.0201776\n",
      "epoch 39,step 2084000, training loss 0.080384\n",
      "epoch 39,step 2088000, training loss 0.0579048\n",
      "epoch 39,step 2092000, training loss 0.0269776\n",
      "epoch 39,step 2096000, training loss 0.0255942\n",
      "epoch 39,step 2100000, training loss 0.0309959\n",
      "epoch 39,step 2104000, training loss 0.0220823\n",
      "epoch 39,step 2108000, training loss 0.0285363\n",
      "epoch 39,step 2112000, training loss 0.0298436\n",
      "epoch 39,step 2116000, training loss 0.0458903\n",
      "epoch 39,step 2120000, training loss 0.035681\n",
      "epoch 39,step 2124000, training loss 0.107225\n",
      "epoch 39,step 2128000, training loss 0.0874051\n",
      "epoch 39,step 2132000, training loss 0.0389367\n",
      "epoch 39,step 2136000, training loss 0.0408534\n",
      "epoch 39,step 2140000, training loss 0.0350164\n",
      "epoch 39,step 2144000, training loss 0.0269683\n",
      "epoch 39,step 2148000, training loss 0.0252478\n",
      "epoch 39,step 2152000, training loss 0.0269378\n",
      "epoch 39,step 2156000, training loss 0.0289138\n",
      "epoch 39,step 2160000, training loss 0.0264626\n",
      "epoch 39,step 2164000, training loss 0.0675638\n",
      "epoch 39,step 2168000, training loss 0.0672181\n",
      "epoch 39,step 2172000, training loss 0.0413036\n",
      "epoch 39,step 2176000, training loss 0.0460276\n",
      "epoch 39,step 2180000, training loss 0.0360574\n",
      "epoch 39,step 2184000, training loss 0.0249\n",
      "epoch 39,step 2188000, training loss 0.0193209\n",
      "epoch 39,step 2192000, training loss 0.02796\n",
      "epoch 39,step 2196000, training loss 0.0262707\n",
      "epoch 39,step 2200000, training loss 0.0438238\n",
      "epoch 39,step 2204000, training loss 0.0729359\n",
      "epoch 39,step 2208000, training loss 0.0429415\n",
      "epoch 39,step 2212000, training loss 0.0304509\n",
      "epoch 39,step 2216000, training loss 0.0350752\n",
      "epoch 39,step 2220000, training loss 0.0307433\n",
      "epoch 39,step 2224000, training loss 0.0300518\n",
      "epoch 39,step 2228000, training loss 0.0261349\n",
      "epoch 39,step 2232000, training loss 0.0365309\n",
      "epoch 39,step 2236000, training loss 0.024926\n",
      "epoch 39,step 2240000, training loss 0.0341209\n",
      "epoch 39,step 2244000, training loss 0.0821662\n",
      "epoch 39,step 2248000, training loss 0.0643997\n",
      "epoch 39,step 2252000, training loss 0.0442884\n",
      "epoch 39,step 2256000, training loss 0.0325641\n",
      "epoch 39,step 2260000, training loss 0.025384\n",
      "epoch 39,step 2264000, training loss 0.0244832\n",
      "epoch 39,step 2268000, training loss 0.0270279\n",
      "epoch 39,step 2272000, training loss 0.0257174\n",
      "epoch 39,step 2276000, training loss 0.0266103\n",
      "epoch 39,step 2280000, training loss 0.0219344\n",
      "epoch 39,step 2284000, training loss 0.0845548\n",
      "epoch 39,step 2288000, training loss 0.059222\n",
      "epoch 39,step 2292000, training loss 0.0395739\n",
      "epoch 39,step 2296000, training loss 0.0356197\n",
      "epoch 39,step 2300000, training loss 0.0425872\n",
      "epoch 39,step 2304000, training loss 0.0332442\n",
      "epoch 39,step 2308000, training loss 0.0298055\n",
      "epoch 39,step 2312000, training loss 0.0321618\n",
      "epoch 39,step 2316000, training loss 0.025197\n",
      "epoch 39,step 2320000, training loss 0.0196692\n",
      "epoch 39,step 2324000, training loss 0.0813548\n",
      "epoch 39,step 2328000, training loss 0.044146\n",
      "epoch 39,step 2332000, training loss 0.0261444\n",
      "epoch 39,step 2336000, training loss 0.0293024\n",
      "epoch 39,step 2340000, training loss 0.0260404\n",
      "epoch 39,step 2344000, training loss 0.0287487\n",
      "epoch 39,step 2348000, training loss 0.0249659\n",
      "epoch 39,step 2352000, training loss 0.0359707\n",
      "epoch 39,step 2356000, training loss 0.0302418\n",
      "epoch 39,step 2360000, training loss 0.0222309\n",
      "epoch 39,step 2364000, training loss 0.0743267\n",
      "epoch 39,step 2368000, training loss 0.0410041\n",
      "epoch 39,step 2372000, training loss 0.0275814\n",
      "epoch 39,step 2376000, training loss 0.0323428\n",
      "epoch 39,step 2380000, training loss 0.0270334\n",
      "epoch 39,step 2384000, training loss 0.0296165\n",
      "epoch 39,step 2388000, training loss 0.020911\n",
      "epoch 39,step 2392000, training loss 0.0245238\n",
      "epoch 39,step 2396000, training loss 0.026471\n",
      "epoch 39,step 2400000, training loss 0.0311292\n",
      "epoch 39,step 2404000, training loss 0.133311\n",
      "epoch 39,step 2408000, training loss 0.0444463\n",
      "epoch 39,step 2412000, training loss 0.0306601\n",
      "epoch 39,step 2416000, training loss 0.0445947\n",
      "epoch 39,step 2420000, training loss 0.0208535\n",
      "epoch 39,step 2424000, training loss 0.0282773\n",
      "epoch 39,step 2428000, training loss 0.0237061\n",
      "epoch 39,step 2432000, training loss 0.0209656\n",
      "epoch 39,step 2436000, training loss 0.026321\n",
      "epoch 39,step 2440000, training loss 0.0205291\n",
      "epoch 39,step 2444000, training loss 0.0954441\n",
      "epoch 39,step 2448000, training loss 0.0541422\n",
      "epoch 39,step 2452000, training loss 0.0313849\n",
      "epoch 39,step 2456000, training loss 0.0221774\n",
      "epoch 39,step 2460000, training loss 0.0224342\n",
      "epoch 39,step 2464000, training loss 0.0239902\n",
      "epoch 39,step 2468000, training loss 0.0212471\n",
      "epoch 39,step 2472000, training loss 0.0206765\n",
      "epoch 39,step 2476000, training loss 0.0212426\n",
      "epoch 39,step 2480000, training loss 0.032068\n",
      "epoch 39,step 2484000, training loss 0.117671\n",
      "epoch 39,step 2488000, training loss 0.106172\n",
      "epoch 39,step 2492000, training loss 0.0387179\n",
      "epoch 39,step 2496000, training loss 0.0407149\n",
      "epoch 39,step 2500000, training loss 0.0323808\n",
      "epoch 39,step 2504000, training loss 0.0384961\n",
      "epoch 39,step 2508000, training loss 0.0224015\n",
      "epoch 39,step 2512000, training loss 0.0248259\n",
      "epoch 39,step 2516000, training loss 0.0245353\n",
      "epoch 39,step 2520000, training loss 0.0279373\n",
      "epoch 39,step 2524000, training loss 0.0790072\n",
      "epoch 39,step 2528000, training loss 0.0442528\n",
      "epoch 39,step 2532000, training loss 0.0240129\n",
      "epoch 39,step 2536000, training loss 0.0293612\n",
      "epoch 39,step 2540000, training loss 0.0213443\n",
      "epoch 39,step 2544000, training loss 0.0196075\n",
      "epoch 39,step 2548000, training loss 0.0211125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39,step 2552000, training loss 0.0294193\n",
      "epoch 39,step 2556000, training loss 0.022966\n",
      "epoch 39,step 2560000, training loss 0.0283775\n",
      "epoch 39,step 2564000, training loss 0.122079\n",
      "epoch 39,step 2568000, training loss 0.057408\n",
      "epoch 39,step 2572000, training loss 0.0248725\n",
      "epoch 39,step 2576000, training loss 0.022992\n",
      "epoch 39,step 2580000, training loss 0.0325434\n",
      "epoch 39,step 2584000, training loss 0.0316471\n",
      "epoch 39,step 2588000, training loss 0.0684691\n",
      "epoch 39,step 2592000, training loss 0.023309\n",
      "epoch 39,step 2596000, training loss 0.0200615\n",
      "epoch 39,step 2600000, training loss 0.0282811\n",
      "epoch 39,step 2604000, training loss 0.0860606\n",
      "epoch 39,step 2608000, training loss 0.0330152\n",
      "epoch 39,step 2612000, training loss 0.0181446\n",
      "epoch 39,step 2616000, training loss 0.0243034\n",
      "epoch 39,step 2620000, training loss 0.0243423\n",
      "epoch 39,step 2624000, training loss 0.0242148\n",
      "epoch 39,step 2628000, training loss 0.0200067\n",
      "epoch 39,step 2632000, training loss 0.0258855\n",
      "epoch 39,step 2636000, training loss 0.0403061\n",
      "epoch 39,step 2640000, training loss 0.0248725\n",
      "epoch 39,step 2644000, training loss 0.0976614\n",
      "epoch 39,step 2648000, training loss 0.0471037\n",
      "epoch 39,step 2652000, training loss 0.0248456\n",
      "epoch 39,step 2656000, training loss 0.029829\n",
      "epoch 39,step 2660000, training loss 0.0243761\n",
      "epoch 39,step 2664000, training loss 0.0247157\n",
      "epoch 39,step 2668000, training loss 0.0268507\n",
      "epoch 39,step 2672000, training loss 0.0344535\n",
      "epoch 39,step 2676000, training loss 0.0257802\n",
      "epoch 39,step 2680000, training loss 0.0292319\n",
      "epoch 39,step 2684000, training loss 0.0967259\n",
      "epoch 39,step 2688000, training loss 0.08323\n",
      "epoch 39,step 2692000, training loss 0.0302461\n",
      "epoch 39,step 2696000, training loss 0.035175\n",
      "epoch 39,step 2700000, training loss 0.0216632\n",
      "epoch 39,step 2704000, training loss 0.0339425\n",
      "epoch 39,step 2708000, training loss 0.0362894\n",
      "epoch 39,step 2712000, training loss 0.0258021\n",
      "epoch 39,step 2716000, training loss 0.02337\n",
      "epoch 39,step 2720000, training loss 0.0277229\n",
      "epoch 39,step 2724000, training loss 0.0931581\n",
      "epoch 39,step 2728000, training loss 0.045067\n",
      "epoch 39,step 2732000, training loss 0.0345407\n",
      "epoch 39,step 2736000, training loss 0.0349132\n",
      "epoch 39,step 2740000, training loss 0.0254758\n",
      "epoch 39,step 2744000, training loss 0.0266855\n",
      "epoch 39,step 2748000, training loss 0.0276305\n",
      "epoch 39,step 2752000, training loss 0.0329613\n",
      "epoch 39,step 2756000, training loss 0.0292497\n",
      "epoch 39,step 2760000, training loss 0.0292412\n",
      "epoch 39,step 2764000, training loss 0.0882511\n",
      "epoch 39,step 2768000, training loss 0.0500551\n",
      "epoch 39,step 2772000, training loss 0.0231167\n",
      "epoch 39,step 2776000, training loss 0.0277043\n",
      "epoch 39,step 2780000, training loss 0.0293286\n",
      "epoch 39,step 2784000, training loss 0.0250581\n",
      "epoch 39,step 2788000, training loss 0.0252701\n",
      "epoch 39,step 2792000, training loss 0.0302299\n",
      "epoch 39,step 2796000, training loss 0.028198\n",
      "epoch 39,step 2800000, training loss 0.0231716\n",
      "epoch 39,step 2804000, training loss 0.081\n",
      "epoch 39,step 2808000, training loss 0.0595817\n",
      "epoch 39,step 2812000, training loss 0.0293267\n",
      "epoch 39,step 2816000, training loss 0.0241083\n",
      "epoch 39,step 2820000, training loss 0.0263437\n",
      "epoch 39,step 2824000, training loss 0.0248406\n",
      "epoch 39,step 2828000, training loss 0.0276933\n",
      "epoch 39,step 2832000, training loss 0.0199538\n",
      "epoch 39,step 2836000, training loss 0.0225233\n",
      "epoch 39,step 2840000, training loss 0.024106\n",
      "epoch 39,step 2844000, training loss 0.0803889\n",
      "epoch 39,step 2848000, training loss 0.052296\n",
      "epoch 39,step 2852000, training loss 0.0283886\n",
      "epoch 39,step 2856000, training loss 0.0230927\n",
      "epoch 39,step 2860000, training loss 0.0289416\n",
      "epoch 39,step 2864000, training loss 0.0259058\n",
      "epoch 39,step 2868000, training loss 0.0334146\n",
      "epoch 39,step 2872000, training loss 0.0327621\n",
      "epoch 39,step 2876000, training loss 0.0232022\n",
      "epoch 39,step 2880000, training loss 0.0241786\n",
      "epoch 39,step 2884000, training loss 0.0584648\n",
      "epoch 39,step 2888000, training loss 0.041026\n",
      "epoch 39,step 2892000, training loss 0.0384754\n",
      "epoch 39,step 2896000, training loss 0.0308528\n",
      "epoch 39,step 2900000, training loss 0.0340782\n",
      "epoch 39,step 2904000, training loss 0.0237217\n",
      "epoch 39,step 2908000, training loss 0.0365303\n",
      "epoch 39,step 2912000, training loss 0.0264897\n",
      "epoch 39,step 2916000, training loss 0.0251842\n",
      "epoch 39,step 2920000, training loss 0.023368\n",
      "epoch 39,step 2924000, training loss 0.0836184\n",
      "epoch 39,step 2928000, training loss 0.0418968\n",
      "epoch 39,step 2932000, training loss 0.0265936\n",
      "epoch 39,step 2936000, training loss 0.0409683\n",
      "epoch 39,step 2940000, training loss 0.0263349\n",
      "epoch 39,step 2944000, training loss 0.0258932\n",
      "epoch 39,step 2948000, training loss 0.0268494\n",
      "epoch 39,step 2952000, training loss 0.03661\n",
      "epoch 39,step 2956000, training loss 0.050892\n",
      "epoch 39,step 2960000, training loss 0.0333173\n",
      "epoch 39,step 2964000, training loss 0.0636266\n",
      "epoch 39,step 2968000, training loss 0.0394613\n",
      "epoch 39,step 2972000, training loss 0.0254824\n",
      "epoch 39,step 2976000, training loss 0.0222706\n",
      "epoch 39,step 2980000, training loss 0.023019\n",
      "epoch 39,step 2984000, training loss 0.0257011\n",
      "epoch 39,step 2988000, training loss 0.0249911\n",
      "epoch 39,step 2992000, training loss 0.0236126\n",
      "epoch 39,step 2996000, training loss 0.0223004\n",
      "epoch 39,step 3000000, training loss 0.0535603\n",
      "epoch 39,step 3004000, training loss 0.0692\n",
      "epoch 39,step 3008000, training loss 0.0371632\n",
      "epoch 39,step 3012000, training loss 0.0275841\n",
      "epoch 39,step 3016000, training loss 0.0243716\n",
      "epoch 39,step 3020000, training loss 0.0222843\n",
      "epoch 39,step 3024000, training loss 0.0260676\n",
      "epoch 39,step 3028000, training loss 0.0257636\n",
      "epoch 39,step 3032000, training loss 0.0242292\n",
      "epoch 39,step 3036000, training loss 0.0167617\n",
      "epoch 39,step 3040000, training loss 0.0209771\n",
      "epoch 39,step 3044000, training loss 0.0744975\n",
      "epoch 39,step 3048000, training loss 0.0685452\n",
      "epoch 39,step 3052000, training loss 0.0276545\n",
      "epoch 39,step 3056000, training loss 0.0229997\n",
      "epoch 39,step 3060000, training loss 0.0349308\n",
      "epoch 39,step 3064000, training loss 0.0404403\n",
      "epoch 39,step 3068000, training loss 0.0216579\n",
      "epoch 39,step 3072000, training loss 0.0341441\n",
      "epoch 39,step 3076000, training loss 0.0258921\n",
      "epoch 39,step 3080000, training loss 0.0182683\n",
      "epoch 39,step 3084000, training loss 0.101902\n",
      "epoch 39,step 3088000, training loss 0.0709722\n",
      "epoch 39,step 3092000, training loss 0.0340104\n",
      "epoch 39,step 3096000, training loss 0.0326996\n",
      "epoch 39,step 3100000, training loss 0.0276377\n",
      "epoch 39,step 3104000, training loss 0.0216572\n",
      "epoch 39,step 3108000, training loss 0.0235964\n",
      "epoch 39,step 3112000, training loss 0.0316834\n",
      "epoch 39,step 3116000, training loss 0.0324206\n",
      "epoch 39,step 3120000, training loss 0.0201943\n",
      "epoch 39,step 3124000, training loss 0.074182\n",
      "epoch 39,step 3128000, training loss 0.057127\n",
      "epoch 39,step 3132000, training loss 0.0379362\n",
      "epoch 39,step 3136000, training loss 0.0285876\n",
      "epoch 39,step 3140000, training loss 0.0279398\n",
      "epoch 39,step 3144000, training loss 0.0209469\n",
      "epoch 39,step 3148000, training loss 0.0445995\n",
      "epoch 39,step 3152000, training loss 0.0328503\n",
      "epoch 39,step 3156000, training loss 0.0266943\n",
      "epoch 39,step 3160000, training loss 0.026664\n",
      "epoch 39,step 3164000, training loss 0.107487\n",
      "epoch 39,step 3168000, training loss 0.0542835\n",
      "epoch 39,step 3172000, training loss 0.0426811\n",
      "epoch 39,step 3176000, training loss 0.0421057\n",
      "epoch 39,step 3180000, training loss 0.0294385\n",
      "epoch 39,step 3184000, training loss 0.0307655\n",
      "epoch 39,step 3188000, training loss 0.0211559\n",
      "epoch 39,step 3192000, training loss 0.0290833\n",
      "epoch 39,step 3196000, training loss 0.0221045\n",
      "epoch 39,step 3200000, training loss 0.0247832\n",
      "epoch 39,step 3204000, training loss 0.0889621\n",
      "epoch 39,step 3208000, training loss 0.0365512\n",
      "epoch 39,step 3212000, training loss 0.0339908\n",
      "epoch 39,step 3216000, training loss 0.0358503\n",
      "epoch 39,step 3220000, training loss 0.0278903\n",
      "epoch 39,step 3224000, training loss 0.0345877\n",
      "epoch 39,step 3228000, training loss 0.0290942\n",
      "epoch 39,step 3232000, training loss 0.034001\n",
      "epoch 39,step 3236000, training loss 0.0276953\n",
      "epoch 39,step 3240000, training loss 0.0234035\n",
      "epoch 39,step 3244000, training loss 0.081576\n",
      "epoch 39,step 3248000, training loss 0.0620257\n",
      "epoch 39,step 3252000, training loss 0.0337926\n",
      "epoch 39,step 3256000, training loss 0.0362357\n",
      "epoch 39,step 3260000, training loss 0.0310178\n",
      "epoch 39,step 3264000, training loss 0.0237695\n",
      "epoch 39,step 3268000, training loss 0.0264124\n",
      "epoch 39,step 3272000, training loss 0.0320923\n",
      "epoch 39,step 3276000, training loss 0.0291031\n",
      "epoch 39,step 3280000, training loss 0.0320926\n",
      "epoch 39,step 3284000, training loss 0.0861031\n",
      "epoch 39,step 3288000, training loss 0.0361739\n",
      "epoch 39,step 3292000, training loss 0.0268145\n",
      "epoch 39,step 3296000, training loss 0.0357184\n",
      "epoch 39,step 3300000, training loss 0.0416482\n",
      "epoch 39,step 3304000, training loss 0.0308191\n",
      "epoch 39,step 3308000, training loss 0.0305552\n",
      "epoch 39,step 3312000, training loss 0.046564\n",
      "epoch 39,step 3316000, training loss 0.0374725\n",
      "epoch 39,step 3320000, training loss 0.0308414\n",
      "epoch 39,step 3324000, training loss 0.0799105\n",
      "epoch 39,step 3328000, training loss 0.0569297\n",
      "epoch 39,step 3332000, training loss 0.0264401\n",
      "epoch 39,step 3336000, training loss 0.0302166\n",
      "epoch 39,step 3340000, training loss 0.0284044\n",
      "epoch 39,step 3344000, training loss 0.0328383\n",
      "epoch 39,step 3348000, training loss 0.0235088\n",
      "epoch 39,step 3352000, training loss 0.0227804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39,step 3356000, training loss 0.0336503\n",
      "epoch 39,step 3360000, training loss 0.0243025\n",
      "epoch 39,step 3364000, training loss 0.092653\n",
      "epoch 39,step 3368000, training loss 0.065542\n",
      "epoch 39,step 3372000, training loss 0.0335431\n",
      "epoch 39,step 3376000, training loss 0.0289932\n",
      "epoch 39,step 3380000, training loss 0.034725\n",
      "epoch 39,step 3384000, training loss 0.0289395\n",
      "epoch 39,step 3388000, training loss 0.042559\n",
      "epoch 39,step 3392000, training loss 0.0201141\n",
      "epoch 39,step 3396000, training loss 0.0261501\n",
      "epoch 39,step 3400000, training loss 0.0315099\n",
      "epoch 39,step 3404000, training loss 0.0740822\n",
      "epoch 39,step 3408000, training loss 0.0462177\n",
      "epoch 39,step 3412000, training loss 0.0284125\n",
      "epoch 39,step 3416000, training loss 0.0377282\n",
      "epoch 39,step 3420000, training loss 0.0365569\n",
      "epoch 39,step 3424000, training loss 0.0319374\n",
      "epoch 39,step 3428000, training loss 0.0269678\n",
      "epoch 39,step 3432000, training loss 0.0338481\n",
      "epoch 39,step 3436000, training loss 0.0318294\n",
      "epoch 39,step 3440000, training loss 0.0370928\n",
      "epoch 39,step 3444000, training loss 0.0718161\n",
      "epoch 39,step 3448000, training loss 0.0470435\n",
      "epoch 39,step 3452000, training loss 0.0251496\n",
      "epoch 39,step 3456000, training loss 0.0272919\n",
      "epoch 39,step 3460000, training loss 0.0283334\n",
      "epoch 39,step 3464000, training loss 0.0233105\n",
      "epoch 39,step 3468000, training loss 0.0261091\n",
      "epoch 39,step 3472000, training loss 0.0250092\n",
      "epoch 39,step 3476000, training loss 0.0312026\n",
      "epoch 39,step 3480000, training loss 0.0341796\n",
      "epoch 39,step 3484000, training loss 0.0781456\n",
      "epoch 39,step 3488000, training loss 0.0402475\n",
      "epoch 39,step 3492000, training loss 0.0243168\n",
      "epoch 39,step 3496000, training loss 0.0251852\n",
      "epoch 39,step 3500000, training loss 0.0241439\n",
      "epoch 39,step 3504000, training loss 0.0373439\n",
      "epoch 39,step 3508000, training loss 0.0362974\n",
      "epoch 39,step 3512000, training loss 0.0237681\n",
      "epoch 39,step 3516000, training loss 0.0180934\n",
      "epoch 39,step 3520000, training loss 0.0239899\n",
      "epoch 39,step 3524000, training loss 0.11214\n",
      "epoch 39,step 3528000, training loss 0.0697216\n",
      "epoch 39,step 3532000, training loss 0.0433261\n",
      "epoch 39,step 3536000, training loss 0.0267834\n",
      "epoch 39,step 3540000, training loss 0.0282314\n",
      "epoch 39,step 3544000, training loss 0.0231832\n",
      "epoch 39,step 3548000, training loss 0.0219691\n",
      "epoch 39,step 3552000, training loss 0.022415\n",
      "epoch 39,step 3556000, training loss 0.0474997\n",
      "epoch 39,step 3560000, training loss 0.0226267\n",
      "epoch 39,step 3564000, training loss 0.0879299\n",
      "epoch 39,step 3568000, training loss 0.0794683\n",
      "epoch 39,step 3572000, training loss 0.0362731\n",
      "epoch 39,step 3576000, training loss 0.0275961\n",
      "epoch 39,step 3580000, training loss 0.0346599\n",
      "epoch 39,step 3584000, training loss 0.0337732\n",
      "epoch 39,step 3588000, training loss 0.0292978\n",
      "epoch 39,step 3592000, training loss 0.0281864\n",
      "epoch 39,step 3596000, training loss 0.026611\n",
      "epoch 39,step 3600000, training loss 0.0563889\n",
      "epoch 39,step 3604000, training loss 0.0690152\n",
      "epoch 39,step 3608000, training loss 0.0292508\n",
      "epoch 39,step 3612000, training loss 0.0226181\n",
      "epoch 39,step 3616000, training loss 0.0375971\n",
      "epoch 39,step 3620000, training loss 0.0257572\n",
      "epoch 39,step 3624000, training loss 0.0237863\n",
      "epoch 39,step 3628000, training loss 0.0191806\n",
      "epoch 39,step 3632000, training loss 0.0201091\n",
      "epoch 39,step 3636000, training loss 0.0242222\n",
      "epoch 39,step 3640000, training loss 0.023839\n",
      "epoch 39,step 3644000, training loss 0.0769418\n",
      "epoch 39,step 3648000, training loss 0.0439479\n",
      "epoch 39,step 3652000, training loss 0.0280802\n",
      "epoch 39,step 3656000, training loss 0.0320881\n",
      "epoch 39,step 3660000, training loss 0.0318108\n",
      "epoch 39,step 3664000, training loss 0.0252296\n",
      "epoch 39,step 3668000, training loss 0.0247865\n",
      "epoch 39,step 3672000, training loss 0.0307065\n",
      "epoch 39,step 3676000, training loss 0.0289775\n",
      "epoch 39,step 3680000, training loss 0.0244858\n",
      "epoch 39,step 3684000, training loss 0.0657058\n",
      "epoch 39,step 3688000, training loss 0.0598493\n",
      "epoch 39,step 3692000, training loss 0.0266721\n",
      "epoch 39,step 3696000, training loss 0.0232053\n",
      "epoch 39,step 3700000, training loss 0.0197958\n",
      "epoch 39,step 3704000, training loss 0.0274229\n",
      "epoch 39,step 3708000, training loss 0.0232755\n",
      "epoch 39,step 3712000, training loss 0.0187857\n",
      "epoch 39,step 3716000, training loss 0.0228906\n",
      "epoch 39,step 3720000, training loss 0.0200878\n",
      "epoch 39,step 3724000, training loss 0.0764019\n",
      "epoch 39,step 3728000, training loss 0.0406184\n",
      "epoch 39,step 3732000, training loss 0.0211574\n",
      "epoch 39,step 3736000, training loss 0.026699\n",
      "epoch 39,step 3740000, training loss 0.0329579\n",
      "epoch 39,step 3744000, training loss 0.0239487\n",
      "epoch 39,step 3748000, training loss 0.0188595\n",
      "epoch 39,step 3752000, training loss 0.03354\n",
      "epoch 39,step 3756000, training loss 0.0223179\n",
      "epoch 39,step 3760000, training loss 0.0225725\n",
      "epoch 39,step 3764000, training loss 0.0821509\n",
      "epoch 39,step 3768000, training loss 0.0516001\n",
      "epoch 39,step 3772000, training loss 0.0214159\n",
      "epoch 39,step 3776000, training loss 0.0298818\n",
      "epoch 39,step 3780000, training loss 0.0259742\n",
      "epoch 39,step 3784000, training loss 0.0307261\n",
      "epoch 39,step 3788000, training loss 0.0322975\n",
      "epoch 39,step 3792000, training loss 0.0216778\n",
      "epoch 39,step 3796000, training loss 0.0290717\n",
      "epoch 39,step 3800000, training loss 0.0178145\n",
      "epoch 39,step 3804000, training loss 0.0800115\n",
      "epoch 39,step 3808000, training loss 0.0697424\n",
      "epoch 39,step 3812000, training loss 0.0264098\n",
      "epoch 39,step 3816000, training loss 0.0210112\n",
      "epoch 39,step 3820000, training loss 0.0227086\n",
      "epoch 39,step 3824000, training loss 0.0292114\n",
      "epoch 39,step 3828000, training loss 0.0277027\n",
      "epoch 39,step 3832000, training loss 0.0338046\n",
      "epoch 39,step 3836000, training loss 0.0236283\n",
      "epoch 39,step 3840000, training loss 0.0271748\n",
      "epoch 39,step 3844000, training loss 0.0832013\n",
      "epoch 39,step 3848000, training loss 0.0434736\n",
      "epoch 39,step 3852000, training loss 0.0182526\n",
      "epoch 39,step 3856000, training loss 0.0182824\n",
      "epoch 39,step 3860000, training loss 0.0203795\n",
      "epoch 39,step 3864000, training loss 0.0241336\n",
      "epoch 39,step 3868000, training loss 0.0221503\n",
      "epoch 39,step 3872000, training loss 0.0207682\n",
      "epoch 39,step 3876000, training loss 0.0229015\n",
      "epoch 39,step 3880000, training loss 0.0263867\n",
      "epoch 39,step 3884000, training loss 0.0910498\n",
      "epoch 39,step 3888000, training loss 0.0432919\n",
      "epoch 39,step 3892000, training loss 0.0230272\n",
      "epoch 39,step 3896000, training loss 0.0238289\n",
      "epoch 39,step 3900000, training loss 0.0298993\n",
      "epoch 39,step 3904000, training loss 0.0206392\n",
      "epoch 39,step 3908000, training loss 0.0216607\n",
      "epoch 39,step 3912000, training loss 0.0246389\n",
      "epoch 39,step 3916000, training loss 0.0208607\n",
      "epoch 39,step 3920000, training loss 0.0240299\n",
      "epoch 39,step 3924000, training loss 0.0778767\n",
      "epoch 39,step 3928000, training loss 0.0513024\n",
      "epoch 39,step 3932000, training loss 0.0363344\n",
      "epoch 39,step 3936000, training loss 0.024695\n",
      "epoch 39,step 3940000, training loss 0.0198608\n",
      "epoch 39,step 3944000, training loss 0.0470107\n",
      "epoch 39,step 3948000, training loss 0.024121\n",
      "epoch 39,step 3952000, training loss 0.0269514\n",
      "epoch 39,step 3956000, training loss 0.0224166\n",
      "epoch 39,step 3960000, training loss 0.0204467\n",
      "epoch 39,step 3964000, training loss 0.0848805\n",
      "epoch 39,step 3968000, training loss 0.0375681\n",
      "epoch 39,step 3972000, training loss 0.0287589\n",
      "epoch 39,step 3976000, training loss 0.0191194\n",
      "epoch 39,step 3980000, training loss 0.0197738\n",
      "epoch 39,step 3984000, training loss 0.022987\n",
      "epoch 39,step 3988000, training loss 0.0183963\n",
      "epoch 39,step 3992000, training loss 0.0243453\n",
      "epoch 39,step 3996000, training loss 0.0337804\n",
      "epoch 39,training loss 0.0337804 ,test loss 0.0362259\n",
      "epoch 40,step 20500, training loss 0.0304991\n",
      "epoch 40,step 41000, training loss 0.038823\n",
      "epoch 40,step 61500, training loss 0.0316375\n",
      "epoch 40,step 82000, training loss 0.0188692\n",
      "epoch 40,step 102500, training loss 0.0223663\n",
      "epoch 40,step 123000, training loss 0.0210145\n",
      "epoch 40,step 143500, training loss 0.0262138\n",
      "epoch 40,step 164000, training loss 0.0211293\n",
      "epoch 40,step 184500, training loss 0.0239082\n",
      "epoch 40,step 205000, training loss 0.0220828\n",
      "epoch 40,step 225500, training loss 0.0229728\n",
      "epoch 40,step 246000, training loss 0.0311088\n",
      "epoch 40,step 266500, training loss 0.0477085\n",
      "epoch 40,step 287000, training loss 0.0244623\n",
      "epoch 40,step 307500, training loss 0.0370451\n",
      "epoch 40,step 328000, training loss 0.0262148\n",
      "epoch 40,step 348500, training loss 0.0226572\n",
      "epoch 40,step 369000, training loss 0.0214145\n",
      "epoch 40,step 389500, training loss 0.0189485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40,step 410000, training loss 0.0285068\n",
      "epoch 40,step 430500, training loss 0.0251438\n",
      "epoch 40,step 451000, training loss 0.0291152\n",
      "epoch 40,step 471500, training loss 0.040581\n",
      "epoch 40,step 492000, training loss 0.0326701\n",
      "epoch 40,step 512500, training loss 0.0293213\n",
      "epoch 40,step 533000, training loss 0.0168197\n",
      "epoch 40,step 553500, training loss 0.0420729\n",
      "epoch 40,step 574000, training loss 0.0265445\n",
      "epoch 40,step 594500, training loss 0.0308259\n",
      "epoch 40,step 615000, training loss 0.0243966\n",
      "epoch 40,step 635500, training loss 0.0380763\n",
      "epoch 40,step 656000, training loss 0.0226184\n",
      "epoch 40,step 676500, training loss 0.0263476\n",
      "epoch 40,step 697000, training loss 0.0250552\n",
      "epoch 40,step 717500, training loss 0.0315697\n",
      "epoch 40,step 738000, training loss 0.0258485\n",
      "epoch 40,step 758500, training loss 0.0303558\n",
      "epoch 40,step 779000, training loss 0.0292416\n",
      "epoch 40,step 799500, training loss 0.027117\n",
      "epoch 40,step 820000, training loss 0.0229618\n",
      "epoch 40,step 840500, training loss 0.0286398\n",
      "epoch 40,step 861000, training loss 0.0227913\n",
      "epoch 40,step 881500, training loss 0.0250168\n",
      "epoch 40,step 902000, training loss 0.026963\n",
      "epoch 40,step 922500, training loss 0.0219684\n",
      "epoch 40,step 943000, training loss 0.0289576\n",
      "epoch 40,step 963500, training loss 0.0193724\n",
      "epoch 40,step 984000, training loss 0.0238283\n",
      "epoch 40,step 1004500, training loss 0.0270668\n",
      "epoch 40,step 1025000, training loss 0.0242024\n",
      "epoch 40,step 1045500, training loss 0.0281312\n",
      "epoch 40,step 1066000, training loss 0.0296703\n",
      "epoch 40,step 1086500, training loss 0.0191829\n",
      "epoch 40,step 1107000, training loss 0.0247074\n",
      "epoch 40,step 1127500, training loss 0.041902\n",
      "epoch 40,step 1148000, training loss 0.0222751\n",
      "epoch 40,step 1168500, training loss 0.0197433\n",
      "epoch 40,step 1189000, training loss 0.0260692\n",
      "epoch 40,step 1209500, training loss 0.0310449\n",
      "epoch 40,step 1230000, training loss 0.0236359\n",
      "epoch 40,step 1250500, training loss 0.0289461\n",
      "epoch 40,step 1271000, training loss 0.0267066\n",
      "epoch 40,step 1291500, training loss 0.024863\n",
      "epoch 40,step 1312000, training loss 0.034219\n",
      "epoch 40,step 1332500, training loss 0.0381726\n",
      "epoch 40,step 1353000, training loss 0.0239803\n",
      "epoch 40,step 1373500, training loss 0.0250684\n",
      "epoch 40,step 1394000, training loss 0.0359879\n",
      "epoch 40,step 1414500, training loss 0.0213501\n",
      "epoch 40,step 1435000, training loss 0.0259118\n",
      "epoch 40,step 1455500, training loss 0.0321121\n",
      "epoch 40,step 1476000, training loss 0.0190028\n",
      "epoch 40,step 1496500, training loss 0.0413285\n",
      "epoch 40,step 1517000, training loss 0.0357047\n",
      "epoch 40,step 1537500, training loss 0.0304312\n",
      "epoch 40,step 1558000, training loss 0.0251509\n",
      "epoch 40,step 1578500, training loss 0.0252755\n",
      "epoch 40,step 1599000, training loss 0.0231158\n",
      "epoch 40,step 1619500, training loss 0.0296507\n",
      "epoch 40,step 1640000, training loss 0.0187608\n",
      "epoch 40,step 1660500, training loss 0.0205629\n",
      "epoch 40,step 1681000, training loss 0.0228636\n",
      "epoch 40,step 1701500, training loss 0.023794\n",
      "epoch 40,step 1722000, training loss 0.0223458\n",
      "epoch 40,step 1742500, training loss 0.0348163\n",
      "epoch 40,step 1763000, training loss 0.0249391\n",
      "epoch 40,step 1783500, training loss 0.0326163\n",
      "epoch 40,step 1804000, training loss 0.0267823\n",
      "epoch 40,step 1824500, training loss 0.0185004\n",
      "epoch 40,step 1845000, training loss 0.0249253\n",
      "epoch 40,step 1865500, training loss 0.0227678\n",
      "epoch 40,step 1886000, training loss 0.0241587\n",
      "epoch 40,step 1906500, training loss 0.0351788\n",
      "epoch 40,step 1927000, training loss 0.0261339\n",
      "epoch 40,step 1947500, training loss 0.0210637\n",
      "epoch 40,step 1968000, training loss 0.0257422\n",
      "epoch 40,step 1988500, training loss 0.0233392\n",
      "epoch 40,step 2009000, training loss 0.0286717\n",
      "epoch 40,step 2029500, training loss 0.0321782\n",
      "epoch 40,step 2050000, training loss 0.0255075\n",
      "epoch 40,step 2070500, training loss 0.0259334\n",
      "epoch 40,step 2091000, training loss 0.0225765\n",
      "epoch 40,step 2111500, training loss 0.0323213\n",
      "epoch 40,step 2132000, training loss 0.0198505\n",
      "epoch 40,step 2152500, training loss 0.0300133\n",
      "epoch 40,step 2173000, training loss 0.035501\n",
      "epoch 40,step 2193500, training loss 0.0347951\n",
      "epoch 40,step 2214000, training loss 0.0263504\n",
      "epoch 40,step 2234500, training loss 0.0342915\n",
      "epoch 40,step 2255000, training loss 0.0432667\n",
      "epoch 40,step 2275500, training loss 0.0323402\n",
      "epoch 40,step 2296000, training loss 0.0364076\n",
      "epoch 40,step 2316500, training loss 0.0261596\n",
      "epoch 40,step 2337000, training loss 0.0213951\n",
      "epoch 40,step 2357500, training loss 0.0416\n",
      "epoch 40,step 2378000, training loss 0.0196083\n",
      "epoch 40,step 2398500, training loss 0.0261565\n",
      "epoch 40,step 2419000, training loss 0.0215297\n",
      "epoch 40,step 2439500, training loss 0.0275275\n",
      "epoch 40,step 2460000, training loss 0.0316307\n",
      "epoch 40,step 2480500, training loss 0.0204291\n",
      "epoch 40,step 2501000, training loss 0.0202465\n",
      "epoch 40,step 2521500, training loss 0.0223924\n",
      "epoch 40,step 2542000, training loss 0.031752\n",
      "epoch 40,step 2562500, training loss 0.0316667\n",
      "epoch 40,step 2583000, training loss 0.0268774\n",
      "epoch 40,step 2603500, training loss 0.0214273\n",
      "epoch 40,step 2624000, training loss 0.0282069\n",
      "epoch 40,step 2644500, training loss 0.0314503\n",
      "epoch 40,step 2665000, training loss 0.0278355\n",
      "epoch 40,step 2685500, training loss 0.0243763\n",
      "epoch 40,step 2706000, training loss 0.0244254\n",
      "epoch 40,step 2726500, training loss 0.0236418\n",
      "epoch 40,step 2747000, training loss 0.0283519\n",
      "epoch 40,step 2767500, training loss 0.0226721\n",
      "epoch 40,step 2788000, training loss 0.0286677\n",
      "epoch 40,step 2808500, training loss 0.0258211\n",
      "epoch 40,step 2829000, training loss 0.0290044\n",
      "epoch 40,step 2849500, training loss 0.0281943\n",
      "epoch 40,step 2870000, training loss 0.0225942\n",
      "epoch 40,step 2890500, training loss 0.0259193\n",
      "epoch 40,step 2911000, training loss 0.0240531\n",
      "epoch 40,step 2931500, training loss 0.027637\n",
      "epoch 40,step 2952000, training loss 0.024314\n",
      "epoch 40,step 2972500, training loss 0.0333104\n",
      "epoch 40,step 2993000, training loss 0.0219624\n",
      "epoch 40,step 3013500, training loss 0.0250221\n",
      "epoch 40,step 3034000, training loss 0.0325297\n",
      "epoch 40,step 3054500, training loss 0.0218024\n",
      "epoch 40,step 3075000, training loss 0.0523986\n",
      "epoch 40,step 3095500, training loss 0.0218119\n",
      "epoch 40,step 3116000, training loss 0.0203672\n",
      "epoch 40,step 3136500, training loss 0.034084\n",
      "epoch 40,step 3157000, training loss 0.0180902\n",
      "epoch 40,step 3177500, training loss 0.0275986\n",
      "epoch 40,step 3198000, training loss 0.0197663\n",
      "epoch 40,step 3218500, training loss 0.0253269\n",
      "epoch 40,step 3239000, training loss 0.0247011\n",
      "epoch 40,step 3259500, training loss 0.0266021\n",
      "epoch 40,step 3280000, training loss 0.0247798\n",
      "epoch 40,step 3300500, training loss 0.0263548\n",
      "epoch 40,step 3321000, training loss 0.0224936\n",
      "epoch 40,step 3341500, training loss 0.0291866\n",
      "epoch 40,step 3362000, training loss 0.0300698\n",
      "epoch 40,step 3382500, training loss 0.0401209\n",
      "epoch 40,step 3403000, training loss 0.0304484\n",
      "epoch 40,step 3423500, training loss 0.0274872\n",
      "epoch 40,step 3444000, training loss 0.0230828\n",
      "epoch 40,step 3464500, training loss 0.0342019\n",
      "epoch 40,step 3485000, training loss 0.0309658\n",
      "epoch 40,step 3505500, training loss 0.0360619\n",
      "epoch 40,step 3526000, training loss 0.0364922\n",
      "epoch 40,step 3546500, training loss 0.0285604\n",
      "epoch 40,step 3567000, training loss 0.0335706\n",
      "epoch 40,step 3587500, training loss 0.0240327\n",
      "epoch 40,step 3608000, training loss 0.0236206\n",
      "epoch 40,step 3628500, training loss 0.0279227\n",
      "epoch 40,step 3649000, training loss 0.0224131\n",
      "epoch 40,step 3669500, training loss 0.0326104\n",
      "epoch 40,step 3690000, training loss 0.054664\n",
      "epoch 40,step 3710500, training loss 0.0254604\n",
      "epoch 40,step 3731000, training loss 0.0247959\n",
      "epoch 40,step 3751500, training loss 0.0313832\n",
      "epoch 40,step 3772000, training loss 0.0240772\n",
      "epoch 40,step 3792500, training loss 0.0188429\n",
      "epoch 40,step 3813000, training loss 0.0196143\n",
      "epoch 40,step 3833500, training loss 0.0324982\n",
      "epoch 40,step 3854000, training loss 0.0220113\n",
      "epoch 40,step 3874500, training loss 0.0250931\n",
      "epoch 40,step 3895000, training loss 0.017449\n",
      "epoch 40,step 3915500, training loss 0.0229606\n",
      "epoch 40,step 3936000, training loss 0.0271548\n",
      "epoch 40,step 3956500, training loss 0.0199522\n",
      "epoch 40,step 3977000, training loss 0.0260026\n",
      "epoch 40,step 3997500, training loss 0.0294641\n",
      "epoch 40,step 4018000, training loss 0.023738\n",
      "epoch 40,step 4038500, training loss 0.0191336\n",
      "epoch 40,step 4059000, training loss 0.020096\n",
      "epoch 40,step 4079500, training loss 0.0201525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40,training loss 0.033114 ,test loss 0.0365244\n",
      "epoch 41,step 21000, training loss 0.0306114\n",
      "epoch 41,step 42000, training loss 0.0382986\n",
      "epoch 41,step 63000, training loss 0.0317611\n",
      "epoch 41,step 84000, training loss 0.0182069\n",
      "epoch 41,step 105000, training loss 0.0212674\n",
      "epoch 41,step 126000, training loss 0.0207856\n",
      "epoch 41,step 147000, training loss 0.0256685\n",
      "epoch 41,step 168000, training loss 0.021347\n",
      "epoch 41,step 189000, training loss 0.0230747\n",
      "epoch 41,step 210000, training loss 0.0226205\n",
      "epoch 41,step 231000, training loss 0.0222231\n",
      "epoch 41,step 252000, training loss 0.0305132\n",
      "epoch 41,step 273000, training loss 0.0469395\n",
      "epoch 41,step 294000, training loss 0.0243707\n",
      "epoch 41,step 315000, training loss 0.0355966\n",
      "epoch 41,step 336000, training loss 0.0266517\n",
      "epoch 41,step 357000, training loss 0.0216496\n",
      "epoch 41,step 378000, training loss 0.0206338\n",
      "epoch 41,step 399000, training loss 0.0184205\n",
      "epoch 41,step 420000, training loss 0.0280594\n",
      "epoch 41,step 441000, training loss 0.0243263\n",
      "epoch 41,step 462000, training loss 0.0294499\n",
      "epoch 41,step 483000, training loss 0.0393917\n",
      "epoch 41,step 504000, training loss 0.0330073\n",
      "epoch 41,step 525000, training loss 0.0287907\n",
      "epoch 41,step 546000, training loss 0.0172323\n",
      "epoch 41,step 567000, training loss 0.0399542\n",
      "epoch 41,step 588000, training loss 0.0261079\n",
      "epoch 41,step 609000, training loss 0.0317019\n",
      "epoch 41,step 630000, training loss 0.0237354\n",
      "epoch 41,step 651000, training loss 0.0371175\n",
      "epoch 41,step 672000, training loss 0.0222596\n",
      "epoch 41,step 693000, training loss 0.0256463\n",
      "epoch 41,step 714000, training loss 0.0259524\n",
      "epoch 41,step 735000, training loss 0.0318449\n",
      "epoch 41,step 756000, training loss 0.0259049\n",
      "epoch 41,step 777000, training loss 0.0310162\n",
      "epoch 41,step 798000, training loss 0.0296915\n",
      "epoch 41,step 819000, training loss 0.0267596\n",
      "epoch 41,step 840000, training loss 0.0227518\n",
      "epoch 41,step 861000, training loss 0.0287219\n",
      "epoch 41,step 882000, training loss 0.0226217\n",
      "epoch 41,step 903000, training loss 0.026112\n",
      "epoch 41,step 924000, training loss 0.0272428\n",
      "epoch 41,step 945000, training loss 0.0222068\n",
      "epoch 41,step 966000, training loss 0.0291298\n",
      "epoch 41,step 987000, training loss 0.0197773\n",
      "epoch 41,step 1008000, training loss 0.0239236\n",
      "epoch 41,step 1029000, training loss 0.0265857\n",
      "epoch 41,step 1050000, training loss 0.0239986\n",
      "epoch 41,step 1071000, training loss 0.0286768\n",
      "epoch 41,step 1092000, training loss 0.029254\n",
      "epoch 41,step 1113000, training loss 0.0190045\n",
      "epoch 41,step 1134000, training loss 0.0235854\n",
      "epoch 41,step 1155000, training loss 0.0413617\n",
      "epoch 41,step 1176000, training loss 0.0226896\n",
      "epoch 41,step 1197000, training loss 0.0194775\n",
      "epoch 41,step 1218000, training loss 0.0257066\n",
      "epoch 41,step 1239000, training loss 0.0309487\n",
      "epoch 41,step 1260000, training loss 0.0228734\n",
      "epoch 41,step 1281000, training loss 0.0284448\n",
      "epoch 41,step 1302000, training loss 0.0269423\n",
      "epoch 41,step 1323000, training loss 0.0247811\n",
      "epoch 41,step 1344000, training loss 0.0341093\n",
      "epoch 41,step 1365000, training loss 0.0374387\n",
      "epoch 41,step 1386000, training loss 0.0237932\n",
      "epoch 41,step 1407000, training loss 0.0252786\n",
      "epoch 41,step 1428000, training loss 0.0348361\n",
      "epoch 41,step 1449000, training loss 0.021136\n",
      "epoch 41,step 1470000, training loss 0.0266022\n",
      "epoch 41,step 1491000, training loss 0.0307858\n",
      "epoch 41,step 1512000, training loss 0.0182558\n",
      "epoch 41,step 1533000, training loss 0.040795\n",
      "epoch 41,step 1554000, training loss 0.0354227\n",
      "epoch 41,step 1575000, training loss 0.0304871\n",
      "epoch 41,step 1596000, training loss 0.0252244\n",
      "epoch 41,step 1617000, training loss 0.0253095\n",
      "epoch 41,step 1638000, training loss 0.0229496\n",
      "epoch 41,step 1659000, training loss 0.0299286\n",
      "epoch 41,step 1680000, training loss 0.0180394\n",
      "epoch 41,step 1701000, training loss 0.0207153\n",
      "epoch 41,step 1722000, training loss 0.0223108\n",
      "epoch 41,step 1743000, training loss 0.023397\n",
      "epoch 41,step 1764000, training loss 0.0226564\n",
      "epoch 41,step 1785000, training loss 0.0348708\n",
      "epoch 41,step 1806000, training loss 0.0246791\n",
      "epoch 41,step 1827000, training loss 0.0327055\n",
      "epoch 41,step 1848000, training loss 0.0263892\n",
      "epoch 41,step 1869000, training loss 0.0190099\n",
      "epoch 41,step 1890000, training loss 0.0246052\n",
      "epoch 41,step 1911000, training loss 0.0228775\n",
      "epoch 41,step 1932000, training loss 0.0236404\n",
      "epoch 41,step 1953000, training loss 0.0345902\n",
      "epoch 41,step 1974000, training loss 0.0268133\n",
      "epoch 41,step 1995000, training loss 0.0213457\n",
      "epoch 41,step 2016000, training loss 0.0251926\n",
      "epoch 41,step 2037000, training loss 0.0229334\n",
      "epoch 41,step 2058000, training loss 0.0283788\n",
      "epoch 41,step 2079000, training loss 0.0320038\n",
      "epoch 41,step 2100000, training loss 0.0252073\n",
      "epoch 41,step 2121000, training loss 0.0247337\n",
      "epoch 41,step 2142000, training loss 0.0221443\n",
      "epoch 41,step 2163000, training loss 0.031863\n",
      "epoch 41,step 2184000, training loss 0.019643\n",
      "epoch 41,step 2205000, training loss 0.030233\n",
      "epoch 41,step 2226000, training loss 0.034601\n",
      "epoch 41,step 2247000, training loss 0.0339071\n",
      "epoch 41,step 2268000, training loss 0.0266922\n",
      "epoch 41,step 2289000, training loss 0.0358717\n",
      "epoch 41,step 2310000, training loss 0.0450864\n",
      "epoch 41,step 2331000, training loss 0.0329504\n",
      "epoch 41,step 2352000, training loss 0.0352742\n",
      "epoch 41,step 2373000, training loss 0.0266675\n",
      "epoch 41,step 2394000, training loss 0.0215037\n",
      "epoch 41,step 2415000, training loss 0.0419959\n",
      "epoch 41,step 2436000, training loss 0.0199365\n",
      "epoch 41,step 2457000, training loss 0.0269086\n",
      "epoch 41,step 2478000, training loss 0.0214491\n",
      "epoch 41,step 2499000, training loss 0.0271218\n",
      "epoch 41,step 2520000, training loss 0.0318324\n",
      "epoch 41,step 2541000, training loss 0.0209042\n",
      "epoch 41,step 2562000, training loss 0.0206997\n",
      "epoch 41,step 2583000, training loss 0.0226763\n",
      "epoch 41,step 2604000, training loss 0.0316613\n",
      "epoch 41,step 2625000, training loss 0.0311112\n",
      "epoch 41,step 2646000, training loss 0.0264308\n",
      "epoch 41,step 2667000, training loss 0.0211915\n",
      "epoch 41,step 2688000, training loss 0.0279506\n",
      "epoch 41,step 2709000, training loss 0.0316956\n",
      "epoch 41,step 2730000, training loss 0.0275919\n",
      "epoch 41,step 2751000, training loss 0.023696\n",
      "epoch 41,step 2772000, training loss 0.0237563\n",
      "epoch 41,step 2793000, training loss 0.0232891\n",
      "epoch 41,step 2814000, training loss 0.0280811\n",
      "epoch 41,step 2835000, training loss 0.0226915\n",
      "epoch 41,step 2856000, training loss 0.0283605\n",
      "epoch 41,step 2877000, training loss 0.0254187\n",
      "epoch 41,step 2898000, training loss 0.0284438\n",
      "epoch 41,step 2919000, training loss 0.0288774\n",
      "epoch 41,step 2940000, training loss 0.0223871\n",
      "epoch 41,step 2961000, training loss 0.02506\n",
      "epoch 41,step 2982000, training loss 0.0238536\n",
      "epoch 41,step 3003000, training loss 0.0280994\n",
      "epoch 41,step 3024000, training loss 0.0239999\n",
      "epoch 41,step 3045000, training loss 0.0338136\n",
      "epoch 41,step 3066000, training loss 0.0228509\n",
      "epoch 41,step 3087000, training loss 0.0253258\n",
      "epoch 41,step 3108000, training loss 0.0335616\n",
      "epoch 41,step 3129000, training loss 0.0237766\n",
      "epoch 41,step 3150000, training loss 0.0534407\n",
      "epoch 41,step 3171000, training loss 0.0218871\n",
      "epoch 41,step 3192000, training loss 0.0213686\n",
      "epoch 41,step 3213000, training loss 0.0359085\n",
      "epoch 41,step 3234000, training loss 0.0184744\n",
      "epoch 41,step 3255000, training loss 0.0279074\n",
      "epoch 41,step 3276000, training loss 0.0204861\n",
      "epoch 41,step 3297000, training loss 0.0280309\n",
      "epoch 41,step 3318000, training loss 0.0258673\n",
      "epoch 41,step 3339000, training loss 0.029466\n",
      "epoch 41,step 3360000, training loss 0.0252304\n",
      "epoch 41,step 3381000, training loss 0.0281253\n",
      "epoch 41,step 3402000, training loss 0.0241085\n",
      "epoch 41,step 3423000, training loss 0.0312835\n",
      "epoch 41,step 3444000, training loss 0.0320146\n",
      "epoch 41,step 3465000, training loss 0.0408142\n",
      "epoch 41,step 3486000, training loss 0.029516\n",
      "epoch 41,step 3507000, training loss 0.0278523\n",
      "epoch 41,step 3528000, training loss 0.0232713\n",
      "epoch 41,step 3549000, training loss 0.0333847\n",
      "epoch 41,step 3570000, training loss 0.0301725\n",
      "epoch 41,step 3591000, training loss 0.0365307\n",
      "epoch 41,step 3612000, training loss 0.0361652\n",
      "epoch 41,step 3633000, training loss 0.0277325\n",
      "epoch 41,step 3654000, training loss 0.0335382\n",
      "epoch 41,step 3675000, training loss 0.0233661\n",
      "epoch 41,step 3696000, training loss 0.0237103\n",
      "epoch 41,step 3717000, training loss 0.0286834\n",
      "epoch 41,step 3738000, training loss 0.0226234\n",
      "epoch 41,step 3759000, training loss 0.0344313\n",
      "epoch 41,step 3780000, training loss 0.0557654\n",
      "epoch 41,step 3801000, training loss 0.0256715\n",
      "epoch 41,step 3822000, training loss 0.0239536\n",
      "epoch 41,step 3843000, training loss 0.0314696\n",
      "epoch 41,step 3864000, training loss 0.0241959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41,step 3885000, training loss 0.0193486\n",
      "epoch 41,step 3906000, training loss 0.0196962\n",
      "epoch 41,step 3927000, training loss 0.0322965\n",
      "epoch 41,step 3948000, training loss 0.0215136\n",
      "epoch 41,step 3969000, training loss 0.0256747\n",
      "epoch 41,step 3990000, training loss 0.0174077\n",
      "epoch 41,step 4011000, training loss 0.0231627\n",
      "epoch 41,step 4032000, training loss 0.0268844\n",
      "epoch 41,step 4053000, training loss 0.020117\n",
      "epoch 41,step 4074000, training loss 0.0263804\n",
      "epoch 41,step 4095000, training loss 0.0289185\n",
      "epoch 41,step 4116000, training loss 0.0236549\n",
      "epoch 41,step 4137000, training loss 0.0195787\n",
      "epoch 41,step 4158000, training loss 0.019898\n",
      "epoch 41,step 4179000, training loss 0.0199825\n",
      "epoch 41,training loss 0.0330392 ,test loss 0.0357823\n",
      "epoch 42,step 21500, training loss 0.0298698\n",
      "epoch 42,step 43000, training loss 0.0373264\n",
      "epoch 42,step 64500, training loss 0.0301928\n",
      "epoch 42,step 86000, training loss 0.0175711\n",
      "epoch 42,step 107500, training loss 0.0217026\n",
      "epoch 42,step 129000, training loss 0.0209132\n",
      "epoch 42,step 150500, training loss 0.025685\n",
      "epoch 42,step 172000, training loss 0.0205691\n",
      "epoch 42,step 193500, training loss 0.0238556\n",
      "epoch 42,step 215000, training loss 0.0219715\n",
      "epoch 42,step 236500, training loss 0.022388\n",
      "epoch 42,step 258000, training loss 0.0302859\n",
      "epoch 42,step 279500, training loss 0.0479717\n",
      "epoch 42,step 301000, training loss 0.0244702\n",
      "epoch 42,step 322500, training loss 0.0358041\n",
      "epoch 42,step 344000, training loss 0.0267572\n",
      "epoch 42,step 365500, training loss 0.0224911\n",
      "epoch 42,step 387000, training loss 0.0210387\n",
      "epoch 42,step 408500, training loss 0.0178615\n",
      "epoch 42,step 430000, training loss 0.0276545\n",
      "epoch 42,step 451500, training loss 0.0247159\n",
      "epoch 42,step 473000, training loss 0.028986\n",
      "epoch 42,step 494500, training loss 0.0395358\n",
      "epoch 42,step 516000, training loss 0.0315814\n",
      "epoch 42,step 537500, training loss 0.0282549\n",
      "epoch 42,step 559000, training loss 0.0171732\n",
      "epoch 42,step 580500, training loss 0.0410718\n",
      "epoch 42,step 602000, training loss 0.0257833\n",
      "epoch 42,step 623500, training loss 0.0301459\n",
      "epoch 42,step 645000, training loss 0.023389\n",
      "epoch 42,step 666500, training loss 0.0379919\n",
      "epoch 42,step 688000, training loss 0.0223488\n",
      "epoch 42,step 709500, training loss 0.0258667\n",
      "epoch 42,step 731000, training loss 0.0251276\n",
      "epoch 42,step 752500, training loss 0.0316297\n",
      "epoch 42,step 774000, training loss 0.0260032\n",
      "epoch 42,step 795500, training loss 0.03044\n",
      "epoch 42,step 817000, training loss 0.0285059\n",
      "epoch 42,step 838500, training loss 0.0262131\n",
      "epoch 42,step 860000, training loss 0.0225347\n",
      "epoch 42,step 881500, training loss 0.0283826\n",
      "epoch 42,step 903000, training loss 0.022314\n",
      "epoch 42,step 924500, training loss 0.0253946\n",
      "epoch 42,step 946000, training loss 0.0268925\n",
      "epoch 42,step 967500, training loss 0.0215102\n",
      "epoch 42,step 989000, training loss 0.0292168\n",
      "epoch 42,step 1010500, training loss 0.0191695\n",
      "epoch 42,step 1032000, training loss 0.0238455\n",
      "epoch 42,step 1053500, training loss 0.0272514\n",
      "epoch 42,step 1075000, training loss 0.0237628\n",
      "epoch 42,step 1096500, training loss 0.0285143\n",
      "epoch 42,step 1118000, training loss 0.0306163\n",
      "epoch 42,step 1139500, training loss 0.0201234\n",
      "epoch 42,step 1161000, training loss 0.0239605\n",
      "epoch 42,step 1182500, training loss 0.0405561\n",
      "epoch 42,step 1204000, training loss 0.0212946\n",
      "epoch 42,step 1225500, training loss 0.0194344\n",
      "epoch 42,step 1247000, training loss 0.0254096\n",
      "epoch 42,step 1268500, training loss 0.030756\n",
      "epoch 42,step 1290000, training loss 0.0226008\n",
      "epoch 42,step 1311500, training loss 0.0288039\n",
      "epoch 42,step 1333000, training loss 0.0260575\n",
      "epoch 42,step 1354500, training loss 0.0248769\n",
      "epoch 42,step 1376000, training loss 0.0341726\n",
      "epoch 42,step 1397500, training loss 0.0374643\n",
      "epoch 42,step 1419000, training loss 0.0234349\n",
      "epoch 42,step 1440500, training loss 0.0251888\n",
      "epoch 42,step 1462000, training loss 0.0353278\n",
      "epoch 42,step 1483500, training loss 0.0213443\n",
      "epoch 42,step 1505000, training loss 0.026497\n",
      "epoch 42,step 1526500, training loss 0.0317408\n",
      "epoch 42,step 1548000, training loss 0.0182125\n",
      "epoch 42,step 1569500, training loss 0.0402426\n",
      "epoch 42,step 1591000, training loss 0.0352789\n",
      "epoch 42,step 1612500, training loss 0.0297351\n",
      "epoch 42,step 1634000, training loss 0.0245063\n",
      "epoch 42,step 1655500, training loss 0.023611\n",
      "epoch 42,step 1677000, training loss 0.0234218\n",
      "epoch 42,step 1698500, training loss 0.0290085\n",
      "epoch 42,step 1720000, training loss 0.0179283\n",
      "epoch 42,step 1741500, training loss 0.020439\n",
      "epoch 42,step 1763000, training loss 0.0222657\n",
      "epoch 42,step 1784500, training loss 0.0234964\n",
      "epoch 42,step 1806000, training loss 0.0217555\n",
      "epoch 42,step 1827500, training loss 0.0341089\n",
      "epoch 42,step 1849000, training loss 0.0249433\n",
      "epoch 42,step 1870500, training loss 0.0322965\n",
      "epoch 42,step 1892000, training loss 0.025478\n",
      "epoch 42,step 1913500, training loss 0.0183874\n",
      "epoch 42,step 1935000, training loss 0.0247682\n",
      "epoch 42,step 1956500, training loss 0.0224904\n",
      "epoch 42,step 1978000, training loss 0.023093\n",
      "epoch 42,step 1999500, training loss 0.0332406\n",
      "epoch 42,step 2021000, training loss 0.0258026\n",
      "epoch 42,step 2042500, training loss 0.0209797\n",
      "epoch 42,step 2064000, training loss 0.0249323\n",
      "epoch 42,step 2085500, training loss 0.023451\n",
      "epoch 42,step 2107000, training loss 0.0290118\n",
      "epoch 42,step 2128500, training loss 0.0314544\n",
      "epoch 42,step 2150000, training loss 0.0252029\n",
      "epoch 42,step 2171500, training loss 0.0247544\n",
      "epoch 42,step 2193000, training loss 0.0222929\n",
      "epoch 42,step 2214500, training loss 0.0315497\n",
      "epoch 42,step 2236000, training loss 0.0194235\n",
      "epoch 42,step 2257500, training loss 0.0304727\n",
      "epoch 42,step 2279000, training loss 0.0349418\n",
      "epoch 42,step 2300500, training loss 0.0331382\n",
      "epoch 42,step 2322000, training loss 0.0259552\n",
      "epoch 42,step 2343500, training loss 0.0356757\n",
      "epoch 42,step 2365000, training loss 0.0434905\n",
      "epoch 42,step 2386500, training loss 0.0319518\n",
      "epoch 42,step 2408000, training loss 0.0342189\n",
      "epoch 42,step 2429500, training loss 0.0273197\n",
      "epoch 42,step 2451000, training loss 0.0218758\n",
      "epoch 42,step 2472500, training loss 0.0412231\n",
      "epoch 42,step 2494000, training loss 0.0191243\n",
      "epoch 42,step 2515500, training loss 0.0261415\n",
      "epoch 42,step 2537000, training loss 0.0209387\n",
      "epoch 42,step 2558500, training loss 0.026211\n",
      "epoch 42,step 2580000, training loss 0.0478922\n",
      "epoch 42,step 2601500, training loss 0.0211053\n",
      "epoch 42,step 2623000, training loss 0.0206724\n",
      "epoch 42,step 2644500, training loss 0.0228226\n",
      "epoch 42,step 2666000, training loss 0.0319202\n",
      "epoch 42,step 2687500, training loss 0.0300122\n",
      "epoch 42,step 2709000, training loss 0.026804\n",
      "epoch 42,step 2730500, training loss 0.0214166\n",
      "epoch 42,step 2752000, training loss 0.0280816\n",
      "epoch 42,step 2773500, training loss 0.0299476\n",
      "epoch 42,step 2795000, training loss 0.0275746\n",
      "epoch 42,step 2816500, training loss 0.0239224\n",
      "epoch 42,step 2838000, training loss 0.0234332\n",
      "epoch 42,step 2859500, training loss 0.0216896\n",
      "epoch 42,step 2881000, training loss 0.0279964\n",
      "epoch 42,step 2902500, training loss 0.0217338\n",
      "epoch 42,step 2924000, training loss 0.0274528\n",
      "epoch 42,step 2945500, training loss 0.024191\n",
      "epoch 42,step 2967000, training loss 0.0259761\n",
      "epoch 42,step 2988500, training loss 0.0273722\n",
      "epoch 42,step 3010000, training loss 0.0226557\n",
      "epoch 42,step 3031500, training loss 0.0249105\n",
      "epoch 42,step 3053000, training loss 0.0230585\n",
      "epoch 42,step 3074500, training loss 0.027254\n",
      "epoch 42,step 3096000, training loss 0.0226102\n",
      "epoch 42,step 3117500, training loss 0.0327131\n",
      "epoch 42,step 3139000, training loss 0.02207\n",
      "epoch 42,step 3160500, training loss 0.0257548\n",
      "epoch 42,step 3182000, training loss 0.0323295\n",
      "epoch 42,step 3203500, training loss 0.0229919\n",
      "epoch 42,step 3225000, training loss 0.052351\n",
      "epoch 42,step 3246500, training loss 0.0225334\n",
      "epoch 42,step 3268000, training loss 0.0213044\n",
      "epoch 42,step 3289500, training loss 0.0347202\n",
      "epoch 42,step 3311000, training loss 0.0192156\n",
      "epoch 42,step 3332500, training loss 0.0275311\n",
      "epoch 42,step 3354000, training loss 0.0199767\n",
      "epoch 42,step 3375500, training loss 0.0260623\n",
      "epoch 42,step 3397000, training loss 0.0256914\n",
      "epoch 42,step 3418500, training loss 0.0273868\n",
      "epoch 42,step 3440000, training loss 0.0236392\n",
      "epoch 42,step 3461500, training loss 0.0270019\n",
      "epoch 42,step 3483000, training loss 0.0230916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42,step 3504500, training loss 0.0293303\n",
      "epoch 42,step 3526000, training loss 0.0301274\n",
      "epoch 42,step 3547500, training loss 0.0393669\n",
      "epoch 42,step 3569000, training loss 0.028769\n",
      "epoch 42,step 3590500, training loss 0.0262409\n",
      "epoch 42,step 3612000, training loss 0.0228741\n",
      "epoch 42,step 3633500, training loss 0.0330644\n",
      "epoch 42,step 3655000, training loss 0.0302465\n",
      "epoch 42,step 3676500, training loss 0.0360315\n",
      "epoch 42,step 3698000, training loss 0.0362274\n",
      "epoch 42,step 3719500, training loss 0.0278243\n",
      "epoch 42,step 3741000, training loss 0.033602\n",
      "epoch 42,step 3762500, training loss 0.0238609\n",
      "epoch 42,step 3784000, training loss 0.0231775\n",
      "epoch 42,step 3805500, training loss 0.0259728\n",
      "epoch 42,step 3827000, training loss 0.0214147\n",
      "epoch 42,step 3848500, training loss 0.0324471\n",
      "epoch 42,step 3870000, training loss 0.0536539\n",
      "epoch 42,step 3891500, training loss 0.0254749\n",
      "epoch 42,step 3913000, training loss 0.0235844\n",
      "epoch 42,step 3934500, training loss 0.0314032\n",
      "epoch 42,step 3956000, training loss 0.0243402\n",
      "epoch 42,step 3977500, training loss 0.0206675\n",
      "epoch 42,step 3999000, training loss 0.0198006\n",
      "epoch 42,step 4020500, training loss 0.0322188\n",
      "epoch 42,step 4042000, training loss 0.0253356\n",
      "epoch 42,step 4063500, training loss 0.0271721\n",
      "epoch 42,step 4085000, training loss 0.0176116\n",
      "epoch 42,step 4106500, training loss 0.022967\n",
      "epoch 42,step 4128000, training loss 0.027365\n",
      "epoch 42,step 4149500, training loss 0.0197265\n",
      "epoch 42,step 4171000, training loss 0.0260078\n",
      "epoch 42,step 4192500, training loss 0.0289218\n",
      "epoch 42,step 4214000, training loss 0.0236273\n",
      "epoch 42,step 4235500, training loss 0.0187795\n",
      "epoch 42,step 4257000, training loss 0.0195372\n",
      "epoch 42,step 4278500, training loss 0.0197198\n",
      "epoch 42,training loss 0.0323098 ,test loss 0.0354709\n",
      "epoch 43,step 22000, training loss 0.0300921\n",
      "epoch 43,step 44000, training loss 0.0355011\n",
      "epoch 43,step 66000, training loss 0.0299717\n",
      "epoch 43,step 88000, training loss 0.0175588\n",
      "epoch 43,step 110000, training loss 0.0210152\n",
      "epoch 43,step 132000, training loss 0.0202968\n",
      "epoch 43,step 154000, training loss 0.0254362\n",
      "epoch 43,step 176000, training loss 0.0205863\n",
      "epoch 43,step 198000, training loss 0.0233816\n",
      "epoch 43,step 220000, training loss 0.022037\n",
      "epoch 43,step 242000, training loss 0.0227888\n",
      "epoch 43,step 264000, training loss 0.0301611\n",
      "epoch 43,step 286000, training loss 0.0464981\n",
      "epoch 43,step 308000, training loss 0.0239049\n",
      "epoch 43,step 330000, training loss 0.0372047\n",
      "epoch 43,step 352000, training loss 0.026517\n",
      "epoch 43,step 374000, training loss 0.0210835\n",
      "epoch 43,step 396000, training loss 0.0203522\n",
      "epoch 43,step 418000, training loss 0.0186243\n",
      "epoch 43,step 440000, training loss 0.0280895\n",
      "epoch 43,step 462000, training loss 0.0250435\n",
      "epoch 43,step 484000, training loss 0.0296034\n",
      "epoch 43,step 506000, training loss 0.0390449\n",
      "epoch 43,step 528000, training loss 0.0318203\n",
      "epoch 43,step 550000, training loss 0.0281012\n",
      "epoch 43,step 572000, training loss 0.0171938\n",
      "epoch 43,step 594000, training loss 0.0394804\n",
      "epoch 43,step 616000, training loss 0.025495\n",
      "epoch 43,step 638000, training loss 0.0296808\n",
      "epoch 43,step 660000, training loss 0.0231236\n",
      "epoch 43,step 682000, training loss 0.0365926\n",
      "epoch 43,step 704000, training loss 0.0216675\n",
      "epoch 43,step 726000, training loss 0.0262589\n",
      "epoch 43,step 748000, training loss 0.0262165\n",
      "epoch 43,step 770000, training loss 0.0303988\n",
      "epoch 43,step 792000, training loss 0.0245163\n",
      "epoch 43,step 814000, training loss 0.0308505\n",
      "epoch 43,step 836000, training loss 0.0285864\n",
      "epoch 43,step 858000, training loss 0.0270706\n",
      "epoch 43,step 880000, training loss 0.0225778\n",
      "epoch 43,step 902000, training loss 0.0278048\n",
      "epoch 43,step 924000, training loss 0.0220152\n",
      "epoch 43,step 946000, training loss 0.0254089\n",
      "epoch 43,step 968000, training loss 0.0260857\n",
      "epoch 43,step 990000, training loss 0.0212215\n",
      "epoch 43,step 1012000, training loss 0.0284916\n",
      "epoch 43,step 1034000, training loss 0.0185597\n",
      "epoch 43,step 1056000, training loss 0.0232547\n",
      "epoch 43,step 1078000, training loss 0.0269446\n",
      "epoch 43,step 1100000, training loss 0.0235247\n",
      "epoch 43,step 1122000, training loss 0.027179\n",
      "epoch 43,step 1144000, training loss 0.0294683\n",
      "epoch 43,step 1166000, training loss 0.020339\n",
      "epoch 43,step 1188000, training loss 0.0236706\n",
      "epoch 43,step 1210000, training loss 0.0406871\n",
      "epoch 43,step 1232000, training loss 0.0223124\n",
      "epoch 43,step 1254000, training loss 0.0197781\n",
      "epoch 43,step 1276000, training loss 0.02577\n",
      "epoch 43,step 1298000, training loss 0.0311336\n",
      "epoch 43,step 1320000, training loss 0.023359\n",
      "epoch 43,step 1342000, training loss 0.028979\n",
      "epoch 43,step 1364000, training loss 0.0274537\n",
      "epoch 43,step 1386000, training loss 0.0243895\n",
      "epoch 43,step 1408000, training loss 0.0340204\n",
      "epoch 43,step 1430000, training loss 0.03689\n",
      "epoch 43,step 1452000, training loss 0.0235357\n",
      "epoch 43,step 1474000, training loss 0.023973\n",
      "epoch 43,step 1496000, training loss 0.0344792\n",
      "epoch 43,step 1518000, training loss 0.0213138\n",
      "epoch 43,step 1540000, training loss 0.0257953\n",
      "epoch 43,step 1562000, training loss 0.030694\n",
      "epoch 43,step 1584000, training loss 0.0183138\n",
      "epoch 43,step 1606000, training loss 0.0392594\n",
      "epoch 43,step 1628000, training loss 0.0347928\n",
      "epoch 43,step 1650000, training loss 0.0303477\n",
      "epoch 43,step 1672000, training loss 0.0247293\n",
      "epoch 43,step 1694000, training loss 0.0245658\n",
      "epoch 43,step 1716000, training loss 0.0230557\n",
      "epoch 43,step 1738000, training loss 0.0294392\n",
      "epoch 43,step 1760000, training loss 0.0180511\n",
      "epoch 43,step 1782000, training loss 0.0210203\n",
      "epoch 43,step 1804000, training loss 0.0218924\n",
      "epoch 43,step 1826000, training loss 0.0223552\n",
      "epoch 43,step 1848000, training loss 0.0220334\n",
      "epoch 43,step 1870000, training loss 0.0329094\n",
      "epoch 43,step 1892000, training loss 0.0240151\n",
      "epoch 43,step 1914000, training loss 0.0324113\n",
      "epoch 43,step 1936000, training loss 0.0251664\n",
      "epoch 43,step 1958000, training loss 0.0185705\n",
      "epoch 43,step 1980000, training loss 0.0241921\n",
      "epoch 43,step 2002000, training loss 0.0225556\n",
      "epoch 43,step 2024000, training loss 0.0236333\n",
      "epoch 43,step 2046000, training loss 0.0344067\n",
      "epoch 43,step 2068000, training loss 0.0270016\n",
      "epoch 43,step 2090000, training loss 0.022532\n",
      "epoch 43,step 2112000, training loss 0.0254141\n",
      "epoch 43,step 2134000, training loss 0.0237262\n",
      "epoch 43,step 2156000, training loss 0.0290682\n",
      "epoch 43,step 2178000, training loss 0.0333546\n",
      "epoch 43,step 2200000, training loss 0.0255558\n",
      "epoch 43,step 2222000, training loss 0.0247396\n",
      "epoch 43,step 2244000, training loss 0.0228152\n",
      "epoch 43,step 2266000, training loss 0.0317987\n",
      "epoch 43,step 2288000, training loss 0.0194968\n",
      "epoch 43,step 2310000, training loss 0.03103\n",
      "epoch 43,step 2332000, training loss 0.0354928\n",
      "epoch 43,step 2354000, training loss 0.0325189\n",
      "epoch 43,step 2376000, training loss 0.0258486\n",
      "epoch 43,step 2398000, training loss 0.0343088\n",
      "epoch 43,step 2420000, training loss 0.0442024\n",
      "epoch 43,step 2442000, training loss 0.0336243\n",
      "epoch 43,step 2464000, training loss 0.0373084\n",
      "epoch 43,step 2486000, training loss 0.0269039\n",
      "epoch 43,step 2508000, training loss 0.0212\n",
      "epoch 43,step 2530000, training loss 0.0402893\n",
      "epoch 43,step 2552000, training loss 0.0203718\n",
      "epoch 43,step 2574000, training loss 0.0250924\n",
      "epoch 43,step 2596000, training loss 0.0212646\n",
      "epoch 43,step 2618000, training loss 0.0265843\n",
      "epoch 43,step 2640000, training loss 0.031508\n",
      "epoch 43,step 2662000, training loss 0.0221257\n",
      "epoch 43,step 2684000, training loss 0.0204679\n",
      "epoch 43,step 2706000, training loss 0.0222517\n",
      "epoch 43,step 2728000, training loss 0.0322987\n",
      "epoch 43,step 2750000, training loss 0.0308445\n",
      "epoch 43,step 2772000, training loss 0.0273898\n",
      "epoch 43,step 2794000, training loss 0.0213811\n",
      "epoch 43,step 2816000, training loss 0.0277761\n",
      "epoch 43,step 2838000, training loss 0.0308064\n",
      "epoch 43,step 2860000, training loss 0.0266442\n",
      "epoch 43,step 2882000, training loss 0.0235811\n",
      "epoch 43,step 2904000, training loss 0.02351\n",
      "epoch 43,step 2926000, training loss 0.0224222\n",
      "epoch 43,step 2948000, training loss 0.028763\n",
      "epoch 43,step 2970000, training loss 0.0215398\n",
      "epoch 43,step 2992000, training loss 0.0279832\n",
      "epoch 43,step 3014000, training loss 0.0249582\n",
      "epoch 43,step 3036000, training loss 0.0258906\n",
      "epoch 43,step 3058000, training loss 0.0272902\n",
      "epoch 43,step 3080000, training loss 0.0223356\n",
      "epoch 43,step 3102000, training loss 0.0248417\n",
      "epoch 43,step 3124000, training loss 0.022495\n",
      "epoch 43,step 3146000, training loss 0.0278652\n",
      "epoch 43,step 3168000, training loss 0.0227188\n",
      "epoch 43,step 3190000, training loss 0.0333301\n",
      "epoch 43,step 3212000, training loss 0.021419\n",
      "epoch 43,step 3234000, training loss 0.026754\n",
      "epoch 43,step 3256000, training loss 0.0319557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43,step 3278000, training loss 0.023954\n",
      "epoch 43,step 3300000, training loss 0.0524299\n",
      "epoch 43,step 3322000, training loss 0.0227692\n",
      "epoch 43,step 3344000, training loss 0.0206749\n",
      "epoch 43,step 3366000, training loss 0.0334034\n",
      "epoch 43,step 3388000, training loss 0.0189321\n",
      "epoch 43,step 3410000, training loss 0.0286187\n",
      "epoch 43,step 3432000, training loss 0.0203806\n",
      "epoch 43,step 3454000, training loss 0.025771\n",
      "epoch 43,step 3476000, training loss 0.0249088\n",
      "epoch 43,step 3498000, training loss 0.027195\n",
      "epoch 43,step 3520000, training loss 0.0230096\n",
      "epoch 43,step 3542000, training loss 0.0270345\n",
      "epoch 43,step 3564000, training loss 0.0224831\n",
      "epoch 43,step 3586000, training loss 0.028281\n",
      "epoch 43,step 3608000, training loss 0.0292437\n",
      "epoch 43,step 3630000, training loss 0.039811\n",
      "epoch 43,step 3652000, training loss 0.0296125\n",
      "epoch 43,step 3674000, training loss 0.0255434\n",
      "epoch 43,step 3696000, training loss 0.0223439\n",
      "epoch 43,step 3718000, training loss 0.0332554\n",
      "epoch 43,step 3740000, training loss 0.029542\n",
      "epoch 43,step 3762000, training loss 0.0357446\n",
      "epoch 43,step 3784000, training loss 0.0352342\n",
      "epoch 43,step 3806000, training loss 0.026156\n",
      "epoch 43,step 3828000, training loss 0.0324606\n",
      "epoch 43,step 3850000, training loss 0.0233466\n",
      "epoch 43,step 3872000, training loss 0.0230609\n",
      "epoch 43,step 3894000, training loss 0.0267602\n",
      "epoch 43,step 3916000, training loss 0.0213161\n",
      "epoch 43,step 3938000, training loss 0.032017\n",
      "epoch 43,step 3960000, training loss 0.0532926\n",
      "epoch 43,step 3982000, training loss 0.0251325\n",
      "epoch 43,step 4004000, training loss 0.0233852\n",
      "epoch 43,step 4026000, training loss 0.0303516\n",
      "epoch 43,step 4048000, training loss 0.0233807\n",
      "epoch 43,step 4070000, training loss 0.0190837\n",
      "epoch 43,step 4092000, training loss 0.019601\n",
      "epoch 43,step 4114000, training loss 0.0322133\n",
      "epoch 43,step 4136000, training loss 0.0212777\n",
      "epoch 43,step 4158000, training loss 0.0254361\n",
      "epoch 43,step 4180000, training loss 0.0176759\n",
      "epoch 43,step 4202000, training loss 0.0231813\n",
      "epoch 43,step 4224000, training loss 0.0265915\n",
      "epoch 43,step 4246000, training loss 0.0202623\n",
      "epoch 43,step 4268000, training loss 0.0257938\n",
      "epoch 43,step 4290000, training loss 0.0287788\n",
      "epoch 43,step 4312000, training loss 0.0227479\n",
      "epoch 43,step 4334000, training loss 0.0180945\n",
      "epoch 43,step 4356000, training loss 0.0190928\n",
      "epoch 43,step 4378000, training loss 0.0192748\n",
      "epoch 43,training loss 0.0320266 ,test loss 0.0346835\n",
      "epoch 44,step 4500, training loss 0.0909107\n",
      "epoch 44,step 9000, training loss 0.0575305\n",
      "epoch 44,step 13500, training loss 0.0268552\n",
      "epoch 44,step 18000, training loss 0.0192266\n",
      "epoch 44,step 22500, training loss 0.0294301\n",
      "epoch 44,step 27000, training loss 0.0202349\n",
      "epoch 44,step 31500, training loss 0.0189841\n",
      "epoch 44,step 36000, training loss 0.0195477\n",
      "epoch 44,step 40500, training loss 0.025596\n",
      "epoch 44,step 45000, training loss 0.0357873\n",
      "epoch 44,step 49500, training loss 0.109695\n",
      "epoch 44,step 54000, training loss 0.0579007\n",
      "epoch 44,step 58500, training loss 0.0355833\n",
      "epoch 44,step 63000, training loss 0.0366911\n",
      "epoch 44,step 67500, training loss 0.0306809\n",
      "epoch 44,step 72000, training loss 0.0216929\n",
      "epoch 44,step 76500, training loss 0.0338516\n",
      "epoch 44,step 81000, training loss 0.0242371\n",
      "epoch 44,step 85500, training loss 0.0427208\n",
      "epoch 44,step 90000, training loss 0.0174938\n",
      "epoch 44,step 94500, training loss 0.0812182\n",
      "epoch 44,step 99000, training loss 0.0372874\n",
      "epoch 44,step 103500, training loss 0.0361875\n",
      "epoch 44,step 108000, training loss 0.0320499\n",
      "epoch 44,step 112500, training loss 0.0214115\n",
      "epoch 44,step 117000, training loss 0.0263041\n",
      "epoch 44,step 121500, training loss 0.0216436\n",
      "epoch 44,step 126000, training loss 0.0195275\n",
      "epoch 44,step 130500, training loss 0.0244296\n",
      "epoch 44,step 135000, training loss 0.0205156\n",
      "epoch 44,step 139500, training loss 0.0641437\n",
      "epoch 44,step 144000, training loss 0.0361534\n",
      "epoch 44,step 148500, training loss 0.0288637\n",
      "epoch 44,step 153000, training loss 0.0255454\n",
      "epoch 44,step 157500, training loss 0.0248551\n",
      "epoch 44,step 162000, training loss 0.037693\n",
      "epoch 44,step 166500, training loss 0.0230304\n",
      "epoch 44,step 171000, training loss 0.0232079\n",
      "epoch 44,step 175500, training loss 0.0197061\n",
      "epoch 44,step 180000, training loss 0.0202055\n",
      "epoch 44,step 184500, training loss 0.0654544\n",
      "epoch 44,step 189000, training loss 0.0321879\n",
      "epoch 44,step 193500, training loss 0.0208205\n",
      "epoch 44,step 198000, training loss 0.0338464\n",
      "epoch 44,step 202500, training loss 0.023844\n",
      "epoch 44,step 207000, training loss 0.0227079\n",
      "epoch 44,step 211500, training loss 0.0297921\n",
      "epoch 44,step 216000, training loss 0.0219126\n",
      "epoch 44,step 220500, training loss 0.0215712\n",
      "epoch 44,step 225000, training loss 0.0222227\n",
      "epoch 44,step 229500, training loss 0.0737309\n",
      "epoch 44,step 234000, training loss 0.0580049\n",
      "epoch 44,step 238500, training loss 0.0291663\n",
      "epoch 44,step 243000, training loss 0.0242246\n",
      "epoch 44,step 247500, training loss 0.0222435\n",
      "epoch 44,step 252000, training loss 0.0224634\n",
      "epoch 44,step 256500, training loss 0.0191417\n",
      "epoch 44,step 261000, training loss 0.0299341\n",
      "epoch 44,step 265500, training loss 0.0332564\n",
      "epoch 44,step 270000, training loss 0.0297289\n",
      "epoch 44,step 274500, training loss 0.0686135\n",
      "epoch 44,step 279000, training loss 0.0470078\n",
      "epoch 44,step 283500, training loss 0.0274976\n",
      "epoch 44,step 288000, training loss 0.0252374\n",
      "epoch 44,step 292500, training loss 0.0461757\n",
      "epoch 44,step 297000, training loss 0.0425399\n",
      "epoch 44,step 301500, training loss 0.0329398\n",
      "epoch 44,step 306000, training loss 0.0279779\n",
      "epoch 44,step 310500, training loss 0.0211576\n",
      "epoch 44,step 315000, training loss 0.0236023\n",
      "epoch 44,step 319500, training loss 0.0745728\n",
      "epoch 44,step 324000, training loss 0.0322551\n",
      "epoch 44,step 328500, training loss 0.0244949\n",
      "epoch 44,step 333000, training loss 0.0221251\n",
      "epoch 44,step 337500, training loss 0.036412\n",
      "epoch 44,step 342000, training loss 0.0317597\n",
      "epoch 44,step 346500, training loss 0.0293465\n",
      "epoch 44,step 351000, training loss 0.0208277\n",
      "epoch 44,step 355500, training loss 0.0248269\n",
      "epoch 44,step 360000, training loss 0.0268585\n",
      "epoch 44,step 364500, training loss 0.0856396\n",
      "epoch 44,step 369000, training loss 0.0385583\n",
      "epoch 44,step 373500, training loss 0.0602757\n",
      "epoch 44,step 378000, training loss 0.0232262\n",
      "epoch 44,step 382500, training loss 0.0211163\n",
      "epoch 44,step 387000, training loss 0.0212019\n",
      "epoch 44,step 391500, training loss 0.0236632\n",
      "epoch 44,step 396000, training loss 0.0216804\n",
      "epoch 44,step 400500, training loss 0.0181122\n",
      "epoch 44,step 405000, training loss 0.0210181\n",
      "epoch 44,step 409500, training loss 0.0718118\n",
      "epoch 44,step 414000, training loss 0.0568661\n",
      "epoch 44,step 418500, training loss 0.029983\n",
      "epoch 44,step 423000, training loss 0.0262881\n",
      "epoch 44,step 427500, training loss 0.0179359\n",
      "epoch 44,step 432000, training loss 0.02276\n",
      "epoch 44,step 436500, training loss 0.0213935\n",
      "epoch 44,step 441000, training loss 0.0228007\n",
      "epoch 44,step 445500, training loss 0.0256697\n",
      "epoch 44,step 450000, training loss 0.0277967\n",
      "epoch 44,step 454500, training loss 0.0815734\n",
      "epoch 44,step 459000, training loss 0.0371504\n",
      "epoch 44,step 463500, training loss 0.0258189\n",
      "epoch 44,step 468000, training loss 0.0199543\n",
      "epoch 44,step 472500, training loss 0.0234929\n",
      "epoch 44,step 477000, training loss 0.0230541\n",
      "epoch 44,step 481500, training loss 0.0164089\n",
      "epoch 44,step 486000, training loss 0.0328528\n",
      "epoch 44,step 490500, training loss 0.030162\n",
      "epoch 44,step 495000, training loss 0.0281118\n",
      "epoch 44,step 499500, training loss 0.0767061\n",
      "epoch 44,step 504000, training loss 0.0515737\n",
      "epoch 44,step 508500, training loss 0.0282005\n",
      "epoch 44,step 513000, training loss 0.0404983\n",
      "epoch 44,step 517500, training loss 0.0387379\n",
      "epoch 44,step 522000, training loss 0.0489195\n",
      "epoch 44,step 526500, training loss 0.0287924\n",
      "epoch 44,step 531000, training loss 0.0455934\n",
      "epoch 44,step 535500, training loss 0.0497045\n",
      "epoch 44,step 540000, training loss 0.0319727\n",
      "epoch 44,step 544500, training loss 0.0706187\n",
      "epoch 44,step 549000, training loss 0.0434202\n",
      "epoch 44,step 553500, training loss 0.028442\n",
      "epoch 44,step 558000, training loss 0.0246663\n",
      "epoch 44,step 562500, training loss 0.0286717\n",
      "epoch 44,step 567000, training loss 0.0193027\n",
      "epoch 44,step 571500, training loss 0.025973\n",
      "epoch 44,step 576000, training loss 0.0243983\n",
      "epoch 44,step 580500, training loss 0.0186021\n",
      "epoch 44,step 585000, training loss 0.0167861\n",
      "epoch 44,step 589500, training loss 0.0673406\n",
      "epoch 44,step 594000, training loss 0.0559389\n",
      "epoch 44,step 598500, training loss 0.024657\n",
      "epoch 44,step 603000, training loss 0.0247338\n",
      "epoch 44,step 607500, training loss 0.0389574\n",
      "epoch 44,step 612000, training loss 0.0185266\n",
      "epoch 44,step 616500, training loss 0.0372349\n",
      "epoch 44,step 621000, training loss 0.0208595\n",
      "epoch 44,step 625500, training loss 0.0293902\n",
      "epoch 44,step 630000, training loss 0.0248552\n",
      "epoch 44,step 634500, training loss 0.106628\n",
      "epoch 44,step 639000, training loss 0.0677823\n",
      "epoch 44,step 643500, training loss 0.0382709\n",
      "epoch 44,step 648000, training loss 0.0240025\n",
      "epoch 44,step 652500, training loss 0.0296547\n",
      "epoch 44,step 657000, training loss 0.0218745\n",
      "epoch 44,step 661500, training loss 0.0332332\n",
      "epoch 44,step 666000, training loss 0.0233577\n",
      "epoch 44,step 670500, training loss 0.0242909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44,step 675000, training loss 0.02301\n",
      "epoch 44,step 679500, training loss 0.0687024\n",
      "epoch 44,step 684000, training loss 0.0538467\n",
      "epoch 44,step 688500, training loss 0.0348115\n",
      "epoch 44,step 693000, training loss 0.0316342\n",
      "epoch 44,step 697500, training loss 0.0355212\n",
      "epoch 44,step 702000, training loss 0.0352796\n",
      "epoch 44,step 706500, training loss 0.0279871\n",
      "epoch 44,step 711000, training loss 0.0188516\n",
      "epoch 44,step 715500, training loss 0.0272928\n",
      "epoch 44,step 720000, training loss 0.0217794\n",
      "epoch 44,step 724500, training loss 0.0623862\n",
      "epoch 44,step 729000, training loss 0.0399339\n",
      "epoch 44,step 733500, training loss 0.0260689\n",
      "epoch 44,step 738000, training loss 0.018555\n",
      "epoch 44,step 742500, training loss 0.0255035\n",
      "epoch 44,step 747000, training loss 0.0188937\n",
      "epoch 44,step 751500, training loss 0.0217254\n",
      "epoch 44,step 756000, training loss 0.0279432\n",
      "epoch 44,step 760500, training loss 0.020004\n",
      "epoch 44,step 765000, training loss 0.0243514\n",
      "epoch 44,step 769500, training loss 0.0954636\n",
      "epoch 44,step 774000, training loss 0.0427741\n",
      "epoch 44,step 778500, training loss 0.0311943\n",
      "epoch 44,step 783000, training loss 0.0458408\n",
      "epoch 44,step 787500, training loss 0.0299279\n",
      "epoch 44,step 792000, training loss 0.025504\n",
      "epoch 44,step 796500, training loss 0.035981\n",
      "epoch 44,step 801000, training loss 0.0272958\n",
      "epoch 44,step 805500, training loss 0.0262471\n",
      "epoch 44,step 810000, training loss 0.024623\n",
      "epoch 44,step 814500, training loss 0.0672893\n",
      "epoch 44,step 819000, training loss 0.0439485\n",
      "epoch 44,step 823500, training loss 0.0317564\n",
      "epoch 44,step 828000, training loss 0.0371977\n",
      "epoch 44,step 832500, training loss 0.0310059\n",
      "epoch 44,step 837000, training loss 0.0587073\n",
      "epoch 44,step 841500, training loss 0.0327562\n",
      "epoch 44,step 846000, training loss 0.0319075\n",
      "epoch 44,step 850500, training loss 0.0286046\n",
      "epoch 44,step 855000, training loss 0.0273925\n",
      "epoch 44,step 859500, training loss 0.063582\n",
      "epoch 44,step 864000, training loss 0.0342558\n",
      "epoch 44,step 868500, training loss 0.0207859\n",
      "epoch 44,step 873000, training loss 0.0228928\n",
      "epoch 44,step 877500, training loss 0.0266082\n",
      "epoch 44,step 882000, training loss 0.0282831\n",
      "epoch 44,step 886500, training loss 0.0200058\n",
      "epoch 44,step 891000, training loss 0.0222371\n",
      "epoch 44,step 895500, training loss 0.0226306\n",
      "epoch 44,step 900000, training loss 0.0222342\n",
      "epoch 44,step 904500, training loss 0.0655246\n",
      "epoch 44,step 909000, training loss 0.0637749\n",
      "epoch 44,step 913500, training loss 0.0308893\n",
      "epoch 44,step 918000, training loss 0.0224763\n",
      "epoch 44,step 922500, training loss 0.0276778\n",
      "epoch 44,step 927000, training loss 0.0353217\n",
      "epoch 44,step 931500, training loss 0.0228369\n",
      "epoch 44,step 936000, training loss 0.0191897\n",
      "epoch 44,step 940500, training loss 0.0211893\n",
      "epoch 44,step 945000, training loss 0.0220027\n",
      "epoch 44,step 949500, training loss 0.0872872\n",
      "epoch 44,step 954000, training loss 0.0366393\n",
      "epoch 44,step 958500, training loss 0.0286137\n",
      "epoch 44,step 963000, training loss 0.0276866\n",
      "epoch 44,step 967500, training loss 0.0249984\n",
      "epoch 44,step 972000, training loss 0.0265282\n",
      "epoch 44,step 976500, training loss 0.0250724\n",
      "epoch 44,step 981000, training loss 0.0294068\n",
      "epoch 44,step 985500, training loss 0.026056\n",
      "epoch 44,step 990000, training loss 0.0266194\n",
      "epoch 44,step 994500, training loss 0.0868361\n",
      "epoch 44,step 999000, training loss 0.0436986\n",
      "epoch 44,step 1003500, training loss 0.0368001\n",
      "epoch 44,step 1008000, training loss 0.0295341\n",
      "epoch 44,step 1012500, training loss 0.021423\n",
      "epoch 44,step 1017000, training loss 0.0204035\n",
      "epoch 44,step 1021500, training loss 0.0263026\n",
      "epoch 44,step 1026000, training loss 0.0257757\n",
      "epoch 44,step 1030500, training loss 0.0225606\n",
      "epoch 44,step 1035000, training loss 0.0288852\n",
      "epoch 44,step 1039500, training loss 0.0674042\n",
      "epoch 44,step 1044000, training loss 0.0390256\n",
      "epoch 44,step 1048500, training loss 0.0224469\n",
      "epoch 44,step 1053000, training loss 0.0259831\n",
      "epoch 44,step 1057500, training loss 0.0192874\n",
      "epoch 44,step 1062000, training loss 0.0196945\n",
      "epoch 44,step 1066500, training loss 0.0270688\n",
      "epoch 44,step 1071000, training loss 0.0215846\n",
      "epoch 44,step 1075500, training loss 0.0240124\n",
      "epoch 44,step 1080000, training loss 0.0231943\n",
      "epoch 44,step 1084500, training loss 0.0849606\n",
      "epoch 44,step 1089000, training loss 0.0332414\n",
      "epoch 44,step 1093500, training loss 0.0308681\n",
      "epoch 44,step 1098000, training loss 0.0310584\n",
      "epoch 44,step 1102500, training loss 0.0273415\n",
      "epoch 44,step 1107000, training loss 0.0330258\n",
      "epoch 44,step 1111500, training loss 0.024313\n",
      "epoch 44,step 1116000, training loss 0.0233314\n",
      "epoch 44,step 1120500, training loss 0.0283088\n",
      "epoch 44,step 1125000, training loss 0.0241133\n",
      "epoch 44,step 1129500, training loss 0.129679\n",
      "epoch 44,step 1134000, training loss 0.0412666\n",
      "epoch 44,step 1138500, training loss 0.0198451\n",
      "epoch 44,step 1143000, training loss 0.0278881\n",
      "epoch 44,step 1147500, training loss 0.0289883\n",
      "epoch 44,step 1152000, training loss 0.0166969\n",
      "epoch 44,step 1156500, training loss 0.0288317\n",
      "epoch 44,step 1161000, training loss 0.0252315\n",
      "epoch 44,step 1165500, training loss 0.0391523\n",
      "epoch 44,step 1170000, training loss 0.0309868\n",
      "epoch 44,step 1174500, training loss 0.100954\n",
      "epoch 44,step 1179000, training loss 0.0595319\n",
      "epoch 44,step 1183500, training loss 0.0486299\n",
      "epoch 44,step 1188000, training loss 0.0279786\n",
      "epoch 44,step 1192500, training loss 0.0191528\n",
      "epoch 44,step 1197000, training loss 0.0205664\n",
      "epoch 44,step 1201500, training loss 0.0230196\n",
      "epoch 44,step 1206000, training loss 0.0225873\n",
      "epoch 44,step 1210500, training loss 0.0198368\n",
      "epoch 44,step 1215000, training loss 0.0230644\n",
      "epoch 44,step 1219500, training loss 0.0793693\n",
      "epoch 44,step 1224000, training loss 0.0334226\n",
      "epoch 44,step 1228500, training loss 0.0286073\n",
      "epoch 44,step 1233000, training loss 0.0368966\n",
      "epoch 44,step 1237500, training loss 0.0403449\n",
      "epoch 44,step 1242000, training loss 0.0269718\n",
      "epoch 44,step 1246500, training loss 0.0201842\n",
      "epoch 44,step 1251000, training loss 0.0218388\n",
      "epoch 44,step 1255500, training loss 0.0235706\n",
      "epoch 44,step 1260000, training loss 0.0215947\n",
      "epoch 44,step 1264500, training loss 0.0918092\n",
      "epoch 44,step 1269000, training loss 0.045366\n",
      "epoch 44,step 1273500, training loss 0.0212044\n",
      "epoch 44,step 1278000, training loss 0.0232375\n",
      "epoch 44,step 1282500, training loss 0.0191814\n",
      "epoch 44,step 1287000, training loss 0.0215401\n",
      "epoch 44,step 1291500, training loss 0.0295748\n",
      "epoch 44,step 1296000, training loss 0.0210068\n",
      "epoch 44,step 1300500, training loss 0.0232815\n",
      "epoch 44,step 1305000, training loss 0.0252988\n",
      "epoch 44,step 1309500, training loss 0.0758347\n",
      "epoch 44,step 1314000, training loss 0.0328085\n",
      "epoch 44,step 1318500, training loss 0.0244897\n",
      "epoch 44,step 1323000, training loss 0.036995\n",
      "epoch 44,step 1327500, training loss 0.0303407\n",
      "epoch 44,step 1332000, training loss 0.0303478\n",
      "epoch 44,step 1336500, training loss 0.0227944\n",
      "epoch 44,step 1341000, training loss 0.0293544\n",
      "epoch 44,step 1345500, training loss 0.0233524\n",
      "epoch 44,step 1350000, training loss 0.022352\n",
      "epoch 44,step 1354500, training loss 0.0690036\n",
      "epoch 44,step 1359000, training loss 0.0470434\n",
      "epoch 44,step 1363500, training loss 0.0300118\n",
      "epoch 44,step 1368000, training loss 0.0293001\n",
      "epoch 44,step 1372500, training loss 0.0295562\n",
      "epoch 44,step 1377000, training loss 0.0286599\n",
      "epoch 44,step 1381500, training loss 0.0245445\n",
      "epoch 44,step 1386000, training loss 0.0375348\n",
      "epoch 44,step 1390500, training loss 0.0213024\n",
      "epoch 44,step 1395000, training loss 0.0268071\n",
      "epoch 44,step 1399500, training loss 0.0800255\n",
      "epoch 44,step 1404000, training loss 0.0611953\n",
      "epoch 44,step 1408500, training loss 0.0203472\n",
      "epoch 44,step 1413000, training loss 0.0311445\n",
      "epoch 44,step 1417500, training loss 0.0249858\n",
      "epoch 44,step 1422000, training loss 0.0355802\n",
      "epoch 44,step 1426500, training loss 0.0457526\n",
      "epoch 44,step 1431000, training loss 0.0233502\n",
      "epoch 44,step 1435500, training loss 0.0411905\n",
      "epoch 44,step 1440000, training loss 0.0339481\n",
      "epoch 44,step 1444500, training loss 0.0813425\n",
      "epoch 44,step 1449000, training loss 0.0571014\n",
      "epoch 44,step 1453500, training loss 0.032467\n",
      "epoch 44,step 1458000, training loss 0.0318946\n",
      "epoch 44,step 1462500, training loss 0.037796\n",
      "epoch 44,step 1467000, training loss 0.0235777\n",
      "epoch 44,step 1471500, training loss 0.0270948\n",
      "epoch 44,step 1476000, training loss 0.0246361\n",
      "epoch 44,step 1480500, training loss 0.0320021\n",
      "epoch 44,step 1485000, training loss 0.0231835\n",
      "epoch 44,step 1489500, training loss 0.0699944\n",
      "epoch 44,step 1494000, training loss 0.0650809\n",
      "epoch 44,step 1498500, training loss 0.028359\n",
      "epoch 44,step 1503000, training loss 0.041099\n",
      "epoch 44,step 1507500, training loss 0.0248176\n",
      "epoch 44,step 1512000, training loss 0.0183428\n",
      "epoch 44,step 1516500, training loss 0.0267235\n",
      "epoch 44,step 1521000, training loss 0.0248571\n",
      "epoch 44,step 1525500, training loss 0.0263393\n",
      "epoch 44,step 1530000, training loss 0.0341381\n",
      "epoch 44,step 1534500, training loss 0.0785319\n",
      "epoch 44,step 1539000, training loss 0.0471152\n",
      "epoch 44,step 1543500, training loss 0.0380829\n",
      "epoch 44,step 1548000, training loss 0.0367314\n",
      "epoch 44,step 1552500, training loss 0.0214035\n",
      "epoch 44,step 1557000, training loss 0.0229122\n",
      "epoch 44,step 1561500, training loss 0.0237655\n",
      "epoch 44,step 1566000, training loss 0.0278051\n",
      "epoch 44,step 1570500, training loss 0.0288091\n",
      "epoch 44,step 1575000, training loss 0.0259631\n",
      "epoch 44,step 1579500, training loss 0.0702925\n",
      "epoch 44,step 1584000, training loss 0.0354194\n",
      "epoch 44,step 1588500, training loss 0.0341381\n",
      "epoch 44,step 1593000, training loss 0.0280287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44,step 1597500, training loss 0.031345\n",
      "epoch 44,step 1602000, training loss 0.0238059\n",
      "epoch 44,step 1606500, training loss 0.0217482\n",
      "epoch 44,step 1611000, training loss 0.0216943\n",
      "epoch 44,step 1615500, training loss 0.0216548\n",
      "epoch 44,step 1620000, training loss 0.0177475\n",
      "epoch 44,step 1624500, training loss 0.0771689\n",
      "epoch 44,step 1629000, training loss 0.0413237\n",
      "epoch 44,step 1633500, training loss 0.0298203\n",
      "epoch 44,step 1638000, training loss 0.0245179\n",
      "epoch 44,step 1642500, training loss 0.0407749\n",
      "epoch 44,step 1647000, training loss 0.0316238\n",
      "epoch 44,step 1651500, training loss 0.0282321\n",
      "epoch 44,step 1656000, training loss 0.0314856\n",
      "epoch 44,step 1660500, training loss 0.0267846\n",
      "epoch 44,step 1665000, training loss 0.035498\n",
      "epoch 44,step 1669500, training loss 0.0798085\n",
      "epoch 44,step 1674000, training loss 0.0444682\n",
      "epoch 44,step 1678500, training loss 0.0232199\n",
      "epoch 44,step 1683000, training loss 0.0404607\n",
      "epoch 44,step 1687500, training loss 0.0286055\n",
      "epoch 44,step 1692000, training loss 0.0265212\n",
      "epoch 44,step 1696500, training loss 0.0248393\n",
      "epoch 44,step 1701000, training loss 0.0350668\n",
      "epoch 44,step 1705500, training loss 0.0202649\n",
      "epoch 44,step 1710000, training loss 0.0240698\n",
      "epoch 44,step 1714500, training loss 0.0726852\n",
      "epoch 44,step 1719000, training loss 0.0951956\n",
      "epoch 44,step 1723500, training loss 0.0392493\n",
      "epoch 44,step 1728000, training loss 0.0283658\n",
      "epoch 44,step 1732500, training loss 0.0237176\n",
      "epoch 44,step 1737000, training loss 0.0267594\n",
      "epoch 44,step 1741500, training loss 0.0279724\n",
      "epoch 44,step 1746000, training loss 0.0242884\n",
      "epoch 44,step 1750500, training loss 0.0294797\n",
      "epoch 44,step 1755000, training loss 0.0223943\n",
      "epoch 44,step 1759500, training loss 0.0658244\n",
      "epoch 44,step 1764000, training loss 0.0497016\n",
      "epoch 44,step 1768500, training loss 0.0201311\n",
      "epoch 44,step 1773000, training loss 0.0247504\n",
      "epoch 44,step 1777500, training loss 0.0279224\n",
      "epoch 44,step 1782000, training loss 0.0192127\n",
      "epoch 44,step 1786500, training loss 0.020091\n",
      "epoch 44,step 1791000, training loss 0.0310808\n",
      "epoch 44,step 1795500, training loss 0.0242065\n",
      "epoch 44,step 1800000, training loss 0.0171239\n",
      "epoch 44,step 1804500, training loss 0.0838258\n",
      "epoch 44,step 1809000, training loss 0.10306\n",
      "epoch 44,step 1813500, training loss 0.0256586\n",
      "epoch 44,step 1818000, training loss 0.024304\n",
      "epoch 44,step 1822500, training loss 0.0207203\n",
      "epoch 44,step 1827000, training loss 0.0227362\n",
      "epoch 44,step 1831500, training loss 0.0196303\n",
      "epoch 44,step 1836000, training loss 0.026303\n",
      "epoch 44,step 1840500, training loss 0.0313203\n",
      "epoch 44,step 1845000, training loss 0.0217333\n",
      "epoch 44,step 1849500, training loss 0.0700004\n",
      "epoch 44,step 1854000, training loss 0.0378115\n",
      "epoch 44,step 1858500, training loss 0.0282694\n",
      "epoch 44,step 1863000, training loss 0.0294415\n",
      "epoch 44,step 1867500, training loss 0.0222149\n",
      "epoch 44,step 1872000, training loss 0.0319984\n",
      "epoch 44,step 1876500, training loss 0.0282579\n",
      "epoch 44,step 1881000, training loss 0.0243059\n",
      "epoch 44,step 1885500, training loss 0.0182822\n",
      "epoch 44,step 1890000, training loss 0.0215141\n",
      "epoch 44,step 1894500, training loss 0.0699929\n",
      "epoch 44,step 1899000, training loss 0.0402836\n",
      "epoch 44,step 1903500, training loss 0.0128146\n",
      "epoch 44,step 1908000, training loss 0.022516\n",
      "epoch 44,step 1912500, training loss 0.032602\n",
      "epoch 44,step 1917000, training loss 0.0276137\n",
      "epoch 44,step 1921500, training loss 0.0289167\n",
      "epoch 44,step 1926000, training loss 0.0328715\n",
      "epoch 44,step 1930500, training loss 0.0212552\n",
      "epoch 44,step 1935000, training loss 0.0241889\n",
      "epoch 44,step 1939500, training loss 0.0719305\n",
      "epoch 44,step 1944000, training loss 0.0555451\n",
      "epoch 44,step 1948500, training loss 0.0308924\n",
      "epoch 44,step 1953000, training loss 0.0240205\n",
      "epoch 44,step 1957500, training loss 0.0314403\n",
      "epoch 44,step 1962000, training loss 0.0224452\n",
      "epoch 44,step 1966500, training loss 0.0258971\n",
      "epoch 44,step 1971000, training loss 0.0255776\n",
      "epoch 44,step 1975500, training loss 0.0245738\n",
      "epoch 44,step 1980000, training loss 0.0244947\n",
      "epoch 44,step 1984500, training loss 0.0825565\n",
      "epoch 44,step 1989000, training loss 0.0390641\n",
      "epoch 44,step 1993500, training loss 0.0314853\n",
      "epoch 44,step 1998000, training loss 0.0217229\n",
      "epoch 44,step 2002500, training loss 0.0179388\n",
      "epoch 44,step 2007000, training loss 0.0178922\n",
      "epoch 44,step 2011500, training loss 0.0260213\n",
      "epoch 44,step 2016000, training loss 0.0229561\n",
      "epoch 44,step 2020500, training loss 0.0245956\n",
      "epoch 44,step 2025000, training loss 0.0244569\n",
      "epoch 44,step 2029500, training loss 0.070212\n",
      "epoch 44,step 2034000, training loss 0.0331527\n",
      "epoch 44,step 2038500, training loss 0.0295811\n",
      "epoch 44,step 2043000, training loss 0.0277276\n",
      "epoch 44,step 2047500, training loss 0.0222641\n",
      "epoch 44,step 2052000, training loss 0.0197851\n",
      "epoch 44,step 2056500, training loss 0.0239955\n",
      "epoch 44,step 2061000, training loss 0.0232956\n",
      "epoch 44,step 2065500, training loss 0.0178091\n",
      "epoch 44,step 2070000, training loss 0.0236861\n",
      "epoch 44,step 2074500, training loss 0.0827971\n",
      "epoch 44,step 2079000, training loss 0.0522316\n",
      "epoch 44,step 2083500, training loss 0.032499\n",
      "epoch 44,step 2088000, training loss 0.030779\n",
      "epoch 44,step 2092500, training loss 0.0332321\n",
      "epoch 44,step 2097000, training loss 0.0243787\n",
      "epoch 44,step 2101500, training loss 0.0388709\n",
      "epoch 44,step 2106000, training loss 0.0285323\n",
      "epoch 44,step 2110500, training loss 0.0241194\n",
      "epoch 44,step 2115000, training loss 0.0260717\n",
      "epoch 44,step 2119500, training loss 0.0786651\n",
      "epoch 44,step 2124000, training loss 0.0792262\n",
      "epoch 44,step 2128500, training loss 0.0324598\n",
      "epoch 44,step 2133000, training loss 0.0278484\n",
      "epoch 44,step 2137500, training loss 0.0209632\n",
      "epoch 44,step 2142000, training loss 0.0380723\n",
      "epoch 44,step 2146500, training loss 0.0279179\n",
      "epoch 44,step 2151000, training loss 0.0291356\n",
      "epoch 44,step 2155500, training loss 0.0236602\n",
      "epoch 44,step 2160000, training loss 0.0247226\n",
      "epoch 44,step 2164500, training loss 0.0650384\n",
      "epoch 44,step 2169000, training loss 0.0298201\n",
      "epoch 44,step 2173500, training loss 0.0312364\n",
      "epoch 44,step 2178000, training loss 0.027738\n",
      "epoch 44,step 2182500, training loss 0.0231279\n",
      "epoch 44,step 2187000, training loss 0.0275555\n",
      "epoch 44,step 2191500, training loss 0.0219374\n",
      "epoch 44,step 2196000, training loss 0.0225879\n",
      "epoch 44,step 2200500, training loss 0.0244494\n",
      "epoch 44,step 2205000, training loss 0.0288488\n",
      "epoch 44,step 2209500, training loss 0.0815956\n",
      "epoch 44,step 2214000, training loss 0.0356839\n",
      "epoch 44,step 2218500, training loss 0.0221368\n",
      "epoch 44,step 2223000, training loss 0.0376757\n",
      "epoch 44,step 2227500, training loss 0.0314125\n",
      "epoch 44,step 2232000, training loss 0.0238047\n",
      "epoch 44,step 2236500, training loss 0.0246473\n",
      "epoch 44,step 2241000, training loss 0.026294\n",
      "epoch 44,step 2245500, training loss 0.0556765\n",
      "epoch 44,step 2250000, training loss 0.0247376\n",
      "epoch 44,step 2254500, training loss 0.0806665\n",
      "epoch 44,step 2259000, training loss 0.047569\n",
      "epoch 44,step 2263500, training loss 0.0313104\n",
      "epoch 44,step 2268000, training loss 0.0314367\n",
      "epoch 44,step 2272500, training loss 0.0245539\n",
      "epoch 44,step 2277000, training loss 0.0255625\n",
      "epoch 44,step 2281500, training loss 0.0190619\n",
      "epoch 44,step 2286000, training loss 0.0236835\n",
      "epoch 44,step 2290500, training loss 0.0286577\n",
      "epoch 44,step 2295000, training loss 0.021744\n",
      "epoch 44,step 2299500, training loss 0.0900429\n",
      "epoch 44,step 2304000, training loss 0.0875895\n",
      "epoch 44,step 2308500, training loss 0.0241337\n",
      "epoch 44,step 2313000, training loss 0.0261742\n",
      "epoch 44,step 2317500, training loss 0.0312682\n",
      "epoch 44,step 2322000, training loss 0.0309766\n",
      "epoch 44,step 2326500, training loss 0.0288544\n",
      "epoch 44,step 2331000, training loss 0.0233742\n",
      "epoch 44,step 2335500, training loss 0.0278269\n",
      "epoch 44,step 2340000, training loss 0.0194845\n",
      "epoch 44,step 2344500, training loss 0.0764519\n",
      "epoch 44,step 2349000, training loss 0.0559912\n",
      "epoch 44,step 2353500, training loss 0.0272738\n",
      "epoch 44,step 2358000, training loss 0.0254582\n",
      "epoch 44,step 2362500, training loss 0.0296726\n",
      "epoch 44,step 2367000, training loss 0.0226538\n",
      "epoch 44,step 2371500, training loss 0.0288609\n",
      "epoch 44,step 2376000, training loss 0.0287366\n",
      "epoch 44,step 2380500, training loss 0.0443816\n",
      "epoch 44,step 2385000, training loss 0.0351918\n",
      "epoch 44,step 2389500, training loss 0.106532\n",
      "epoch 44,step 2394000, training loss 0.0862872\n",
      "epoch 44,step 2398500, training loss 0.0404206\n",
      "epoch 44,step 2403000, training loss 0.0389626\n",
      "epoch 44,step 2407500, training loss 0.0337363\n",
      "epoch 44,step 2412000, training loss 0.0269012\n",
      "epoch 44,step 2416500, training loss 0.024539\n",
      "epoch 44,step 2421000, training loss 0.0273337\n",
      "epoch 44,step 2425500, training loss 0.0279526\n",
      "epoch 44,step 2430000, training loss 0.0267597\n",
      "epoch 44,step 2434500, training loss 0.0726295\n",
      "epoch 44,step 2439000, training loss 0.0660706\n",
      "epoch 44,step 2443500, training loss 0.0404654\n",
      "epoch 44,step 2448000, training loss 0.0439098\n",
      "epoch 44,step 2452500, training loss 0.0340301\n",
      "epoch 44,step 2457000, training loss 0.0245212\n",
      "epoch 44,step 2461500, training loss 0.0209025\n",
      "epoch 44,step 2466000, training loss 0.028858\n",
      "epoch 44,step 2470500, training loss 0.0248979\n",
      "epoch 44,step 2475000, training loss 0.0437463\n",
      "epoch 44,step 2479500, training loss 0.0713017\n",
      "epoch 44,step 2484000, training loss 0.0410103\n",
      "epoch 44,step 2488500, training loss 0.0321448\n",
      "epoch 44,step 2493000, training loss 0.036793\n",
      "epoch 44,step 2497500, training loss 0.0337437\n",
      "epoch 44,step 2502000, training loss 0.0318205\n",
      "epoch 44,step 2506500, training loss 0.0258333\n",
      "epoch 44,step 2511000, training loss 0.035843\n",
      "epoch 44,step 2515500, training loss 0.0260263\n",
      "epoch 44,step 2520000, training loss 0.0358439\n",
      "epoch 44,step 2524500, training loss 0.0835759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44,step 2529000, training loss 0.0638265\n",
      "epoch 44,step 2533500, training loss 0.04204\n",
      "epoch 44,step 2538000, training loss 0.0329095\n",
      "epoch 44,step 2542500, training loss 0.0265169\n",
      "epoch 44,step 2547000, training loss 0.0268808\n",
      "epoch 44,step 2551500, training loss 0.0263792\n",
      "epoch 44,step 2556000, training loss 0.0241309\n",
      "epoch 44,step 2560500, training loss 0.0256206\n",
      "epoch 44,step 2565000, training loss 0.0215073\n",
      "epoch 44,step 2569500, training loss 0.0797036\n",
      "epoch 44,step 2574000, training loss 0.0567658\n",
      "epoch 44,step 2578500, training loss 0.0399001\n",
      "epoch 44,step 2583000, training loss 0.0343682\n",
      "epoch 44,step 2587500, training loss 0.0408151\n",
      "epoch 44,step 2592000, training loss 0.0318815\n",
      "epoch 44,step 2596500, training loss 0.0285247\n",
      "epoch 44,step 2601000, training loss 0.0349696\n",
      "epoch 44,step 2605500, training loss 0.023588\n",
      "epoch 44,step 2610000, training loss 0.0195337\n",
      "epoch 44,step 2614500, training loss 0.0783848\n",
      "epoch 44,step 2619000, training loss 0.0431704\n",
      "epoch 44,step 2623500, training loss 0.026416\n",
      "epoch 44,step 2628000, training loss 0.0298438\n",
      "epoch 44,step 2632500, training loss 0.0258342\n",
      "epoch 44,step 2637000, training loss 0.0285736\n",
      "epoch 44,step 2641500, training loss 0.0241404\n",
      "epoch 44,step 2646000, training loss 0.0351051\n",
      "epoch 44,step 2650500, training loss 0.0304183\n",
      "epoch 44,step 2655000, training loss 0.0207498\n",
      "epoch 44,step 2659500, training loss 0.0695482\n",
      "epoch 44,step 2664000, training loss 0.0406319\n",
      "epoch 44,step 2668500, training loss 0.0273356\n",
      "epoch 44,step 2673000, training loss 0.0315951\n",
      "epoch 44,step 2677500, training loss 0.0278655\n",
      "epoch 44,step 2682000, training loss 0.0280462\n",
      "epoch 44,step 2686500, training loss 0.0203415\n",
      "epoch 44,step 2691000, training loss 0.0233308\n",
      "epoch 44,step 2695500, training loss 0.0266287\n",
      "epoch 44,step 2700000, training loss 0.0437019\n",
      "epoch 44,step 2704500, training loss 0.131235\n",
      "epoch 44,step 2709000, training loss 0.0427352\n",
      "epoch 44,step 2713500, training loss 0.0307594\n",
      "epoch 44,step 2718000, training loss 0.0430511\n",
      "epoch 44,step 2722500, training loss 0.0216227\n",
      "epoch 44,step 2727000, training loss 0.0274594\n",
      "epoch 44,step 2731500, training loss 0.023258\n",
      "epoch 44,step 2736000, training loss 0.0210112\n",
      "epoch 44,step 2740500, training loss 0.0255579\n",
      "epoch 44,step 2745000, training loss 0.0204385\n",
      "epoch 44,step 2749500, training loss 0.0956238\n",
      "epoch 44,step 2754000, training loss 0.0523783\n",
      "epoch 44,step 2758500, training loss 0.0324326\n",
      "epoch 44,step 2763000, training loss 0.0222563\n",
      "epoch 44,step 2767500, training loss 0.0222365\n",
      "epoch 44,step 2772000, training loss 0.0236385\n",
      "epoch 44,step 2776500, training loss 0.0209772\n",
      "epoch 44,step 2781000, training loss 0.0209343\n",
      "epoch 44,step 2785500, training loss 0.0208621\n",
      "epoch 44,step 2790000, training loss 0.0307225\n",
      "epoch 44,step 2794500, training loss 0.10338\n",
      "epoch 44,step 2799000, training loss 0.102836\n",
      "epoch 44,step 2803500, training loss 0.0357639\n",
      "epoch 44,step 2808000, training loss 0.0384555\n",
      "epoch 44,step 2812500, training loss 0.029298\n",
      "epoch 44,step 2817000, training loss 0.0365368\n",
      "epoch 44,step 2821500, training loss 0.0207053\n",
      "epoch 44,step 2826000, training loss 0.0246163\n",
      "epoch 44,step 2830500, training loss 0.0226077\n",
      "epoch 44,step 2835000, training loss 0.0254775\n",
      "epoch 44,step 2839500, training loss 0.0747016\n",
      "epoch 44,step 2844000, training loss 0.0423974\n",
      "epoch 44,step 2848500, training loss 0.0225981\n",
      "epoch 44,step 2853000, training loss 0.0284639\n",
      "epoch 44,step 2857500, training loss 0.0210531\n",
      "epoch 44,step 2862000, training loss 0.0186167\n",
      "epoch 44,step 2866500, training loss 0.0195991\n",
      "epoch 44,step 2871000, training loss 0.0271942\n",
      "epoch 44,step 2875500, training loss 0.0215745\n",
      "epoch 44,step 2880000, training loss 0.0278177\n",
      "epoch 44,step 2884500, training loss 0.113108\n",
      "epoch 44,step 2889000, training loss 0.0548673\n",
      "epoch 44,step 2893500, training loss 0.0227774\n",
      "epoch 44,step 2898000, training loss 0.0217999\n",
      "epoch 44,step 2902500, training loss 0.0312219\n",
      "epoch 44,step 2907000, training loss 0.0295294\n",
      "epoch 44,step 2911500, training loss 0.0650801\n",
      "epoch 44,step 2916000, training loss 0.0207623\n",
      "epoch 44,step 2920500, training loss 0.0194932\n",
      "epoch 44,step 2925000, training loss 0.027363\n",
      "epoch 44,step 2929500, training loss 0.0884132\n",
      "epoch 44,step 2934000, training loss 0.0320201\n",
      "epoch 44,step 2938500, training loss 0.0183452\n",
      "epoch 44,step 2943000, training loss 0.0224418\n",
      "epoch 44,step 2947500, training loss 0.0235965\n",
      "epoch 44,step 2952000, training loss 0.0222987\n",
      "epoch 44,step 2956500, training loss 0.0183067\n",
      "epoch 44,step 2961000, training loss 0.0265728\n",
      "epoch 44,step 2965500, training loss 0.039631\n",
      "epoch 44,step 2970000, training loss 0.023496\n",
      "epoch 44,step 2974500, training loss 0.101483\n",
      "epoch 44,step 2979000, training loss 0.0463021\n",
      "epoch 44,step 2983500, training loss 0.0259481\n",
      "epoch 44,step 2988000, training loss 0.0295525\n",
      "epoch 44,step 2992500, training loss 0.0218241\n",
      "epoch 44,step 2997000, training loss 0.0243181\n",
      "epoch 44,step 3001500, training loss 0.0272855\n",
      "epoch 44,step 3006000, training loss 0.0346131\n",
      "epoch 44,step 3010500, training loss 0.0256412\n",
      "epoch 44,step 3015000, training loss 0.0275502\n",
      "epoch 44,step 3019500, training loss 0.0931508\n",
      "epoch 44,step 3024000, training loss 0.0813619\n",
      "epoch 44,step 3028500, training loss 0.0300465\n",
      "epoch 44,step 3033000, training loss 0.048828\n",
      "epoch 44,step 3037500, training loss 0.0220002\n",
      "epoch 44,step 3042000, training loss 0.0331411\n",
      "epoch 44,step 3046500, training loss 0.0355507\n",
      "epoch 44,step 3051000, training loss 0.0252498\n",
      "epoch 44,step 3055500, training loss 0.0218016\n",
      "epoch 44,step 3060000, training loss 0.0273763\n",
      "epoch 44,step 3064500, training loss 0.0907296\n",
      "epoch 44,step 3069000, training loss 0.0448423\n",
      "epoch 44,step 3073500, training loss 0.0330061\n",
      "epoch 44,step 3078000, training loss 0.0316205\n",
      "epoch 44,step 3082500, training loss 0.0242937\n",
      "epoch 44,step 3087000, training loss 0.0254003\n",
      "epoch 44,step 3091500, training loss 0.025929\n",
      "epoch 44,step 3096000, training loss 0.0310972\n",
      "epoch 44,step 3100500, training loss 0.0277149\n",
      "epoch 44,step 3105000, training loss 0.0270411\n",
      "epoch 44,step 3109500, training loss 0.0872059\n",
      "epoch 44,step 3114000, training loss 0.0498006\n",
      "epoch 44,step 3118500, training loss 0.0228136\n",
      "epoch 44,step 3123000, training loss 0.0264372\n",
      "epoch 44,step 3127500, training loss 0.0276433\n",
      "epoch 44,step 3132000, training loss 0.0250466\n",
      "epoch 44,step 3136500, training loss 0.0251034\n",
      "epoch 44,step 3141000, training loss 0.0291167\n",
      "epoch 44,step 3145500, training loss 0.0272696\n",
      "epoch 44,step 3150000, training loss 0.0224671\n",
      "epoch 44,step 3154500, training loss 0.0779212\n",
      "epoch 44,step 3159000, training loss 0.0606006\n",
      "epoch 44,step 3163500, training loss 0.0273266\n",
      "epoch 44,step 3168000, training loss 0.0229595\n",
      "epoch 44,step 3172500, training loss 0.0252272\n",
      "epoch 44,step 3177000, training loss 0.0240751\n",
      "epoch 44,step 3181500, training loss 0.0273985\n",
      "epoch 44,step 3186000, training loss 0.0198141\n",
      "epoch 44,step 3190500, training loss 0.0217742\n",
      "epoch 44,step 3195000, training loss 0.0229258\n",
      "epoch 44,step 3199500, training loss 0.0775556\n",
      "epoch 44,step 3204000, training loss 0.0512618\n",
      "epoch 44,step 3208500, training loss 0.028176\n",
      "epoch 44,step 3213000, training loss 0.0219073\n",
      "epoch 44,step 3217500, training loss 0.0281284\n",
      "epoch 44,step 3222000, training loss 0.025318\n",
      "epoch 44,step 3226500, training loss 0.03295\n",
      "epoch 44,step 3231000, training loss 0.0322458\n",
      "epoch 44,step 3235500, training loss 0.022344\n",
      "epoch 44,step 3240000, training loss 0.0229413\n",
      "epoch 44,step 3244500, training loss 0.0563206\n",
      "epoch 44,step 3249000, training loss 0.0403969\n",
      "epoch 44,step 3253500, training loss 0.035778\n",
      "epoch 44,step 3258000, training loss 0.0290827\n",
      "epoch 44,step 3262500, training loss 0.0329304\n",
      "epoch 44,step 3267000, training loss 0.0242574\n",
      "epoch 44,step 3271500, training loss 0.0358313\n",
      "epoch 44,step 3276000, training loss 0.0243142\n",
      "epoch 44,step 3280500, training loss 0.0235698\n",
      "epoch 44,step 3285000, training loss 0.0215572\n",
      "epoch 44,step 3289500, training loss 0.0814449\n",
      "epoch 44,step 3294000, training loss 0.0415951\n",
      "epoch 44,step 3298500, training loss 0.0258033\n",
      "epoch 44,step 3303000, training loss 0.0400386\n",
      "epoch 44,step 3307500, training loss 0.0260519\n",
      "epoch 44,step 3312000, training loss 0.0255007\n",
      "epoch 44,step 3316500, training loss 0.0263705\n",
      "epoch 44,step 3321000, training loss 0.0356973\n",
      "epoch 44,step 3325500, training loss 0.0496611\n",
      "epoch 44,step 3330000, training loss 0.0312889\n",
      "epoch 44,step 3334500, training loss 0.0622606\n",
      "epoch 44,step 3339000, training loss 0.038184\n",
      "epoch 44,step 3343500, training loss 0.0231259\n",
      "epoch 44,step 3348000, training loss 0.0202775\n",
      "epoch 44,step 3352500, training loss 0.0229847\n",
      "epoch 44,step 3357000, training loss 0.0253028\n",
      "epoch 44,step 3361500, training loss 0.0228123\n",
      "epoch 44,step 3366000, training loss 0.020016\n",
      "epoch 44,step 3370500, training loss 0.021384\n",
      "epoch 44,step 3375000, training loss 0.0525579\n",
      "epoch 44,step 3379500, training loss 0.0711793\n",
      "epoch 44,step 3384000, training loss 0.036324\n",
      "epoch 44,step 3388500, training loss 0.0270895\n",
      "epoch 44,step 3393000, training loss 0.0233515\n",
      "epoch 44,step 3397500, training loss 0.0219339\n",
      "epoch 44,step 3402000, training loss 0.0250212\n",
      "epoch 44,step 3406500, training loss 0.0259836\n",
      "epoch 44,step 3411000, training loss 0.0236734\n",
      "epoch 44,step 3415500, training loss 0.0152835\n",
      "epoch 44,step 3420000, training loss 0.0206595\n",
      "epoch 44,step 3424500, training loss 0.0749774\n",
      "epoch 44,step 3429000, training loss 0.0670021\n",
      "epoch 44,step 3433500, training loss 0.0255286\n",
      "epoch 44,step 3438000, training loss 0.0202499\n",
      "epoch 44,step 3442500, training loss 0.0328207\n",
      "epoch 44,step 3447000, training loss 0.0382248\n",
      "epoch 44,step 3451500, training loss 0.0216877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44,step 3456000, training loss 0.0315732\n",
      "epoch 44,step 3460500, training loss 0.0232101\n",
      "epoch 44,step 3465000, training loss 0.0188559\n",
      "epoch 44,step 3469500, training loss 0.0998464\n",
      "epoch 44,step 3474000, training loss 0.0676842\n",
      "epoch 44,step 3478500, training loss 0.0336371\n",
      "epoch 44,step 3483000, training loss 0.0321586\n",
      "epoch 44,step 3487500, training loss 0.0272673\n",
      "epoch 44,step 3492000, training loss 0.0207917\n",
      "epoch 44,step 3496500, training loss 0.0235741\n",
      "epoch 44,step 3501000, training loss 0.0300157\n",
      "epoch 44,step 3505500, training loss 0.0309159\n",
      "epoch 44,step 3510000, training loss 0.019366\n",
      "epoch 44,step 3514500, training loss 0.0688889\n",
      "epoch 44,step 3519000, training loss 0.0559914\n",
      "epoch 44,step 3523500, training loss 0.0355613\n",
      "epoch 44,step 3528000, training loss 0.0247354\n",
      "epoch 44,step 3532500, training loss 0.0246744\n",
      "epoch 44,step 3537000, training loss 0.0208363\n",
      "epoch 44,step 3541500, training loss 0.0443903\n",
      "epoch 44,step 3546000, training loss 0.0294848\n",
      "epoch 44,step 3550500, training loss 0.0242687\n",
      "epoch 44,step 3555000, training loss 0.0245043\n",
      "epoch 44,step 3559500, training loss 0.104595\n",
      "epoch 44,step 3564000, training loss 0.0537692\n",
      "epoch 44,step 3568500, training loss 0.0411425\n",
      "epoch 44,step 3573000, training loss 0.0400953\n",
      "epoch 44,step 3577500, training loss 0.0273936\n",
      "epoch 44,step 3582000, training loss 0.0291425\n",
      "epoch 44,step 3586500, training loss 0.0203068\n",
      "epoch 44,step 3591000, training loss 0.0277283\n",
      "epoch 44,step 3595500, training loss 0.0211854\n",
      "epoch 44,step 3600000, training loss 0.0230656\n",
      "epoch 44,step 3604500, training loss 0.0902728\n",
      "epoch 44,step 3609000, training loss 0.0347515\n",
      "epoch 44,step 3613500, training loss 0.0337853\n",
      "epoch 44,step 3618000, training loss 0.0340157\n",
      "epoch 44,step 3622500, training loss 0.0256978\n",
      "epoch 44,step 3627000, training loss 0.0332923\n",
      "epoch 44,step 3631500, training loss 0.0277544\n",
      "epoch 44,step 3636000, training loss 0.0319867\n",
      "epoch 44,step 3640500, training loss 0.0253249\n",
      "epoch 44,step 3645000, training loss 0.0227448\n",
      "epoch 44,step 3649500, training loss 0.0818743\n",
      "epoch 44,step 3654000, training loss 0.0591838\n",
      "epoch 44,step 3658500, training loss 0.0318096\n",
      "epoch 44,step 3663000, training loss 0.0357248\n",
      "epoch 44,step 3667500, training loss 0.029276\n",
      "epoch 44,step 3672000, training loss 0.0236868\n",
      "epoch 44,step 3676500, training loss 0.0244601\n",
      "epoch 44,step 3681000, training loss 0.0310972\n",
      "epoch 44,step 3685500, training loss 0.0292036\n",
      "epoch 44,step 3690000, training loss 0.0295649\n",
      "epoch 44,step 3694500, training loss 0.0838071\n",
      "epoch 44,step 3699000, training loss 0.0355382\n",
      "epoch 44,step 3703500, training loss 0.0252725\n",
      "epoch 44,step 3708000, training loss 0.0351839\n",
      "epoch 44,step 3712500, training loss 0.0382339\n",
      "epoch 44,step 3717000, training loss 0.0290971\n",
      "epoch 44,step 3721500, training loss 0.0307163\n",
      "epoch 44,step 3726000, training loss 0.0467219\n",
      "epoch 44,step 3730500, training loss 0.0377817\n",
      "epoch 44,step 3735000, training loss 0.0288065\n",
      "epoch 44,step 3739500, training loss 0.0776933\n",
      "epoch 44,step 3744000, training loss 0.056759\n",
      "epoch 44,step 3748500, training loss 0.0259923\n",
      "epoch 44,step 3753000, training loss 0.0281918\n",
      "epoch 44,step 3757500, training loss 0.0255284\n",
      "epoch 44,step 3762000, training loss 0.0317296\n",
      "epoch 44,step 3766500, training loss 0.0223476\n",
      "epoch 44,step 3771000, training loss 0.021344\n",
      "epoch 44,step 3775500, training loss 0.032472\n",
      "epoch 44,step 3780000, training loss 0.0228893\n",
      "epoch 44,step 3784500, training loss 0.089124\n",
      "epoch 44,step 3789000, training loss 0.0625493\n",
      "epoch 44,step 3793500, training loss 0.0324168\n",
      "epoch 44,step 3798000, training loss 0.0275799\n",
      "epoch 44,step 3802500, training loss 0.0333271\n",
      "epoch 44,step 3807000, training loss 0.0293295\n",
      "epoch 44,step 3811500, training loss 0.0421944\n",
      "epoch 44,step 3816000, training loss 0.0198311\n",
      "epoch 44,step 3820500, training loss 0.0255121\n",
      "epoch 44,step 3825000, training loss 0.0303111\n",
      "epoch 44,step 3829500, training loss 0.0731848\n",
      "epoch 44,step 3834000, training loss 0.0445239\n",
      "epoch 44,step 3838500, training loss 0.0276486\n",
      "epoch 44,step 3843000, training loss 0.0344559\n",
      "epoch 44,step 3847500, training loss 0.0350003\n",
      "epoch 44,step 3852000, training loss 0.0291009\n",
      "epoch 44,step 3856500, training loss 0.0253862\n",
      "epoch 44,step 3861000, training loss 0.0299477\n",
      "epoch 44,step 3865500, training loss 0.0305579\n",
      "epoch 44,step 3870000, training loss 0.0356363\n",
      "epoch 44,step 3874500, training loss 0.0673881\n",
      "epoch 44,step 3879000, training loss 0.045313\n",
      "epoch 44,step 3883500, training loss 0.0224522\n",
      "epoch 44,step 3888000, training loss 0.0254876\n",
      "epoch 44,step 3892500, training loss 0.026785\n",
      "epoch 44,step 3897000, training loss 0.0212396\n",
      "epoch 44,step 3901500, training loss 0.0243575\n",
      "epoch 44,step 3906000, training loss 0.0236988\n",
      "epoch 44,step 3910500, training loss 0.0302061\n",
      "epoch 44,step 3915000, training loss 0.0330533\n",
      "epoch 44,step 3919500, training loss 0.0752081\n",
      "epoch 44,step 3924000, training loss 0.0370334\n",
      "epoch 44,step 3928500, training loss 0.0222647\n",
      "epoch 44,step 3933000, training loss 0.0243193\n",
      "epoch 44,step 3937500, training loss 0.0237831\n",
      "epoch 44,step 3942000, training loss 0.034964\n",
      "epoch 44,step 3946500, training loss 0.0334325\n",
      "epoch 44,step 3951000, training loss 0.0222878\n",
      "epoch 44,step 3955500, training loss 0.0177973\n",
      "epoch 44,step 3960000, training loss 0.022991\n",
      "epoch 44,step 3964500, training loss 0.108153\n",
      "epoch 44,step 3969000, training loss 0.0673245\n",
      "epoch 44,step 3973500, training loss 0.0398655\n",
      "epoch 44,step 3978000, training loss 0.0241945\n",
      "epoch 44,step 3982500, training loss 0.025741\n",
      "epoch 44,step 3987000, training loss 0.0222824\n",
      "epoch 44,step 3991500, training loss 0.0210864\n",
      "epoch 44,step 3996000, training loss 0.0210093\n",
      "epoch 44,step 4000500, training loss 0.0434475\n",
      "epoch 44,step 4005000, training loss 0.0212306\n",
      "epoch 44,step 4009500, training loss 0.0840182\n",
      "epoch 44,step 4014000, training loss 0.0776524\n",
      "epoch 44,step 4018500, training loss 0.034319\n",
      "epoch 44,step 4023000, training loss 0.0261911\n",
      "epoch 44,step 4027500, training loss 0.0309328\n",
      "epoch 44,step 4032000, training loss 0.0328765\n",
      "epoch 44,step 4036500, training loss 0.0283496\n",
      "epoch 44,step 4041000, training loss 0.0261807\n",
      "epoch 44,step 4045500, training loss 0.0244817\n",
      "epoch 44,step 4050000, training loss 0.0524764\n",
      "epoch 44,step 4054500, training loss 0.0655897\n",
      "epoch 44,step 4059000, training loss 0.0279299\n",
      "epoch 44,step 4063500, training loss 0.0220546\n",
      "epoch 44,step 4068000, training loss 0.0376265\n",
      "epoch 44,step 4072500, training loss 0.0251354\n",
      "epoch 44,step 4077000, training loss 0.0213809\n",
      "epoch 44,step 4081500, training loss 0.0180371\n",
      "epoch 44,step 4086000, training loss 0.0199115\n",
      "epoch 44,step 4090500, training loss 0.0230188\n",
      "epoch 44,step 4095000, training loss 0.0239571\n",
      "epoch 44,step 4099500, training loss 0.0739536\n",
      "epoch 44,step 4104000, training loss 0.0417923\n",
      "epoch 44,step 4108500, training loss 0.0274953\n",
      "epoch 44,step 4113000, training loss 0.0315875\n",
      "epoch 44,step 4117500, training loss 0.0302809\n",
      "epoch 44,step 4122000, training loss 0.0250189\n",
      "epoch 44,step 4126500, training loss 0.0236527\n",
      "epoch 44,step 4131000, training loss 0.0291977\n",
      "epoch 44,step 4135500, training loss 0.0279134\n",
      "epoch 44,step 4140000, training loss 0.0235038\n",
      "epoch 44,step 4144500, training loss 0.0637582\n",
      "epoch 44,step 4149000, training loss 0.0577602\n",
      "epoch 44,step 4153500, training loss 0.0263018\n",
      "epoch 44,step 4158000, training loss 0.0227919\n",
      "epoch 44,step 4162500, training loss 0.0192267\n",
      "epoch 44,step 4167000, training loss 0.0269207\n",
      "epoch 44,step 4171500, training loss 0.0222081\n",
      "epoch 44,step 4176000, training loss 0.0180088\n",
      "epoch 44,step 4180500, training loss 0.0218419\n",
      "epoch 44,step 4185000, training loss 0.0193961\n",
      "epoch 44,step 4189500, training loss 0.0740899\n",
      "epoch 44,step 4194000, training loss 0.0382724\n",
      "epoch 44,step 4198500, training loss 0.0204953\n",
      "epoch 44,step 4203000, training loss 0.0255213\n",
      "epoch 44,step 4207500, training loss 0.0316655\n",
      "epoch 44,step 4212000, training loss 0.0238327\n",
      "epoch 44,step 4216500, training loss 0.0180176\n",
      "epoch 44,step 4221000, training loss 0.0316872\n",
      "epoch 44,step 4225500, training loss 0.0212462\n",
      "epoch 44,step 4230000, training loss 0.0209999\n",
      "epoch 44,step 4234500, training loss 0.0822835\n",
      "epoch 44,step 4239000, training loss 0.0504004\n",
      "epoch 44,step 4243500, training loss 0.0210646\n",
      "epoch 44,step 4248000, training loss 0.0295462\n",
      "epoch 44,step 4252500, training loss 0.0260635\n",
      "epoch 44,step 4257000, training loss 0.030126\n",
      "epoch 44,step 4261500, training loss 0.0315224\n",
      "epoch 44,step 4266000, training loss 0.0216507\n",
      "epoch 44,step 4270500, training loss 0.0281905\n",
      "epoch 44,step 4275000, training loss 0.0171483\n",
      "epoch 44,step 4279500, training loss 0.0770336\n",
      "epoch 44,step 4284000, training loss 0.066816\n",
      "epoch 44,step 4288500, training loss 0.0275272\n",
      "epoch 44,step 4293000, training loss 0.0209877\n",
      "epoch 44,step 4297500, training loss 0.0231056\n",
      "epoch 44,step 4302000, training loss 0.0284916\n",
      "epoch 44,step 4306500, training loss 0.0268286\n",
      "epoch 44,step 4311000, training loss 0.0326238\n",
      "epoch 44,step 4315500, training loss 0.0236783\n",
      "epoch 44,step 4320000, training loss 0.0266505\n",
      "epoch 44,step 4324500, training loss 0.0817627\n",
      "epoch 44,step 4329000, training loss 0.0406799\n",
      "epoch 44,step 4333500, training loss 0.0184506\n",
      "epoch 44,step 4338000, training loss 0.01856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44,step 4342500, training loss 0.0198796\n",
      "epoch 44,step 4347000, training loss 0.022742\n",
      "epoch 44,step 4351500, training loss 0.02015\n",
      "epoch 44,step 4356000, training loss 0.020861\n",
      "epoch 44,step 4360500, training loss 0.023163\n",
      "epoch 44,step 4365000, training loss 0.0256275\n",
      "epoch 44,step 4369500, training loss 0.0907422\n",
      "epoch 44,step 4374000, training loss 0.0426259\n",
      "epoch 44,step 4378500, training loss 0.0223697\n",
      "epoch 44,step 4383000, training loss 0.0226793\n",
      "epoch 44,step 4387500, training loss 0.0289016\n",
      "epoch 44,step 4392000, training loss 0.0197042\n",
      "epoch 44,step 4396500, training loss 0.0211261\n",
      "epoch 44,step 4401000, training loss 0.0232477\n",
      "epoch 44,step 4405500, training loss 0.019864\n",
      "epoch 44,step 4410000, training loss 0.0229432\n",
      "epoch 44,step 4414500, training loss 0.0711589\n",
      "epoch 44,step 4419000, training loss 0.0483884\n",
      "epoch 44,step 4423500, training loss 0.0336361\n",
      "epoch 44,step 4428000, training loss 0.0224281\n",
      "epoch 44,step 4432500, training loss 0.0180829\n",
      "epoch 44,step 4437000, training loss 0.0451244\n",
      "epoch 44,step 4441500, training loss 0.0226547\n",
      "epoch 44,step 4446000, training loss 0.0253423\n",
      "epoch 44,step 4450500, training loss 0.0211476\n",
      "epoch 44,step 4455000, training loss 0.0189401\n",
      "epoch 44,step 4459500, training loss 0.0762101\n",
      "epoch 44,step 4464000, training loss 0.0374515\n",
      "epoch 44,step 4468500, training loss 0.0276001\n",
      "epoch 44,step 4473000, training loss 0.0180183\n",
      "epoch 44,step 4477500, training loss 0.0187439\n",
      "epoch 44,step 4482000, training loss 0.0214626\n",
      "epoch 44,step 4486500, training loss 0.0169875\n",
      "epoch 44,step 4491000, training loss 0.0235139\n",
      "epoch 44,step 4495500, training loss 0.0318936\n",
      "epoch 44,training loss 0.0318936 ,test loss 0.0350638\n",
      "epoch 45,step 23000, training loss 0.0289006\n",
      "epoch 45,step 46000, training loss 0.0364025\n",
      "epoch 45,step 69000, training loss 0.0293324\n",
      "epoch 45,step 92000, training loss 0.0175503\n",
      "epoch 45,step 115000, training loss 0.0214476\n",
      "epoch 45,step 138000, training loss 0.0203941\n",
      "epoch 45,step 161000, training loss 0.0252308\n",
      "epoch 45,step 184000, training loss 0.020699\n",
      "epoch 45,step 207000, training loss 0.0230895\n",
      "epoch 45,step 230000, training loss 0.0217169\n",
      "epoch 45,step 253000, training loss 0.0221975\n",
      "epoch 45,step 276000, training loss 0.030246\n",
      "epoch 45,step 299000, training loss 0.04667\n",
      "epoch 45,step 322000, training loss 0.0235913\n",
      "epoch 45,step 345000, training loss 0.0360471\n",
      "epoch 45,step 368000, training loss 0.026105\n",
      "epoch 45,step 391000, training loss 0.0212171\n",
      "epoch 45,step 414000, training loss 0.0200843\n",
      "epoch 45,step 437000, training loss 0.0175842\n",
      "epoch 45,step 460000, training loss 0.0274234\n",
      "epoch 45,step 483000, training loss 0.0248715\n",
      "epoch 45,step 506000, training loss 0.0284912\n",
      "epoch 45,step 529000, training loss 0.0392962\n",
      "epoch 45,step 552000, training loss 0.0310109\n",
      "epoch 45,step 575000, training loss 0.0280356\n",
      "epoch 45,step 598000, training loss 0.0169523\n",
      "epoch 45,step 621000, training loss 0.0399135\n",
      "epoch 45,step 644000, training loss 0.0249934\n",
      "epoch 45,step 667000, training loss 0.0288645\n",
      "epoch 45,step 690000, training loss 0.0223756\n",
      "epoch 45,step 713000, training loss 0.035847\n",
      "epoch 45,step 736000, training loss 0.021683\n",
      "epoch 45,step 759000, training loss 0.025652\n",
      "epoch 45,step 782000, training loss 0.0239958\n",
      "epoch 45,step 805000, training loss 0.0303821\n",
      "epoch 45,step 828000, training loss 0.0243002\n",
      "epoch 45,step 851000, training loss 0.0301884\n",
      "epoch 45,step 874000, training loss 0.0282112\n",
      "epoch 45,step 897000, training loss 0.0267614\n",
      "epoch 45,step 920000, training loss 0.0222287\n",
      "epoch 45,step 943000, training loss 0.0273893\n",
      "epoch 45,step 966000, training loss 0.0219971\n",
      "epoch 45,step 989000, training loss 0.0252037\n",
      "epoch 45,step 1012000, training loss 0.026596\n",
      "epoch 45,step 1035000, training loss 0.0211605\n",
      "epoch 45,step 1058000, training loss 0.028192\n",
      "epoch 45,step 1081000, training loss 0.0188477\n",
      "epoch 45,step 1104000, training loss 0.0230145\n",
      "epoch 45,step 1127000, training loss 0.0274596\n",
      "epoch 45,step 1150000, training loss 0.0229481\n",
      "epoch 45,step 1173000, training loss 0.0274492\n",
      "epoch 45,step 1196000, training loss 0.0297255\n",
      "epoch 45,step 1219000, training loss 0.0191203\n",
      "epoch 45,step 1242000, training loss 0.0233307\n",
      "epoch 45,step 1265000, training loss 0.0410219\n",
      "epoch 45,step 1288000, training loss 0.0216291\n",
      "epoch 45,step 1311000, training loss 0.0196249\n",
      "epoch 45,step 1334000, training loss 0.0251967\n",
      "epoch 45,step 1357000, training loss 0.0298262\n",
      "epoch 45,step 1380000, training loss 0.022224\n",
      "epoch 45,step 1403000, training loss 0.0286448\n",
      "epoch 45,step 1426000, training loss 0.0265586\n",
      "epoch 45,step 1449000, training loss 0.0241327\n",
      "epoch 45,step 1472000, training loss 0.0341425\n",
      "epoch 45,step 1495000, training loss 0.0366714\n",
      "epoch 45,step 1518000, training loss 0.0229034\n",
      "epoch 45,step 1541000, training loss 0.0241184\n",
      "epoch 45,step 1564000, training loss 0.0337418\n",
      "epoch 45,step 1587000, training loss 0.0215544\n",
      "epoch 45,step 1610000, training loss 0.025372\n",
      "epoch 45,step 1633000, training loss 0.0307097\n",
      "epoch 45,step 1656000, training loss 0.0180824\n",
      "epoch 45,step 1679000, training loss 0.0402975\n",
      "epoch 45,step 1702000, training loss 0.0344982\n",
      "epoch 45,step 1725000, training loss 0.0300076\n",
      "epoch 45,step 1748000, training loss 0.0245738\n",
      "epoch 45,step 1771000, training loss 0.0239925\n",
      "epoch 45,step 1794000, training loss 0.0220974\n",
      "epoch 45,step 1817000, training loss 0.0278857\n",
      "epoch 45,step 1840000, training loss 0.0177424\n",
      "epoch 45,step 1863000, training loss 0.0205641\n",
      "epoch 45,step 1886000, training loss 0.0218734\n",
      "epoch 45,step 1909000, training loss 0.0228169\n",
      "epoch 45,step 1932000, training loss 0.0207616\n",
      "epoch 45,step 1955000, training loss 0.0330757\n",
      "epoch 45,step 1978000, training loss 0.024024\n",
      "epoch 45,step 2001000, training loss 0.0323168\n",
      "epoch 45,step 2024000, training loss 0.0254507\n",
      "epoch 45,step 2047000, training loss 0.017933\n",
      "epoch 45,step 2070000, training loss 0.0239176\n",
      "epoch 45,step 2093000, training loss 0.0225298\n",
      "epoch 45,step 2116000, training loss 0.0234597\n",
      "epoch 45,step 2139000, training loss 0.0334534\n",
      "epoch 45,step 2162000, training loss 0.0268994\n",
      "epoch 45,step 2185000, training loss 0.0212622\n",
      "epoch 45,step 2208000, training loss 0.025087\n",
      "epoch 45,step 2231000, training loss 0.023078\n",
      "epoch 45,step 2254000, training loss 0.0286689\n",
      "epoch 45,step 2277000, training loss 0.0320566\n",
      "epoch 45,step 2300000, training loss 0.0251173\n",
      "epoch 45,step 2323000, training loss 0.0240343\n",
      "epoch 45,step 2346000, training loss 0.0213944\n",
      "epoch 45,step 2369000, training loss 0.0313638\n",
      "epoch 45,step 2392000, training loss 0.0198744\n",
      "epoch 45,step 2415000, training loss 0.0295316\n",
      "epoch 45,step 2438000, training loss 0.0351742\n",
      "epoch 45,step 2461000, training loss 0.0330343\n",
      "epoch 45,step 2484000, training loss 0.0260357\n",
      "epoch 45,step 2507000, training loss 0.0342334\n",
      "epoch 45,step 2530000, training loss 0.0441147\n",
      "epoch 45,step 2553000, training loss 0.033916\n",
      "epoch 45,step 2576000, training loss 0.0369932\n",
      "epoch 45,step 2599000, training loss 0.0269891\n",
      "epoch 45,step 2622000, training loss 0.0210339\n",
      "epoch 45,step 2645000, training loss 0.0405986\n",
      "epoch 45,step 2668000, training loss 0.0197253\n",
      "epoch 45,step 2691000, training loss 0.0257726\n",
      "epoch 45,step 2714000, training loss 0.0210736\n",
      "epoch 45,step 2737000, training loss 0.0264075\n",
      "epoch 45,step 2760000, training loss 0.0292333\n",
      "epoch 45,step 2783000, training loss 0.0208381\n",
      "epoch 45,step 2806000, training loss 0.0201592\n",
      "epoch 45,step 2829000, training loss 0.0216685\n",
      "epoch 45,step 2852000, training loss 0.0311912\n",
      "epoch 45,step 2875000, training loss 0.0295914\n",
      "epoch 45,step 2898000, training loss 0.0265663\n",
      "epoch 45,step 2921000, training loss 0.0220199\n",
      "epoch 45,step 2944000, training loss 0.0286858\n",
      "epoch 45,step 2967000, training loss 0.0313162\n",
      "epoch 45,step 2990000, training loss 0.026945\n",
      "epoch 45,step 3013000, training loss 0.0242269\n",
      "epoch 45,step 3036000, training loss 0.0233008\n",
      "epoch 45,step 3059000, training loss 0.0219078\n",
      "epoch 45,step 3082000, training loss 0.0280405\n",
      "epoch 45,step 3105000, training loss 0.0220971\n",
      "epoch 45,step 3128000, training loss 0.0278865\n",
      "epoch 45,step 3151000, training loss 0.0252993\n",
      "epoch 45,step 3174000, training loss 0.0263756\n",
      "epoch 45,step 3197000, training loss 0.0277402\n",
      "epoch 45,step 3220000, training loss 0.0221859\n",
      "epoch 45,step 3243000, training loss 0.0250128\n",
      "epoch 45,step 3266000, training loss 0.0232637\n",
      "epoch 45,step 3289000, training loss 0.0273081\n",
      "epoch 45,step 3312000, training loss 0.0224009\n",
      "epoch 45,step 3335000, training loss 0.0340361\n",
      "epoch 45,step 3358000, training loss 0.0228969\n",
      "epoch 45,step 3381000, training loss 0.0249945\n",
      "epoch 45,step 3404000, training loss 0.0307976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45,step 3427000, training loss 0.0234515\n",
      "epoch 45,step 3450000, training loss 0.0529287\n",
      "epoch 45,step 3473000, training loss 0.0214356\n",
      "epoch 45,step 3496000, training loss 0.0209662\n",
      "epoch 45,step 3519000, training loss 0.0347888\n",
      "epoch 45,step 3542000, training loss 0.0183849\n",
      "epoch 45,step 3565000, training loss 0.0278131\n",
      "epoch 45,step 3588000, training loss 0.0199148\n",
      "epoch 45,step 3611000, training loss 0.0265442\n",
      "epoch 45,step 3634000, training loss 0.0249335\n",
      "epoch 45,step 3657000, training loss 0.028107\n",
      "epoch 45,step 3680000, training loss 0.0232979\n",
      "epoch 45,step 3703000, training loss 0.0256936\n",
      "epoch 45,step 3726000, training loss 0.0227512\n",
      "epoch 45,step 3749000, training loss 0.0289362\n",
      "epoch 45,step 3772000, training loss 0.0298052\n",
      "epoch 45,step 3795000, training loss 0.039321\n",
      "epoch 45,step 3818000, training loss 0.0292135\n",
      "epoch 45,step 3841000, training loss 0.025298\n",
      "epoch 45,step 3864000, training loss 0.0224869\n",
      "epoch 45,step 3887000, training loss 0.0324694\n",
      "epoch 45,step 3910000, training loss 0.0296402\n",
      "epoch 45,step 3933000, training loss 0.0353343\n",
      "epoch 45,step 3956000, training loss 0.0348377\n",
      "epoch 45,step 3979000, training loss 0.026595\n",
      "epoch 45,step 4002000, training loss 0.0323841\n",
      "epoch 45,step 4025000, training loss 0.0227099\n",
      "epoch 45,step 4048000, training loss 0.0224761\n",
      "epoch 45,step 4071000, training loss 0.025606\n",
      "epoch 45,step 4094000, training loss 0.0212385\n",
      "epoch 45,step 4117000, training loss 0.0311636\n",
      "epoch 45,step 4140000, training loss 0.0521218\n",
      "epoch 45,step 4163000, training loss 0.0245307\n",
      "epoch 45,step 4186000, training loss 0.0225337\n",
      "epoch 45,step 4209000, training loss 0.0303547\n",
      "epoch 45,step 4232000, training loss 0.0231725\n",
      "epoch 45,step 4255000, training loss 0.0187857\n",
      "epoch 45,step 4278000, training loss 0.0191833\n",
      "epoch 45,step 4301000, training loss 0.0312297\n",
      "epoch 45,step 4324000, training loss 0.021413\n",
      "epoch 45,step 4347000, training loss 0.0254011\n",
      "epoch 45,step 4370000, training loss 0.0168347\n",
      "epoch 45,step 4393000, training loss 0.0214777\n",
      "epoch 45,step 4416000, training loss 0.0253446\n",
      "epoch 45,step 4439000, training loss 0.0196408\n",
      "epoch 45,step 4462000, training loss 0.0251129\n",
      "epoch 45,step 4485000, training loss 0.0283086\n",
      "epoch 45,step 4508000, training loss 0.0228505\n",
      "epoch 45,step 4531000, training loss 0.017955\n",
      "epoch 45,step 4554000, training loss 0.0190037\n",
      "epoch 45,step 4577000, training loss 0.0184467\n",
      "epoch 45,training loss 0.0314036 ,test loss 0.0346599\n",
      "epoch 46,step 23500, training loss 0.0286081\n",
      "epoch 46,step 47000, training loss 0.0356101\n",
      "epoch 46,step 70500, training loss 0.0291845\n",
      "epoch 46,step 94000, training loss 0.0169537\n",
      "epoch 46,step 117500, training loss 0.0216907\n",
      "epoch 46,step 141000, training loss 0.0206252\n",
      "epoch 46,step 164500, training loss 0.0248248\n",
      "epoch 46,step 188000, training loss 0.0201707\n",
      "epoch 46,step 211500, training loss 0.0233857\n",
      "epoch 46,step 235000, training loss 0.0220013\n",
      "epoch 46,step 258500, training loss 0.0218054\n",
      "epoch 46,step 282000, training loss 0.0296613\n",
      "epoch 46,step 305500, training loss 0.0462132\n",
      "epoch 46,step 329000, training loss 0.0235252\n",
      "epoch 46,step 352500, training loss 0.0360098\n",
      "epoch 46,step 376000, training loss 0.0261475\n",
      "epoch 46,step 399500, training loss 0.0202492\n",
      "epoch 46,step 423000, training loss 0.0199294\n",
      "epoch 46,step 446500, training loss 0.0170608\n",
      "epoch 46,step 470000, training loss 0.0264242\n",
      "epoch 46,step 493500, training loss 0.0239401\n",
      "epoch 46,step 517000, training loss 0.0280588\n",
      "epoch 46,step 540500, training loss 0.0376721\n",
      "epoch 46,step 564000, training loss 0.0313319\n",
      "epoch 46,step 587500, training loss 0.0278842\n",
      "epoch 46,step 611000, training loss 0.0168039\n",
      "epoch 46,step 634500, training loss 0.0390201\n",
      "epoch 46,step 658000, training loss 0.0241614\n",
      "epoch 46,step 681500, training loss 0.0284315\n",
      "epoch 46,step 705000, training loss 0.0219499\n",
      "epoch 46,step 728500, training loss 0.035469\n",
      "epoch 46,step 752000, training loss 0.0211965\n",
      "epoch 46,step 775500, training loss 0.0255593\n",
      "epoch 46,step 799000, training loss 0.0242614\n",
      "epoch 46,step 822500, training loss 0.0296119\n",
      "epoch 46,step 846000, training loss 0.0235557\n",
      "epoch 46,step 869500, training loss 0.0299915\n",
      "epoch 46,step 893000, training loss 0.0280017\n",
      "epoch 46,step 916500, training loss 0.0257762\n",
      "epoch 46,step 940000, training loss 0.022425\n",
      "epoch 46,step 963500, training loss 0.0275728\n",
      "epoch 46,step 987000, training loss 0.0221895\n",
      "epoch 46,step 1010500, training loss 0.0249984\n",
      "epoch 46,step 1034000, training loss 0.0256043\n",
      "epoch 46,step 1057500, training loss 0.020926\n",
      "epoch 46,step 1081000, training loss 0.0280152\n",
      "epoch 46,step 1104500, training loss 0.0183833\n",
      "epoch 46,step 1128000, training loss 0.022783\n",
      "epoch 46,step 1151500, training loss 0.0272686\n",
      "epoch 46,step 1175000, training loss 0.023382\n",
      "epoch 46,step 1198500, training loss 0.0270696\n",
      "epoch 46,step 1222000, training loss 0.0293016\n",
      "epoch 46,step 1245500, training loss 0.0187715\n",
      "epoch 46,step 1269000, training loss 0.0226535\n",
      "epoch 46,step 1292500, training loss 0.0391643\n",
      "epoch 46,step 1316000, training loss 0.0214567\n",
      "epoch 46,step 1339500, training loss 0.0191182\n",
      "epoch 46,step 1363000, training loss 0.0248123\n",
      "epoch 46,step 1386500, training loss 0.029007\n",
      "epoch 46,step 1410000, training loss 0.0212663\n",
      "epoch 46,step 1433500, training loss 0.0287995\n",
      "epoch 46,step 1457000, training loss 0.026815\n",
      "epoch 46,step 1480500, training loss 0.0246469\n",
      "epoch 46,step 1504000, training loss 0.0340387\n",
      "epoch 46,step 1527500, training loss 0.0367012\n",
      "epoch 46,step 1551000, training loss 0.0230248\n",
      "epoch 46,step 1574500, training loss 0.0239423\n",
      "epoch 46,step 1598000, training loss 0.0341239\n",
      "epoch 46,step 1621500, training loss 0.0220852\n",
      "epoch 46,step 1645000, training loss 0.0249507\n",
      "epoch 46,step 1668500, training loss 0.0302234\n",
      "epoch 46,step 1692000, training loss 0.0173431\n",
      "epoch 46,step 1715500, training loss 0.0384837\n",
      "epoch 46,step 1739000, training loss 0.034739\n",
      "epoch 46,step 1762500, training loss 0.0291639\n",
      "epoch 46,step 1786000, training loss 0.0244383\n",
      "epoch 46,step 1809500, training loss 0.0229768\n",
      "epoch 46,step 1833000, training loss 0.0221065\n",
      "epoch 46,step 1856500, training loss 0.0284729\n",
      "epoch 46,step 1880000, training loss 0.0174536\n",
      "epoch 46,step 1903500, training loss 0.0204168\n",
      "epoch 46,step 1927000, training loss 0.0222464\n",
      "epoch 46,step 1950500, training loss 0.0227283\n",
      "epoch 46,step 1974000, training loss 0.0204296\n",
      "epoch 46,step 1997500, training loss 0.0327809\n",
      "epoch 46,step 2021000, training loss 0.0236285\n",
      "epoch 46,step 2044500, training loss 0.0319361\n",
      "epoch 46,step 2068000, training loss 0.0250759\n",
      "epoch 46,step 2091500, training loss 0.0181843\n",
      "epoch 46,step 2115000, training loss 0.0244997\n",
      "epoch 46,step 2138500, training loss 0.0221091\n",
      "epoch 46,step 2162000, training loss 0.0232197\n",
      "epoch 46,step 2185500, training loss 0.0338905\n",
      "epoch 46,step 2209000, training loss 0.0263701\n",
      "epoch 46,step 2232500, training loss 0.0212016\n",
      "epoch 46,step 2256000, training loss 0.0254238\n",
      "epoch 46,step 2279500, training loss 0.0232622\n",
      "epoch 46,step 2303000, training loss 0.0290675\n",
      "epoch 46,step 2326500, training loss 0.0309208\n",
      "epoch 46,step 2350000, training loss 0.0247867\n",
      "epoch 46,step 2373500, training loss 0.024345\n",
      "epoch 46,step 2397000, training loss 0.022201\n",
      "epoch 46,step 2420500, training loss 0.0309993\n",
      "epoch 46,step 2444000, training loss 0.0201172\n",
      "epoch 46,step 2467500, training loss 0.0303226\n",
      "epoch 46,step 2491000, training loss 0.0345508\n",
      "epoch 46,step 2514500, training loss 0.0331938\n",
      "epoch 46,step 2538000, training loss 0.0255499\n",
      "epoch 46,step 2561500, training loss 0.0347312\n",
      "epoch 46,step 2585000, training loss 0.0438294\n",
      "epoch 46,step 2608500, training loss 0.0335569\n",
      "epoch 46,step 2632000, training loss 0.0356575\n",
      "epoch 46,step 2655500, training loss 0.0258439\n",
      "epoch 46,step 2679000, training loss 0.0208776\n",
      "epoch 46,step 2702500, training loss 0.0402386\n",
      "epoch 46,step 2726000, training loss 0.0187817\n",
      "epoch 46,step 2749500, training loss 0.0241952\n",
      "epoch 46,step 2773000, training loss 0.0203485\n",
      "epoch 46,step 2796500, training loss 0.0266144\n",
      "epoch 46,step 2820000, training loss 0.0296576\n",
      "epoch 46,step 2843500, training loss 0.0206558\n",
      "epoch 46,step 2867000, training loss 0.0204707\n",
      "epoch 46,step 2890500, training loss 0.0217558\n",
      "epoch 46,step 2914000, training loss 0.0303226\n",
      "epoch 46,step 2937500, training loss 0.0289242\n",
      "epoch 46,step 2961000, training loss 0.0249805\n",
      "epoch 46,step 2984500, training loss 0.0218061\n",
      "epoch 46,step 3008000, training loss 0.0288106\n",
      "epoch 46,step 3031500, training loss 0.0319618\n",
      "epoch 46,step 3055000, training loss 0.027221\n",
      "epoch 46,step 3078500, training loss 0.0227212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46,step 3102000, training loss 0.0226918\n",
      "epoch 46,step 3125500, training loss 0.0234727\n",
      "epoch 46,step 3149000, training loss 0.0280746\n",
      "epoch 46,step 3172500, training loss 0.0224575\n",
      "epoch 46,step 3196000, training loss 0.0273194\n",
      "epoch 46,step 3219500, training loss 0.0246846\n",
      "epoch 46,step 3243000, training loss 0.0265117\n",
      "epoch 46,step 3266500, training loss 0.0267721\n",
      "epoch 46,step 3290000, training loss 0.0214687\n",
      "epoch 46,step 3313500, training loss 0.0255131\n",
      "epoch 46,step 3337000, training loss 0.0235677\n",
      "epoch 46,step 3360500, training loss 0.0278871\n",
      "epoch 46,step 3384000, training loss 0.0223708\n",
      "epoch 46,step 3407500, training loss 0.0341246\n",
      "epoch 46,step 3431000, training loss 0.0220784\n",
      "epoch 46,step 3454500, training loss 0.0253174\n",
      "epoch 46,step 3478000, training loss 0.0308776\n",
      "epoch 46,step 3501500, training loss 0.0231376\n",
      "epoch 46,step 3525000, training loss 0.0531552\n",
      "epoch 46,step 3548500, training loss 0.0218354\n",
      "epoch 46,step 3572000, training loss 0.0205882\n",
      "epoch 46,step 3595500, training loss 0.0341685\n",
      "epoch 46,step 3619000, training loss 0.0184679\n",
      "epoch 46,step 3642500, training loss 0.0277156\n",
      "epoch 46,step 3666000, training loss 0.0200881\n",
      "epoch 46,step 3689500, training loss 0.026731\n",
      "epoch 46,step 3713000, training loss 0.0248014\n",
      "epoch 46,step 3736500, training loss 0.0277849\n",
      "epoch 46,step 3760000, training loss 0.0229243\n",
      "epoch 46,step 3783500, training loss 0.0263553\n",
      "epoch 46,step 3807000, training loss 0.0228818\n",
      "epoch 46,step 3830500, training loss 0.0293083\n",
      "epoch 46,step 3854000, training loss 0.0303641\n",
      "epoch 46,step 3877500, training loss 0.038236\n",
      "epoch 46,step 3901000, training loss 0.0275939\n",
      "epoch 46,step 3924500, training loss 0.0263502\n",
      "epoch 46,step 3948000, training loss 0.0217912\n",
      "epoch 46,step 3971500, training loss 0.0316571\n",
      "epoch 46,step 3995000, training loss 0.0284449\n",
      "epoch 46,step 4018500, training loss 0.0348018\n",
      "epoch 46,step 4042000, training loss 0.0341781\n",
      "epoch 46,step 4065500, training loss 0.0258637\n",
      "epoch 46,step 4089000, training loss 0.0321888\n",
      "epoch 46,step 4112500, training loss 0.0240752\n",
      "epoch 46,step 4136000, training loss 0.022658\n",
      "epoch 46,step 4159500, training loss 0.0259064\n",
      "epoch 46,step 4183000, training loss 0.0217219\n",
      "epoch 46,step 4206500, training loss 0.0307594\n",
      "epoch 46,step 4230000, training loss 0.0525471\n",
      "epoch 46,step 4253500, training loss 0.0248967\n",
      "epoch 46,step 4277000, training loss 0.0232713\n",
      "epoch 46,step 4300500, training loss 0.0312105\n",
      "epoch 46,step 4324000, training loss 0.022998\n",
      "epoch 46,step 4347500, training loss 0.0187265\n",
      "epoch 46,step 4371000, training loss 0.0194206\n",
      "epoch 46,step 4394500, training loss 0.0305291\n",
      "epoch 46,step 4418000, training loss 0.0208958\n",
      "epoch 46,step 4441500, training loss 0.0255931\n",
      "epoch 46,step 4465000, training loss 0.0169842\n",
      "epoch 46,step 4488500, training loss 0.0224488\n",
      "epoch 46,step 4512000, training loss 0.0253525\n",
      "epoch 46,step 4535500, training loss 0.0195181\n",
      "epoch 46,step 4559000, training loss 0.0254351\n",
      "epoch 46,step 4582500, training loss 0.0283245\n",
      "epoch 46,step 4606000, training loss 0.0226904\n",
      "epoch 46,step 4629500, training loss 0.0188286\n",
      "epoch 46,step 4653000, training loss 0.0194736\n",
      "epoch 46,step 4676500, training loss 0.0184943\n",
      "epoch 46,training loss 0.0322416 ,test loss 0.0345167\n",
      "epoch 47,step 24000, training loss 0.0281858\n",
      "epoch 47,step 48000, training loss 0.0353932\n",
      "epoch 47,step 72000, training loss 0.029449\n",
      "epoch 47,step 96000, training loss 0.0167787\n",
      "epoch 47,step 120000, training loss 0.0211223\n",
      "epoch 47,step 144000, training loss 0.0206868\n",
      "epoch 47,step 168000, training loss 0.0247178\n",
      "epoch 47,step 192000, training loss 0.0201896\n",
      "epoch 47,step 216000, training loss 0.022808\n",
      "epoch 47,step 240000, training loss 0.0218636\n",
      "epoch 47,step 264000, training loss 0.0226073\n",
      "epoch 47,step 288000, training loss 0.0300638\n",
      "epoch 47,step 312000, training loss 0.0472535\n",
      "epoch 47,step 336000, training loss 0.0238152\n",
      "epoch 47,step 360000, training loss 0.0357663\n",
      "epoch 47,step 384000, training loss 0.0265507\n",
      "epoch 47,step 408000, training loss 0.021178\n",
      "epoch 47,step 432000, training loss 0.0206507\n",
      "epoch 47,step 456000, training loss 0.0170198\n",
      "epoch 47,step 480000, training loss 0.0266632\n",
      "epoch 47,step 504000, training loss 0.0243873\n",
      "epoch 47,step 528000, training loss 0.0291284\n",
      "epoch 47,step 552000, training loss 0.0390254\n",
      "epoch 47,step 576000, training loss 0.0320238\n",
      "epoch 47,step 600000, training loss 0.027175\n",
      "epoch 47,step 624000, training loss 0.0166065\n",
      "epoch 47,step 648000, training loss 0.0391983\n",
      "epoch 47,step 672000, training loss 0.024412\n",
      "epoch 47,step 696000, training loss 0.0286006\n",
      "epoch 47,step 720000, training loss 0.022275\n",
      "epoch 47,step 744000, training loss 0.0350105\n",
      "epoch 47,step 768000, training loss 0.0214963\n",
      "epoch 47,step 792000, training loss 0.0257815\n",
      "epoch 47,step 816000, training loss 0.0236367\n",
      "epoch 47,step 840000, training loss 0.0292316\n",
      "epoch 47,step 864000, training loss 0.0229144\n",
      "epoch 47,step 888000, training loss 0.0300279\n",
      "epoch 47,step 912000, training loss 0.0270989\n",
      "epoch 47,step 936000, training loss 0.0260098\n",
      "epoch 47,step 960000, training loss 0.0221334\n",
      "epoch 47,step 984000, training loss 0.0266696\n",
      "epoch 47,step 1008000, training loss 0.0223022\n",
      "epoch 47,step 1032000, training loss 0.0246663\n",
      "epoch 47,step 1056000, training loss 0.0254054\n",
      "epoch 47,step 1080000, training loss 0.0203524\n",
      "epoch 47,step 1104000, training loss 0.0270801\n",
      "epoch 47,step 1128000, training loss 0.018779\n",
      "epoch 47,step 1152000, training loss 0.0223619\n",
      "epoch 47,step 1176000, training loss 0.0266412\n",
      "epoch 47,step 1200000, training loss 0.0235358\n",
      "epoch 47,step 1224000, training loss 0.02663\n",
      "epoch 47,step 1248000, training loss 0.0297733\n",
      "epoch 47,step 1272000, training loss 0.0197226\n",
      "epoch 47,step 1296000, training loss 0.0227622\n",
      "epoch 47,step 1320000, training loss 0.0398531\n",
      "epoch 47,step 1344000, training loss 0.0215208\n",
      "epoch 47,step 1368000, training loss 0.0188573\n",
      "epoch 47,step 1392000, training loss 0.0244021\n",
      "epoch 47,step 1416000, training loss 0.0294173\n",
      "epoch 47,step 1440000, training loss 0.0216666\n",
      "epoch 47,step 1464000, training loss 0.0280184\n",
      "epoch 47,step 1488000, training loss 0.0268351\n",
      "epoch 47,step 1512000, training loss 0.0247773\n",
      "epoch 47,step 1536000, training loss 0.0342418\n",
      "epoch 47,step 1560000, training loss 0.0361212\n",
      "epoch 47,step 1584000, training loss 0.0230009\n",
      "epoch 47,step 1608000, training loss 0.0242674\n",
      "epoch 47,step 1632000, training loss 0.0346387\n",
      "epoch 47,step 1656000, training loss 0.0214306\n",
      "epoch 47,step 1680000, training loss 0.0252378\n",
      "epoch 47,step 1704000, training loss 0.0301917\n",
      "epoch 47,step 1728000, training loss 0.017633\n",
      "epoch 47,step 1752000, training loss 0.0389984\n",
      "epoch 47,step 1776000, training loss 0.034318\n",
      "epoch 47,step 1800000, training loss 0.02896\n",
      "epoch 47,step 1824000, training loss 0.0237273\n",
      "epoch 47,step 1848000, training loss 0.0230602\n",
      "epoch 47,step 1872000, training loss 0.0221264\n",
      "epoch 47,step 1896000, training loss 0.0282279\n",
      "epoch 47,step 1920000, training loss 0.0178558\n",
      "epoch 47,step 1944000, training loss 0.0204472\n",
      "epoch 47,step 1968000, training loss 0.0215116\n",
      "epoch 47,step 1992000, training loss 0.0220996\n",
      "epoch 47,step 2016000, training loss 0.020695\n",
      "epoch 47,step 2040000, training loss 0.0325944\n",
      "epoch 47,step 2064000, training loss 0.0241584\n",
      "epoch 47,step 2088000, training loss 0.031014\n",
      "epoch 47,step 2112000, training loss 0.0242021\n",
      "epoch 47,step 2136000, training loss 0.0168705\n",
      "epoch 47,step 2160000, training loss 0.0239792\n",
      "epoch 47,step 2184000, training loss 0.0216323\n",
      "epoch 47,step 2208000, training loss 0.0225155\n",
      "epoch 47,step 2232000, training loss 0.0313493\n",
      "epoch 47,step 2256000, training loss 0.0249847\n",
      "epoch 47,step 2280000, training loss 0.0209228\n",
      "epoch 47,step 2304000, training loss 0.0245978\n",
      "epoch 47,step 2328000, training loss 0.0234799\n",
      "epoch 47,step 2352000, training loss 0.0282224\n",
      "epoch 47,step 2376000, training loss 0.0311421\n",
      "epoch 47,step 2400000, training loss 0.0241364\n",
      "epoch 47,step 2424000, training loss 0.0228063\n",
      "epoch 47,step 2448000, training loss 0.02122\n",
      "epoch 47,step 2472000, training loss 0.0308805\n",
      "epoch 47,step 2496000, training loss 0.0195149\n",
      "epoch 47,step 2520000, training loss 0.0292732\n",
      "epoch 47,step 2544000, training loss 0.0341095\n",
      "epoch 47,step 2568000, training loss 0.0330352\n",
      "epoch 47,step 2592000, training loss 0.0255389\n",
      "epoch 47,step 2616000, training loss 0.0341237\n",
      "epoch 47,step 2640000, training loss 0.0437637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47,step 2664000, training loss 0.0332721\n",
      "epoch 47,step 2688000, training loss 0.0353869\n",
      "epoch 47,step 2712000, training loss 0.0251904\n",
      "epoch 47,step 2736000, training loss 0.020726\n",
      "epoch 47,step 2760000, training loss 0.039661\n",
      "epoch 47,step 2784000, training loss 0.0186772\n",
      "epoch 47,step 2808000, training loss 0.0250261\n",
      "epoch 47,step 2832000, training loss 0.0205248\n",
      "epoch 47,step 2856000, training loss 0.0255065\n",
      "epoch 47,step 2880000, training loss 0.028515\n",
      "epoch 47,step 2904000, training loss 0.0206105\n",
      "epoch 47,step 2928000, training loss 0.0201623\n",
      "epoch 47,step 2952000, training loss 0.0215764\n",
      "epoch 47,step 2976000, training loss 0.0314022\n",
      "epoch 47,step 3000000, training loss 0.029181\n",
      "epoch 47,step 3024000, training loss 0.0261788\n",
      "epoch 47,step 3048000, training loss 0.0213383\n",
      "epoch 47,step 3072000, training loss 0.0278622\n",
      "epoch 47,step 3096000, training loss 0.0308012\n",
      "epoch 47,step 3120000, training loss 0.0266567\n",
      "epoch 47,step 3144000, training loss 0.022941\n",
      "epoch 47,step 3168000, training loss 0.0229052\n",
      "epoch 47,step 3192000, training loss 0.0208502\n",
      "epoch 47,step 3216000, training loss 0.0267288\n",
      "epoch 47,step 3240000, training loss 0.02121\n",
      "epoch 47,step 3264000, training loss 0.0266472\n",
      "epoch 47,step 3288000, training loss 0.0237461\n",
      "epoch 47,step 3312000, training loss 0.0256433\n",
      "epoch 47,step 3336000, training loss 0.0251822\n",
      "epoch 47,step 3360000, training loss 0.0211095\n",
      "epoch 47,step 3384000, training loss 0.0242272\n",
      "epoch 47,step 3408000, training loss 0.0219274\n",
      "epoch 47,step 3432000, training loss 0.0266042\n",
      "epoch 47,step 3456000, training loss 0.0219406\n",
      "epoch 47,step 3480000, training loss 0.0333417\n",
      "epoch 47,step 3504000, training loss 0.0212608\n",
      "epoch 47,step 3528000, training loss 0.0261775\n",
      "epoch 47,step 3552000, training loss 0.0310761\n",
      "epoch 47,step 3576000, training loss 0.0231999\n",
      "epoch 47,step 3600000, training loss 0.0521898\n",
      "epoch 47,step 3624000, training loss 0.0216684\n",
      "epoch 47,step 3648000, training loss 0.0206703\n",
      "epoch 47,step 3672000, training loss 0.031754\n",
      "epoch 47,step 3696000, training loss 0.0174689\n",
      "epoch 47,step 3720000, training loss 0.0269821\n",
      "epoch 47,step 3744000, training loss 0.0199736\n",
      "epoch 47,step 3768000, training loss 0.0246332\n",
      "epoch 47,step 3792000, training loss 0.0236285\n",
      "epoch 47,step 3816000, training loss 0.026504\n",
      "epoch 47,step 3840000, training loss 0.0220776\n",
      "epoch 47,step 3864000, training loss 0.024762\n",
      "epoch 47,step 3888000, training loss 0.0221145\n",
      "epoch 47,step 3912000, training loss 0.0275585\n",
      "epoch 47,step 3936000, training loss 0.028704\n",
      "epoch 47,step 3960000, training loss 0.0368916\n",
      "epoch 47,step 3984000, training loss 0.027816\n",
      "epoch 47,step 4008000, training loss 0.025807\n",
      "epoch 47,step 4032000, training loss 0.0216949\n",
      "epoch 47,step 4056000, training loss 0.0316144\n",
      "epoch 47,step 4080000, training loss 0.028564\n",
      "epoch 47,step 4104000, training loss 0.034248\n",
      "epoch 47,step 4128000, training loss 0.033671\n",
      "epoch 47,step 4152000, training loss 0.0257002\n",
      "epoch 47,step 4176000, training loss 0.0321456\n",
      "epoch 47,step 4200000, training loss 0.0243564\n",
      "epoch 47,step 4224000, training loss 0.0226669\n",
      "epoch 47,step 4248000, training loss 0.0260956\n",
      "epoch 47,step 4272000, training loss 0.0210859\n",
      "epoch 47,step 4296000, training loss 0.029064\n",
      "epoch 47,step 4320000, training loss 0.0508788\n",
      "epoch 47,step 4344000, training loss 0.0235026\n",
      "epoch 47,step 4368000, training loss 0.022352\n",
      "epoch 47,step 4392000, training loss 0.0297074\n",
      "epoch 47,step 4416000, training loss 0.0227724\n",
      "epoch 47,step 4440000, training loss 0.0175708\n",
      "epoch 47,step 4464000, training loss 0.0192017\n",
      "epoch 47,step 4488000, training loss 0.0310738\n",
      "epoch 47,step 4512000, training loss 0.0198892\n",
      "epoch 47,step 4536000, training loss 0.0245504\n",
      "epoch 47,step 4560000, training loss 0.0164864\n",
      "epoch 47,step 4584000, training loss 0.0226489\n",
      "epoch 47,step 4608000, training loss 0.0256338\n",
      "epoch 47,step 4632000, training loss 0.0193762\n",
      "epoch 47,step 4656000, training loss 0.0244049\n",
      "epoch 47,step 4680000, training loss 0.0271404\n",
      "epoch 47,step 4704000, training loss 0.0221786\n",
      "epoch 47,step 4728000, training loss 0.0170639\n",
      "epoch 47,step 4752000, training loss 0.0187021\n",
      "epoch 47,step 4776000, training loss 0.0180291\n",
      "epoch 47,training loss 0.0305798 ,test loss 0.0338086\n",
      "epoch 48,step 24500, training loss 0.0274796\n",
      "epoch 48,step 49000, training loss 0.0352475\n",
      "epoch 48,step 73500, training loss 0.0288385\n",
      "epoch 48,step 98000, training loss 0.0171859\n",
      "epoch 48,step 122500, training loss 0.021569\n",
      "epoch 48,step 147000, training loss 0.020874\n",
      "epoch 48,step 171500, training loss 0.0251559\n",
      "epoch 48,step 196000, training loss 0.0200697\n",
      "epoch 48,step 220500, training loss 0.0229953\n",
      "epoch 48,step 245000, training loss 0.0219319\n",
      "epoch 48,step 269500, training loss 0.0215088\n",
      "epoch 48,step 294000, training loss 0.0291755\n",
      "epoch 48,step 318500, training loss 0.0461561\n",
      "epoch 48,step 343000, training loss 0.0238015\n",
      "epoch 48,step 367500, training loss 0.0353103\n",
      "epoch 48,step 392000, training loss 0.0258862\n",
      "epoch 48,step 416500, training loss 0.0204332\n",
      "epoch 48,step 441000, training loss 0.0200135\n",
      "epoch 48,step 465500, training loss 0.0167137\n",
      "epoch 48,step 490000, training loss 0.0259602\n",
      "epoch 48,step 514500, training loss 0.0229579\n",
      "epoch 48,step 539000, training loss 0.0279897\n",
      "epoch 48,step 563500, training loss 0.0378767\n",
      "epoch 48,step 588000, training loss 0.0317465\n",
      "epoch 48,step 612500, training loss 0.0277823\n",
      "epoch 48,step 637000, training loss 0.0170066\n",
      "epoch 48,step 661500, training loss 0.0386812\n",
      "epoch 48,step 686000, training loss 0.0241959\n",
      "epoch 48,step 710500, training loss 0.0285732\n",
      "epoch 48,step 735000, training loss 0.0223791\n",
      "epoch 48,step 759500, training loss 0.0353563\n",
      "epoch 48,step 784000, training loss 0.0206131\n",
      "epoch 48,step 808500, training loss 0.0259094\n",
      "epoch 48,step 833000, training loss 0.0231478\n",
      "epoch 48,step 857500, training loss 0.0300606\n",
      "epoch 48,step 882000, training loss 0.0240233\n",
      "epoch 48,step 906500, training loss 0.0300544\n",
      "epoch 48,step 931000, training loss 0.027838\n",
      "epoch 48,step 955500, training loss 0.0259806\n",
      "epoch 48,step 980000, training loss 0.0226557\n",
      "epoch 48,step 1004500, training loss 0.02788\n",
      "epoch 48,step 1029000, training loss 0.0219181\n",
      "epoch 48,step 1053500, training loss 0.0253024\n",
      "epoch 48,step 1078000, training loss 0.0258209\n",
      "epoch 48,step 1102500, training loss 0.0210881\n",
      "epoch 48,step 1127000, training loss 0.0280404\n",
      "epoch 48,step 1151500, training loss 0.0184459\n",
      "epoch 48,step 1176000, training loss 0.0220173\n",
      "epoch 48,step 1200500, training loss 0.0273256\n",
      "epoch 48,step 1225000, training loss 0.0230567\n",
      "epoch 48,step 1249500, training loss 0.0269268\n",
      "epoch 48,step 1274000, training loss 0.0291534\n",
      "epoch 48,step 1298500, training loss 0.0187742\n",
      "epoch 48,step 1323000, training loss 0.0224958\n",
      "epoch 48,step 1347500, training loss 0.0396888\n",
      "epoch 48,step 1372000, training loss 0.0211754\n",
      "epoch 48,step 1396500, training loss 0.0195993\n",
      "epoch 48,step 1421000, training loss 0.0237582\n",
      "epoch 48,step 1445500, training loss 0.0282604\n",
      "epoch 48,step 1470000, training loss 0.0214386\n",
      "epoch 48,step 1494500, training loss 0.0285418\n",
      "epoch 48,step 1519000, training loss 0.0267553\n",
      "epoch 48,step 1543500, training loss 0.0243728\n",
      "epoch 48,step 1568000, training loss 0.0337876\n",
      "epoch 48,step 1592500, training loss 0.0353047\n",
      "epoch 48,step 1617000, training loss 0.0227527\n",
      "epoch 48,step 1641500, training loss 0.0229957\n",
      "epoch 48,step 1666000, training loss 0.0329212\n",
      "epoch 48,step 1690500, training loss 0.0215275\n",
      "epoch 48,step 1715000, training loss 0.0243937\n",
      "epoch 48,step 1739500, training loss 0.0297303\n",
      "epoch 48,step 1764000, training loss 0.0173133\n",
      "epoch 48,step 1788500, training loss 0.0391749\n",
      "epoch 48,step 1813000, training loss 0.0336526\n",
      "epoch 48,step 1837500, training loss 0.0272383\n",
      "epoch 48,step 1862000, training loss 0.0236274\n",
      "epoch 48,step 1886500, training loss 0.0226509\n",
      "epoch 48,step 1911000, training loss 0.0221264\n",
      "epoch 48,step 1935500, training loss 0.0277\n",
      "epoch 48,step 1960000, training loss 0.0167072\n",
      "epoch 48,step 1984500, training loss 0.0197333\n",
      "epoch 48,step 2009000, training loss 0.0210213\n",
      "epoch 48,step 2033500, training loss 0.0224769\n",
      "epoch 48,step 2058000, training loss 0.0204399\n",
      "epoch 48,step 2082500, training loss 0.0316765\n",
      "epoch 48,step 2107000, training loss 0.0233456\n",
      "epoch 48,step 2131500, training loss 0.0305854\n",
      "epoch 48,step 2156000, training loss 0.0235148\n",
      "epoch 48,step 2180500, training loss 0.0176046\n",
      "epoch 48,step 2205000, training loss 0.0241789\n",
      "epoch 48,step 2229500, training loss 0.0216296\n",
      "epoch 48,step 2254000, training loss 0.0226665\n",
      "epoch 48,step 2278500, training loss 0.0317799\n",
      "epoch 48,step 2303000, training loss 0.0254208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48,step 2327500, training loss 0.0197425\n",
      "epoch 48,step 2352000, training loss 0.0244256\n",
      "epoch 48,step 2376500, training loss 0.0221821\n",
      "epoch 48,step 2401000, training loss 0.0277562\n",
      "epoch 48,step 2425500, training loss 0.030234\n",
      "epoch 48,step 2450000, training loss 0.0230309\n",
      "epoch 48,step 2474500, training loss 0.0240613\n",
      "epoch 48,step 2499000, training loss 0.0216755\n",
      "epoch 48,step 2523500, training loss 0.030656\n",
      "epoch 48,step 2548000, training loss 0.0195657\n",
      "epoch 48,step 2572500, training loss 0.0287952\n",
      "epoch 48,step 2597000, training loss 0.0337961\n",
      "epoch 48,step 2621500, training loss 0.0343249\n",
      "epoch 48,step 2646000, training loss 0.026867\n",
      "epoch 48,step 2670500, training loss 0.0348959\n",
      "epoch 48,step 2695000, training loss 0.0441796\n",
      "epoch 48,step 2719500, training loss 0.0329118\n",
      "epoch 48,step 2744000, training loss 0.0357578\n",
      "epoch 48,step 2768500, training loss 0.0257795\n",
      "epoch 48,step 2793000, training loss 0.0210467\n",
      "epoch 48,step 2817500, training loss 0.0393834\n",
      "epoch 48,step 2842000, training loss 0.0177975\n",
      "epoch 48,step 2866500, training loss 0.0248693\n",
      "epoch 48,step 2891000, training loss 0.0206387\n",
      "epoch 48,step 2915500, training loss 0.025304\n",
      "epoch 48,step 2940000, training loss 0.0312683\n",
      "epoch 48,step 2964500, training loss 0.0197703\n",
      "epoch 48,step 2989000, training loss 0.0198601\n",
      "epoch 48,step 3013500, training loss 0.0218043\n",
      "epoch 48,step 3038000, training loss 0.0308505\n",
      "epoch 48,step 3062500, training loss 0.02943\n",
      "epoch 48,step 3087000, training loss 0.0255884\n",
      "epoch 48,step 3111500, training loss 0.0217844\n",
      "epoch 48,step 3136000, training loss 0.0272283\n",
      "epoch 48,step 3160500, training loss 0.0308919\n",
      "epoch 48,step 3185000, training loss 0.0263194\n",
      "epoch 48,step 3209500, training loss 0.0228609\n",
      "epoch 48,step 3234000, training loss 0.0229476\n",
      "epoch 48,step 3258500, training loss 0.021777\n",
      "epoch 48,step 3283000, training loss 0.0277059\n",
      "epoch 48,step 3307500, training loss 0.0218558\n",
      "epoch 48,step 3332000, training loss 0.0267215\n",
      "epoch 48,step 3356500, training loss 0.0238202\n",
      "epoch 48,step 3381000, training loss 0.0266554\n",
      "epoch 48,step 3405500, training loss 0.0249744\n",
      "epoch 48,step 3430000, training loss 0.0214189\n",
      "epoch 48,step 3454500, training loss 0.0248891\n",
      "epoch 48,step 3479000, training loss 0.0228673\n",
      "epoch 48,step 3503500, training loss 0.0268932\n",
      "epoch 48,step 3528000, training loss 0.0220584\n",
      "epoch 48,step 3552500, training loss 0.0345653\n",
      "epoch 48,step 3577000, training loss 0.021281\n",
      "epoch 48,step 3601500, training loss 0.0252409\n",
      "epoch 48,step 3626000, training loss 0.0307579\n",
      "epoch 48,step 3650500, training loss 0.0230516\n",
      "epoch 48,step 3675000, training loss 0.0518814\n",
      "epoch 48,step 3699500, training loss 0.0214264\n",
      "epoch 48,step 3724000, training loss 0.0205007\n",
      "epoch 48,step 3748500, training loss 0.0319344\n",
      "epoch 48,step 3773000, training loss 0.0178407\n",
      "epoch 48,step 3797500, training loss 0.0277995\n",
      "epoch 48,step 3822000, training loss 0.01982\n",
      "epoch 48,step 3846500, training loss 0.024775\n",
      "epoch 48,step 3871000, training loss 0.0239605\n",
      "epoch 48,step 3895500, training loss 0.0262419\n",
      "epoch 48,step 3920000, training loss 0.0225277\n",
      "epoch 48,step 3944500, training loss 0.0251615\n",
      "epoch 48,step 3969000, training loss 0.0221878\n",
      "epoch 48,step 3993500, training loss 0.0274596\n",
      "epoch 48,step 4018000, training loss 0.027646\n",
      "epoch 48,step 4042500, training loss 0.0368977\n",
      "epoch 48,step 4067000, training loss 0.0277229\n",
      "epoch 48,step 4091500, training loss 0.0252403\n",
      "epoch 48,step 4116000, training loss 0.0217364\n",
      "epoch 48,step 4140500, training loss 0.0316394\n",
      "epoch 48,step 4165000, training loss 0.0290898\n",
      "epoch 48,step 4189500, training loss 0.0347995\n",
      "epoch 48,step 4214000, training loss 0.0342791\n",
      "epoch 48,step 4238500, training loss 0.0261927\n",
      "epoch 48,step 4263000, training loss 0.0318757\n",
      "epoch 48,step 4287500, training loss 0.0235989\n",
      "epoch 48,step 4312000, training loss 0.0226633\n",
      "epoch 48,step 4336500, training loss 0.0255164\n",
      "epoch 48,step 4361000, training loss 0.0204719\n",
      "epoch 48,step 4385500, training loss 0.0292632\n",
      "epoch 48,step 4410000, training loss 0.0513867\n",
      "epoch 48,step 4434500, training loss 0.0240857\n",
      "epoch 48,step 4459000, training loss 0.0226828\n",
      "epoch 48,step 4483500, training loss 0.0303608\n",
      "epoch 48,step 4508000, training loss 0.0230278\n",
      "epoch 48,step 4532500, training loss 0.0182071\n",
      "epoch 48,step 4557000, training loss 0.0185771\n",
      "epoch 48,step 4581500, training loss 0.0311472\n",
      "epoch 48,step 4606000, training loss 0.0199242\n",
      "epoch 48,step 4630500, training loss 0.025439\n",
      "epoch 48,step 4655000, training loss 0.0173005\n",
      "epoch 48,step 4679500, training loss 0.0227236\n",
      "epoch 48,step 4704000, training loss 0.0253451\n",
      "epoch 48,step 4728500, training loss 0.0198341\n",
      "epoch 48,step 4753000, training loss 0.0248622\n",
      "epoch 48,step 4777500, training loss 0.0265639\n",
      "epoch 48,step 4802000, training loss 0.0227206\n",
      "epoch 48,step 4826500, training loss 0.0173524\n",
      "epoch 48,step 4851000, training loss 0.0184882\n",
      "epoch 48,step 4875500, training loss 0.0179247\n",
      "epoch 48,training loss 0.0310325 ,test loss 0.033865\n",
      "epoch 49,step 5000, training loss 0.0875773\n",
      "epoch 49,step 10000, training loss 0.0554645\n",
      "epoch 49,step 15000, training loss 0.0268667\n",
      "epoch 49,step 20000, training loss 0.0182997\n",
      "epoch 49,step 25000, training loss 0.0281545\n",
      "epoch 49,step 30000, training loss 0.0194853\n",
      "epoch 49,step 35000, training loss 0.019076\n",
      "epoch 49,step 40000, training loss 0.0184693\n",
      "epoch 49,step 45000, training loss 0.0245235\n",
      "epoch 49,step 50000, training loss 0.0343641\n",
      "epoch 49,step 55000, training loss 0.10401\n",
      "epoch 49,step 60000, training loss 0.0555041\n",
      "epoch 49,step 65000, training loss 0.0360171\n",
      "epoch 49,step 70000, training loss 0.0356218\n",
      "epoch 49,step 75000, training loss 0.0297516\n",
      "epoch 49,step 80000, training loss 0.0214528\n",
      "epoch 49,step 85000, training loss 0.0341036\n",
      "epoch 49,step 90000, training loss 0.024663\n",
      "epoch 49,step 95000, training loss 0.0426905\n",
      "epoch 49,step 100000, training loss 0.0167714\n",
      "epoch 49,step 105000, training loss 0.0770134\n",
      "epoch 49,step 110000, training loss 0.0358366\n",
      "epoch 49,step 115000, training loss 0.0352766\n",
      "epoch 49,step 120000, training loss 0.0308188\n",
      "epoch 49,step 125000, training loss 0.0202668\n",
      "epoch 49,step 130000, training loss 0.0252599\n",
      "epoch 49,step 135000, training loss 0.0203067\n",
      "epoch 49,step 140000, training loss 0.018564\n",
      "epoch 49,step 145000, training loss 0.0235435\n",
      "epoch 49,step 150000, training loss 0.0200947\n",
      "epoch 49,step 155000, training loss 0.0637866\n",
      "epoch 49,step 160000, training loss 0.035484\n",
      "epoch 49,step 165000, training loss 0.02704\n",
      "epoch 49,step 170000, training loss 0.0238195\n",
      "epoch 49,step 175000, training loss 0.0233827\n",
      "epoch 49,step 180000, training loss 0.0371289\n",
      "epoch 49,step 185000, training loss 0.023256\n",
      "epoch 49,step 190000, training loss 0.021922\n",
      "epoch 49,step 195000, training loss 0.0194356\n",
      "epoch 49,step 200000, training loss 0.0200744\n",
      "epoch 49,step 205000, training loss 0.063135\n",
      "epoch 49,step 210000, training loss 0.0307608\n",
      "epoch 49,step 215000, training loss 0.020289\n",
      "epoch 49,step 220000, training loss 0.0337783\n",
      "epoch 49,step 225000, training loss 0.0225812\n",
      "epoch 49,step 230000, training loss 0.0218379\n",
      "epoch 49,step 235000, training loss 0.0291323\n",
      "epoch 49,step 240000, training loss 0.0217445\n",
      "epoch 49,step 245000, training loss 0.0207799\n",
      "epoch 49,step 250000, training loss 0.0218451\n",
      "epoch 49,step 255000, training loss 0.0708017\n",
      "epoch 49,step 260000, training loss 0.0577764\n",
      "epoch 49,step 265000, training loss 0.0272418\n",
      "epoch 49,step 270000, training loss 0.0242675\n",
      "epoch 49,step 275000, training loss 0.0211929\n",
      "epoch 49,step 280000, training loss 0.021875\n",
      "epoch 49,step 285000, training loss 0.0185896\n",
      "epoch 49,step 290000, training loss 0.0300338\n",
      "epoch 49,step 295000, training loss 0.0325698\n",
      "epoch 49,step 300000, training loss 0.0295051\n",
      "epoch 49,step 305000, training loss 0.0663039\n",
      "epoch 49,step 310000, training loss 0.0449892\n",
      "epoch 49,step 315000, training loss 0.0291905\n",
      "epoch 49,step 320000, training loss 0.0253236\n",
      "epoch 49,step 325000, training loss 0.0466725\n",
      "epoch 49,step 330000, training loss 0.0410488\n",
      "epoch 49,step 335000, training loss 0.0322688\n",
      "epoch 49,step 340000, training loss 0.0284358\n",
      "epoch 49,step 345000, training loss 0.0220639\n",
      "epoch 49,step 350000, training loss 0.0239199\n",
      "epoch 49,step 355000, training loss 0.0738143\n",
      "epoch 49,step 360000, training loss 0.0307427\n",
      "epoch 49,step 365000, training loss 0.0230575\n",
      "epoch 49,step 370000, training loss 0.0222354\n",
      "epoch 49,step 375000, training loss 0.0363399\n",
      "epoch 49,step 380000, training loss 0.0306472\n",
      "epoch 49,step 385000, training loss 0.0260795\n",
      "epoch 49,step 390000, training loss 0.0190456\n",
      "epoch 49,step 395000, training loss 0.0246187\n",
      "epoch 49,step 400000, training loss 0.0259602\n",
      "epoch 49,step 405000, training loss 0.0844832\n",
      "epoch 49,step 410000, training loss 0.0371477\n",
      "epoch 49,step 415000, training loss 0.05937\n",
      "epoch 49,step 420000, training loss 0.0230163\n",
      "epoch 49,step 425000, training loss 0.0197877\n",
      "epoch 49,step 430000, training loss 0.0207597\n",
      "epoch 49,step 435000, training loss 0.0228668\n",
      "epoch 49,step 440000, training loss 0.0208362\n",
      "epoch 49,step 445000, training loss 0.0175284\n",
      "epoch 49,step 450000, training loss 0.0201593\n",
      "epoch 49,step 455000, training loss 0.0702912\n",
      "epoch 49,step 460000, training loss 0.0541762\n",
      "epoch 49,step 465000, training loss 0.0279717\n",
      "epoch 49,step 470000, training loss 0.0261082\n",
      "epoch 49,step 475000, training loss 0.0173208\n",
      "epoch 49,step 480000, training loss 0.0220149\n",
      "epoch 49,step 485000, training loss 0.0206772\n",
      "epoch 49,step 490000, training loss 0.0222216\n",
      "epoch 49,step 495000, training loss 0.0251679\n",
      "epoch 49,step 500000, training loss 0.0266099\n",
      "epoch 49,step 505000, training loss 0.0783813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49,step 510000, training loss 0.0356321\n",
      "epoch 49,step 515000, training loss 0.0253428\n",
      "epoch 49,step 520000, training loss 0.0200976\n",
      "epoch 49,step 525000, training loss 0.024123\n",
      "epoch 49,step 530000, training loss 0.0225462\n",
      "epoch 49,step 535000, training loss 0.0156034\n",
      "epoch 49,step 540000, training loss 0.0328439\n",
      "epoch 49,step 545000, training loss 0.0307006\n",
      "epoch 49,step 550000, training loss 0.0285423\n",
      "epoch 49,step 555000, training loss 0.0734593\n",
      "epoch 49,step 560000, training loss 0.0504532\n",
      "epoch 49,step 565000, training loss 0.0274979\n",
      "epoch 49,step 570000, training loss 0.0407047\n",
      "epoch 49,step 575000, training loss 0.0366084\n",
      "epoch 49,step 580000, training loss 0.0482162\n",
      "epoch 49,step 585000, training loss 0.0284693\n",
      "epoch 49,step 590000, training loss 0.0453145\n",
      "epoch 49,step 595000, training loss 0.0475928\n",
      "epoch 49,step 600000, training loss 0.0306289\n",
      "epoch 49,step 605000, training loss 0.0661533\n",
      "epoch 49,step 610000, training loss 0.0426394\n",
      "epoch 49,step 615000, training loss 0.0279953\n",
      "epoch 49,step 620000, training loss 0.0234964\n",
      "epoch 49,step 625000, training loss 0.0272635\n",
      "epoch 49,step 630000, training loss 0.0186239\n",
      "epoch 49,step 635000, training loss 0.0260184\n",
      "epoch 49,step 640000, training loss 0.0238746\n",
      "epoch 49,step 645000, training loss 0.0173952\n",
      "epoch 49,step 650000, training loss 0.0162224\n",
      "epoch 49,step 655000, training loss 0.064787\n",
      "epoch 49,step 660000, training loss 0.0545603\n",
      "epoch 49,step 665000, training loss 0.0241071\n",
      "epoch 49,step 670000, training loss 0.0238306\n",
      "epoch 49,step 675000, training loss 0.0372522\n",
      "epoch 49,step 680000, training loss 0.0177744\n",
      "epoch 49,step 685000, training loss 0.0385619\n",
      "epoch 49,step 690000, training loss 0.0199945\n",
      "epoch 49,step 695000, training loss 0.0286132\n",
      "epoch 49,step 700000, training loss 0.0238043\n",
      "epoch 49,step 705000, training loss 0.101437\n",
      "epoch 49,step 710000, training loss 0.0645776\n",
      "epoch 49,step 715000, training loss 0.0355024\n",
      "epoch 49,step 720000, training loss 0.0220109\n",
      "epoch 49,step 725000, training loss 0.0278975\n",
      "epoch 49,step 730000, training loss 0.0223896\n",
      "epoch 49,step 735000, training loss 0.0315814\n",
      "epoch 49,step 740000, training loss 0.0218315\n",
      "epoch 49,step 745000, training loss 0.0216582\n",
      "epoch 49,step 750000, training loss 0.0216048\n",
      "epoch 49,step 755000, training loss 0.065337\n",
      "epoch 49,step 760000, training loss 0.0524549\n",
      "epoch 49,step 765000, training loss 0.0324002\n",
      "epoch 49,step 770000, training loss 0.0306539\n",
      "epoch 49,step 775000, training loss 0.0341673\n",
      "epoch 49,step 780000, training loss 0.0339901\n",
      "epoch 49,step 785000, training loss 0.0271748\n",
      "epoch 49,step 790000, training loss 0.0171955\n",
      "epoch 49,step 795000, training loss 0.0251813\n",
      "epoch 49,step 800000, training loss 0.0203501\n",
      "epoch 49,step 805000, training loss 0.0619925\n",
      "epoch 49,step 810000, training loss 0.0380176\n",
      "epoch 49,step 815000, training loss 0.0244356\n",
      "epoch 49,step 820000, training loss 0.0184357\n",
      "epoch 49,step 825000, training loss 0.0253045\n",
      "epoch 49,step 830000, training loss 0.0180784\n",
      "epoch 49,step 835000, training loss 0.019806\n",
      "epoch 49,step 840000, training loss 0.0277272\n",
      "epoch 49,step 845000, training loss 0.0197304\n",
      "epoch 49,step 850000, training loss 0.0233816\n",
      "epoch 49,step 855000, training loss 0.0932665\n",
      "epoch 49,step 860000, training loss 0.0413176\n",
      "epoch 49,step 865000, training loss 0.0283915\n",
      "epoch 49,step 870000, training loss 0.0440767\n",
      "epoch 49,step 875000, training loss 0.029115\n",
      "epoch 49,step 880000, training loss 0.0254915\n",
      "epoch 49,step 885000, training loss 0.0349262\n",
      "epoch 49,step 890000, training loss 0.0261345\n",
      "epoch 49,step 895000, training loss 0.0261804\n",
      "epoch 49,step 900000, training loss 0.0233483\n",
      "epoch 49,step 905000, training loss 0.0649657\n",
      "epoch 49,step 910000, training loss 0.0422869\n",
      "epoch 49,step 915000, training loss 0.0310461\n",
      "epoch 49,step 920000, training loss 0.035133\n",
      "epoch 49,step 925000, training loss 0.0289527\n",
      "epoch 49,step 930000, training loss 0.0554479\n",
      "epoch 49,step 935000, training loss 0.0312627\n",
      "epoch 49,step 940000, training loss 0.0307254\n",
      "epoch 49,step 945000, training loss 0.0277414\n",
      "epoch 49,step 950000, training loss 0.0274703\n",
      "epoch 49,step 955000, training loss 0.0601457\n",
      "epoch 49,step 960000, training loss 0.0326718\n",
      "epoch 49,step 965000, training loss 0.0199556\n",
      "epoch 49,step 970000, training loss 0.0219447\n",
      "epoch 49,step 975000, training loss 0.0253193\n",
      "epoch 49,step 980000, training loss 0.0280273\n",
      "epoch 49,step 985000, training loss 0.0192067\n",
      "epoch 49,step 990000, training loss 0.0203975\n",
      "epoch 49,step 995000, training loss 0.0213889\n",
      "epoch 49,step 1000000, training loss 0.0220687\n",
      "epoch 49,step 1005000, training loss 0.0635986\n",
      "epoch 49,step 1010000, training loss 0.0613437\n",
      "epoch 49,step 1015000, training loss 0.0294388\n",
      "epoch 49,step 1020000, training loss 0.0219844\n",
      "epoch 49,step 1025000, training loss 0.0269882\n",
      "epoch 49,step 1030000, training loss 0.0340393\n",
      "epoch 49,step 1035000, training loss 0.0217938\n",
      "epoch 49,step 1040000, training loss 0.0196099\n",
      "epoch 49,step 1045000, training loss 0.0214024\n",
      "epoch 49,step 1050000, training loss 0.0217911\n",
      "epoch 49,step 1055000, training loss 0.0854608\n",
      "epoch 49,step 1060000, training loss 0.0358067\n",
      "epoch 49,step 1065000, training loss 0.0288126\n",
      "epoch 49,step 1070000, training loss 0.0279367\n",
      "epoch 49,step 1075000, training loss 0.025296\n",
      "epoch 49,step 1080000, training loss 0.0274687\n",
      "epoch 49,step 1085000, training loss 0.0243738\n",
      "epoch 49,step 1090000, training loss 0.028592\n",
      "epoch 49,step 1095000, training loss 0.0247511\n",
      "epoch 49,step 1100000, training loss 0.0255501\n",
      "epoch 49,step 1105000, training loss 0.0839896\n",
      "epoch 49,step 1110000, training loss 0.041579\n",
      "epoch 49,step 1115000, training loss 0.0358197\n",
      "epoch 49,step 1120000, training loss 0.0292215\n",
      "epoch 49,step 1125000, training loss 0.0202943\n",
      "epoch 49,step 1130000, training loss 0.0196982\n",
      "epoch 49,step 1135000, training loss 0.0255444\n",
      "epoch 49,step 1140000, training loss 0.025078\n",
      "epoch 49,step 1145000, training loss 0.0208473\n",
      "epoch 49,step 1150000, training loss 0.0275256\n",
      "epoch 49,step 1155000, training loss 0.0659631\n",
      "epoch 49,step 1160000, training loss 0.0369397\n",
      "epoch 49,step 1165000, training loss 0.0211704\n",
      "epoch 49,step 1170000, training loss 0.0253282\n",
      "epoch 49,step 1175000, training loss 0.018228\n",
      "epoch 49,step 1180000, training loss 0.0190369\n",
      "epoch 49,step 1185000, training loss 0.0262688\n",
      "epoch 49,step 1190000, training loss 0.020514\n",
      "epoch 49,step 1195000, training loss 0.0229871\n",
      "epoch 49,step 1200000, training loss 0.0218907\n",
      "epoch 49,step 1205000, training loss 0.0823458\n",
      "epoch 49,step 1210000, training loss 0.0335332\n",
      "epoch 49,step 1215000, training loss 0.0318492\n",
      "epoch 49,step 1220000, training loss 0.0316651\n",
      "epoch 49,step 1225000, training loss 0.0273051\n",
      "epoch 49,step 1230000, training loss 0.0304041\n",
      "epoch 49,step 1235000, training loss 0.0245808\n",
      "epoch 49,step 1240000, training loss 0.0237452\n",
      "epoch 49,step 1245000, training loss 0.0273794\n",
      "epoch 49,step 1250000, training loss 0.0229944\n",
      "epoch 49,step 1255000, training loss 0.12417\n",
      "epoch 49,step 1260000, training loss 0.0399285\n",
      "epoch 49,step 1265000, training loss 0.0208856\n",
      "epoch 49,step 1270000, training loss 0.0283373\n",
      "epoch 49,step 1275000, training loss 0.0273347\n",
      "epoch 49,step 1280000, training loss 0.015636\n",
      "epoch 49,step 1285000, training loss 0.0271212\n",
      "epoch 49,step 1290000, training loss 0.0256403\n",
      "epoch 49,step 1295000, training loss 0.0382309\n",
      "epoch 49,step 1300000, training loss 0.0300868\n",
      "epoch 49,step 1305000, training loss 0.100483\n",
      "epoch 49,step 1310000, training loss 0.0559824\n",
      "epoch 49,step 1315000, training loss 0.0487271\n",
      "epoch 49,step 1320000, training loss 0.0305101\n",
      "epoch 49,step 1325000, training loss 0.0193858\n",
      "epoch 49,step 1330000, training loss 0.0197202\n",
      "epoch 49,step 1335000, training loss 0.0236079\n",
      "epoch 49,step 1340000, training loss 0.0217091\n",
      "epoch 49,step 1345000, training loss 0.0191631\n",
      "epoch 49,step 1350000, training loss 0.0222413\n",
      "epoch 49,step 1355000, training loss 0.0824799\n",
      "epoch 49,step 1360000, training loss 0.0320523\n",
      "epoch 49,step 1365000, training loss 0.0286526\n",
      "epoch 49,step 1370000, training loss 0.0368847\n",
      "epoch 49,step 1375000, training loss 0.0390756\n",
      "epoch 49,step 1380000, training loss 0.024822\n",
      "epoch 49,step 1385000, training loss 0.0193996\n",
      "epoch 49,step 1390000, training loss 0.0213881\n",
      "epoch 49,step 1395000, training loss 0.0246896\n",
      "epoch 49,step 1400000, training loss 0.0211128\n",
      "epoch 49,step 1405000, training loss 0.0904769\n",
      "epoch 49,step 1410000, training loss 0.0445726\n",
      "epoch 49,step 1415000, training loss 0.0199689\n",
      "epoch 49,step 1420000, training loss 0.0236366\n",
      "epoch 49,step 1425000, training loss 0.019119\n",
      "epoch 49,step 1430000, training loss 0.0215594\n",
      "epoch 49,step 1435000, training loss 0.0292078\n",
      "epoch 49,step 1440000, training loss 0.0200336\n",
      "epoch 49,step 1445000, training loss 0.0224257\n",
      "epoch 49,step 1450000, training loss 0.0241618\n",
      "epoch 49,step 1455000, training loss 0.071654\n",
      "epoch 49,step 1460000, training loss 0.0325351\n",
      "epoch 49,step 1465000, training loss 0.0243049\n",
      "epoch 49,step 1470000, training loss 0.0365729\n",
      "epoch 49,step 1475000, training loss 0.0287968\n",
      "epoch 49,step 1480000, training loss 0.029487\n",
      "epoch 49,step 1485000, training loss 0.0234164\n",
      "epoch 49,step 1490000, training loss 0.0289412\n",
      "epoch 49,step 1495000, training loss 0.0226892\n",
      "epoch 49,step 1500000, training loss 0.0213979\n",
      "epoch 49,step 1505000, training loss 0.0692057\n",
      "epoch 49,step 1510000, training loss 0.047472\n",
      "epoch 49,step 1515000, training loss 0.0300186\n",
      "epoch 49,step 1520000, training loss 0.0287449\n",
      "epoch 49,step 1525000, training loss 0.0278345\n",
      "epoch 49,step 1530000, training loss 0.0272423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49,step 1535000, training loss 0.0247428\n",
      "epoch 49,step 1540000, training loss 0.0353681\n",
      "epoch 49,step 1545000, training loss 0.0212153\n",
      "epoch 49,step 1550000, training loss 0.0266726\n",
      "epoch 49,step 1555000, training loss 0.0779364\n",
      "epoch 49,step 1560000, training loss 0.0589258\n",
      "epoch 49,step 1565000, training loss 0.0198635\n",
      "epoch 49,step 1570000, training loss 0.0302378\n",
      "epoch 49,step 1575000, training loss 0.0251086\n",
      "epoch 49,step 1580000, training loss 0.0350064\n",
      "epoch 49,step 1585000, training loss 0.0439807\n",
      "epoch 49,step 1590000, training loss 0.0231892\n",
      "epoch 49,step 1595000, training loss 0.0399601\n",
      "epoch 49,step 1600000, training loss 0.0344164\n",
      "epoch 49,step 1605000, training loss 0.0773826\n",
      "epoch 49,step 1610000, training loss 0.0540992\n",
      "epoch 49,step 1615000, training loss 0.0319747\n",
      "epoch 49,step 1620000, training loss 0.0303878\n",
      "epoch 49,step 1625000, training loss 0.0348856\n",
      "epoch 49,step 1630000, training loss 0.0223345\n",
      "epoch 49,step 1635000, training loss 0.0260692\n",
      "epoch 49,step 1640000, training loss 0.0231888\n",
      "epoch 49,step 1645000, training loss 0.0308466\n",
      "epoch 49,step 1650000, training loss 0.0221364\n",
      "epoch 49,step 1655000, training loss 0.0691814\n",
      "epoch 49,step 1660000, training loss 0.0635792\n",
      "epoch 49,step 1665000, training loss 0.0263729\n",
      "epoch 49,step 1670000, training loss 0.035523\n",
      "epoch 49,step 1675000, training loss 0.0236898\n",
      "epoch 49,step 1680000, training loss 0.0181943\n",
      "epoch 49,step 1685000, training loss 0.0258313\n",
      "epoch 49,step 1690000, training loss 0.0243865\n",
      "epoch 49,step 1695000, training loss 0.0248763\n",
      "epoch 49,step 1700000, training loss 0.033357\n",
      "epoch 49,step 1705000, training loss 0.0801336\n",
      "epoch 49,step 1710000, training loss 0.0462784\n",
      "epoch 49,step 1715000, training loss 0.0355269\n",
      "epoch 49,step 1720000, training loss 0.0362317\n",
      "epoch 49,step 1725000, training loss 0.0211354\n",
      "epoch 49,step 1730000, training loss 0.0212911\n",
      "epoch 49,step 1735000, training loss 0.0227318\n",
      "epoch 49,step 1740000, training loss 0.0263108\n",
      "epoch 49,step 1745000, training loss 0.0286031\n",
      "epoch 49,step 1750000, training loss 0.0243075\n",
      "epoch 49,step 1755000, training loss 0.0714413\n",
      "epoch 49,step 1760000, training loss 0.0339453\n",
      "epoch 49,step 1765000, training loss 0.0336086\n",
      "epoch 49,step 1770000, training loss 0.0257076\n",
      "epoch 49,step 1775000, training loss 0.0305105\n",
      "epoch 49,step 1780000, training loss 0.0232721\n",
      "epoch 49,step 1785000, training loss 0.0207767\n",
      "epoch 49,step 1790000, training loss 0.0197949\n",
      "epoch 49,step 1795000, training loss 0.0211292\n",
      "epoch 49,step 1800000, training loss 0.0177893\n",
      "epoch 49,step 1805000, training loss 0.0748229\n",
      "epoch 49,step 1810000, training loss 0.0402653\n",
      "epoch 49,step 1815000, training loss 0.0283502\n",
      "epoch 49,step 1820000, training loss 0.0225538\n",
      "epoch 49,step 1825000, training loss 0.0383073\n",
      "epoch 49,step 1830000, training loss 0.0295893\n",
      "epoch 49,step 1835000, training loss 0.0265584\n",
      "epoch 49,step 1840000, training loss 0.0309076\n",
      "epoch 49,step 1845000, training loss 0.0252704\n",
      "epoch 49,step 1850000, training loss 0.0339224\n",
      "epoch 49,step 1855000, training loss 0.079003\n",
      "epoch 49,step 1860000, training loss 0.0426927\n",
      "epoch 49,step 1865000, training loss 0.0237202\n",
      "epoch 49,step 1870000, training loss 0.0385511\n",
      "epoch 49,step 1875000, training loss 0.0271469\n",
      "epoch 49,step 1880000, training loss 0.0249819\n",
      "epoch 49,step 1885000, training loss 0.0247247\n",
      "epoch 49,step 1890000, training loss 0.0298061\n",
      "epoch 49,step 1895000, training loss 0.0202768\n",
      "epoch 49,step 1900000, training loss 0.0235004\n",
      "epoch 49,step 1905000, training loss 0.0695364\n",
      "epoch 49,step 1910000, training loss 0.0904915\n",
      "epoch 49,step 1915000, training loss 0.0371292\n",
      "epoch 49,step 1920000, training loss 0.0262123\n",
      "epoch 49,step 1925000, training loss 0.0225481\n",
      "epoch 49,step 1930000, training loss 0.0252512\n",
      "epoch 49,step 1935000, training loss 0.0276558\n",
      "epoch 49,step 1940000, training loss 0.0238832\n",
      "epoch 49,step 1945000, training loss 0.0277213\n",
      "epoch 49,step 1950000, training loss 0.021806\n",
      "epoch 49,step 1955000, training loss 0.0631438\n",
      "epoch 49,step 1960000, training loss 0.048025\n",
      "epoch 49,step 1965000, training loss 0.0207699\n",
      "epoch 49,step 1970000, training loss 0.0248675\n",
      "epoch 49,step 1975000, training loss 0.027167\n",
      "epoch 49,step 1980000, training loss 0.0183862\n",
      "epoch 49,step 1985000, training loss 0.0196989\n",
      "epoch 49,step 1990000, training loss 0.0308078\n",
      "epoch 49,step 1995000, training loss 0.0233951\n",
      "epoch 49,step 2000000, training loss 0.0167844\n",
      "epoch 49,step 2005000, training loss 0.081617\n",
      "epoch 49,step 2010000, training loss 0.11185\n",
      "epoch 49,step 2015000, training loss 0.0245577\n",
      "epoch 49,step 2020000, training loss 0.0239827\n",
      "epoch 49,step 2025000, training loss 0.0208159\n",
      "epoch 49,step 2030000, training loss 0.0222851\n",
      "epoch 49,step 2035000, training loss 0.0193504\n",
      "epoch 49,step 2040000, training loss 0.026302\n",
      "epoch 49,step 2045000, training loss 0.0302558\n",
      "epoch 49,step 2050000, training loss 0.0225919\n",
      "epoch 49,step 2055000, training loss 0.0678409\n",
      "epoch 49,step 2060000, training loss 0.0365191\n",
      "epoch 49,step 2065000, training loss 0.0286984\n",
      "epoch 49,step 2070000, training loss 0.0298045\n",
      "epoch 49,step 2075000, training loss 0.0220422\n",
      "epoch 49,step 2080000, training loss 0.0317331\n",
      "epoch 49,step 2085000, training loss 0.0273052\n",
      "epoch 49,step 2090000, training loss 0.0239404\n",
      "epoch 49,step 2095000, training loss 0.0188507\n",
      "epoch 49,step 2100000, training loss 0.0205802\n",
      "epoch 49,step 2105000, training loss 0.0712317\n",
      "epoch 49,step 2110000, training loss 0.0362421\n",
      "epoch 49,step 2115000, training loss 0.0127431\n",
      "epoch 49,step 2120000, training loss 0.0232032\n",
      "epoch 49,step 2125000, training loss 0.0316741\n",
      "epoch 49,step 2130000, training loss 0.0271469\n",
      "epoch 49,step 2135000, training loss 0.0288824\n",
      "epoch 49,step 2140000, training loss 0.0312899\n",
      "epoch 49,step 2145000, training loss 0.0213377\n",
      "epoch 49,step 2150000, training loss 0.0240211\n",
      "epoch 49,step 2155000, training loss 0.067903\n",
      "epoch 49,step 2160000, training loss 0.0545066\n",
      "epoch 49,step 2165000, training loss 0.0302542\n",
      "epoch 49,step 2170000, training loss 0.0242021\n",
      "epoch 49,step 2175000, training loss 0.0314102\n",
      "epoch 49,step 2180000, training loss 0.0214644\n",
      "epoch 49,step 2185000, training loss 0.0246456\n",
      "epoch 49,step 2190000, training loss 0.0251924\n",
      "epoch 49,step 2195000, training loss 0.0248336\n",
      "epoch 49,step 2200000, training loss 0.0238157\n",
      "epoch 49,step 2205000, training loss 0.0735656\n",
      "epoch 49,step 2210000, training loss 0.0380976\n",
      "epoch 49,step 2215000, training loss 0.0300752\n",
      "epoch 49,step 2220000, training loss 0.0206499\n",
      "epoch 49,step 2225000, training loss 0.0169988\n",
      "epoch 49,step 2230000, training loss 0.0176084\n",
      "epoch 49,step 2235000, training loss 0.0259926\n",
      "epoch 49,step 2240000, training loss 0.0219793\n",
      "epoch 49,step 2245000, training loss 0.0228295\n",
      "epoch 49,step 2250000, training loss 0.0234187\n",
      "epoch 49,step 2255000, training loss 0.0699241\n",
      "epoch 49,step 2260000, training loss 0.0338877\n",
      "epoch 49,step 2265000, training loss 0.0284039\n",
      "epoch 49,step 2270000, training loss 0.0268335\n",
      "epoch 49,step 2275000, training loss 0.0221744\n",
      "epoch 49,step 2280000, training loss 0.0193188\n",
      "epoch 49,step 2285000, training loss 0.0224303\n",
      "epoch 49,step 2290000, training loss 0.0228719\n",
      "epoch 49,step 2295000, training loss 0.0172416\n",
      "epoch 49,step 2300000, training loss 0.0230689\n",
      "epoch 49,step 2305000, training loss 0.0791015\n",
      "epoch 49,step 2310000, training loss 0.0509033\n",
      "epoch 49,step 2315000, training loss 0.0332237\n",
      "epoch 49,step 2320000, training loss 0.0318702\n",
      "epoch 49,step 2325000, training loss 0.0325006\n",
      "epoch 49,step 2330000, training loss 0.0246898\n",
      "epoch 49,step 2335000, training loss 0.0386268\n",
      "epoch 49,step 2340000, training loss 0.0294033\n",
      "epoch 49,step 2345000, training loss 0.024273\n",
      "epoch 49,step 2350000, training loss 0.0250746\n",
      "epoch 49,step 2355000, training loss 0.0750604\n",
      "epoch 49,step 2360000, training loss 0.075761\n",
      "epoch 49,step 2365000, training loss 0.0310448\n",
      "epoch 49,step 2370000, training loss 0.0260358\n",
      "epoch 49,step 2375000, training loss 0.0197522\n",
      "epoch 49,step 2380000, training loss 0.0371366\n",
      "epoch 49,step 2385000, training loss 0.0278468\n",
      "epoch 49,step 2390000, training loss 0.0269957\n",
      "epoch 49,step 2395000, training loss 0.0220188\n",
      "epoch 49,step 2400000, training loss 0.0241322\n",
      "epoch 49,step 2405000, training loss 0.0619531\n",
      "epoch 49,step 2410000, training loss 0.0293089\n",
      "epoch 49,step 2415000, training loss 0.0317111\n",
      "epoch 49,step 2420000, training loss 0.027498\n",
      "epoch 49,step 2425000, training loss 0.0223373\n",
      "epoch 49,step 2430000, training loss 0.0265504\n",
      "epoch 49,step 2435000, training loss 0.0226951\n",
      "epoch 49,step 2440000, training loss 0.0240941\n",
      "epoch 49,step 2445000, training loss 0.0236691\n",
      "epoch 49,step 2450000, training loss 0.027427\n",
      "epoch 49,step 2455000, training loss 0.077025\n",
      "epoch 49,step 2460000, training loss 0.0338196\n",
      "epoch 49,step 2465000, training loss 0.0206318\n",
      "epoch 49,step 2470000, training loss 0.0353798\n",
      "epoch 49,step 2475000, training loss 0.0287836\n",
      "epoch 49,step 2480000, training loss 0.0221899\n",
      "epoch 49,step 2485000, training loss 0.0237771\n",
      "epoch 49,step 2490000, training loss 0.0244845\n",
      "epoch 49,step 2495000, training loss 0.0668028\n",
      "epoch 49,step 2500000, training loss 0.0226696\n",
      "epoch 49,step 2505000, training loss 0.0763929\n",
      "epoch 49,step 2510000, training loss 0.0463788\n",
      "epoch 49,step 2515000, training loss 0.0305558\n",
      "epoch 49,step 2520000, training loss 0.0301998\n",
      "epoch 49,step 2525000, training loss 0.023506\n",
      "epoch 49,step 2530000, training loss 0.023485\n",
      "epoch 49,step 2535000, training loss 0.0181167\n",
      "epoch 49,step 2540000, training loss 0.0233017\n",
      "epoch 49,step 2545000, training loss 0.0278242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49,step 2550000, training loss 0.0206567\n",
      "epoch 49,step 2555000, training loss 0.0907979\n",
      "epoch 49,step 2560000, training loss 0.0866535\n",
      "epoch 49,step 2565000, training loss 0.0237286\n",
      "epoch 49,step 2570000, training loss 0.0262616\n",
      "epoch 49,step 2575000, training loss 0.0306915\n",
      "epoch 49,step 2580000, training loss 0.0300195\n",
      "epoch 49,step 2585000, training loss 0.0285352\n",
      "epoch 49,step 2590000, training loss 0.023252\n",
      "epoch 49,step 2595000, training loss 0.027367\n",
      "epoch 49,step 2600000, training loss 0.0201999\n",
      "epoch 49,step 2605000, training loss 0.0781298\n",
      "epoch 49,step 2610000, training loss 0.0525947\n",
      "epoch 49,step 2615000, training loss 0.0251466\n",
      "epoch 49,step 2620000, training loss 0.0248326\n",
      "epoch 49,step 2625000, training loss 0.028571\n",
      "epoch 49,step 2630000, training loss 0.0220661\n",
      "epoch 49,step 2635000, training loss 0.0278226\n",
      "epoch 49,step 2640000, training loss 0.0271924\n",
      "epoch 49,step 2645000, training loss 0.0427064\n",
      "epoch 49,step 2650000, training loss 0.0331474\n",
      "epoch 49,step 2655000, training loss 0.0994975\n",
      "epoch 49,step 2660000, training loss 0.0800585\n",
      "epoch 49,step 2665000, training loss 0.0373518\n",
      "epoch 49,step 2670000, training loss 0.0385022\n",
      "epoch 49,step 2675000, training loss 0.0331037\n",
      "epoch 49,step 2680000, training loss 0.0255277\n",
      "epoch 49,step 2685000, training loss 0.0240131\n",
      "epoch 49,step 2690000, training loss 0.0244811\n",
      "epoch 49,step 2695000, training loss 0.0267616\n",
      "epoch 49,step 2700000, training loss 0.0259525\n",
      "epoch 49,step 2705000, training loss 0.0691759\n",
      "epoch 49,step 2710000, training loss 0.0622552\n",
      "epoch 49,step 2715000, training loss 0.0416796\n",
      "epoch 49,step 2720000, training loss 0.0438485\n",
      "epoch 49,step 2725000, training loss 0.0337045\n",
      "epoch 49,step 2730000, training loss 0.02317\n",
      "epoch 49,step 2735000, training loss 0.020068\n",
      "epoch 49,step 2740000, training loss 0.0275431\n",
      "epoch 49,step 2745000, training loss 0.0244901\n",
      "epoch 49,step 2750000, training loss 0.0418749\n",
      "epoch 49,step 2755000, training loss 0.0644484\n",
      "epoch 49,step 2760000, training loss 0.0415912\n",
      "epoch 49,step 2765000, training loss 0.0297673\n",
      "epoch 49,step 2770000, training loss 0.0341438\n",
      "epoch 49,step 2775000, training loss 0.0321354\n",
      "epoch 49,step 2780000, training loss 0.0306076\n",
      "epoch 49,step 2785000, training loss 0.0247902\n",
      "epoch 49,step 2790000, training loss 0.0342727\n",
      "epoch 49,step 2795000, training loss 0.0248885\n",
      "epoch 49,step 2800000, training loss 0.0349744\n",
      "epoch 49,step 2805000, training loss 0.0823202\n",
      "epoch 49,step 2810000, training loss 0.0581557\n",
      "epoch 49,step 2815000, training loss 0.0399289\n",
      "epoch 49,step 2820000, training loss 0.0318234\n",
      "epoch 49,step 2825000, training loss 0.0266578\n",
      "epoch 49,step 2830000, training loss 0.026526\n",
      "epoch 49,step 2835000, training loss 0.0262455\n",
      "epoch 49,step 2840000, training loss 0.0232935\n",
      "epoch 49,step 2845000, training loss 0.0247951\n",
      "epoch 49,step 2850000, training loss 0.0206376\n",
      "epoch 49,step 2855000, training loss 0.0816412\n",
      "epoch 49,step 2860000, training loss 0.0565276\n",
      "epoch 49,step 2865000, training loss 0.0401449\n",
      "epoch 49,step 2870000, training loss 0.0344203\n",
      "epoch 49,step 2875000, training loss 0.0395159\n",
      "epoch 49,step 2880000, training loss 0.0312971\n",
      "epoch 49,step 2885000, training loss 0.0282937\n",
      "epoch 49,step 2890000, training loss 0.0324432\n",
      "epoch 49,step 2895000, training loss 0.0230124\n",
      "epoch 49,step 2900000, training loss 0.0186699\n",
      "epoch 49,step 2905000, training loss 0.0774582\n",
      "epoch 49,step 2910000, training loss 0.0424791\n",
      "epoch 49,step 2915000, training loss 0.025249\n",
      "epoch 49,step 2920000, training loss 0.0301121\n",
      "epoch 49,step 2925000, training loss 0.0261361\n",
      "epoch 49,step 2930000, training loss 0.027371\n",
      "epoch 49,step 2935000, training loss 0.0236248\n",
      "epoch 49,step 2940000, training loss 0.0326603\n",
      "epoch 49,step 2945000, training loss 0.029347\n",
      "epoch 49,step 2950000, training loss 0.0212548\n",
      "epoch 49,step 2955000, training loss 0.0681247\n",
      "epoch 49,step 2960000, training loss 0.0397123\n",
      "epoch 49,step 2965000, training loss 0.026805\n",
      "epoch 49,step 2970000, training loss 0.0300684\n",
      "epoch 49,step 2975000, training loss 0.0258163\n",
      "epoch 49,step 2980000, training loss 0.0273971\n",
      "epoch 49,step 2985000, training loss 0.0201851\n",
      "epoch 49,step 2990000, training loss 0.0230858\n",
      "epoch 49,step 2995000, training loss 0.0251655\n",
      "epoch 49,step 3000000, training loss 0.0373803\n",
      "epoch 49,step 3005000, training loss 0.125653\n",
      "epoch 49,step 3010000, training loss 0.0423498\n",
      "epoch 49,step 3015000, training loss 0.0281269\n",
      "epoch 49,step 3020000, training loss 0.0412757\n",
      "epoch 49,step 3025000, training loss 0.0198268\n",
      "epoch 49,step 3030000, training loss 0.0267288\n",
      "epoch 49,step 3035000, training loss 0.0232818\n",
      "epoch 49,step 3040000, training loss 0.0205356\n",
      "epoch 49,step 3045000, training loss 0.0255379\n",
      "epoch 49,step 3050000, training loss 0.0199008\n",
      "epoch 49,step 3055000, training loss 0.088248\n",
      "epoch 49,step 3060000, training loss 0.0512385\n",
      "epoch 49,step 3065000, training loss 0.0304724\n",
      "epoch 49,step 3070000, training loss 0.0214917\n",
      "epoch 49,step 3075000, training loss 0.0227848\n",
      "epoch 49,step 3080000, training loss 0.0229955\n",
      "epoch 49,step 3085000, training loss 0.0199942\n",
      "epoch 49,step 3090000, training loss 0.0205139\n",
      "epoch 49,step 3095000, training loss 0.0204688\n",
      "epoch 49,step 3100000, training loss 0.0309302\n",
      "epoch 49,step 3105000, training loss 0.102082\n",
      "epoch 49,step 3110000, training loss 0.0966875\n",
      "epoch 49,step 3115000, training loss 0.0353382\n",
      "epoch 49,step 3120000, training loss 0.0368286\n",
      "epoch 49,step 3125000, training loss 0.0285376\n",
      "epoch 49,step 3130000, training loss 0.0355545\n",
      "epoch 49,step 3135000, training loss 0.0204446\n",
      "epoch 49,step 3140000, training loss 0.0243337\n",
      "epoch 49,step 3145000, training loss 0.0223669\n",
      "epoch 49,step 3150000, training loss 0.025461\n",
      "epoch 49,step 3155000, training loss 0.0719112\n",
      "epoch 49,step 3160000, training loss 0.0416852\n",
      "epoch 49,step 3165000, training loss 0.0232226\n",
      "epoch 49,step 3170000, training loss 0.0282279\n",
      "epoch 49,step 3175000, training loss 0.0212802\n",
      "epoch 49,step 3180000, training loss 0.0180285\n",
      "epoch 49,step 3185000, training loss 0.0194156\n",
      "epoch 49,step 3190000, training loss 0.0267418\n",
      "epoch 49,step 3195000, training loss 0.0217343\n",
      "epoch 49,step 3200000, training loss 0.026875\n",
      "epoch 49,step 3205000, training loss 0.108445\n",
      "epoch 49,step 3210000, training loss 0.0527193\n",
      "epoch 49,step 3215000, training loss 0.0215171\n",
      "epoch 49,step 3220000, training loss 0.0214359\n",
      "epoch 49,step 3225000, training loss 0.0315276\n",
      "epoch 49,step 3230000, training loss 0.029264\n",
      "epoch 49,step 3235000, training loss 0.062674\n",
      "epoch 49,step 3240000, training loss 0.0195296\n",
      "epoch 49,step 3245000, training loss 0.0187019\n",
      "epoch 49,step 3250000, training loss 0.0255802\n",
      "epoch 49,step 3255000, training loss 0.0823989\n",
      "epoch 49,step 3260000, training loss 0.0311262\n",
      "epoch 49,step 3265000, training loss 0.0169736\n",
      "epoch 49,step 3270000, training loss 0.0204235\n",
      "epoch 49,step 3275000, training loss 0.0212751\n",
      "epoch 49,step 3280000, training loss 0.0211688\n",
      "epoch 49,step 3285000, training loss 0.0176665\n",
      "epoch 49,step 3290000, training loss 0.0253534\n",
      "epoch 49,step 3295000, training loss 0.037724\n",
      "epoch 49,step 3300000, training loss 0.0222476\n",
      "epoch 49,step 3305000, training loss 0.0950614\n",
      "epoch 49,step 3310000, training loss 0.0457488\n",
      "epoch 49,step 3315000, training loss 0.0258487\n",
      "epoch 49,step 3320000, training loss 0.0295957\n",
      "epoch 49,step 3325000, training loss 0.0209547\n",
      "epoch 49,step 3330000, training loss 0.0226001\n",
      "epoch 49,step 3335000, training loss 0.0252377\n",
      "epoch 49,step 3340000, training loss 0.0327147\n",
      "epoch 49,step 3345000, training loss 0.0245598\n",
      "epoch 49,step 3350000, training loss 0.0268553\n",
      "epoch 49,step 3355000, training loss 0.0945847\n",
      "epoch 49,step 3360000, training loss 0.0794364\n",
      "epoch 49,step 3365000, training loss 0.0306351\n",
      "epoch 49,step 3370000, training loss 0.0381474\n",
      "epoch 49,step 3375000, training loss 0.0221101\n",
      "epoch 49,step 3380000, training loss 0.0321942\n",
      "epoch 49,step 3385000, training loss 0.0336822\n",
      "epoch 49,step 3390000, training loss 0.0237234\n",
      "epoch 49,step 3395000, training loss 0.0224418\n",
      "epoch 49,step 3400000, training loss 0.0268225\n",
      "epoch 49,step 3405000, training loss 0.0876904\n",
      "epoch 49,step 3410000, training loss 0.0415199\n",
      "epoch 49,step 3415000, training loss 0.0306245\n",
      "epoch 49,step 3420000, training loss 0.029936\n",
      "epoch 49,step 3425000, training loss 0.0236121\n",
      "epoch 49,step 3430000, training loss 0.0243966\n",
      "epoch 49,step 3435000, training loss 0.0255752\n",
      "epoch 49,step 3440000, training loss 0.030918\n",
      "epoch 49,step 3445000, training loss 0.0247576\n",
      "epoch 49,step 3450000, training loss 0.0252419\n",
      "epoch 49,step 3455000, training loss 0.0817806\n",
      "epoch 49,step 3460000, training loss 0.0484004\n",
      "epoch 49,step 3465000, training loss 0.0216315\n",
      "epoch 49,step 3470000, training loss 0.0238331\n",
      "epoch 49,step 3475000, training loss 0.0255811\n",
      "epoch 49,step 3480000, training loss 0.0231355\n",
      "epoch 49,step 3485000, training loss 0.0241856\n",
      "epoch 49,step 3490000, training loss 0.0282149\n",
      "epoch 49,step 3495000, training loss 0.0250396\n",
      "epoch 49,step 3500000, training loss 0.0205574\n",
      "epoch 49,step 3505000, training loss 0.0751159\n",
      "epoch 49,step 3510000, training loss 0.0576388\n",
      "epoch 49,step 3515000, training loss 0.0275951\n",
      "epoch 49,step 3520000, training loss 0.0229241\n",
      "epoch 49,step 3525000, training loss 0.0245925\n",
      "epoch 49,step 3530000, training loss 0.0226186\n",
      "epoch 49,step 3535000, training loss 0.0257116\n",
      "epoch 49,step 3540000, training loss 0.0186585\n",
      "epoch 49,step 3545000, training loss 0.0207843\n",
      "epoch 49,step 3550000, training loss 0.0226751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49,step 3555000, training loss 0.0756053\n",
      "epoch 49,step 3560000, training loss 0.0496447\n",
      "epoch 49,step 3565000, training loss 0.0255611\n",
      "epoch 49,step 3570000, training loss 0.0190525\n",
      "epoch 49,step 3575000, training loss 0.0270792\n",
      "epoch 49,step 3580000, training loss 0.0243318\n",
      "epoch 49,step 3585000, training loss 0.0306171\n",
      "epoch 49,step 3590000, training loss 0.029375\n",
      "epoch 49,step 3595000, training loss 0.02075\n",
      "epoch 49,step 3600000, training loss 0.0214319\n",
      "epoch 49,step 3605000, training loss 0.0543007\n",
      "epoch 49,step 3610000, training loss 0.0382992\n",
      "epoch 49,step 3615000, training loss 0.034306\n",
      "epoch 49,step 3620000, training loss 0.0276144\n",
      "epoch 49,step 3625000, training loss 0.0322181\n",
      "epoch 49,step 3630000, training loss 0.0216196\n",
      "epoch 49,step 3635000, training loss 0.0334305\n",
      "epoch 49,step 3640000, training loss 0.0236171\n",
      "epoch 49,step 3645000, training loss 0.0232184\n",
      "epoch 49,step 3650000, training loss 0.0206343\n",
      "epoch 49,step 3655000, training loss 0.0782428\n",
      "epoch 49,step 3660000, training loss 0.0385457\n",
      "epoch 49,step 3665000, training loss 0.0246256\n",
      "epoch 49,step 3670000, training loss 0.0393696\n",
      "epoch 49,step 3675000, training loss 0.0232262\n",
      "epoch 49,step 3680000, training loss 0.0244425\n",
      "epoch 49,step 3685000, training loss 0.0251191\n",
      "epoch 49,step 3690000, training loss 0.0343263\n",
      "epoch 49,step 3695000, training loss 0.0476634\n",
      "epoch 49,step 3700000, training loss 0.0297633\n",
      "epoch 49,step 3705000, training loss 0.0597086\n",
      "epoch 49,step 3710000, training loss 0.0357058\n",
      "epoch 49,step 3715000, training loss 0.024187\n",
      "epoch 49,step 3720000, training loss 0.0214873\n",
      "epoch 49,step 3725000, training loss 0.0216174\n",
      "epoch 49,step 3730000, training loss 0.0231287\n",
      "epoch 49,step 3735000, training loss 0.0237433\n",
      "epoch 49,step 3740000, training loss 0.0208239\n",
      "epoch 49,step 3745000, training loss 0.0206257\n",
      "epoch 49,step 3750000, training loss 0.0518321\n",
      "epoch 49,step 3755000, training loss 0.0666839\n",
      "epoch 49,step 3760000, training loss 0.0346877\n",
      "epoch 49,step 3765000, training loss 0.0259338\n",
      "epoch 49,step 3770000, training loss 0.0231306\n",
      "epoch 49,step 3775000, training loss 0.0210333\n",
      "epoch 49,step 3780000, training loss 0.0239299\n",
      "epoch 49,step 3785000, training loss 0.0239216\n",
      "epoch 49,step 3790000, training loss 0.023658\n",
      "epoch 49,step 3795000, training loss 0.0161389\n",
      "epoch 49,step 3800000, training loss 0.0201772\n",
      "epoch 49,step 3805000, training loss 0.0700403\n",
      "epoch 49,step 3810000, training loss 0.0639855\n",
      "epoch 49,step 3815000, training loss 0.0266922\n",
      "epoch 49,step 3820000, training loss 0.0216435\n",
      "epoch 49,step 3825000, training loss 0.0324106\n",
      "epoch 49,step 3830000, training loss 0.0371154\n",
      "epoch 49,step 3835000, training loss 0.020505\n",
      "epoch 49,step 3840000, training loss 0.0315183\n",
      "epoch 49,step 3845000, training loss 0.0234495\n",
      "epoch 49,step 3850000, training loss 0.0176915\n",
      "epoch 49,step 3855000, training loss 0.0941164\n",
      "epoch 49,step 3860000, training loss 0.0626695\n",
      "epoch 49,step 3865000, training loss 0.0306929\n",
      "epoch 49,step 3870000, training loss 0.0308258\n",
      "epoch 49,step 3875000, training loss 0.0267503\n",
      "epoch 49,step 3880000, training loss 0.0201023\n",
      "epoch 49,step 3885000, training loss 0.0219564\n",
      "epoch 49,step 3890000, training loss 0.0284822\n",
      "epoch 49,step 3895000, training loss 0.0309836\n",
      "epoch 49,step 3900000, training loss 0.0196036\n",
      "epoch 49,step 3905000, training loss 0.0689661\n",
      "epoch 49,step 3910000, training loss 0.0553079\n",
      "epoch 49,step 3915000, training loss 0.0358466\n",
      "epoch 49,step 3920000, training loss 0.0276103\n",
      "epoch 49,step 3925000, training loss 0.0262043\n",
      "epoch 49,step 3930000, training loss 0.020785\n",
      "epoch 49,step 3935000, training loss 0.042109\n",
      "epoch 49,step 3940000, training loss 0.0313755\n",
      "epoch 49,step 3945000, training loss 0.0254385\n",
      "epoch 49,step 3950000, training loss 0.0239538\n",
      "epoch 49,step 3955000, training loss 0.102675\n",
      "epoch 49,step 3960000, training loss 0.0503867\n",
      "epoch 49,step 3965000, training loss 0.040206\n",
      "epoch 49,step 3970000, training loss 0.0399868\n",
      "epoch 49,step 3975000, training loss 0.026871\n",
      "epoch 49,step 3980000, training loss 0.0285649\n",
      "epoch 49,step 3985000, training loss 0.0202182\n",
      "epoch 49,step 3990000, training loss 0.0266123\n",
      "epoch 49,step 3995000, training loss 0.0211214\n",
      "epoch 49,step 4000000, training loss 0.0226647\n",
      "epoch 49,step 4005000, training loss 0.0876482\n",
      "epoch 49,step 4010000, training loss 0.0333431\n",
      "epoch 49,step 4015000, training loss 0.0337465\n",
      "epoch 49,step 4020000, training loss 0.0358769\n",
      "epoch 49,step 4025000, training loss 0.0266958\n",
      "epoch 49,step 4030000, training loss 0.0328986\n",
      "epoch 49,step 4035000, training loss 0.0258273\n",
      "epoch 49,step 4040000, training loss 0.0324645\n",
      "epoch 49,step 4045000, training loss 0.0267311\n",
      "epoch 49,step 4050000, training loss 0.0230224\n",
      "epoch 49,step 4055000, training loss 0.0802967\n",
      "epoch 49,step 4060000, training loss 0.0591765\n",
      "epoch 49,step 4065000, training loss 0.0317197\n",
      "epoch 49,step 4070000, training loss 0.0304526\n",
      "epoch 49,step 4075000, training loss 0.027859\n",
      "epoch 49,step 4080000, training loss 0.0223077\n",
      "epoch 49,step 4085000, training loss 0.0246953\n",
      "epoch 49,step 4090000, training loss 0.0293427\n",
      "epoch 49,step 4095000, training loss 0.0266526\n",
      "epoch 49,step 4100000, training loss 0.0299162\n",
      "epoch 49,step 4105000, training loss 0.0810291\n",
      "epoch 49,step 4110000, training loss 0.0335902\n",
      "epoch 49,step 4115000, training loss 0.0246362\n",
      "epoch 49,step 4120000, training loss 0.0345769\n",
      "epoch 49,step 4125000, training loss 0.0380526\n",
      "epoch 49,step 4130000, training loss 0.0287433\n",
      "epoch 49,step 4135000, training loss 0.0301516\n",
      "epoch 49,step 4140000, training loss 0.043359\n",
      "epoch 49,step 4145000, training loss 0.0350338\n",
      "epoch 49,step 4150000, training loss 0.0273099\n",
      "epoch 49,step 4155000, training loss 0.0758336\n",
      "epoch 49,step 4160000, training loss 0.0532993\n",
      "epoch 49,step 4165000, training loss 0.0244684\n",
      "epoch 49,step 4170000, training loss 0.0259988\n",
      "epoch 49,step 4175000, training loss 0.0245291\n",
      "epoch 49,step 4180000, training loss 0.0312311\n",
      "epoch 49,step 4185000, training loss 0.0220407\n",
      "epoch 49,step 4190000, training loss 0.0209047\n",
      "epoch 49,step 4195000, training loss 0.0300613\n",
      "epoch 49,step 4200000, training loss 0.0211758\n",
      "epoch 49,step 4205000, training loss 0.0850035\n",
      "epoch 49,step 4210000, training loss 0.058157\n",
      "epoch 49,step 4215000, training loss 0.0303349\n",
      "epoch 49,step 4220000, training loss 0.0257424\n",
      "epoch 49,step 4225000, training loss 0.0308769\n",
      "epoch 49,step 4230000, training loss 0.0278128\n",
      "epoch 49,step 4235000, training loss 0.0386783\n",
      "epoch 49,step 4240000, training loss 0.0185429\n",
      "epoch 49,step 4245000, training loss 0.0250516\n",
      "epoch 49,step 4250000, training loss 0.0279835\n",
      "epoch 49,step 4255000, training loss 0.0714007\n",
      "epoch 49,step 4260000, training loss 0.0428958\n",
      "epoch 49,step 4265000, training loss 0.0271352\n",
      "epoch 49,step 4270000, training loss 0.0337601\n",
      "epoch 49,step 4275000, training loss 0.0337793\n",
      "epoch 49,step 4280000, training loss 0.0284045\n",
      "epoch 49,step 4285000, training loss 0.0237695\n",
      "epoch 49,step 4290000, training loss 0.0292375\n",
      "epoch 49,step 4295000, training loss 0.0294955\n",
      "epoch 49,step 4300000, training loss 0.0335157\n",
      "epoch 49,step 4305000, training loss 0.0665631\n",
      "epoch 49,step 4310000, training loss 0.044366\n",
      "epoch 49,step 4315000, training loss 0.0221679\n",
      "epoch 49,step 4320000, training loss 0.0245722\n",
      "epoch 49,step 4325000, training loss 0.0246985\n",
      "epoch 49,step 4330000, training loss 0.0199442\n",
      "epoch 49,step 4335000, training loss 0.0235946\n",
      "epoch 49,step 4340000, training loss 0.0230264\n",
      "epoch 49,step 4345000, training loss 0.0290146\n",
      "epoch 49,step 4350000, training loss 0.0309708\n",
      "epoch 49,step 4355000, training loss 0.073874\n",
      "epoch 49,step 4360000, training loss 0.0349296\n",
      "epoch 49,step 4365000, training loss 0.0211681\n",
      "epoch 49,step 4370000, training loss 0.0248929\n",
      "epoch 49,step 4375000, training loss 0.0229784\n",
      "epoch 49,step 4380000, training loss 0.0330357\n",
      "epoch 49,step 4385000, training loss 0.032295\n",
      "epoch 49,step 4390000, training loss 0.0214621\n",
      "epoch 49,step 4395000, training loss 0.0168377\n",
      "epoch 49,step 4400000, training loss 0.0222613\n",
      "epoch 49,step 4405000, training loss 0.10229\n",
      "epoch 49,step 4410000, training loss 0.0644132\n",
      "epoch 49,step 4415000, training loss 0.038498\n",
      "epoch 49,step 4420000, training loss 0.024319\n",
      "epoch 49,step 4425000, training loss 0.0250228\n",
      "epoch 49,step 4430000, training loss 0.0215971\n",
      "epoch 49,step 4435000, training loss 0.0202907\n",
      "epoch 49,step 4440000, training loss 0.0210548\n",
      "epoch 49,step 4445000, training loss 0.042212\n",
      "epoch 49,step 4450000, training loss 0.0206741\n",
      "epoch 49,step 4455000, training loss 0.0830051\n",
      "epoch 49,step 4460000, training loss 0.0741722\n",
      "epoch 49,step 4465000, training loss 0.0324982\n",
      "epoch 49,step 4470000, training loss 0.0243607\n",
      "epoch 49,step 4475000, training loss 0.0290897\n",
      "epoch 49,step 4480000, training loss 0.0318591\n",
      "epoch 49,step 4485000, training loss 0.0274948\n",
      "epoch 49,step 4490000, training loss 0.0245474\n",
      "epoch 49,step 4495000, training loss 0.023836\n",
      "epoch 49,step 4500000, training loss 0.0507012\n",
      "epoch 49,step 4505000, training loss 0.0629414\n",
      "epoch 49,step 4510000, training loss 0.0278105\n",
      "epoch 49,step 4515000, training loss 0.0219106\n",
      "epoch 49,step 4520000, training loss 0.0385964\n",
      "epoch 49,step 4525000, training loss 0.0236171\n",
      "epoch 49,step 4530000, training loss 0.0195108\n",
      "epoch 49,step 4535000, training loss 0.0180557\n",
      "epoch 49,step 4540000, training loss 0.0198992\n",
      "epoch 49,step 4545000, training loss 0.0225547\n",
      "epoch 49,step 4550000, training loss 0.0227043\n",
      "epoch 49,step 4555000, training loss 0.0724611\n",
      "epoch 49,step 4560000, training loss 0.0408404\n",
      "epoch 49,step 4565000, training loss 0.0259297\n",
      "epoch 49,step 4570000, training loss 0.0291935\n",
      "epoch 49,step 4575000, training loss 0.0298888\n",
      "epoch 49,step 4580000, training loss 0.0246831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49,step 4585000, training loss 0.0232922\n",
      "epoch 49,step 4590000, training loss 0.0274827\n",
      "epoch 49,step 4595000, training loss 0.0283634\n",
      "epoch 49,step 4600000, training loss 0.0231025\n",
      "epoch 49,step 4605000, training loss 0.0605955\n",
      "epoch 49,step 4610000, training loss 0.0556325\n",
      "epoch 49,step 4615000, training loss 0.0251239\n",
      "epoch 49,step 4620000, training loss 0.0210293\n",
      "epoch 49,step 4625000, training loss 0.0181829\n",
      "epoch 49,step 4630000, training loss 0.0266065\n",
      "epoch 49,step 4635000, training loss 0.0214839\n",
      "epoch 49,step 4640000, training loss 0.0174116\n",
      "epoch 49,step 4645000, training loss 0.0214115\n",
      "epoch 49,step 4650000, training loss 0.0190269\n",
      "epoch 49,step 4655000, training loss 0.0747574\n",
      "epoch 49,step 4660000, training loss 0.0377651\n",
      "epoch 49,step 4665000, training loss 0.0209232\n",
      "epoch 49,step 4670000, training loss 0.025658\n",
      "epoch 49,step 4675000, training loss 0.031885\n",
      "epoch 49,step 4680000, training loss 0.0232258\n",
      "epoch 49,step 4685000, training loss 0.0179148\n",
      "epoch 49,step 4690000, training loss 0.0321903\n",
      "epoch 49,step 4695000, training loss 0.0205604\n",
      "epoch 49,step 4700000, training loss 0.0206803\n",
      "epoch 49,step 4705000, training loss 0.0783693\n",
      "epoch 49,step 4710000, training loss 0.0492722\n",
      "epoch 49,step 4715000, training loss 0.0200468\n",
      "epoch 49,step 4720000, training loss 0.0280164\n",
      "epoch 49,step 4725000, training loss 0.0247831\n",
      "epoch 49,step 4730000, training loss 0.0290993\n",
      "epoch 49,step 4735000, training loss 0.029166\n",
      "epoch 49,step 4740000, training loss 0.0211825\n",
      "epoch 49,step 4745000, training loss 0.0274355\n",
      "epoch 49,step 4750000, training loss 0.016831\n",
      "epoch 49,step 4755000, training loss 0.075428\n",
      "epoch 49,step 4760000, training loss 0.0645529\n",
      "epoch 49,step 4765000, training loss 0.0271512\n",
      "epoch 49,step 4770000, training loss 0.0207671\n",
      "epoch 49,step 4775000, training loss 0.0218569\n",
      "epoch 49,step 4780000, training loss 0.0265181\n",
      "epoch 49,step 4785000, training loss 0.0263118\n",
      "epoch 49,step 4790000, training loss 0.0330884\n",
      "epoch 49,step 4795000, training loss 0.0234699\n",
      "epoch 49,step 4800000, training loss 0.0243439\n",
      "epoch 49,step 4805000, training loss 0.0767223\n",
      "epoch 49,step 4810000, training loss 0.0385267\n",
      "epoch 49,step 4815000, training loss 0.0192506\n",
      "epoch 49,step 4820000, training loss 0.0182636\n",
      "epoch 49,step 4825000, training loss 0.0190164\n",
      "epoch 49,step 4830000, training loss 0.0204448\n",
      "epoch 49,step 4835000, training loss 0.0196744\n",
      "epoch 49,step 4840000, training loss 0.0199774\n",
      "epoch 49,step 4845000, training loss 0.0221056\n",
      "epoch 49,step 4850000, training loss 0.0248382\n",
      "epoch 49,step 4855000, training loss 0.0871997\n",
      "epoch 49,step 4860000, training loss 0.0421521\n",
      "epoch 49,step 4865000, training loss 0.0218936\n",
      "epoch 49,step 4870000, training loss 0.0216332\n",
      "epoch 49,step 4875000, training loss 0.027557\n",
      "epoch 49,step 4880000, training loss 0.0194343\n",
      "epoch 49,step 4885000, training loss 0.0200083\n",
      "epoch 49,step 4890000, training loss 0.0219049\n",
      "epoch 49,step 4895000, training loss 0.0197166\n",
      "epoch 49,step 4900000, training loss 0.0226226\n",
      "epoch 49,step 4905000, training loss 0.077565\n",
      "epoch 49,step 4910000, training loss 0.0473704\n",
      "epoch 49,step 4915000, training loss 0.0329029\n",
      "epoch 49,step 4920000, training loss 0.0222091\n",
      "epoch 49,step 4925000, training loss 0.0178714\n",
      "epoch 49,step 4930000, training loss 0.0436114\n",
      "epoch 49,step 4935000, training loss 0.0218462\n",
      "epoch 49,step 4940000, training loss 0.0247658\n",
      "epoch 49,step 4945000, training loss 0.0215567\n",
      "epoch 49,step 4950000, training loss 0.0190709\n",
      "epoch 49,step 4955000, training loss 0.0759332\n",
      "epoch 49,step 4960000, training loss 0.0359067\n",
      "epoch 49,step 4965000, training loss 0.0259355\n",
      "epoch 49,step 4970000, training loss 0.0169245\n",
      "epoch 49,step 4975000, training loss 0.0185349\n",
      "epoch 49,step 4980000, training loss 0.0211077\n",
      "epoch 49,step 4985000, training loss 0.0161393\n",
      "epoch 49,step 4990000, training loss 0.022543\n",
      "epoch 49,step 4995000, training loss 0.0313118\n",
      "epoch 49,training loss 0.0313118 ,test loss 0.0342586\n"
     ]
    }
   ],
   "source": [
    "train(expert_data, batch_size = 100, epochs = 50, train_from_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./tmp/model.ckpt\")\n",
    "    prediction = pred.eval(feed_dict={x: batch_x})\n",
    "    print(prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_policy(envname = \"Humanoid-v1\", render = True, max_timesteps = None, num_rollouts = 20, verbose = False):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, \"./tmp/model.ckpt\")\n",
    "\n",
    "        import gym\n",
    "        env = gym.make( envname)\n",
    "        max_steps =  max_timesteps or env.spec.timestep_limit\n",
    "\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "        for i in range(num_rollouts):\n",
    "            if verbose:\n",
    "                print('-----> iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "#                 action = policy_fn(obs[None,:])\n",
    "                len_obs = len(obs)\n",
    "                batch_x = np.array(obs).reshape(1,len_obs)\n",
    "                action = pred.eval(feed_dict={x: batch_x})\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "                obs, r, done, _ = env.step(action)\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if verbose:\n",
    "                    if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "\n",
    "        if verbose:\n",
    "            print('returns', returns)\n",
    "            print('mean return', np.mean(returns))\n",
    "            print('std of return', np.std(returns))\n",
    "\n",
    "        immit_data = {'observations': np.array(observations),\n",
    "                       'actions': np.array(actions)}\n",
    "        return immit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 15:24:03,087] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 15:24:03,123] Making new env: Humanoid-v1\n"
     ]
    }
   ],
   "source": [
    "immit_data = try_policy(num_rollouts = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14397, 376)\n",
      "(14397, 1, 1, 17)\n"
     ]
    }
   ],
   "source": [
    "print(immit_data[\"observations\"].shape)\n",
    "print(immit_data[\"actions\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the expert to evalute observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expert_with_obs(observation_ip,expert_policy_file = \"experts/Humanoid-v1.pkl\", verbose = False):\n",
    "    if verbose:\n",
    "        print('loading and building expert policy')\n",
    "    policy_fn = load_policy.load_policy( expert_policy_file)\n",
    "    if verbose:\n",
    "        print('loaded and built')\n",
    "\n",
    "    with tf.Session():\n",
    "        tf_util.initialize()\n",
    "        actions = []\n",
    "        for i in range(observation_ip.shape[0]):\n",
    "            if verbose:\n",
    "                print('-----> iter', i)\n",
    "            action = policy_fn(observation_ip[i:i+1,:])\n",
    "            actions.append(action)\n",
    "\n",
    "        expert_data = {'observations': observation_ip,\n",
    "                       'actions': np.array(actions)}\n",
    "        return expert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 376) (1, 376)\n",
      "loaded and built\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:04:40,480] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:04:40,509] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 0\n",
      "-----> iter 1\n",
      "-----> iter 2\n",
      "-----> iter 3\n",
      "-----> iter 4\n",
      "-----> iter 5\n",
      "-----> iter 6\n",
      "-----> iter 7\n",
      "-----> iter 8\n",
      "-----> iter 9\n",
      "-----> iter 10\n",
      "-----> iter 11\n",
      "-----> iter 12\n",
      "-----> iter 13\n",
      "-----> iter 14\n",
      "-----> iter 15\n",
      "-----> iter 16\n",
      "-----> iter 17\n",
      "-----> iter 18\n",
      "-----> iter 19\n",
      "-----> iter 20\n",
      "-----> iter 21\n",
      "-----> iter 22\n",
      "-----> iter 23\n",
      "-----> iter 24\n",
      "-----> iter 25\n",
      "-----> iter 26\n",
      "-----> iter 27\n",
      "-----> iter 28\n",
      "-----> iter 29\n",
      "-----> iter 30\n",
      "-----> iter 31\n",
      "-----> iter 32\n",
      "-----> iter 33\n",
      "-----> iter 34\n",
      "-----> iter 35\n",
      "-----> iter 36\n",
      "-----> iter 37\n",
      "-----> iter 38\n",
      "-----> iter 39\n",
      "-----> iter 40\n",
      "-----> iter 41\n",
      "-----> iter 42\n",
      "-----> iter 43\n",
      "-----> iter 44\n",
      "-----> iter 45\n",
      "-----> iter 46\n",
      "-----> iter 47\n",
      "-----> iter 48\n",
      "-----> iter 49\n",
      "-----> iter 50\n",
      "-----> iter 51\n",
      "-----> iter 52\n",
      "-----> iter 53\n",
      "-----> iter 54\n",
      "-----> iter 55\n",
      "-----> iter 56\n",
      "-----> iter 57\n",
      "-----> iter 58\n",
      "-----> iter 59\n",
      "-----> iter 60\n",
      "-----> iter 61\n",
      "-----> iter 62\n",
      "-----> iter 63\n",
      "-----> iter 64\n",
      "-----> iter 65\n",
      "-----> iter 66\n",
      "-----> iter 67\n",
      "-----> iter 68\n",
      "-----> iter 69\n",
      "-----> iter 70\n",
      "-----> iter 71\n",
      "-----> iter 72\n",
      "-----> iter 73\n",
      "-----> iter 74\n",
      "-----> iter 75\n",
      "-----> iter 76\n",
      "-----> iter 77\n",
      "-----> iter 78\n",
      "-----> iter 79\n",
      "-----> iter 80\n",
      "-----> iter 81\n",
      "-----> iter 82\n",
      "-----> iter 83\n",
      "-----> iter 84\n",
      "-----> iter 85\n",
      "-----> iter 86\n",
      "-----> iter 87\n",
      "-----> iter 88\n",
      "-----> iter 89\n",
      "-----> iter 90\n",
      "-----> iter 91\n",
      "-----> iter 92\n",
      "-----> iter 93\n",
      "-----> iter 94\n",
      "-----> iter 95\n",
      "-----> iter 96\n",
      "-----> iter 97\n",
      "-----> iter 98\n",
      "-----> iter 99\n",
      "-----> iter 100\n",
      "-----> iter 101\n",
      "-----> iter 102\n",
      "-----> iter 103\n",
      "-----> iter 104\n",
      "-----> iter 105\n",
      "-----> iter 106\n",
      "-----> iter 107\n",
      "-----> iter 108\n",
      "-----> iter 109\n",
      "-----> iter 110\n",
      "-----> iter 111\n",
      "-----> iter 112\n",
      "-----> iter 113\n",
      "-----> iter 114\n",
      "-----> iter 115\n",
      "-----> iter 116\n",
      "-----> iter 117\n",
      "-----> iter 118\n",
      "-----> iter 119\n",
      "-----> iter 120\n",
      "-----> iter 121\n",
      "-----> iter 122\n",
      "-----> iter 123\n",
      "-----> iter 124\n",
      "-----> iter 125\n",
      "-----> iter 126\n",
      "-----> iter 127\n",
      "-----> iter 128\n",
      "-----> iter 129\n",
      "-----> iter 130\n",
      "-----> iter 131\n",
      "-----> iter 132\n",
      "-----> iter 133\n",
      "-----> iter 134\n",
      "-----> iter 135\n",
      "-----> iter 136\n",
      "-----> iter 137\n",
      "-----> iter 138\n",
      "-----> iter 139\n",
      "-----> iter 140\n",
      "-----> iter 141\n",
      "-----> iter 142\n",
      "-----> iter 143\n",
      "-----> iter 144\n",
      "-----> iter 145\n",
      "-----> iter 146\n",
      "-----> iter 147\n",
      "-----> iter 148\n",
      "-----> iter 149\n",
      "-----> iter 150\n",
      "-----> iter 151\n",
      "-----> iter 152\n",
      "-----> iter 153\n",
      "-----> iter 154\n",
      "-----> iter 155\n",
      "-----> iter 156\n",
      "-----> iter 157\n",
      "-----> iter 158\n",
      "-----> iter 159\n",
      "-----> iter 160\n",
      "-----> iter 161\n",
      "-----> iter 162\n",
      "-----> iter 163\n",
      "-----> iter 164\n",
      "-----> iter 165\n",
      "-----> iter 166\n",
      "-----> iter 167\n",
      "-----> iter 168\n",
      "-----> iter 169\n",
      "-----> iter 170\n",
      "-----> iter 171\n",
      "-----> iter 172\n",
      "-----> iter 173\n",
      "-----> iter 174\n",
      "-----> iter 175\n",
      "-----> iter 176\n",
      "-----> iter 177\n",
      "-----> iter 178\n",
      "-----> iter 179\n",
      "-----> iter 180\n",
      "-----> iter 181\n",
      "-----> iter 182\n",
      "-----> iter 183\n",
      "-----> iter 184\n",
      "-----> iter 185\n",
      "-----> iter 186\n",
      "-----> iter 187\n",
      "-----> iter 188\n",
      "-----> iter 189\n",
      "-----> iter 190\n",
      "-----> iter 191\n",
      "-----> iter 192\n",
      "-----> iter 193\n",
      "-----> iter 194\n",
      "-----> iter 195\n",
      "-----> iter 196\n",
      "-----> iter 197\n",
      "-----> iter 198\n",
      "-----> iter 199\n",
      "-----> iter 200\n",
      "-----> iter 201\n",
      "-----> iter 202\n",
      "-----> iter 203\n",
      "-----> iter 204\n",
      "-----> iter 205\n",
      "-----> iter 206\n",
      "-----> iter 207\n",
      "-----> iter 208\n",
      "-----> iter 209\n",
      "-----> iter 210\n",
      "-----> iter 211\n",
      "-----> iter 212\n",
      "-----> iter 213\n",
      "-----> iter 214\n",
      "-----> iter 215\n",
      "-----> iter 216\n",
      "-----> iter 217\n",
      "-----> iter 218\n",
      "-----> iter 219\n",
      "-----> iter 220\n",
      "-----> iter 221\n",
      "-----> iter 222\n",
      "-----> iter 223\n",
      "-----> iter 224\n",
      "-----> iter 225\n",
      "-----> iter 226\n",
      "-----> iter 227\n",
      "-----> iter 228\n",
      "-----> iter 229\n",
      "-----> iter 230\n",
      "-----> iter 231\n",
      "-----> iter 232\n",
      "-----> iter 233\n",
      "-----> iter 234\n",
      "-----> iter 235\n",
      "-----> iter 236\n",
      "-----> iter 237\n",
      "-----> iter 238\n",
      "-----> iter 239\n",
      "-----> iter 240\n",
      "-----> iter 241\n",
      "-----> iter 242\n",
      "-----> iter 243\n",
      "-----> iter 244\n",
      "-----> iter 245\n",
      "-----> iter 246\n",
      "-----> iter 247\n",
      "-----> iter 248\n",
      "-----> iter 249\n",
      "-----> iter 250\n",
      "-----> iter 251\n",
      "-----> iter 252\n",
      "-----> iter 253\n",
      "-----> iter 254\n",
      "-----> iter 255\n",
      "-----> iter 256\n",
      "-----> iter 257\n",
      "-----> iter 258\n",
      "-----> iter 259\n",
      "-----> iter 260\n",
      "-----> iter 261\n",
      "-----> iter 262\n",
      "-----> iter 263\n",
      "-----> iter 264\n",
      "-----> iter 265\n",
      "-----> iter 266\n",
      "-----> iter 267\n",
      "-----> iter 268\n",
      "-----> iter 269\n",
      "-----> iter 270\n",
      "-----> iter 271\n",
      "-----> iter 272\n",
      "-----> iter 273\n",
      "-----> iter 274\n",
      "-----> iter 275\n",
      "-----> iter 276\n",
      "-----> iter 277\n",
      "-----> iter 278\n",
      "-----> iter 279\n",
      "-----> iter 280\n",
      "-----> iter 281\n",
      "-----> iter 282\n",
      "-----> iter 283\n",
      "-----> iter 284\n",
      "-----> iter 285\n",
      "-----> iter 286\n",
      "-----> iter 287\n",
      "-----> iter 288\n",
      "-----> iter 289\n",
      "-----> iter 290\n",
      "-----> iter 291\n",
      "-----> iter 292\n",
      "-----> iter 293\n",
      "-----> iter 294\n",
      "-----> iter 295\n",
      "-----> iter 296\n",
      "-----> iter 297\n",
      "-----> iter 298\n",
      "-----> iter 299\n",
      "-----> iter 300\n",
      "-----> iter 301\n",
      "-----> iter 302\n",
      "-----> iter 303\n",
      "-----> iter 304\n",
      "-----> iter 305\n",
      "-----> iter 306\n",
      "-----> iter 307\n",
      "-----> iter 308\n",
      "-----> iter 309\n",
      "-----> iter 310\n",
      "-----> iter 311\n",
      "-----> iter 312\n",
      "-----> iter 313\n",
      "-----> iter 314\n",
      "-----> iter 315\n",
      "-----> iter 316\n",
      "-----> iter 317\n",
      "-----> iter 318\n",
      "-----> iter 319\n",
      "-----> iter 320\n",
      "-----> iter 321\n",
      "-----> iter 322\n",
      "-----> iter 323\n",
      "-----> iter 324\n",
      "-----> iter 325\n",
      "-----> iter 326\n",
      "-----> iter 327\n",
      "-----> iter 328\n",
      "-----> iter 329\n",
      "-----> iter 330\n",
      "-----> iter 331\n",
      "-----> iter 332\n",
      "-----> iter 333\n",
      "-----> iter 334\n",
      "-----> iter 335\n",
      "-----> iter 336\n",
      "-----> iter 337\n",
      "-----> iter 338\n",
      "-----> iter 339\n",
      "-----> iter 340\n",
      "-----> iter 341\n",
      "-----> iter 342\n",
      "-----> iter 343\n",
      "-----> iter 344\n",
      "-----> iter 345\n",
      "-----> iter 346\n",
      "-----> iter 347\n",
      "-----> iter 348\n",
      "-----> iter 349\n",
      "-----> iter 350\n",
      "-----> iter 351\n",
      "-----> iter 352\n",
      "-----> iter 353\n",
      "-----> iter 354\n",
      "-----> iter 355\n",
      "-----> iter 356\n",
      "-----> iter 357\n",
      "-----> iter 358\n",
      "-----> iter 359\n",
      "-----> iter 360\n",
      "-----> iter 361\n",
      "-----> iter 362\n",
      "-----> iter 363\n",
      "-----> iter 364\n",
      "-----> iter 365\n",
      "-----> iter 366\n",
      "-----> iter 367\n",
      "-----> iter 368\n",
      "-----> iter 369\n",
      "-----> iter 370\n",
      "-----> iter 371\n",
      "-----> iter 372\n",
      "-----> iter 373\n",
      "-----> iter 374\n",
      "-----> iter 375\n",
      "-----> iter 376\n",
      "-----> iter 377\n",
      "-----> iter 378\n",
      "-----> iter 379\n",
      "-----> iter 380\n",
      "-----> iter 381\n",
      "-----> iter 382\n",
      "-----> iter 383\n",
      "-----> iter 384\n",
      "-----> iter 385\n",
      "-----> iter 386\n",
      "-----> iter 387\n",
      "-----> iter 388\n",
      "-----> iter 389\n",
      "-----> iter 390\n",
      "-----> iter 391\n",
      "-----> iter 392\n",
      "-----> iter 393\n",
      "-----> iter 394\n",
      "-----> iter 395\n",
      "-----> iter 396\n",
      "-----> iter 397\n",
      "-----> iter 398\n",
      "-----> iter 399\n",
      "-----> iter 400\n",
      "-----> iter 401\n",
      "-----> iter 402\n",
      "-----> iter 403\n",
      "-----> iter 404\n",
      "-----> iter 405\n",
      "-----> iter 406\n",
      "-----> iter 407\n",
      "-----> iter 408\n",
      "-----> iter 409\n",
      "-----> iter 410\n",
      "-----> iter 411\n",
      "-----> iter 412\n",
      "-----> iter 413\n",
      "-----> iter 414\n",
      "-----> iter 415\n",
      "-----> iter 416\n",
      "-----> iter 417\n",
      "-----> iter 418\n",
      "-----> iter 419\n",
      "-----> iter 420\n",
      "-----> iter 421\n",
      "-----> iter 422\n",
      "-----> iter 423\n",
      "-----> iter 424\n",
      "-----> iter 425\n",
      "-----> iter 426\n",
      "-----> iter 427\n",
      "-----> iter 428\n",
      "-----> iter 429\n",
      "-----> iter 430\n",
      "-----> iter 431\n",
      "-----> iter 432\n",
      "-----> iter 433\n",
      "-----> iter 434\n",
      "-----> iter 435\n",
      "-----> iter 436\n",
      "-----> iter 437\n",
      "-----> iter 438\n",
      "-----> iter 439\n",
      "-----> iter 440\n",
      "-----> iter 441\n",
      "-----> iter 442\n",
      "-----> iter 443\n",
      "-----> iter 444\n",
      "-----> iter 445\n",
      "-----> iter 446\n",
      "-----> iter 447\n",
      "-----> iter 448\n",
      "-----> iter 449\n",
      "-----> iter 450\n",
      "-----> iter 451\n",
      "-----> iter 452\n",
      "-----> iter 453\n",
      "-----> iter 454\n",
      "-----> iter 455\n",
      "-----> iter 456\n",
      "-----> iter 457\n",
      "-----> iter 458\n",
      "-----> iter 459\n",
      "-----> iter 460\n",
      "-----> iter 461\n",
      "-----> iter 462\n",
      "-----> iter 463\n",
      "-----> iter 464\n",
      "-----> iter 465\n",
      "-----> iter 466\n",
      "-----> iter 467\n",
      "-----> iter 468\n",
      "-----> iter 469\n",
      "-----> iter 470\n",
      "-----> iter 471\n",
      "-----> iter 472\n",
      "-----> iter 473\n",
      "-----> iter 474\n",
      "-----> iter 475\n",
      "-----> iter 476\n",
      "-----> iter 477\n",
      "-----> iter 478\n",
      "-----> iter 479\n",
      "-----> iter 480\n",
      "-----> iter 481\n",
      "-----> iter 482\n",
      "-----> iter 483\n",
      "-----> iter 484\n",
      "-----> iter 485\n",
      "-----> iter 486\n",
      "-----> iter 487\n",
      "-----> iter 488\n",
      "-----> iter 489\n",
      "-----> iter 490\n",
      "-----> iter 491\n",
      "-----> iter 492\n",
      "-----> iter 493\n",
      "-----> iter 494\n",
      "-----> iter 495\n",
      "-----> iter 496\n",
      "-----> iter 497\n",
      "-----> iter 498\n",
      "-----> iter 499\n",
      "-----> iter 500\n",
      "-----> iter 501\n",
      "-----> iter 502\n",
      "-----> iter 503\n",
      "-----> iter 504\n",
      "-----> iter 505\n",
      "-----> iter 506\n",
      "-----> iter 507\n",
      "-----> iter 508\n",
      "-----> iter 509\n",
      "-----> iter 510\n",
      "-----> iter 511\n",
      "-----> iter 512\n",
      "-----> iter 513\n",
      "-----> iter 514\n",
      "-----> iter 515\n",
      "-----> iter 516\n",
      "-----> iter 517\n",
      "-----> iter 518\n",
      "-----> iter 519\n",
      "-----> iter 520\n",
      "-----> iter 521\n",
      "-----> iter 522\n",
      "-----> iter 523\n",
      "-----> iter 524\n",
      "-----> iter 525\n",
      "-----> iter 526\n",
      "-----> iter 527\n",
      "-----> iter 528\n",
      "-----> iter 529\n",
      "-----> iter 530\n",
      "-----> iter 531\n",
      "-----> iter 532\n",
      "-----> iter 533\n",
      "-----> iter 534\n",
      "-----> iter 535\n",
      "-----> iter 536\n",
      "-----> iter 537\n",
      "-----> iter 538\n",
      "-----> iter 539\n",
      "-----> iter 540\n",
      "-----> iter 541\n",
      "-----> iter 542\n",
      "-----> iter 543\n",
      "-----> iter 544\n",
      "-----> iter 545\n",
      "-----> iter 546\n",
      "-----> iter 547\n",
      "-----> iter 548\n",
      "-----> iter 549\n",
      "-----> iter 550\n",
      "-----> iter 551\n",
      "-----> iter 552\n",
      "-----> iter 553\n",
      "-----> iter 554\n",
      "-----> iter 555\n",
      "-----> iter 556\n",
      "-----> iter 557\n",
      "-----> iter 558\n",
      "-----> iter 559\n",
      "-----> iter 560\n",
      "-----> iter 561\n",
      "-----> iter 562\n",
      "-----> iter 563\n",
      "-----> iter 564\n",
      "-----> iter 565\n",
      "-----> iter 566\n",
      "-----> iter 567\n",
      "-----> iter 568\n",
      "-----> iter 569\n",
      "-----> iter 570\n",
      "-----> iter 571\n",
      "-----> iter 572\n",
      "-----> iter 573\n",
      "-----> iter 574\n",
      "-----> iter 575\n",
      "-----> iter 576\n",
      "-----> iter 577\n",
      "-----> iter 578\n",
      "-----> iter 579\n",
      "-----> iter 580\n",
      "-----> iter 581\n",
      "-----> iter 582\n",
      "-----> iter 583\n",
      "-----> iter 584\n",
      "-----> iter 585\n",
      "-----> iter 586\n",
      "-----> iter 587\n",
      "-----> iter 588\n",
      "-----> iter 589\n",
      "-----> iter 590\n",
      "-----> iter 591\n",
      "-----> iter 592\n",
      "-----> iter 593\n",
      "-----> iter 594\n",
      "-----> iter 595\n",
      "-----> iter 596\n",
      "-----> iter 597\n",
      "-----> iter 598\n",
      "-----> iter 599\n",
      "-----> iter 600\n",
      "-----> iter 601\n",
      "-----> iter 602\n",
      "-----> iter 603\n",
      "-----> iter 604\n",
      "-----> iter 605\n",
      "-----> iter 606\n",
      "-----> iter 607\n",
      "-----> iter 608\n",
      "-----> iter 609\n",
      "-----> iter 610\n",
      "-----> iter 611\n",
      "-----> iter 612\n",
      "-----> iter 613\n",
      "-----> iter 614\n",
      "-----> iter 615\n",
      "-----> iter 616\n",
      "-----> iter 617\n",
      "-----> iter 618\n",
      "-----> iter 619\n",
      "-----> iter 620\n",
      "-----> iter 621\n",
      "-----> iter 622\n",
      "-----> iter 623\n",
      "-----> iter 624\n",
      "-----> iter 625\n",
      "-----> iter 626\n",
      "-----> iter 627\n",
      "-----> iter 628\n",
      "-----> iter 629\n",
      "-----> iter 630\n",
      "-----> iter 631\n",
      "-----> iter 632\n",
      "-----> iter 633\n",
      "-----> iter 634\n",
      "-----> iter 635\n",
      "-----> iter 636\n",
      "-----> iter 637\n",
      "-----> iter 638\n",
      "-----> iter 639\n",
      "-----> iter 640\n",
      "-----> iter 641\n",
      "-----> iter 642\n",
      "-----> iter 643\n",
      "-----> iter 644\n",
      "-----> iter 645\n",
      "-----> iter 646\n",
      "-----> iter 647\n",
      "-----> iter 648\n",
      "-----> iter 649\n",
      "-----> iter 650\n",
      "-----> iter 651\n",
      "-----> iter 652\n",
      "-----> iter 653\n",
      "-----> iter 654\n",
      "-----> iter 655\n",
      "-----> iter 656\n",
      "-----> iter 657\n",
      "-----> iter 658\n",
      "-----> iter 659\n",
      "-----> iter 660\n",
      "-----> iter 661\n",
      "-----> iter 662\n",
      "-----> iter 663\n",
      "-----> iter 664\n",
      "-----> iter 665\n",
      "-----> iter 666\n",
      "-----> iter 667\n",
      "-----> iter 668\n",
      "-----> iter 669\n",
      "-----> iter 670\n",
      "-----> iter 671\n",
      "-----> iter 672\n",
      "-----> iter 673\n",
      "-----> iter 674\n",
      "-----> iter 675\n",
      "-----> iter 676\n",
      "-----> iter 677\n",
      "-----> iter 678\n",
      "-----> iter 679\n",
      "-----> iter 680\n",
      "-----> iter 681\n",
      "-----> iter 682\n",
      "-----> iter 683\n",
      "-----> iter 684\n",
      "-----> iter 685\n",
      "-----> iter 686\n",
      "-----> iter 687\n",
      "-----> iter 688\n",
      "-----> iter 689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 690\n",
      "-----> iter 691\n",
      "-----> iter 692\n",
      "-----> iter 693\n",
      "-----> iter 694\n",
      "-----> iter 695\n",
      "-----> iter 696\n",
      "-----> iter 697\n",
      "-----> iter 698\n",
      "-----> iter 699\n",
      "-----> iter 700\n",
      "-----> iter 701\n",
      "-----> iter 702\n",
      "-----> iter 703\n",
      "-----> iter 704\n",
      "-----> iter 705\n",
      "-----> iter 706\n",
      "-----> iter 707\n",
      "-----> iter 708\n",
      "-----> iter 709\n",
      "-----> iter 710\n",
      "-----> iter 711\n",
      "-----> iter 712\n",
      "-----> iter 713\n",
      "-----> iter 714\n",
      "-----> iter 715\n",
      "-----> iter 716\n",
      "-----> iter 717\n",
      "-----> iter 718\n",
      "-----> iter 719\n",
      "-----> iter 720\n",
      "-----> iter 721\n",
      "-----> iter 722\n",
      "-----> iter 723\n",
      "-----> iter 724\n",
      "-----> iter 725\n",
      "-----> iter 726\n",
      "-----> iter 727\n",
      "-----> iter 728\n",
      "-----> iter 729\n",
      "-----> iter 730\n",
      "-----> iter 731\n",
      "-----> iter 732\n",
      "-----> iter 733\n",
      "-----> iter 734\n",
      "-----> iter 735\n",
      "-----> iter 736\n",
      "-----> iter 737\n",
      "-----> iter 738\n",
      "-----> iter 739\n",
      "-----> iter 740\n",
      "-----> iter 741\n",
      "-----> iter 742\n",
      "-----> iter 743\n",
      "-----> iter 744\n",
      "-----> iter 745\n",
      "-----> iter 746\n",
      "-----> iter 747\n",
      "-----> iter 748\n",
      "-----> iter 749\n",
      "-----> iter 750\n",
      "-----> iter 751\n",
      "-----> iter 752\n",
      "-----> iter 753\n",
      "-----> iter 754\n",
      "-----> iter 755\n",
      "-----> iter 756\n",
      "-----> iter 757\n",
      "-----> iter 758\n",
      "-----> iter 759\n",
      "-----> iter 760\n",
      "-----> iter 761\n",
      "-----> iter 762\n",
      "-----> iter 763\n",
      "-----> iter 764\n",
      "-----> iter 765\n",
      "-----> iter 766\n",
      "-----> iter 767\n",
      "-----> iter 768\n",
      "-----> iter 769\n",
      "-----> iter 770\n",
      "-----> iter 771\n",
      "-----> iter 772\n",
      "-----> iter 773\n",
      "-----> iter 774\n",
      "-----> iter 775\n",
      "-----> iter 776\n",
      "-----> iter 777\n",
      "-----> iter 778\n",
      "-----> iter 779\n",
      "-----> iter 780\n",
      "-----> iter 781\n",
      "-----> iter 782\n",
      "-----> iter 783\n",
      "-----> iter 784\n",
      "-----> iter 785\n",
      "-----> iter 786\n",
      "-----> iter 787\n",
      "-----> iter 788\n",
      "-----> iter 789\n",
      "-----> iter 790\n",
      "-----> iter 791\n",
      "-----> iter 792\n",
      "-----> iter 793\n",
      "-----> iter 794\n",
      "-----> iter 795\n",
      "-----> iter 796\n",
      "-----> iter 797\n",
      "-----> iter 798\n",
      "-----> iter 799\n",
      "-----> iter 800\n",
      "-----> iter 801\n",
      "-----> iter 802\n",
      "-----> iter 803\n",
      "-----> iter 804\n",
      "-----> iter 805\n",
      "-----> iter 806\n",
      "-----> iter 807\n",
      "-----> iter 808\n",
      "-----> iter 809\n",
      "-----> iter 810\n",
      "-----> iter 811\n",
      "-----> iter 812\n",
      "-----> iter 813\n",
      "-----> iter 814\n",
      "-----> iter 815\n",
      "-----> iter 816\n",
      "-----> iter 817\n",
      "-----> iter 818\n",
      "-----> iter 819\n",
      "-----> iter 820\n",
      "-----> iter 821\n",
      "-----> iter 822\n",
      "-----> iter 823\n",
      "-----> iter 824\n",
      "-----> iter 825\n",
      "-----> iter 826\n",
      "-----> iter 827\n",
      "-----> iter 828\n",
      "-----> iter 829\n",
      "-----> iter 830\n",
      "-----> iter 831\n",
      "-----> iter 832\n",
      "-----> iter 833\n",
      "-----> iter 834\n",
      "-----> iter 835\n",
      "-----> iter 836\n",
      "-----> iter 837\n",
      "-----> iter 838\n",
      "-----> iter 839\n",
      "-----> iter 840\n",
      "-----> iter 841\n",
      "-----> iter 842\n",
      "-----> iter 843\n",
      "-----> iter 844\n",
      "-----> iter 845\n",
      "-----> iter 846\n",
      "-----> iter 847\n",
      "-----> iter 848\n",
      "-----> iter 849\n",
      "-----> iter 850\n",
      "-----> iter 851\n",
      "-----> iter 852\n",
      "-----> iter 853\n",
      "-----> iter 854\n",
      "-----> iter 855\n",
      "-----> iter 856\n",
      "-----> iter 857\n",
      "-----> iter 858\n",
      "-----> iter 859\n",
      "-----> iter 860\n",
      "-----> iter 861\n",
      "-----> iter 862\n",
      "-----> iter 863\n",
      "-----> iter 864\n",
      "-----> iter 865\n",
      "-----> iter 866\n",
      "-----> iter 867\n",
      "-----> iter 868\n",
      "-----> iter 869\n",
      "-----> iter 870\n",
      "-----> iter 871\n",
      "-----> iter 872\n",
      "-----> iter 873\n",
      "-----> iter 874\n",
      "-----> iter 875\n",
      "-----> iter 876\n",
      "-----> iter 877\n",
      "-----> iter 878\n",
      "-----> iter 879\n",
      "-----> iter 880\n",
      "-----> iter 881\n",
      "-----> iter 882\n",
      "-----> iter 883\n",
      "-----> iter 884\n",
      "-----> iter 885\n",
      "-----> iter 886\n",
      "-----> iter 887\n",
      "-----> iter 888\n",
      "-----> iter 889\n",
      "-----> iter 890\n",
      "-----> iter 891\n",
      "-----> iter 892\n",
      "-----> iter 893\n",
      "-----> iter 894\n",
      "-----> iter 895\n",
      "-----> iter 896\n",
      "-----> iter 897\n",
      "-----> iter 898\n",
      "-----> iter 899\n",
      "-----> iter 900\n",
      "-----> iter 901\n",
      "-----> iter 902\n",
      "-----> iter 903\n",
      "-----> iter 904\n",
      "-----> iter 905\n",
      "-----> iter 906\n",
      "-----> iter 907\n",
      "-----> iter 908\n",
      "-----> iter 909\n",
      "-----> iter 910\n",
      "-----> iter 911\n",
      "-----> iter 912\n",
      "-----> iter 913\n",
      "-----> iter 914\n",
      "-----> iter 915\n",
      "-----> iter 916\n",
      "-----> iter 917\n",
      "-----> iter 918\n",
      "-----> iter 919\n",
      "-----> iter 920\n",
      "-----> iter 921\n",
      "-----> iter 922\n",
      "-----> iter 923\n",
      "-----> iter 924\n",
      "-----> iter 925\n",
      "-----> iter 926\n",
      "-----> iter 927\n",
      "-----> iter 928\n",
      "-----> iter 929\n",
      "-----> iter 930\n",
      "-----> iter 931\n",
      "-----> iter 932\n",
      "-----> iter 933\n",
      "-----> iter 934\n",
      "-----> iter 935\n",
      "-----> iter 936\n",
      "-----> iter 937\n",
      "-----> iter 938\n",
      "-----> iter 939\n",
      "-----> iter 940\n",
      "-----> iter 941\n",
      "-----> iter 942\n",
      "-----> iter 943\n",
      "-----> iter 944\n",
      "-----> iter 945\n",
      "-----> iter 946\n",
      "-----> iter 947\n",
      "-----> iter 948\n",
      "-----> iter 949\n",
      "-----> iter 950\n",
      "-----> iter 951\n",
      "-----> iter 952\n",
      "-----> iter 953\n",
      "-----> iter 954\n",
      "-----> iter 955\n",
      "-----> iter 956\n",
      "-----> iter 957\n",
      "-----> iter 958\n",
      "-----> iter 959\n",
      "-----> iter 960\n",
      "-----> iter 961\n",
      "-----> iter 962\n",
      "-----> iter 963\n",
      "-----> iter 964\n",
      "-----> iter 965\n",
      "-----> iter 966\n",
      "-----> iter 967\n",
      "-----> iter 968\n",
      "-----> iter 969\n",
      "-----> iter 970\n",
      "-----> iter 971\n",
      "-----> iter 972\n",
      "-----> iter 973\n",
      "-----> iter 974\n",
      "-----> iter 975\n",
      "-----> iter 976\n",
      "-----> iter 977\n",
      "-----> iter 978\n",
      "-----> iter 979\n",
      "-----> iter 980\n",
      "-----> iter 981\n",
      "-----> iter 982\n",
      "-----> iter 983\n",
      "-----> iter 984\n",
      "-----> iter 985\n",
      "-----> iter 986\n",
      "-----> iter 987\n",
      "-----> iter 988\n",
      "-----> iter 989\n",
      "-----> iter 990\n",
      "-----> iter 991\n",
      "-----> iter 992\n",
      "-----> iter 993\n",
      "-----> iter 994\n",
      "-----> iter 995\n",
      "-----> iter 996\n",
      "-----> iter 997\n",
      "-----> iter 998\n",
      "-----> iter 999\n",
      "-----> iter 1000\n",
      "-----> iter 1001\n",
      "-----> iter 1002\n",
      "-----> iter 1003\n",
      "-----> iter 1004\n",
      "-----> iter 1005\n",
      "-----> iter 1006\n",
      "-----> iter 1007\n",
      "-----> iter 1008\n",
      "-----> iter 1009\n",
      "-----> iter 1010\n",
      "-----> iter 1011\n",
      "-----> iter 1012\n",
      "-----> iter 1013\n",
      "-----> iter 1014\n",
      "-----> iter 1015\n",
      "-----> iter 1016\n",
      "-----> iter 1017\n",
      "-----> iter 1018\n",
      "-----> iter 1019\n",
      "-----> iter 1020\n",
      "-----> iter 1021\n",
      "-----> iter 1022\n",
      "-----> iter 1023\n",
      "-----> iter 1024\n",
      "-----> iter 1025\n",
      "-----> iter 1026\n",
      "-----> iter 1027\n",
      "-----> iter 1028\n",
      "-----> iter 1029\n",
      "-----> iter 1030\n",
      "-----> iter 1031\n",
      "-----> iter 1032\n",
      "-----> iter 1033\n",
      "-----> iter 1034\n",
      "-----> iter 1035\n",
      "-----> iter 1036\n",
      "-----> iter 1037\n",
      "-----> iter 1038\n",
      "-----> iter 1039\n",
      "-----> iter 1040\n",
      "-----> iter 1041\n",
      "-----> iter 1042\n",
      "-----> iter 1043\n",
      "-----> iter 1044\n",
      "-----> iter 1045\n",
      "-----> iter 1046\n",
      "-----> iter 1047\n",
      "-----> iter 1048\n",
      "-----> iter 1049\n",
      "-----> iter 1050\n",
      "-----> iter 1051\n",
      "-----> iter 1052\n",
      "-----> iter 1053\n",
      "-----> iter 1054\n",
      "-----> iter 1055\n",
      "-----> iter 1056\n",
      "-----> iter 1057\n",
      "-----> iter 1058\n",
      "-----> iter 1059\n",
      "-----> iter 1060\n",
      "-----> iter 1061\n",
      "-----> iter 1062\n",
      "-----> iter 1063\n",
      "-----> iter 1064\n",
      "-----> iter 1065\n",
      "-----> iter 1066\n",
      "-----> iter 1067\n",
      "-----> iter 1068\n",
      "-----> iter 1069\n",
      "-----> iter 1070\n",
      "-----> iter 1071\n",
      "-----> iter 1072\n",
      "-----> iter 1073\n",
      "-----> iter 1074\n",
      "-----> iter 1075\n",
      "-----> iter 1076\n",
      "-----> iter 1077\n",
      "-----> iter 1078\n",
      "-----> iter 1079\n",
      "-----> iter 1080\n",
      "-----> iter 1081\n",
      "-----> iter 1082\n",
      "-----> iter 1083\n",
      "-----> iter 1084\n",
      "-----> iter 1085\n",
      "-----> iter 1086\n",
      "-----> iter 1087\n",
      "-----> iter 1088\n",
      "-----> iter 1089\n",
      "-----> iter 1090\n",
      "-----> iter 1091\n",
      "-----> iter 1092\n",
      "-----> iter 1093\n",
      "-----> iter 1094\n",
      "-----> iter 1095\n",
      "-----> iter 1096\n",
      "-----> iter 1097\n",
      "-----> iter 1098\n",
      "-----> iter 1099\n",
      "-----> iter 1100\n",
      "-----> iter 1101\n",
      "-----> iter 1102\n",
      "-----> iter 1103\n",
      "-----> iter 1104\n",
      "-----> iter 1105\n",
      "-----> iter 1106\n",
      "-----> iter 1107\n",
      "-----> iter 1108\n",
      "-----> iter 1109\n",
      "-----> iter 1110\n",
      "-----> iter 1111\n",
      "-----> iter 1112\n",
      "-----> iter 1113\n",
      "-----> iter 1114\n",
      "-----> iter 1115\n",
      "-----> iter 1116\n",
      "-----> iter 1117\n",
      "-----> iter 1118\n",
      "-----> iter 1119\n",
      "-----> iter 1120\n",
      "-----> iter 1121\n",
      "-----> iter 1122\n",
      "-----> iter 1123\n",
      "-----> iter 1124\n",
      "-----> iter 1125\n",
      "-----> iter 1126\n",
      "-----> iter 1127\n",
      "-----> iter 1128\n",
      "-----> iter 1129\n",
      "-----> iter 1130\n",
      "-----> iter 1131\n",
      "-----> iter 1132\n",
      "-----> iter 1133\n",
      "-----> iter 1134\n",
      "-----> iter 1135\n",
      "-----> iter 1136\n",
      "-----> iter 1137\n",
      "-----> iter 1138\n",
      "-----> iter 1139\n",
      "-----> iter 1140\n",
      "-----> iter 1141\n",
      "-----> iter 1142\n",
      "-----> iter 1143\n",
      "-----> iter 1144\n",
      "-----> iter 1145\n",
      "-----> iter 1146\n",
      "-----> iter 1147\n",
      "-----> iter 1148\n",
      "-----> iter 1149\n",
      "-----> iter 1150\n",
      "-----> iter 1151\n",
      "-----> iter 1152\n",
      "-----> iter 1153\n",
      "-----> iter 1154\n",
      "-----> iter 1155\n",
      "-----> iter 1156\n",
      "-----> iter 1157\n",
      "-----> iter 1158\n",
      "-----> iter 1159\n",
      "-----> iter 1160\n",
      "-----> iter 1161\n",
      "-----> iter 1162\n",
      "-----> iter 1163\n",
      "-----> iter 1164\n",
      "-----> iter 1165\n",
      "-----> iter 1166\n",
      "-----> iter 1167\n",
      "-----> iter 1168\n",
      "-----> iter 1169\n",
      "-----> iter 1170\n",
      "-----> iter 1171\n",
      "-----> iter 1172\n",
      "-----> iter 1173\n",
      "-----> iter 1174\n",
      "-----> iter 1175\n",
      "-----> iter 1176\n",
      "-----> iter 1177\n",
      "-----> iter 1178\n",
      "-----> iter 1179\n",
      "-----> iter 1180\n",
      "-----> iter 1181\n",
      "-----> iter 1182\n",
      "-----> iter 1183\n",
      "-----> iter 1184\n",
      "-----> iter 1185\n",
      "-----> iter 1186\n",
      "-----> iter 1187\n",
      "-----> iter 1188\n",
      "-----> iter 1189\n",
      "-----> iter 1190\n",
      "-----> iter 1191\n",
      "-----> iter 1192\n",
      "-----> iter 1193\n",
      "-----> iter 1194\n",
      "-----> iter 1195\n",
      "-----> iter 1196\n",
      "-----> iter 1197\n",
      "-----> iter 1198\n",
      "-----> iter 1199\n",
      "-----> iter 1200\n",
      "-----> iter 1201\n",
      "-----> iter 1202\n",
      "-----> iter 1203\n",
      "-----> iter 1204\n",
      "-----> iter 1205\n",
      "-----> iter 1206\n",
      "-----> iter 1207\n",
      "-----> iter 1208\n",
      "-----> iter 1209\n",
      "-----> iter 1210\n",
      "-----> iter 1211\n",
      "-----> iter 1212\n",
      "-----> iter 1213\n",
      "-----> iter 1214\n",
      "-----> iter 1215\n",
      "-----> iter 1216\n",
      "-----> iter 1217\n",
      "-----> iter 1218\n",
      "-----> iter 1219\n",
      "-----> iter 1220\n",
      "-----> iter 1221\n",
      "-----> iter 1222\n",
      "-----> iter 1223\n",
      "-----> iter 1224\n",
      "-----> iter 1225\n",
      "-----> iter 1226\n",
      "-----> iter 1227\n",
      "-----> iter 1228\n",
      "-----> iter 1229\n",
      "-----> iter 1230\n",
      "-----> iter 1231\n",
      "-----> iter 1232\n",
      "-----> iter 1233\n",
      "-----> iter 1234\n",
      "-----> iter 1235\n",
      "-----> iter 1236\n",
      "-----> iter 1237\n",
      "-----> iter 1238\n",
      "-----> iter 1239\n",
      "-----> iter 1240\n",
      "-----> iter 1241\n",
      "-----> iter 1242\n",
      "-----> iter 1243\n",
      "-----> iter 1244\n",
      "-----> iter 1245\n",
      "-----> iter 1246\n",
      "-----> iter 1247\n",
      "-----> iter 1248\n",
      "-----> iter 1249\n",
      "-----> iter 1250\n",
      "-----> iter 1251\n",
      "-----> iter 1252\n",
      "-----> iter 1253\n",
      "-----> iter 1254\n",
      "-----> iter 1255\n",
      "-----> iter 1256\n",
      "-----> iter 1257\n",
      "-----> iter 1258\n",
      "-----> iter 1259\n",
      "-----> iter 1260\n",
      "-----> iter 1261\n",
      "-----> iter 1262\n",
      "-----> iter 1263\n",
      "-----> iter 1264\n",
      "-----> iter 1265\n",
      "-----> iter 1266\n",
      "-----> iter 1267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 1268\n",
      "-----> iter 1269\n",
      "-----> iter 1270\n",
      "-----> iter 1271\n",
      "-----> iter 1272\n",
      "-----> iter 1273\n",
      "-----> iter 1274\n",
      "-----> iter 1275\n",
      "-----> iter 1276\n",
      "-----> iter 1277\n",
      "-----> iter 1278\n",
      "-----> iter 1279\n",
      "-----> iter 1280\n",
      "-----> iter 1281\n",
      "-----> iter 1282\n",
      "-----> iter 1283\n",
      "-----> iter 1284\n",
      "-----> iter 1285\n",
      "-----> iter 1286\n",
      "-----> iter 1287\n",
      "-----> iter 1288\n",
      "-----> iter 1289\n",
      "-----> iter 1290\n",
      "-----> iter 1291\n",
      "-----> iter 1292\n",
      "-----> iter 1293\n",
      "-----> iter 1294\n",
      "-----> iter 1295\n",
      "-----> iter 1296\n",
      "-----> iter 1297\n",
      "-----> iter 1298\n",
      "-----> iter 1299\n",
      "-----> iter 1300\n",
      "-----> iter 1301\n",
      "-----> iter 1302\n",
      "-----> iter 1303\n",
      "-----> iter 1304\n",
      "-----> iter 1305\n",
      "-----> iter 1306\n",
      "-----> iter 1307\n",
      "-----> iter 1308\n",
      "-----> iter 1309\n",
      "-----> iter 1310\n",
      "-----> iter 1311\n",
      "-----> iter 1312\n",
      "-----> iter 1313\n",
      "-----> iter 1314\n",
      "-----> iter 1315\n",
      "-----> iter 1316\n",
      "-----> iter 1317\n",
      "-----> iter 1318\n",
      "-----> iter 1319\n",
      "-----> iter 1320\n",
      "-----> iter 1321\n",
      "-----> iter 1322\n",
      "-----> iter 1323\n",
      "-----> iter 1324\n",
      "-----> iter 1325\n",
      "-----> iter 1326\n",
      "-----> iter 1327\n",
      "-----> iter 1328\n",
      "-----> iter 1329\n",
      "-----> iter 1330\n",
      "-----> iter 1331\n",
      "-----> iter 1332\n",
      "-----> iter 1333\n",
      "-----> iter 1334\n",
      "-----> iter 1335\n",
      "-----> iter 1336\n",
      "-----> iter 1337\n",
      "-----> iter 1338\n",
      "-----> iter 1339\n",
      "-----> iter 1340\n",
      "-----> iter 1341\n",
      "-----> iter 1342\n",
      "-----> iter 1343\n",
      "-----> iter 1344\n",
      "-----> iter 1345\n",
      "-----> iter 1346\n",
      "-----> iter 1347\n",
      "-----> iter 1348\n",
      "-----> iter 1349\n",
      "-----> iter 1350\n",
      "-----> iter 1351\n",
      "-----> iter 1352\n",
      "-----> iter 1353\n",
      "-----> iter 1354\n",
      "-----> iter 1355\n",
      "-----> iter 1356\n",
      "-----> iter 1357\n",
      "-----> iter 1358\n",
      "-----> iter 1359\n",
      "-----> iter 1360\n",
      "-----> iter 1361\n",
      "-----> iter 1362\n",
      "-----> iter 1363\n",
      "-----> iter 1364\n",
      "-----> iter 1365\n",
      "-----> iter 1366\n",
      "-----> iter 1367\n",
      "-----> iter 1368\n",
      "-----> iter 1369\n",
      "-----> iter 1370\n",
      "-----> iter 1371\n",
      "-----> iter 1372\n",
      "-----> iter 1373\n",
      "-----> iter 1374\n",
      "-----> iter 1375\n",
      "-----> iter 1376\n",
      "-----> iter 1377\n",
      "-----> iter 1378\n",
      "-----> iter 1379\n",
      "-----> iter 1380\n",
      "-----> iter 1381\n",
      "-----> iter 1382\n",
      "-----> iter 1383\n",
      "-----> iter 1384\n",
      "-----> iter 1385\n",
      "-----> iter 1386\n",
      "-----> iter 1387\n",
      "-----> iter 1388\n",
      "-----> iter 1389\n",
      "-----> iter 1390\n",
      "-----> iter 1391\n",
      "-----> iter 1392\n",
      "-----> iter 1393\n",
      "-----> iter 1394\n",
      "-----> iter 1395\n",
      "-----> iter 1396\n",
      "-----> iter 1397\n",
      "-----> iter 1398\n",
      "-----> iter 1399\n",
      "-----> iter 1400\n",
      "-----> iter 1401\n",
      "-----> iter 1402\n",
      "-----> iter 1403\n",
      "-----> iter 1404\n",
      "-----> iter 1405\n",
      "-----> iter 1406\n",
      "-----> iter 1407\n",
      "-----> iter 1408\n",
      "-----> iter 1409\n",
      "-----> iter 1410\n",
      "-----> iter 1411\n",
      "-----> iter 1412\n",
      "-----> iter 1413\n",
      "-----> iter 1414\n",
      "-----> iter 1415\n",
      "-----> iter 1416\n",
      "-----> iter 1417\n",
      "-----> iter 1418\n",
      "-----> iter 1419\n",
      "-----> iter 1420\n",
      "-----> iter 1421\n",
      "-----> iter 1422\n",
      "-----> iter 1423\n",
      "-----> iter 1424\n",
      "-----> iter 1425\n",
      "-----> iter 1426\n",
      "-----> iter 1427\n",
      "-----> iter 1428\n",
      "-----> iter 1429\n",
      "-----> iter 1430\n",
      "-----> iter 1431\n",
      "-----> iter 1432\n",
      "-----> iter 1433\n",
      "-----> iter 1434\n",
      "-----> iter 1435\n",
      "-----> iter 1436\n",
      "-----> iter 1437\n",
      "-----> iter 1438\n",
      "-----> iter 1439\n",
      "-----> iter 1440\n",
      "-----> iter 1441\n",
      "-----> iter 1442\n",
      "-----> iter 1443\n",
      "-----> iter 1444\n",
      "-----> iter 1445\n",
      "-----> iter 1446\n",
      "-----> iter 1447\n",
      "-----> iter 1448\n",
      "-----> iter 1449\n",
      "-----> iter 1450\n",
      "-----> iter 1451\n",
      "-----> iter 1452\n",
      "-----> iter 1453\n",
      "-----> iter 1454\n",
      "-----> iter 1455\n",
      "-----> iter 1456\n",
      "-----> iter 1457\n",
      "-----> iter 1458\n",
      "-----> iter 1459\n",
      "-----> iter 1460\n",
      "-----> iter 1461\n",
      "-----> iter 1462\n",
      "-----> iter 1463\n",
      "-----> iter 1464\n",
      "-----> iter 1465\n",
      "-----> iter 1466\n",
      "-----> iter 1467\n",
      "-----> iter 1468\n",
      "-----> iter 1469\n",
      "-----> iter 1470\n",
      "-----> iter 1471\n",
      "-----> iter 1472\n",
      "-----> iter 1473\n",
      "-----> iter 1474\n",
      "-----> iter 1475\n",
      "-----> iter 1476\n",
      "-----> iter 1477\n",
      "-----> iter 1478\n",
      "-----> iter 1479\n",
      "-----> iter 1480\n",
      "-----> iter 1481\n",
      "-----> iter 1482\n",
      "-----> iter 1483\n",
      "-----> iter 1484\n",
      "-----> iter 1485\n",
      "-----> iter 1486\n",
      "-----> iter 1487\n",
      "-----> iter 1488\n",
      "-----> iter 1489\n",
      "-----> iter 1490\n",
      "-----> iter 1491\n",
      "-----> iter 1492\n",
      "-----> iter 1493\n",
      "-----> iter 1494\n",
      "-----> iter 1495\n",
      "-----> iter 1496\n",
      "-----> iter 1497\n",
      "-----> iter 1498\n",
      "-----> iter 1499\n",
      "-----> iter 1500\n",
      "-----> iter 1501\n",
      "-----> iter 1502\n",
      "-----> iter 1503\n",
      "-----> iter 1504\n",
      "-----> iter 1505\n",
      "-----> iter 1506\n",
      "-----> iter 1507\n",
      "-----> iter 1508\n",
      "-----> iter 1509\n",
      "-----> iter 1510\n",
      "-----> iter 1511\n",
      "-----> iter 1512\n",
      "-----> iter 1513\n",
      "-----> iter 1514\n",
      "-----> iter 1515\n",
      "-----> iter 1516\n",
      "-----> iter 1517\n",
      "-----> iter 1518\n",
      "-----> iter 1519\n",
      "-----> iter 1520\n",
      "-----> iter 1521\n",
      "-----> iter 1522\n",
      "-----> iter 1523\n",
      "-----> iter 1524\n",
      "-----> iter 1525\n",
      "-----> iter 1526\n",
      "-----> iter 1527\n",
      "-----> iter 1528\n",
      "-----> iter 1529\n",
      "-----> iter 1530\n",
      "-----> iter 1531\n",
      "-----> iter 1532\n",
      "-----> iter 1533\n",
      "-----> iter 1534\n",
      "-----> iter 1535\n",
      "-----> iter 1536\n",
      "-----> iter 1537\n",
      "-----> iter 1538\n",
      "-----> iter 1539\n",
      "-----> iter 1540\n",
      "-----> iter 1541\n",
      "-----> iter 1542\n",
      "-----> iter 1543\n",
      "-----> iter 1544\n",
      "-----> iter 1545\n",
      "-----> iter 1546\n",
      "-----> iter 1547\n",
      "-----> iter 1548\n",
      "-----> iter 1549\n",
      "-----> iter 1550\n",
      "-----> iter 1551\n",
      "-----> iter 1552\n",
      "-----> iter 1553\n",
      "-----> iter 1554\n",
      "-----> iter 1555\n",
      "-----> iter 1556\n",
      "-----> iter 1557\n",
      "-----> iter 1558\n",
      "-----> iter 1559\n",
      "-----> iter 1560\n",
      "-----> iter 1561\n",
      "-----> iter 1562\n",
      "-----> iter 1563\n",
      "-----> iter 1564\n",
      "-----> iter 1565\n",
      "-----> iter 1566\n",
      "-----> iter 1567\n",
      "-----> iter 1568\n",
      "-----> iter 1569\n",
      "-----> iter 1570\n",
      "-----> iter 1571\n",
      "-----> iter 1572\n",
      "-----> iter 1573\n",
      "-----> iter 1574\n",
      "-----> iter 1575\n",
      "-----> iter 1576\n",
      "-----> iter 1577\n",
      "-----> iter 1578\n",
      "-----> iter 1579\n",
      "-----> iter 1580\n",
      "-----> iter 1581\n",
      "-----> iter 1582\n",
      "-----> iter 1583\n",
      "-----> iter 1584\n",
      "-----> iter 1585\n",
      "-----> iter 1586\n",
      "-----> iter 1587\n",
      "-----> iter 1588\n",
      "-----> iter 1589\n",
      "-----> iter 1590\n",
      "-----> iter 1591\n",
      "-----> iter 1592\n",
      "-----> iter 1593\n",
      "-----> iter 1594\n",
      "-----> iter 1595\n",
      "-----> iter 1596\n",
      "-----> iter 1597\n",
      "-----> iter 1598\n",
      "-----> iter 1599\n",
      "-----> iter 1600\n",
      "-----> iter 1601\n",
      "-----> iter 1602\n",
      "-----> iter 1603\n",
      "-----> iter 1604\n",
      "-----> iter 1605\n",
      "-----> iter 1606\n",
      "-----> iter 1607\n",
      "-----> iter 1608\n",
      "-----> iter 1609\n",
      "-----> iter 1610\n",
      "-----> iter 1611\n",
      "-----> iter 1612\n",
      "-----> iter 1613\n",
      "-----> iter 1614\n",
      "-----> iter 1615\n",
      "-----> iter 1616\n",
      "-----> iter 1617\n",
      "-----> iter 1618\n",
      "-----> iter 1619\n",
      "-----> iter 1620\n",
      "-----> iter 1621\n",
      "-----> iter 1622\n",
      "-----> iter 1623\n",
      "-----> iter 1624\n",
      "-----> iter 1625\n",
      "-----> iter 1626\n",
      "-----> iter 1627\n",
      "-----> iter 1628\n",
      "-----> iter 1629\n",
      "-----> iter 1630\n",
      "-----> iter 1631\n",
      "-----> iter 1632\n",
      "-----> iter 1633\n",
      "-----> iter 1634\n",
      "-----> iter 1635\n",
      "-----> iter 1636\n",
      "-----> iter 1637\n",
      "-----> iter 1638\n",
      "-----> iter 1639\n",
      "-----> iter 1640\n",
      "-----> iter 1641\n",
      "-----> iter 1642\n",
      "-----> iter 1643\n",
      "-----> iter 1644\n",
      "-----> iter 1645\n",
      "-----> iter 1646\n",
      "-----> iter 1647\n",
      "-----> iter 1648\n",
      "-----> iter 1649\n",
      "-----> iter 1650\n",
      "-----> iter 1651\n",
      "-----> iter 1652\n",
      "-----> iter 1653\n",
      "-----> iter 1654\n",
      "-----> iter 1655\n",
      "-----> iter 1656\n",
      "-----> iter 1657\n",
      "-----> iter 1658\n",
      "-----> iter 1659\n",
      "-----> iter 1660\n",
      "-----> iter 1661\n",
      "-----> iter 1662\n",
      "-----> iter 1663\n",
      "-----> iter 1664\n",
      "-----> iter 1665\n",
      "-----> iter 1666\n",
      "-----> iter 1667\n",
      "-----> iter 1668\n",
      "-----> iter 1669\n",
      "-----> iter 1670\n",
      "-----> iter 1671\n",
      "-----> iter 1672\n",
      "-----> iter 1673\n",
      "-----> iter 1674\n",
      "-----> iter 1675\n",
      "-----> iter 1676\n",
      "-----> iter 1677\n",
      "-----> iter 1678\n",
      "-----> iter 1679\n",
      "-----> iter 1680\n",
      "-----> iter 1681\n",
      "-----> iter 1682\n",
      "-----> iter 1683\n",
      "-----> iter 1684\n",
      "-----> iter 1685\n",
      "-----> iter 1686\n",
      "-----> iter 1687\n",
      "-----> iter 1688\n",
      "-----> iter 1689\n",
      "-----> iter 1690\n",
      "-----> iter 1691\n",
      "-----> iter 1692\n",
      "-----> iter 1693\n",
      "-----> iter 1694\n",
      "-----> iter 1695\n",
      "-----> iter 1696\n",
      "-----> iter 1697\n",
      "-----> iter 1698\n",
      "-----> iter 1699\n",
      "-----> iter 1700\n",
      "-----> iter 1701\n",
      "-----> iter 1702\n",
      "-----> iter 1703\n",
      "-----> iter 1704\n",
      "-----> iter 1705\n",
      "-----> iter 1706\n",
      "-----> iter 1707\n",
      "-----> iter 1708\n",
      "-----> iter 1709\n",
      "-----> iter 1710\n",
      "-----> iter 1711\n",
      "-----> iter 1712\n",
      "-----> iter 1713\n",
      "-----> iter 1714\n",
      "-----> iter 1715\n",
      "-----> iter 1716\n",
      "-----> iter 1717\n",
      "-----> iter 1718\n",
      "-----> iter 1719\n",
      "-----> iter 1720\n",
      "-----> iter 1721\n",
      "-----> iter 1722\n",
      "-----> iter 1723\n",
      "-----> iter 1724\n",
      "-----> iter 1725\n",
      "-----> iter 1726\n",
      "-----> iter 1727\n",
      "-----> iter 1728\n",
      "-----> iter 1729\n",
      "-----> iter 1730\n",
      "-----> iter 1731\n",
      "-----> iter 1732\n",
      "-----> iter 1733\n",
      "-----> iter 1734\n",
      "-----> iter 1735\n",
      "-----> iter 1736\n",
      "-----> iter 1737\n",
      "-----> iter 1738\n",
      "-----> iter 1739\n",
      "-----> iter 1740\n",
      "-----> iter 1741\n",
      "-----> iter 1742\n",
      "-----> iter 1743\n",
      "-----> iter 1744\n",
      "-----> iter 1745\n",
      "-----> iter 1746\n",
      "-----> iter 1747\n",
      "-----> iter 1748\n",
      "-----> iter 1749\n",
      "-----> iter 1750\n",
      "-----> iter 1751\n",
      "-----> iter 1752\n",
      "-----> iter 1753\n",
      "-----> iter 1754\n",
      "-----> iter 1755\n",
      "-----> iter 1756\n",
      "-----> iter 1757\n",
      "-----> iter 1758\n",
      "-----> iter 1759\n",
      "-----> iter 1760\n",
      "-----> iter 1761\n",
      "-----> iter 1762\n",
      "-----> iter 1763\n",
      "-----> iter 1764\n",
      "-----> iter 1765\n",
      "-----> iter 1766\n",
      "-----> iter 1767\n",
      "-----> iter 1768\n",
      "-----> iter 1769\n",
      "-----> iter 1770\n",
      "-----> iter 1771\n",
      "-----> iter 1772\n",
      "-----> iter 1773\n",
      "-----> iter 1774\n",
      "-----> iter 1775\n",
      "-----> iter 1776\n",
      "-----> iter 1777\n",
      "-----> iter 1778\n",
      "-----> iter 1779\n",
      "-----> iter 1780\n",
      "-----> iter 1781\n",
      "-----> iter 1782\n",
      "-----> iter 1783\n",
      "-----> iter 1784\n",
      "-----> iter 1785\n",
      "-----> iter 1786\n",
      "-----> iter 1787\n",
      "-----> iter 1788\n",
      "-----> iter 1789\n",
      "-----> iter 1790\n",
      "-----> iter 1791\n",
      "-----> iter 1792\n",
      "-----> iter 1793\n",
      "-----> iter 1794\n",
      "-----> iter 1795\n",
      "-----> iter 1796\n",
      "-----> iter 1797\n",
      "-----> iter 1798\n",
      "-----> iter 1799\n",
      "-----> iter 1800\n",
      "-----> iter 1801\n",
      "-----> iter 1802\n",
      "-----> iter 1803\n",
      "-----> iter 1804\n",
      "-----> iter 1805\n",
      "-----> iter 1806\n",
      "-----> iter 1807\n",
      "-----> iter 1808\n",
      "-----> iter 1809\n",
      "-----> iter 1810\n",
      "-----> iter 1811\n",
      "-----> iter 1812\n",
      "-----> iter 1813\n",
      "-----> iter 1814\n",
      "-----> iter 1815\n",
      "-----> iter 1816\n",
      "-----> iter 1817\n",
      "-----> iter 1818\n",
      "-----> iter 1819\n",
      "-----> iter 1820\n",
      "-----> iter 1821\n",
      "-----> iter 1822\n",
      "-----> iter 1823\n",
      "-----> iter 1824\n",
      "-----> iter 1825\n",
      "-----> iter 1826\n",
      "-----> iter 1827\n",
      "-----> iter 1828\n",
      "-----> iter 1829\n",
      "-----> iter 1830\n",
      "-----> iter 1831\n",
      "-----> iter 1832\n",
      "-----> iter 1833\n",
      "-----> iter 1834\n",
      "-----> iter 1835\n",
      "-----> iter 1836\n",
      "-----> iter 1837\n",
      "-----> iter 1838\n",
      "-----> iter 1839\n",
      "-----> iter 1840\n",
      "-----> iter 1841\n",
      "-----> iter 1842\n",
      "-----> iter 1843\n",
      "-----> iter 1844\n",
      "-----> iter 1845\n",
      "-----> iter 1846\n",
      "-----> iter 1847\n",
      "-----> iter 1848\n",
      "-----> iter 1849\n",
      "-----> iter 1850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 1851\n",
      "-----> iter 1852\n",
      "-----> iter 1853\n",
      "-----> iter 1854\n",
      "-----> iter 1855\n",
      "-----> iter 1856\n",
      "-----> iter 1857\n",
      "-----> iter 1858\n",
      "-----> iter 1859\n",
      "-----> iter 1860\n",
      "-----> iter 1861\n",
      "-----> iter 1862\n",
      "-----> iter 1863\n",
      "-----> iter 1864\n",
      "-----> iter 1865\n",
      "-----> iter 1866\n",
      "-----> iter 1867\n",
      "-----> iter 1868\n",
      "-----> iter 1869\n",
      "-----> iter 1870\n",
      "-----> iter 1871\n",
      "-----> iter 1872\n",
      "-----> iter 1873\n",
      "-----> iter 1874\n",
      "-----> iter 1875\n",
      "-----> iter 1876\n",
      "-----> iter 1877\n",
      "-----> iter 1878\n",
      "-----> iter 1879\n",
      "-----> iter 1880\n",
      "-----> iter 1881\n",
      "-----> iter 1882\n",
      "-----> iter 1883\n",
      "-----> iter 1884\n",
      "-----> iter 1885\n",
      "-----> iter 1886\n",
      "-----> iter 1887\n",
      "-----> iter 1888\n",
      "-----> iter 1889\n",
      "-----> iter 1890\n",
      "-----> iter 1891\n",
      "-----> iter 1892\n",
      "-----> iter 1893\n",
      "-----> iter 1894\n",
      "-----> iter 1895\n",
      "-----> iter 1896\n",
      "-----> iter 1897\n",
      "-----> iter 1898\n",
      "-----> iter 1899\n",
      "-----> iter 1900\n",
      "-----> iter 1901\n",
      "-----> iter 1902\n",
      "-----> iter 1903\n",
      "-----> iter 1904\n",
      "-----> iter 1905\n",
      "-----> iter 1906\n",
      "-----> iter 1907\n",
      "-----> iter 1908\n",
      "-----> iter 1909\n",
      "-----> iter 1910\n",
      "-----> iter 1911\n",
      "-----> iter 1912\n",
      "-----> iter 1913\n",
      "-----> iter 1914\n",
      "-----> iter 1915\n",
      "-----> iter 1916\n",
      "-----> iter 1917\n",
      "-----> iter 1918\n",
      "-----> iter 1919\n",
      "-----> iter 1920\n",
      "-----> iter 1921\n",
      "-----> iter 1922\n",
      "-----> iter 1923\n",
      "-----> iter 1924\n",
      "-----> iter 1925\n",
      "-----> iter 1926\n",
      "-----> iter 1927\n",
      "-----> iter 1928\n",
      "-----> iter 1929\n",
      "-----> iter 1930\n",
      "-----> iter 1931\n",
      "-----> iter 1932\n",
      "-----> iter 1933\n",
      "-----> iter 1934\n",
      "-----> iter 1935\n",
      "-----> iter 1936\n",
      "-----> iter 1937\n",
      "-----> iter 1938\n",
      "-----> iter 1939\n",
      "-----> iter 1940\n",
      "-----> iter 1941\n",
      "-----> iter 1942\n",
      "-----> iter 1943\n",
      "-----> iter 1944\n",
      "-----> iter 1945\n",
      "-----> iter 1946\n",
      "-----> iter 1947\n",
      "-----> iter 1948\n",
      "-----> iter 1949\n",
      "-----> iter 1950\n",
      "-----> iter 1951\n",
      "-----> iter 1952\n",
      "-----> iter 1953\n",
      "-----> iter 1954\n",
      "-----> iter 1955\n",
      "-----> iter 1956\n",
      "-----> iter 1957\n",
      "-----> iter 1958\n",
      "-----> iter 1959\n",
      "-----> iter 1960\n",
      "-----> iter 1961\n",
      "-----> iter 1962\n",
      "-----> iter 1963\n",
      "-----> iter 1964\n",
      "-----> iter 1965\n",
      "-----> iter 1966\n",
      "-----> iter 1967\n",
      "-----> iter 1968\n",
      "-----> iter 1969\n",
      "-----> iter 1970\n",
      "-----> iter 1971\n",
      "-----> iter 1972\n",
      "-----> iter 1973\n",
      "-----> iter 1974\n",
      "-----> iter 1975\n",
      "-----> iter 1976\n",
      "-----> iter 1977\n",
      "-----> iter 1978\n",
      "-----> iter 1979\n",
      "-----> iter 1980\n",
      "-----> iter 1981\n",
      "-----> iter 1982\n",
      "-----> iter 1983\n",
      "-----> iter 1984\n",
      "-----> iter 1985\n",
      "-----> iter 1986\n",
      "-----> iter 1987\n",
      "-----> iter 1988\n",
      "-----> iter 1989\n",
      "-----> iter 1990\n",
      "-----> iter 1991\n",
      "-----> iter 1992\n",
      "-----> iter 1993\n",
      "-----> iter 1994\n",
      "-----> iter 1995\n",
      "-----> iter 1996\n",
      "-----> iter 1997\n",
      "-----> iter 1998\n",
      "-----> iter 1999\n",
      "-----> iter 2000\n",
      "-----> iter 2001\n",
      "-----> iter 2002\n",
      "-----> iter 2003\n",
      "-----> iter 2004\n",
      "-----> iter 2005\n",
      "-----> iter 2006\n",
      "-----> iter 2007\n",
      "-----> iter 2008\n",
      "-----> iter 2009\n",
      "-----> iter 2010\n",
      "-----> iter 2011\n",
      "-----> iter 2012\n",
      "-----> iter 2013\n",
      "-----> iter 2014\n",
      "-----> iter 2015\n",
      "-----> iter 2016\n",
      "-----> iter 2017\n",
      "-----> iter 2018\n",
      "-----> iter 2019\n",
      "-----> iter 2020\n",
      "-----> iter 2021\n",
      "-----> iter 2022\n",
      "-----> iter 2023\n",
      "-----> iter 2024\n",
      "-----> iter 2025\n",
      "-----> iter 2026\n",
      "-----> iter 2027\n",
      "-----> iter 2028\n",
      "-----> iter 2029\n",
      "-----> iter 2030\n",
      "-----> iter 2031\n",
      "-----> iter 2032\n",
      "-----> iter 2033\n",
      "-----> iter 2034\n",
      "-----> iter 2035\n",
      "-----> iter 2036\n",
      "-----> iter 2037\n",
      "-----> iter 2038\n",
      "-----> iter 2039\n",
      "-----> iter 2040\n",
      "-----> iter 2041\n",
      "-----> iter 2042\n",
      "-----> iter 2043\n",
      "-----> iter 2044\n",
      "-----> iter 2045\n",
      "-----> iter 2046\n",
      "-----> iter 2047\n",
      "-----> iter 2048\n",
      "-----> iter 2049\n",
      "-----> iter 2050\n",
      "-----> iter 2051\n",
      "-----> iter 2052\n",
      "-----> iter 2053\n",
      "-----> iter 2054\n",
      "-----> iter 2055\n",
      "-----> iter 2056\n",
      "-----> iter 2057\n",
      "-----> iter 2058\n",
      "-----> iter 2059\n",
      "-----> iter 2060\n",
      "-----> iter 2061\n",
      "-----> iter 2062\n",
      "-----> iter 2063\n",
      "-----> iter 2064\n",
      "-----> iter 2065\n",
      "-----> iter 2066\n",
      "-----> iter 2067\n",
      "-----> iter 2068\n",
      "-----> iter 2069\n",
      "-----> iter 2070\n",
      "-----> iter 2071\n",
      "-----> iter 2072\n",
      "-----> iter 2073\n",
      "-----> iter 2074\n",
      "-----> iter 2075\n",
      "-----> iter 2076\n",
      "-----> iter 2077\n",
      "-----> iter 2078\n",
      "-----> iter 2079\n",
      "-----> iter 2080\n",
      "-----> iter 2081\n",
      "-----> iter 2082\n",
      "-----> iter 2083\n",
      "-----> iter 2084\n",
      "-----> iter 2085\n",
      "-----> iter 2086\n",
      "-----> iter 2087\n",
      "-----> iter 2088\n",
      "-----> iter 2089\n",
      "-----> iter 2090\n",
      "-----> iter 2091\n",
      "-----> iter 2092\n",
      "-----> iter 2093\n",
      "-----> iter 2094\n",
      "-----> iter 2095\n",
      "-----> iter 2096\n",
      "-----> iter 2097\n",
      "-----> iter 2098\n",
      "-----> iter 2099\n",
      "-----> iter 2100\n",
      "-----> iter 2101\n",
      "-----> iter 2102\n",
      "-----> iter 2103\n",
      "-----> iter 2104\n",
      "-----> iter 2105\n",
      "-----> iter 2106\n",
      "-----> iter 2107\n",
      "-----> iter 2108\n",
      "-----> iter 2109\n",
      "-----> iter 2110\n",
      "-----> iter 2111\n",
      "-----> iter 2112\n",
      "-----> iter 2113\n",
      "-----> iter 2114\n",
      "-----> iter 2115\n",
      "-----> iter 2116\n",
      "-----> iter 2117\n",
      "-----> iter 2118\n",
      "-----> iter 2119\n",
      "-----> iter 2120\n",
      "-----> iter 2121\n",
      "-----> iter 2122\n",
      "-----> iter 2123\n",
      "-----> iter 2124\n",
      "-----> iter 2125\n",
      "-----> iter 2126\n",
      "-----> iter 2127\n",
      "-----> iter 2128\n",
      "-----> iter 2129\n",
      "-----> iter 2130\n",
      "-----> iter 2131\n",
      "-----> iter 2132\n",
      "-----> iter 2133\n",
      "-----> iter 2134\n",
      "-----> iter 2135\n",
      "-----> iter 2136\n",
      "-----> iter 2137\n",
      "-----> iter 2138\n",
      "-----> iter 2139\n",
      "-----> iter 2140\n",
      "-----> iter 2141\n",
      "-----> iter 2142\n",
      "-----> iter 2143\n",
      "-----> iter 2144\n",
      "-----> iter 2145\n",
      "-----> iter 2146\n",
      "-----> iter 2147\n",
      "-----> iter 2148\n",
      "-----> iter 2149\n",
      "-----> iter 2150\n",
      "-----> iter 2151\n",
      "-----> iter 2152\n",
      "-----> iter 2153\n",
      "-----> iter 2154\n",
      "-----> iter 2155\n",
      "-----> iter 2156\n",
      "-----> iter 2157\n",
      "-----> iter 2158\n",
      "-----> iter 2159\n",
      "-----> iter 2160\n",
      "-----> iter 2161\n",
      "-----> iter 2162\n",
      "-----> iter 2163\n",
      "-----> iter 2164\n",
      "-----> iter 2165\n",
      "-----> iter 2166\n",
      "-----> iter 2167\n",
      "-----> iter 2168\n",
      "-----> iter 2169\n",
      "-----> iter 2170\n",
      "-----> iter 2171\n",
      "-----> iter 2172\n",
      "-----> iter 2173\n",
      "-----> iter 2174\n",
      "-----> iter 2175\n",
      "-----> iter 2176\n",
      "-----> iter 2177\n",
      "-----> iter 2178\n",
      "-----> iter 2179\n",
      "-----> iter 2180\n",
      "-----> iter 2181\n",
      "-----> iter 2182\n",
      "-----> iter 2183\n",
      "-----> iter 2184\n",
      "-----> iter 2185\n",
      "-----> iter 2186\n",
      "-----> iter 2187\n",
      "-----> iter 2188\n",
      "-----> iter 2189\n",
      "-----> iter 2190\n",
      "-----> iter 2191\n",
      "-----> iter 2192\n",
      "-----> iter 2193\n",
      "-----> iter 2194\n",
      "-----> iter 2195\n",
      "-----> iter 2196\n",
      "-----> iter 2197\n",
      "-----> iter 2198\n",
      "-----> iter 2199\n",
      "-----> iter 2200\n",
      "-----> iter 2201\n",
      "-----> iter 2202\n",
      "-----> iter 2203\n",
      "-----> iter 2204\n",
      "-----> iter 2205\n",
      "-----> iter 2206\n",
      "-----> iter 2207\n",
      "-----> iter 2208\n",
      "-----> iter 2209\n",
      "-----> iter 2210\n",
      "-----> iter 2211\n",
      "-----> iter 2212\n",
      "-----> iter 2213\n",
      "-----> iter 2214\n",
      "-----> iter 2215\n",
      "-----> iter 2216\n",
      "-----> iter 2217\n",
      "-----> iter 2218\n",
      "-----> iter 2219\n",
      "-----> iter 2220\n",
      "-----> iter 2221\n",
      "-----> iter 2222\n",
      "-----> iter 2223\n",
      "-----> iter 2224\n",
      "-----> iter 2225\n",
      "-----> iter 2226\n",
      "-----> iter 2227\n",
      "-----> iter 2228\n",
      "-----> iter 2229\n",
      "-----> iter 2230\n",
      "-----> iter 2231\n",
      "-----> iter 2232\n",
      "-----> iter 2233\n",
      "-----> iter 2234\n",
      "-----> iter 2235\n",
      "-----> iter 2236\n",
      "-----> iter 2237\n",
      "-----> iter 2238\n",
      "-----> iter 2239\n",
      "-----> iter 2240\n",
      "-----> iter 2241\n",
      "-----> iter 2242\n",
      "-----> iter 2243\n",
      "-----> iter 2244\n",
      "-----> iter 2245\n",
      "-----> iter 2246\n",
      "-----> iter 2247\n",
      "-----> iter 2248\n",
      "-----> iter 2249\n",
      "-----> iter 2250\n",
      "-----> iter 2251\n",
      "-----> iter 2252\n",
      "-----> iter 2253\n",
      "-----> iter 2254\n",
      "-----> iter 2255\n",
      "-----> iter 2256\n",
      "-----> iter 2257\n",
      "-----> iter 2258\n",
      "-----> iter 2259\n",
      "-----> iter 2260\n",
      "-----> iter 2261\n",
      "-----> iter 2262\n",
      "-----> iter 2263\n",
      "-----> iter 2264\n",
      "-----> iter 2265\n",
      "-----> iter 2266\n",
      "-----> iter 2267\n",
      "-----> iter 2268\n",
      "-----> iter 2269\n",
      "-----> iter 2270\n",
      "-----> iter 2271\n",
      "-----> iter 2272\n",
      "-----> iter 2273\n",
      "-----> iter 2274\n",
      "-----> iter 2275\n",
      "-----> iter 2276\n",
      "-----> iter 2277\n",
      "-----> iter 2278\n",
      "-----> iter 2279\n",
      "-----> iter 2280\n",
      "-----> iter 2281\n",
      "-----> iter 2282\n",
      "-----> iter 2283\n",
      "-----> iter 2284\n",
      "-----> iter 2285\n",
      "-----> iter 2286\n",
      "-----> iter 2287\n",
      "-----> iter 2288\n",
      "-----> iter 2289\n",
      "-----> iter 2290\n",
      "-----> iter 2291\n",
      "-----> iter 2292\n",
      "-----> iter 2293\n",
      "-----> iter 2294\n",
      "-----> iter 2295\n",
      "-----> iter 2296\n",
      "-----> iter 2297\n",
      "-----> iter 2298\n",
      "-----> iter 2299\n",
      "-----> iter 2300\n",
      "-----> iter 2301\n",
      "-----> iter 2302\n",
      "-----> iter 2303\n",
      "-----> iter 2304\n",
      "-----> iter 2305\n",
      "-----> iter 2306\n",
      "-----> iter 2307\n",
      "-----> iter 2308\n",
      "-----> iter 2309\n",
      "-----> iter 2310\n",
      "-----> iter 2311\n",
      "-----> iter 2312\n",
      "-----> iter 2313\n",
      "-----> iter 2314\n",
      "-----> iter 2315\n",
      "-----> iter 2316\n",
      "-----> iter 2317\n",
      "-----> iter 2318\n",
      "-----> iter 2319\n",
      "-----> iter 2320\n",
      "-----> iter 2321\n",
      "-----> iter 2322\n",
      "-----> iter 2323\n",
      "-----> iter 2324\n",
      "-----> iter 2325\n",
      "-----> iter 2326\n",
      "-----> iter 2327\n",
      "-----> iter 2328\n",
      "-----> iter 2329\n",
      "-----> iter 2330\n",
      "-----> iter 2331\n",
      "-----> iter 2332\n",
      "-----> iter 2333\n",
      "-----> iter 2334\n",
      "-----> iter 2335\n",
      "-----> iter 2336\n",
      "-----> iter 2337\n",
      "-----> iter 2338\n",
      "-----> iter 2339\n",
      "-----> iter 2340\n",
      "-----> iter 2341\n",
      "-----> iter 2342\n",
      "-----> iter 2343\n",
      "-----> iter 2344\n",
      "-----> iter 2345\n",
      "-----> iter 2346\n",
      "-----> iter 2347\n",
      "-----> iter 2348\n",
      "-----> iter 2349\n",
      "-----> iter 2350\n",
      "-----> iter 2351\n",
      "-----> iter 2352\n",
      "-----> iter 2353\n",
      "-----> iter 2354\n",
      "-----> iter 2355\n",
      "-----> iter 2356\n",
      "-----> iter 2357\n",
      "-----> iter 2358\n",
      "-----> iter 2359\n",
      "-----> iter 2360\n",
      "-----> iter 2361\n",
      "-----> iter 2362\n",
      "-----> iter 2363\n",
      "-----> iter 2364\n",
      "-----> iter 2365\n",
      "-----> iter 2366\n",
      "-----> iter 2367\n",
      "-----> iter 2368\n",
      "-----> iter 2369\n",
      "-----> iter 2370\n",
      "-----> iter 2371\n",
      "-----> iter 2372\n",
      "-----> iter 2373\n",
      "-----> iter 2374\n",
      "-----> iter 2375\n",
      "-----> iter 2376\n",
      "-----> iter 2377\n",
      "-----> iter 2378\n",
      "-----> iter 2379\n",
      "-----> iter 2380\n",
      "-----> iter 2381\n",
      "-----> iter 2382\n",
      "-----> iter 2383\n",
      "-----> iter 2384\n",
      "-----> iter 2385\n",
      "-----> iter 2386\n",
      "-----> iter 2387\n",
      "-----> iter 2388\n",
      "-----> iter 2389\n",
      "-----> iter 2390\n",
      "-----> iter 2391\n",
      "-----> iter 2392\n",
      "-----> iter 2393\n",
      "-----> iter 2394\n",
      "-----> iter 2395\n",
      "-----> iter 2396\n",
      "-----> iter 2397\n",
      "-----> iter 2398\n",
      "-----> iter 2399\n",
      "-----> iter 2400\n",
      "-----> iter 2401\n",
      "-----> iter 2402\n",
      "-----> iter 2403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 2404\n",
      "-----> iter 2405\n",
      "-----> iter 2406\n",
      "-----> iter 2407\n",
      "-----> iter 2408\n",
      "-----> iter 2409\n",
      "-----> iter 2410\n",
      "-----> iter 2411\n",
      "-----> iter 2412\n",
      "-----> iter 2413\n",
      "-----> iter 2414\n",
      "-----> iter 2415\n",
      "-----> iter 2416\n",
      "-----> iter 2417\n",
      "-----> iter 2418\n",
      "-----> iter 2419\n",
      "-----> iter 2420\n",
      "-----> iter 2421\n",
      "-----> iter 2422\n",
      "-----> iter 2423\n",
      "-----> iter 2424\n",
      "-----> iter 2425\n",
      "-----> iter 2426\n",
      "-----> iter 2427\n",
      "-----> iter 2428\n",
      "-----> iter 2429\n",
      "-----> iter 2430\n",
      "-----> iter 2431\n",
      "-----> iter 2432\n",
      "-----> iter 2433\n",
      "-----> iter 2434\n",
      "-----> iter 2435\n",
      "-----> iter 2436\n",
      "-----> iter 2437\n",
      "-----> iter 2438\n",
      "-----> iter 2439\n",
      "-----> iter 2440\n",
      "-----> iter 2441\n",
      "-----> iter 2442\n",
      "-----> iter 2443\n",
      "-----> iter 2444\n",
      "-----> iter 2445\n",
      "-----> iter 2446\n",
      "-----> iter 2447\n",
      "-----> iter 2448\n",
      "-----> iter 2449\n",
      "-----> iter 2450\n",
      "-----> iter 2451\n",
      "-----> iter 2452\n",
      "-----> iter 2453\n",
      "-----> iter 2454\n",
      "-----> iter 2455\n",
      "-----> iter 2456\n",
      "-----> iter 2457\n",
      "-----> iter 2458\n",
      "-----> iter 2459\n",
      "-----> iter 2460\n",
      "-----> iter 2461\n",
      "-----> iter 2462\n",
      "-----> iter 2463\n",
      "-----> iter 2464\n",
      "-----> iter 2465\n",
      "-----> iter 2466\n",
      "-----> iter 2467\n",
      "-----> iter 2468\n",
      "-----> iter 2469\n",
      "-----> iter 2470\n",
      "-----> iter 2471\n",
      "-----> iter 2472\n",
      "-----> iter 2473\n",
      "-----> iter 2474\n",
      "-----> iter 2475\n",
      "-----> iter 2476\n",
      "-----> iter 2477\n",
      "-----> iter 2478\n",
      "-----> iter 2479\n",
      "-----> iter 2480\n",
      "-----> iter 2481\n",
      "-----> iter 2482\n",
      "-----> iter 2483\n",
      "-----> iter 2484\n",
      "-----> iter 2485\n",
      "-----> iter 2486\n",
      "-----> iter 2487\n",
      "-----> iter 2488\n",
      "-----> iter 2489\n",
      "-----> iter 2490\n",
      "-----> iter 2491\n",
      "-----> iter 2492\n",
      "-----> iter 2493\n",
      "-----> iter 2494\n",
      "-----> iter 2495\n",
      "-----> iter 2496\n",
      "-----> iter 2497\n",
      "-----> iter 2498\n",
      "-----> iter 2499\n",
      "-----> iter 2500\n",
      "-----> iter 2501\n",
      "-----> iter 2502\n",
      "-----> iter 2503\n",
      "-----> iter 2504\n",
      "-----> iter 2505\n",
      "-----> iter 2506\n",
      "-----> iter 2507\n",
      "-----> iter 2508\n",
      "-----> iter 2509\n",
      "-----> iter 2510\n",
      "-----> iter 2511\n",
      "-----> iter 2512\n",
      "-----> iter 2513\n",
      "-----> iter 2514\n",
      "-----> iter 2515\n",
      "-----> iter 2516\n",
      "-----> iter 2517\n",
      "-----> iter 2518\n",
      "-----> iter 2519\n",
      "-----> iter 2520\n",
      "-----> iter 2521\n",
      "-----> iter 2522\n",
      "-----> iter 2523\n",
      "-----> iter 2524\n",
      "-----> iter 2525\n",
      "-----> iter 2526\n",
      "-----> iter 2527\n",
      "-----> iter 2528\n",
      "-----> iter 2529\n",
      "-----> iter 2530\n",
      "-----> iter 2531\n",
      "-----> iter 2532\n",
      "-----> iter 2533\n",
      "-----> iter 2534\n",
      "-----> iter 2535\n",
      "-----> iter 2536\n",
      "-----> iter 2537\n",
      "-----> iter 2538\n",
      "-----> iter 2539\n",
      "-----> iter 2540\n",
      "-----> iter 2541\n",
      "-----> iter 2542\n",
      "-----> iter 2543\n",
      "-----> iter 2544\n",
      "-----> iter 2545\n",
      "-----> iter 2546\n",
      "-----> iter 2547\n",
      "-----> iter 2548\n",
      "-----> iter 2549\n",
      "-----> iter 2550\n",
      "-----> iter 2551\n",
      "-----> iter 2552\n",
      "-----> iter 2553\n",
      "-----> iter 2554\n",
      "-----> iter 2555\n",
      "-----> iter 2556\n",
      "-----> iter 2557\n",
      "-----> iter 2558\n",
      "-----> iter 2559\n",
      "-----> iter 2560\n",
      "-----> iter 2561\n",
      "-----> iter 2562\n",
      "-----> iter 2563\n",
      "-----> iter 2564\n",
      "-----> iter 2565\n",
      "-----> iter 2566\n",
      "-----> iter 2567\n",
      "-----> iter 2568\n",
      "-----> iter 2569\n",
      "-----> iter 2570\n",
      "-----> iter 2571\n",
      "-----> iter 2572\n",
      "-----> iter 2573\n",
      "-----> iter 2574\n",
      "-----> iter 2575\n",
      "-----> iter 2576\n",
      "-----> iter 2577\n",
      "-----> iter 2578\n",
      "-----> iter 2579\n",
      "-----> iter 2580\n",
      "-----> iter 2581\n",
      "-----> iter 2582\n",
      "-----> iter 2583\n",
      "-----> iter 2584\n",
      "-----> iter 2585\n",
      "-----> iter 2586\n",
      "-----> iter 2587\n",
      "-----> iter 2588\n",
      "-----> iter 2589\n",
      "-----> iter 2590\n",
      "-----> iter 2591\n",
      "-----> iter 2592\n",
      "-----> iter 2593\n",
      "-----> iter 2594\n",
      "-----> iter 2595\n",
      "-----> iter 2596\n",
      "-----> iter 2597\n",
      "-----> iter 2598\n",
      "-----> iter 2599\n",
      "-----> iter 2600\n",
      "-----> iter 2601\n",
      "-----> iter 2602\n",
      "-----> iter 2603\n",
      "-----> iter 2604\n",
      "-----> iter 2605\n",
      "-----> iter 2606\n",
      "-----> iter 2607\n",
      "-----> iter 2608\n",
      "-----> iter 2609\n",
      "-----> iter 2610\n",
      "-----> iter 2611\n",
      "-----> iter 2612\n",
      "-----> iter 2613\n",
      "-----> iter 2614\n",
      "-----> iter 2615\n",
      "-----> iter 2616\n",
      "-----> iter 2617\n",
      "-----> iter 2618\n",
      "-----> iter 2619\n",
      "-----> iter 2620\n",
      "-----> iter 2621\n",
      "-----> iter 2622\n",
      "-----> iter 2623\n",
      "-----> iter 2624\n",
      "-----> iter 2625\n",
      "-----> iter 2626\n",
      "-----> iter 2627\n",
      "-----> iter 2628\n",
      "-----> iter 2629\n",
      "-----> iter 2630\n",
      "-----> iter 2631\n",
      "-----> iter 2632\n",
      "-----> iter 2633\n",
      "-----> iter 2634\n",
      "-----> iter 2635\n",
      "-----> iter 2636\n",
      "-----> iter 2637\n",
      "-----> iter 2638\n",
      "-----> iter 2639\n",
      "-----> iter 2640\n",
      "-----> iter 2641\n",
      "-----> iter 2642\n",
      "-----> iter 2643\n",
      "-----> iter 2644\n",
      "-----> iter 2645\n",
      "-----> iter 2646\n",
      "-----> iter 2647\n",
      "-----> iter 2648\n",
      "-----> iter 2649\n",
      "-----> iter 2650\n",
      "-----> iter 2651\n",
      "-----> iter 2652\n",
      "-----> iter 2653\n",
      "-----> iter 2654\n",
      "-----> iter 2655\n",
      "-----> iter 2656\n",
      "-----> iter 2657\n",
      "-----> iter 2658\n",
      "-----> iter 2659\n",
      "-----> iter 2660\n",
      "-----> iter 2661\n",
      "-----> iter 2662\n",
      "-----> iter 2663\n",
      "-----> iter 2664\n",
      "-----> iter 2665\n",
      "-----> iter 2666\n",
      "-----> iter 2667\n",
      "-----> iter 2668\n",
      "-----> iter 2669\n",
      "-----> iter 2670\n",
      "-----> iter 2671\n",
      "-----> iter 2672\n",
      "-----> iter 2673\n",
      "-----> iter 2674\n",
      "-----> iter 2675\n",
      "-----> iter 2676\n",
      "-----> iter 2677\n",
      "-----> iter 2678\n",
      "-----> iter 2679\n",
      "-----> iter 2680\n",
      "-----> iter 2681\n",
      "-----> iter 2682\n",
      "-----> iter 2683\n",
      "-----> iter 2684\n",
      "-----> iter 2685\n",
      "-----> iter 2686\n",
      "-----> iter 2687\n",
      "-----> iter 2688\n",
      "-----> iter 2689\n",
      "-----> iter 2690\n",
      "-----> iter 2691\n",
      "-----> iter 2692\n",
      "-----> iter 2693\n",
      "-----> iter 2694\n",
      "-----> iter 2695\n",
      "-----> iter 2696\n",
      "-----> iter 2697\n",
      "-----> iter 2698\n",
      "-----> iter 2699\n",
      "-----> iter 2700\n",
      "-----> iter 2701\n",
      "-----> iter 2702\n",
      "-----> iter 2703\n",
      "-----> iter 2704\n",
      "-----> iter 2705\n",
      "-----> iter 2706\n",
      "-----> iter 2707\n",
      "-----> iter 2708\n",
      "-----> iter 2709\n",
      "-----> iter 2710\n",
      "-----> iter 2711\n",
      "-----> iter 2712\n",
      "-----> iter 2713\n",
      "-----> iter 2714\n",
      "-----> iter 2715\n",
      "-----> iter 2716\n",
      "-----> iter 2717\n",
      "-----> iter 2718\n",
      "-----> iter 2719\n",
      "-----> iter 2720\n",
      "-----> iter 2721\n",
      "-----> iter 2722\n",
      "-----> iter 2723\n",
      "-----> iter 2724\n",
      "-----> iter 2725\n",
      "-----> iter 2726\n",
      "-----> iter 2727\n",
      "-----> iter 2728\n",
      "-----> iter 2729\n",
      "-----> iter 2730\n",
      "-----> iter 2731\n",
      "-----> iter 2732\n",
      "-----> iter 2733\n",
      "-----> iter 2734\n",
      "-----> iter 2735\n",
      "-----> iter 2736\n",
      "-----> iter 2737\n",
      "-----> iter 2738\n",
      "-----> iter 2739\n",
      "-----> iter 2740\n",
      "-----> iter 2741\n",
      "-----> iter 2742\n",
      "-----> iter 2743\n",
      "-----> iter 2744\n",
      "-----> iter 2745\n",
      "-----> iter 2746\n",
      "-----> iter 2747\n",
      "-----> iter 2748\n",
      "-----> iter 2749\n",
      "-----> iter 2750\n",
      "-----> iter 2751\n",
      "-----> iter 2752\n",
      "-----> iter 2753\n",
      "-----> iter 2754\n",
      "-----> iter 2755\n",
      "-----> iter 2756\n",
      "-----> iter 2757\n",
      "-----> iter 2758\n",
      "-----> iter 2759\n",
      "-----> iter 2760\n",
      "-----> iter 2761\n",
      "-----> iter 2762\n",
      "-----> iter 2763\n",
      "-----> iter 2764\n",
      "-----> iter 2765\n",
      "-----> iter 2766\n",
      "-----> iter 2767\n",
      "-----> iter 2768\n",
      "-----> iter 2769\n",
      "-----> iter 2770\n",
      "-----> iter 2771\n",
      "-----> iter 2772\n",
      "-----> iter 2773\n",
      "-----> iter 2774\n",
      "-----> iter 2775\n",
      "-----> iter 2776\n",
      "-----> iter 2777\n",
      "-----> iter 2778\n",
      "-----> iter 2779\n",
      "-----> iter 2780\n",
      "-----> iter 2781\n",
      "-----> iter 2782\n",
      "-----> iter 2783\n",
      "-----> iter 2784\n",
      "-----> iter 2785\n",
      "-----> iter 2786\n",
      "-----> iter 2787\n",
      "-----> iter 2788\n",
      "-----> iter 2789\n",
      "-----> iter 2790\n",
      "-----> iter 2791\n",
      "-----> iter 2792\n",
      "-----> iter 2793\n",
      "-----> iter 2794\n",
      "-----> iter 2795\n",
      "-----> iter 2796\n",
      "-----> iter 2797\n",
      "-----> iter 2798\n",
      "-----> iter 2799\n",
      "-----> iter 2800\n",
      "-----> iter 2801\n",
      "-----> iter 2802\n",
      "-----> iter 2803\n",
      "-----> iter 2804\n",
      "-----> iter 2805\n",
      "-----> iter 2806\n",
      "-----> iter 2807\n",
      "-----> iter 2808\n",
      "-----> iter 2809\n",
      "-----> iter 2810\n",
      "-----> iter 2811\n",
      "-----> iter 2812\n",
      "-----> iter 2813\n",
      "-----> iter 2814\n",
      "-----> iter 2815\n",
      "-----> iter 2816\n",
      "-----> iter 2817\n",
      "-----> iter 2818\n",
      "-----> iter 2819\n",
      "-----> iter 2820\n",
      "-----> iter 2821\n",
      "-----> iter 2822\n",
      "-----> iter 2823\n",
      "-----> iter 2824\n",
      "-----> iter 2825\n",
      "-----> iter 2826\n",
      "-----> iter 2827\n",
      "-----> iter 2828\n",
      "-----> iter 2829\n",
      "-----> iter 2830\n",
      "-----> iter 2831\n",
      "-----> iter 2832\n",
      "-----> iter 2833\n",
      "-----> iter 2834\n",
      "-----> iter 2835\n",
      "-----> iter 2836\n",
      "-----> iter 2837\n",
      "-----> iter 2838\n",
      "-----> iter 2839\n",
      "-----> iter 2840\n",
      "-----> iter 2841\n",
      "-----> iter 2842\n",
      "-----> iter 2843\n",
      "-----> iter 2844\n",
      "-----> iter 2845\n",
      "-----> iter 2846\n",
      "-----> iter 2847\n",
      "-----> iter 2848\n",
      "-----> iter 2849\n",
      "-----> iter 2850\n",
      "-----> iter 2851\n",
      "-----> iter 2852\n",
      "-----> iter 2853\n",
      "-----> iter 2854\n",
      "-----> iter 2855\n",
      "-----> iter 2856\n",
      "-----> iter 2857\n",
      "-----> iter 2858\n",
      "-----> iter 2859\n",
      "-----> iter 2860\n",
      "-----> iter 2861\n",
      "-----> iter 2862\n",
      "-----> iter 2863\n",
      "-----> iter 2864\n",
      "-----> iter 2865\n",
      "-----> iter 2866\n",
      "-----> iter 2867\n",
      "-----> iter 2868\n",
      "-----> iter 2869\n",
      "-----> iter 2870\n",
      "-----> iter 2871\n",
      "-----> iter 2872\n",
      "-----> iter 2873\n",
      "-----> iter 2874\n",
      "-----> iter 2875\n",
      "-----> iter 2876\n",
      "-----> iter 2877\n",
      "-----> iter 2878\n",
      "-----> iter 2879\n",
      "-----> iter 2880\n",
      "-----> iter 2881\n",
      "-----> iter 2882\n",
      "-----> iter 2883\n",
      "-----> iter 2884\n",
      "-----> iter 2885\n",
      "-----> iter 2886\n",
      "-----> iter 2887\n",
      "-----> iter 2888\n",
      "-----> iter 2889\n",
      "-----> iter 2890\n",
      "-----> iter 2891\n",
      "-----> iter 2892\n",
      "-----> iter 2893\n",
      "-----> iter 2894\n",
      "-----> iter 2895\n",
      "-----> iter 2896\n",
      "-----> iter 2897\n",
      "-----> iter 2898\n",
      "-----> iter 2899\n",
      "-----> iter 2900\n",
      "-----> iter 2901\n",
      "-----> iter 2902\n",
      "-----> iter 2903\n",
      "-----> iter 2904\n",
      "-----> iter 2905\n",
      "-----> iter 2906\n",
      "-----> iter 2907\n",
      "-----> iter 2908\n",
      "-----> iter 2909\n",
      "-----> iter 2910\n",
      "-----> iter 2911\n",
      "-----> iter 2912\n",
      "-----> iter 2913\n",
      "-----> iter 2914\n",
      "-----> iter 2915\n",
      "-----> iter 2916\n",
      "-----> iter 2917\n",
      "-----> iter 2918\n",
      "-----> iter 2919\n",
      "-----> iter 2920\n",
      "-----> iter 2921\n",
      "-----> iter 2922\n",
      "-----> iter 2923\n",
      "-----> iter 2924\n",
      "-----> iter 2925\n",
      "-----> iter 2926\n",
      "-----> iter 2927\n",
      "-----> iter 2928\n",
      "-----> iter 2929\n",
      "-----> iter 2930\n",
      "-----> iter 2931\n",
      "-----> iter 2932\n",
      "-----> iter 2933\n",
      "-----> iter 2934\n",
      "-----> iter 2935\n",
      "-----> iter 2936\n",
      "-----> iter 2937\n",
      "-----> iter 2938\n",
      "-----> iter 2939\n",
      "-----> iter 2940\n",
      "-----> iter 2941\n",
      "-----> iter 2942\n",
      "-----> iter 2943\n",
      "-----> iter 2944\n",
      "-----> iter 2945\n",
      "-----> iter 2946\n",
      "-----> iter 2947\n",
      "-----> iter 2948\n",
      "-----> iter 2949\n",
      "-----> iter 2950\n",
      "-----> iter 2951\n",
      "-----> iter 2952\n",
      "-----> iter 2953\n",
      "-----> iter 2954\n",
      "-----> iter 2955\n",
      "-----> iter 2956\n",
      "-----> iter 2957\n",
      "-----> iter 2958\n",
      "-----> iter 2959\n",
      "-----> iter 2960\n",
      "-----> iter 2961\n",
      "-----> iter 2962\n",
      "-----> iter 2963\n",
      "-----> iter 2964\n",
      "-----> iter 2965\n",
      "-----> iter 2966\n",
      "-----> iter 2967\n",
      "-----> iter 2968\n",
      "-----> iter 2969\n",
      "-----> iter 2970\n",
      "-----> iter 2971\n",
      "-----> iter 2972\n",
      "-----> iter 2973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 2974\n",
      "-----> iter 2975\n",
      "-----> iter 2976\n",
      "-----> iter 2977\n",
      "-----> iter 2978\n",
      "-----> iter 2979\n",
      "-----> iter 2980\n",
      "-----> iter 2981\n",
      "-----> iter 2982\n",
      "-----> iter 2983\n",
      "-----> iter 2984\n",
      "-----> iter 2985\n",
      "-----> iter 2986\n",
      "-----> iter 2987\n",
      "-----> iter 2988\n",
      "-----> iter 2989\n",
      "-----> iter 2990\n",
      "-----> iter 2991\n",
      "-----> iter 2992\n",
      "-----> iter 2993\n",
      "-----> iter 2994\n",
      "-----> iter 2995\n",
      "-----> iter 2996\n",
      "-----> iter 2997\n",
      "-----> iter 2998\n",
      "-----> iter 2999\n",
      "-----> iter 3000\n",
      "-----> iter 3001\n",
      "-----> iter 3002\n",
      "-----> iter 3003\n",
      "-----> iter 3004\n",
      "-----> iter 3005\n",
      "-----> iter 3006\n",
      "-----> iter 3007\n",
      "-----> iter 3008\n",
      "-----> iter 3009\n",
      "-----> iter 3010\n",
      "-----> iter 3011\n",
      "-----> iter 3012\n",
      "-----> iter 3013\n",
      "-----> iter 3014\n",
      "-----> iter 3015\n",
      "-----> iter 3016\n",
      "-----> iter 3017\n",
      "-----> iter 3018\n",
      "-----> iter 3019\n",
      "-----> iter 3020\n",
      "-----> iter 3021\n",
      "-----> iter 3022\n",
      "-----> iter 3023\n",
      "-----> iter 3024\n",
      "-----> iter 3025\n",
      "-----> iter 3026\n",
      "-----> iter 3027\n",
      "-----> iter 3028\n",
      "-----> iter 3029\n",
      "-----> iter 3030\n",
      "-----> iter 3031\n",
      "-----> iter 3032\n",
      "-----> iter 3033\n",
      "-----> iter 3034\n",
      "-----> iter 3035\n",
      "-----> iter 3036\n",
      "-----> iter 3037\n",
      "-----> iter 3038\n",
      "-----> iter 3039\n",
      "-----> iter 3040\n",
      "-----> iter 3041\n",
      "-----> iter 3042\n",
      "-----> iter 3043\n",
      "-----> iter 3044\n",
      "-----> iter 3045\n",
      "-----> iter 3046\n",
      "-----> iter 3047\n",
      "-----> iter 3048\n",
      "-----> iter 3049\n",
      "-----> iter 3050\n",
      "-----> iter 3051\n",
      "-----> iter 3052\n",
      "-----> iter 3053\n",
      "-----> iter 3054\n",
      "-----> iter 3055\n",
      "-----> iter 3056\n",
      "-----> iter 3057\n",
      "-----> iter 3058\n",
      "-----> iter 3059\n",
      "-----> iter 3060\n",
      "-----> iter 3061\n",
      "-----> iter 3062\n",
      "-----> iter 3063\n",
      "-----> iter 3064\n",
      "-----> iter 3065\n",
      "-----> iter 3066\n",
      "-----> iter 3067\n",
      "-----> iter 3068\n",
      "-----> iter 3069\n",
      "-----> iter 3070\n",
      "-----> iter 3071\n",
      "-----> iter 3072\n",
      "-----> iter 3073\n",
      "-----> iter 3074\n",
      "-----> iter 3075\n",
      "-----> iter 3076\n",
      "-----> iter 3077\n",
      "-----> iter 3078\n",
      "-----> iter 3079\n",
      "-----> iter 3080\n",
      "-----> iter 3081\n",
      "-----> iter 3082\n",
      "-----> iter 3083\n",
      "-----> iter 3084\n",
      "-----> iter 3085\n",
      "-----> iter 3086\n",
      "-----> iter 3087\n",
      "-----> iter 3088\n",
      "-----> iter 3089\n",
      "-----> iter 3090\n",
      "-----> iter 3091\n",
      "-----> iter 3092\n",
      "-----> iter 3093\n",
      "-----> iter 3094\n",
      "-----> iter 3095\n",
      "-----> iter 3096\n",
      "-----> iter 3097\n",
      "-----> iter 3098\n",
      "-----> iter 3099\n",
      "-----> iter 3100\n",
      "-----> iter 3101\n",
      "-----> iter 3102\n",
      "-----> iter 3103\n",
      "-----> iter 3104\n",
      "-----> iter 3105\n",
      "-----> iter 3106\n",
      "-----> iter 3107\n",
      "-----> iter 3108\n",
      "-----> iter 3109\n",
      "-----> iter 3110\n",
      "-----> iter 3111\n",
      "-----> iter 3112\n",
      "-----> iter 3113\n",
      "-----> iter 3114\n",
      "-----> iter 3115\n",
      "-----> iter 3116\n",
      "-----> iter 3117\n",
      "-----> iter 3118\n",
      "-----> iter 3119\n",
      "-----> iter 3120\n",
      "-----> iter 3121\n",
      "-----> iter 3122\n",
      "-----> iter 3123\n",
      "-----> iter 3124\n",
      "-----> iter 3125\n",
      "-----> iter 3126\n",
      "-----> iter 3127\n",
      "-----> iter 3128\n",
      "-----> iter 3129\n",
      "-----> iter 3130\n",
      "-----> iter 3131\n",
      "-----> iter 3132\n",
      "-----> iter 3133\n",
      "-----> iter 3134\n",
      "-----> iter 3135\n",
      "-----> iter 3136\n",
      "-----> iter 3137\n",
      "-----> iter 3138\n",
      "-----> iter 3139\n",
      "-----> iter 3140\n",
      "-----> iter 3141\n",
      "-----> iter 3142\n",
      "-----> iter 3143\n",
      "-----> iter 3144\n",
      "-----> iter 3145\n",
      "-----> iter 3146\n",
      "-----> iter 3147\n",
      "-----> iter 3148\n",
      "-----> iter 3149\n",
      "-----> iter 3150\n",
      "-----> iter 3151\n",
      "-----> iter 3152\n",
      "-----> iter 3153\n",
      "-----> iter 3154\n",
      "-----> iter 3155\n",
      "-----> iter 3156\n",
      "-----> iter 3157\n",
      "-----> iter 3158\n",
      "-----> iter 3159\n",
      "-----> iter 3160\n",
      "-----> iter 3161\n",
      "-----> iter 3162\n",
      "-----> iter 3163\n",
      "-----> iter 3164\n",
      "-----> iter 3165\n",
      "-----> iter 3166\n",
      "-----> iter 3167\n",
      "-----> iter 3168\n",
      "-----> iter 3169\n",
      "-----> iter 3170\n",
      "-----> iter 3171\n",
      "-----> iter 3172\n",
      "-----> iter 3173\n",
      "-----> iter 3174\n",
      "-----> iter 3175\n",
      "-----> iter 3176\n",
      "-----> iter 3177\n",
      "-----> iter 3178\n",
      "-----> iter 3179\n",
      "-----> iter 3180\n",
      "-----> iter 3181\n",
      "-----> iter 3182\n",
      "-----> iter 3183\n",
      "-----> iter 3184\n",
      "-----> iter 3185\n",
      "-----> iter 3186\n",
      "-----> iter 3187\n",
      "-----> iter 3188\n",
      "-----> iter 3189\n",
      "-----> iter 3190\n",
      "-----> iter 3191\n",
      "-----> iter 3192\n",
      "-----> iter 3193\n",
      "-----> iter 3194\n",
      "-----> iter 3195\n",
      "-----> iter 3196\n",
      "-----> iter 3197\n",
      "-----> iter 3198\n",
      "-----> iter 3199\n",
      "-----> iter 3200\n",
      "-----> iter 3201\n",
      "-----> iter 3202\n",
      "-----> iter 3203\n",
      "-----> iter 3204\n",
      "-----> iter 3205\n",
      "-----> iter 3206\n",
      "-----> iter 3207\n",
      "-----> iter 3208\n",
      "-----> iter 3209\n",
      "-----> iter 3210\n",
      "-----> iter 3211\n",
      "-----> iter 3212\n",
      "-----> iter 3213\n",
      "-----> iter 3214\n",
      "-----> iter 3215\n",
      "-----> iter 3216\n",
      "-----> iter 3217\n",
      "-----> iter 3218\n",
      "-----> iter 3219\n",
      "-----> iter 3220\n",
      "-----> iter 3221\n",
      "-----> iter 3222\n",
      "-----> iter 3223\n",
      "-----> iter 3224\n",
      "-----> iter 3225\n",
      "-----> iter 3226\n",
      "-----> iter 3227\n",
      "-----> iter 3228\n",
      "-----> iter 3229\n",
      "-----> iter 3230\n",
      "-----> iter 3231\n",
      "-----> iter 3232\n",
      "-----> iter 3233\n",
      "-----> iter 3234\n",
      "-----> iter 3235\n",
      "-----> iter 3236\n",
      "-----> iter 3237\n",
      "-----> iter 3238\n",
      "-----> iter 3239\n",
      "-----> iter 3240\n",
      "-----> iter 3241\n",
      "-----> iter 3242\n",
      "-----> iter 3243\n",
      "-----> iter 3244\n",
      "-----> iter 3245\n",
      "-----> iter 3246\n",
      "-----> iter 3247\n",
      "-----> iter 3248\n",
      "-----> iter 3249\n",
      "-----> iter 3250\n",
      "-----> iter 3251\n",
      "-----> iter 3252\n",
      "-----> iter 3253\n",
      "-----> iter 3254\n",
      "-----> iter 3255\n",
      "-----> iter 3256\n",
      "-----> iter 3257\n",
      "-----> iter 3258\n",
      "-----> iter 3259\n",
      "-----> iter 3260\n",
      "-----> iter 3261\n",
      "-----> iter 3262\n",
      "-----> iter 3263\n",
      "-----> iter 3264\n",
      "-----> iter 3265\n",
      "-----> iter 3266\n",
      "-----> iter 3267\n",
      "-----> iter 3268\n",
      "-----> iter 3269\n",
      "-----> iter 3270\n",
      "-----> iter 3271\n",
      "-----> iter 3272\n",
      "-----> iter 3273\n",
      "-----> iter 3274\n",
      "-----> iter 3275\n",
      "-----> iter 3276\n",
      "-----> iter 3277\n",
      "-----> iter 3278\n",
      "-----> iter 3279\n",
      "-----> iter 3280\n",
      "-----> iter 3281\n",
      "-----> iter 3282\n",
      "-----> iter 3283\n",
      "-----> iter 3284\n",
      "-----> iter 3285\n",
      "-----> iter 3286\n",
      "-----> iter 3287\n",
      "-----> iter 3288\n",
      "-----> iter 3289\n",
      "-----> iter 3290\n",
      "-----> iter 3291\n",
      "-----> iter 3292\n",
      "-----> iter 3293\n",
      "-----> iter 3294\n",
      "-----> iter 3295\n",
      "-----> iter 3296\n",
      "-----> iter 3297\n",
      "-----> iter 3298\n",
      "-----> iter 3299\n",
      "-----> iter 3300\n",
      "-----> iter 3301\n",
      "-----> iter 3302\n",
      "-----> iter 3303\n",
      "-----> iter 3304\n",
      "-----> iter 3305\n",
      "-----> iter 3306\n",
      "-----> iter 3307\n",
      "-----> iter 3308\n",
      "-----> iter 3309\n",
      "-----> iter 3310\n",
      "-----> iter 3311\n",
      "-----> iter 3312\n",
      "-----> iter 3313\n",
      "-----> iter 3314\n",
      "-----> iter 3315\n",
      "-----> iter 3316\n",
      "-----> iter 3317\n",
      "-----> iter 3318\n",
      "-----> iter 3319\n",
      "-----> iter 3320\n",
      "-----> iter 3321\n",
      "-----> iter 3322\n",
      "-----> iter 3323\n",
      "-----> iter 3324\n",
      "-----> iter 3325\n",
      "-----> iter 3326\n",
      "-----> iter 3327\n",
      "-----> iter 3328\n",
      "-----> iter 3329\n",
      "-----> iter 3330\n",
      "-----> iter 3331\n",
      "-----> iter 3332\n",
      "-----> iter 3333\n",
      "-----> iter 3334\n",
      "-----> iter 3335\n",
      "-----> iter 3336\n",
      "-----> iter 3337\n",
      "-----> iter 3338\n",
      "-----> iter 3339\n",
      "-----> iter 3340\n",
      "-----> iter 3341\n",
      "-----> iter 3342\n",
      "-----> iter 3343\n",
      "-----> iter 3344\n",
      "-----> iter 3345\n",
      "-----> iter 3346\n",
      "-----> iter 3347\n",
      "-----> iter 3348\n",
      "-----> iter 3349\n",
      "-----> iter 3350\n",
      "-----> iter 3351\n",
      "-----> iter 3352\n",
      "-----> iter 3353\n",
      "-----> iter 3354\n",
      "-----> iter 3355\n",
      "-----> iter 3356\n",
      "-----> iter 3357\n",
      "-----> iter 3358\n",
      "-----> iter 3359\n",
      "-----> iter 3360\n",
      "-----> iter 3361\n",
      "-----> iter 3362\n",
      "-----> iter 3363\n",
      "-----> iter 3364\n",
      "-----> iter 3365\n",
      "-----> iter 3366\n",
      "-----> iter 3367\n",
      "-----> iter 3368\n",
      "-----> iter 3369\n",
      "-----> iter 3370\n",
      "-----> iter 3371\n",
      "-----> iter 3372\n",
      "-----> iter 3373\n",
      "-----> iter 3374\n",
      "-----> iter 3375\n",
      "-----> iter 3376\n",
      "-----> iter 3377\n",
      "-----> iter 3378\n",
      "-----> iter 3379\n",
      "-----> iter 3380\n",
      "-----> iter 3381\n",
      "-----> iter 3382\n",
      "-----> iter 3383\n",
      "-----> iter 3384\n",
      "-----> iter 3385\n",
      "-----> iter 3386\n",
      "-----> iter 3387\n",
      "-----> iter 3388\n",
      "-----> iter 3389\n",
      "-----> iter 3390\n",
      "-----> iter 3391\n",
      "-----> iter 3392\n",
      "-----> iter 3393\n",
      "-----> iter 3394\n",
      "-----> iter 3395\n",
      "-----> iter 3396\n",
      "-----> iter 3397\n",
      "-----> iter 3398\n",
      "-----> iter 3399\n",
      "-----> iter 3400\n",
      "-----> iter 3401\n",
      "-----> iter 3402\n",
      "-----> iter 3403\n",
      "-----> iter 3404\n",
      "-----> iter 3405\n",
      "-----> iter 3406\n",
      "-----> iter 3407\n",
      "-----> iter 3408\n",
      "-----> iter 3409\n",
      "-----> iter 3410\n",
      "-----> iter 3411\n",
      "-----> iter 3412\n",
      "-----> iter 3413\n",
      "-----> iter 3414\n",
      "-----> iter 3415\n",
      "-----> iter 3416\n",
      "-----> iter 3417\n",
      "-----> iter 3418\n",
      "-----> iter 3419\n",
      "-----> iter 3420\n",
      "-----> iter 3421\n",
      "-----> iter 3422\n",
      "-----> iter 3423\n",
      "-----> iter 3424\n",
      "-----> iter 3425\n",
      "-----> iter 3426\n",
      "-----> iter 3427\n",
      "-----> iter 3428\n",
      "-----> iter 3429\n",
      "-----> iter 3430\n",
      "-----> iter 3431\n",
      "-----> iter 3432\n",
      "-----> iter 3433\n",
      "-----> iter 3434\n",
      "-----> iter 3435\n",
      "-----> iter 3436\n",
      "-----> iter 3437\n",
      "-----> iter 3438\n",
      "-----> iter 3439\n",
      "-----> iter 3440\n",
      "-----> iter 3441\n",
      "-----> iter 3442\n",
      "-----> iter 3443\n",
      "-----> iter 3444\n",
      "-----> iter 3445\n",
      "-----> iter 3446\n",
      "-----> iter 3447\n",
      "-----> iter 3448\n",
      "-----> iter 3449\n",
      "-----> iter 3450\n",
      "-----> iter 3451\n",
      "-----> iter 3452\n",
      "-----> iter 3453\n",
      "-----> iter 3454\n",
      "-----> iter 3455\n",
      "-----> iter 3456\n",
      "-----> iter 3457\n",
      "-----> iter 3458\n",
      "-----> iter 3459\n",
      "-----> iter 3460\n",
      "-----> iter 3461\n",
      "-----> iter 3462\n",
      "-----> iter 3463\n",
      "-----> iter 3464\n",
      "-----> iter 3465\n",
      "-----> iter 3466\n",
      "-----> iter 3467\n",
      "-----> iter 3468\n",
      "-----> iter 3469\n",
      "-----> iter 3470\n",
      "-----> iter 3471\n",
      "-----> iter 3472\n",
      "-----> iter 3473\n",
      "-----> iter 3474\n",
      "-----> iter 3475\n",
      "-----> iter 3476\n",
      "-----> iter 3477\n",
      "-----> iter 3478\n",
      "-----> iter 3479\n",
      "-----> iter 3480\n",
      "-----> iter 3481\n",
      "-----> iter 3482\n",
      "-----> iter 3483\n",
      "-----> iter 3484\n",
      "-----> iter 3485\n",
      "-----> iter 3486\n",
      "-----> iter 3487\n",
      "-----> iter 3488\n",
      "-----> iter 3489\n",
      "-----> iter 3490\n",
      "-----> iter 3491\n",
      "-----> iter 3492\n",
      "-----> iter 3493\n",
      "-----> iter 3494\n",
      "-----> iter 3495\n",
      "-----> iter 3496\n",
      "-----> iter 3497\n",
      "-----> iter 3498\n",
      "-----> iter 3499\n",
      "-----> iter 3500\n",
      "-----> iter 3501\n",
      "-----> iter 3502\n",
      "-----> iter 3503\n",
      "-----> iter 3504\n",
      "-----> iter 3505\n",
      "-----> iter 3506\n",
      "-----> iter 3507\n",
      "-----> iter 3508\n",
      "-----> iter 3509\n",
      "-----> iter 3510\n",
      "-----> iter 3511\n",
      "-----> iter 3512\n",
      "-----> iter 3513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 3514\n",
      "-----> iter 3515\n",
      "-----> iter 3516\n",
      "-----> iter 3517\n",
      "-----> iter 3518\n",
      "-----> iter 3519\n",
      "-----> iter 3520\n",
      "-----> iter 3521\n",
      "-----> iter 3522\n",
      "-----> iter 3523\n",
      "-----> iter 3524\n",
      "-----> iter 3525\n",
      "-----> iter 3526\n",
      "-----> iter 3527\n",
      "-----> iter 3528\n",
      "-----> iter 3529\n",
      "-----> iter 3530\n",
      "-----> iter 3531\n",
      "-----> iter 3532\n",
      "-----> iter 3533\n",
      "-----> iter 3534\n",
      "-----> iter 3535\n",
      "-----> iter 3536\n",
      "-----> iter 3537\n",
      "-----> iter 3538\n",
      "-----> iter 3539\n",
      "-----> iter 3540\n",
      "-----> iter 3541\n",
      "-----> iter 3542\n",
      "-----> iter 3543\n",
      "-----> iter 3544\n",
      "-----> iter 3545\n",
      "-----> iter 3546\n",
      "-----> iter 3547\n",
      "-----> iter 3548\n",
      "-----> iter 3549\n",
      "-----> iter 3550\n",
      "-----> iter 3551\n",
      "-----> iter 3552\n",
      "-----> iter 3553\n",
      "-----> iter 3554\n",
      "-----> iter 3555\n",
      "-----> iter 3556\n",
      "-----> iter 3557\n",
      "-----> iter 3558\n",
      "-----> iter 3559\n",
      "-----> iter 3560\n",
      "-----> iter 3561\n",
      "-----> iter 3562\n",
      "-----> iter 3563\n",
      "-----> iter 3564\n",
      "-----> iter 3565\n",
      "-----> iter 3566\n",
      "-----> iter 3567\n",
      "-----> iter 3568\n",
      "-----> iter 3569\n",
      "-----> iter 3570\n",
      "-----> iter 3571\n",
      "-----> iter 3572\n",
      "-----> iter 3573\n",
      "-----> iter 3574\n",
      "-----> iter 3575\n",
      "-----> iter 3576\n",
      "-----> iter 3577\n",
      "-----> iter 3578\n",
      "-----> iter 3579\n",
      "-----> iter 3580\n",
      "-----> iter 3581\n",
      "-----> iter 3582\n",
      "-----> iter 3583\n",
      "-----> iter 3584\n",
      "-----> iter 3585\n",
      "-----> iter 3586\n",
      "-----> iter 3587\n",
      "-----> iter 3588\n",
      "-----> iter 3589\n",
      "-----> iter 3590\n",
      "-----> iter 3591\n",
      "-----> iter 3592\n",
      "-----> iter 3593\n",
      "-----> iter 3594\n",
      "-----> iter 3595\n",
      "-----> iter 3596\n",
      "-----> iter 3597\n",
      "-----> iter 3598\n",
      "-----> iter 3599\n",
      "-----> iter 3600\n",
      "-----> iter 3601\n",
      "-----> iter 3602\n",
      "-----> iter 3603\n",
      "-----> iter 3604\n",
      "-----> iter 3605\n",
      "-----> iter 3606\n",
      "-----> iter 3607\n",
      "-----> iter 3608\n",
      "-----> iter 3609\n",
      "-----> iter 3610\n",
      "-----> iter 3611\n",
      "-----> iter 3612\n",
      "-----> iter 3613\n",
      "-----> iter 3614\n",
      "-----> iter 3615\n",
      "-----> iter 3616\n",
      "-----> iter 3617\n",
      "-----> iter 3618\n",
      "-----> iter 3619\n",
      "-----> iter 3620\n",
      "-----> iter 3621\n",
      "-----> iter 3622\n",
      "-----> iter 3623\n",
      "-----> iter 3624\n",
      "-----> iter 3625\n",
      "-----> iter 3626\n",
      "-----> iter 3627\n",
      "-----> iter 3628\n",
      "-----> iter 3629\n",
      "-----> iter 3630\n",
      "-----> iter 3631\n",
      "-----> iter 3632\n",
      "-----> iter 3633\n",
      "-----> iter 3634\n",
      "-----> iter 3635\n",
      "-----> iter 3636\n",
      "-----> iter 3637\n",
      "-----> iter 3638\n",
      "-----> iter 3639\n",
      "-----> iter 3640\n",
      "-----> iter 3641\n",
      "-----> iter 3642\n",
      "-----> iter 3643\n",
      "-----> iter 3644\n",
      "-----> iter 3645\n",
      "-----> iter 3646\n",
      "-----> iter 3647\n",
      "-----> iter 3648\n",
      "-----> iter 3649\n",
      "-----> iter 3650\n",
      "-----> iter 3651\n",
      "-----> iter 3652\n",
      "-----> iter 3653\n",
      "-----> iter 3654\n",
      "-----> iter 3655\n",
      "-----> iter 3656\n",
      "-----> iter 3657\n",
      "-----> iter 3658\n",
      "-----> iter 3659\n",
      "-----> iter 3660\n",
      "-----> iter 3661\n",
      "-----> iter 3662\n",
      "-----> iter 3663\n",
      "-----> iter 3664\n",
      "-----> iter 3665\n",
      "-----> iter 3666\n",
      "-----> iter 3667\n",
      "-----> iter 3668\n",
      "-----> iter 3669\n",
      "-----> iter 3670\n",
      "-----> iter 3671\n",
      "-----> iter 3672\n",
      "-----> iter 3673\n",
      "-----> iter 3674\n",
      "-----> iter 3675\n",
      "-----> iter 3676\n",
      "-----> iter 3677\n",
      "-----> iter 3678\n",
      "-----> iter 3679\n",
      "-----> iter 3680\n",
      "-----> iter 3681\n",
      "-----> iter 3682\n",
      "-----> iter 3683\n",
      "-----> iter 3684\n",
      "-----> iter 3685\n",
      "-----> iter 3686\n",
      "-----> iter 3687\n",
      "-----> iter 3688\n",
      "-----> iter 3689\n",
      "-----> iter 3690\n",
      "-----> iter 3691\n",
      "-----> iter 3692\n",
      "-----> iter 3693\n",
      "-----> iter 3694\n",
      "-----> iter 3695\n",
      "-----> iter 3696\n",
      "-----> iter 3697\n",
      "-----> iter 3698\n",
      "-----> iter 3699\n",
      "-----> iter 3700\n",
      "-----> iter 3701\n",
      "-----> iter 3702\n",
      "-----> iter 3703\n",
      "-----> iter 3704\n",
      "-----> iter 3705\n",
      "-----> iter 3706\n",
      "-----> iter 3707\n",
      "-----> iter 3708\n",
      "-----> iter 3709\n",
      "-----> iter 3710\n",
      "-----> iter 3711\n",
      "-----> iter 3712\n",
      "-----> iter 3713\n",
      "-----> iter 3714\n",
      "-----> iter 3715\n",
      "-----> iter 3716\n",
      "-----> iter 3717\n",
      "-----> iter 3718\n",
      "-----> iter 3719\n",
      "-----> iter 3720\n",
      "-----> iter 3721\n",
      "-----> iter 3722\n",
      "-----> iter 3723\n",
      "-----> iter 3724\n",
      "-----> iter 3725\n",
      "-----> iter 3726\n",
      "-----> iter 3727\n",
      "-----> iter 3728\n",
      "-----> iter 3729\n",
      "-----> iter 3730\n",
      "-----> iter 3731\n",
      "-----> iter 3732\n",
      "-----> iter 3733\n",
      "-----> iter 3734\n",
      "-----> iter 3735\n",
      "-----> iter 3736\n",
      "-----> iter 3737\n",
      "-----> iter 3738\n",
      "-----> iter 3739\n",
      "-----> iter 3740\n",
      "-----> iter 3741\n",
      "-----> iter 3742\n",
      "-----> iter 3743\n",
      "-----> iter 3744\n",
      "-----> iter 3745\n",
      "-----> iter 3746\n",
      "-----> iter 3747\n",
      "-----> iter 3748\n",
      "-----> iter 3749\n",
      "-----> iter 3750\n",
      "-----> iter 3751\n",
      "-----> iter 3752\n",
      "-----> iter 3753\n",
      "-----> iter 3754\n",
      "-----> iter 3755\n",
      "-----> iter 3756\n",
      "-----> iter 3757\n",
      "-----> iter 3758\n",
      "-----> iter 3759\n",
      "-----> iter 3760\n",
      "-----> iter 3761\n",
      "-----> iter 3762\n",
      "-----> iter 3763\n",
      "-----> iter 3764\n",
      "-----> iter 3765\n",
      "-----> iter 3766\n",
      "-----> iter 3767\n",
      "-----> iter 3768\n",
      "-----> iter 3769\n",
      "-----> iter 3770\n",
      "-----> iter 3771\n",
      "-----> iter 3772\n",
      "-----> iter 3773\n",
      "-----> iter 3774\n",
      "-----> iter 3775\n",
      "-----> iter 3776\n",
      "-----> iter 3777\n",
      "-----> iter 3778\n",
      "-----> iter 3779\n",
      "-----> iter 3780\n",
      "-----> iter 3781\n",
      "-----> iter 3782\n",
      "-----> iter 3783\n",
      "-----> iter 3784\n",
      "-----> iter 3785\n",
      "-----> iter 3786\n",
      "-----> iter 3787\n",
      "-----> iter 3788\n",
      "-----> iter 3789\n",
      "-----> iter 3790\n",
      "-----> iter 3791\n",
      "-----> iter 3792\n",
      "-----> iter 3793\n",
      "-----> iter 3794\n",
      "-----> iter 3795\n",
      "-----> iter 3796\n",
      "-----> iter 3797\n",
      "-----> iter 3798\n",
      "-----> iter 3799\n",
      "-----> iter 3800\n",
      "-----> iter 3801\n",
      "-----> iter 3802\n",
      "-----> iter 3803\n",
      "-----> iter 3804\n",
      "-----> iter 3805\n",
      "-----> iter 3806\n",
      "-----> iter 3807\n",
      "-----> iter 3808\n",
      "-----> iter 3809\n",
      "-----> iter 3810\n",
      "-----> iter 3811\n",
      "-----> iter 3812\n",
      "-----> iter 3813\n",
      "-----> iter 3814\n",
      "-----> iter 3815\n",
      "-----> iter 3816\n",
      "-----> iter 3817\n",
      "-----> iter 3818\n",
      "-----> iter 3819\n",
      "-----> iter 3820\n",
      "-----> iter 3821\n",
      "-----> iter 3822\n",
      "-----> iter 3823\n",
      "-----> iter 3824\n",
      "-----> iter 3825\n",
      "-----> iter 3826\n",
      "-----> iter 3827\n",
      "-----> iter 3828\n",
      "-----> iter 3829\n",
      "-----> iter 3830\n",
      "-----> iter 3831\n",
      "-----> iter 3832\n",
      "-----> iter 3833\n",
      "-----> iter 3834\n",
      "-----> iter 3835\n",
      "-----> iter 3836\n",
      "-----> iter 3837\n",
      "-----> iter 3838\n",
      "-----> iter 3839\n",
      "-----> iter 3840\n",
      "-----> iter 3841\n",
      "-----> iter 3842\n",
      "-----> iter 3843\n",
      "-----> iter 3844\n",
      "-----> iter 3845\n",
      "-----> iter 3846\n",
      "-----> iter 3847\n",
      "-----> iter 3848\n",
      "-----> iter 3849\n",
      "-----> iter 3850\n",
      "-----> iter 3851\n",
      "-----> iter 3852\n",
      "-----> iter 3853\n",
      "-----> iter 3854\n",
      "-----> iter 3855\n",
      "-----> iter 3856\n",
      "-----> iter 3857\n",
      "-----> iter 3858\n",
      "-----> iter 3859\n",
      "-----> iter 3860\n",
      "-----> iter 3861\n",
      "-----> iter 3862\n",
      "-----> iter 3863\n",
      "-----> iter 3864\n",
      "-----> iter 3865\n",
      "-----> iter 3866\n",
      "-----> iter 3867\n",
      "-----> iter 3868\n",
      "-----> iter 3869\n",
      "-----> iter 3870\n",
      "-----> iter 3871\n",
      "-----> iter 3872\n",
      "-----> iter 3873\n",
      "-----> iter 3874\n",
      "-----> iter 3875\n",
      "-----> iter 3876\n",
      "-----> iter 3877\n",
      "-----> iter 3878\n",
      "-----> iter 3879\n",
      "-----> iter 3880\n",
      "-----> iter 3881\n",
      "-----> iter 3882\n",
      "-----> iter 3883\n",
      "-----> iter 3884\n",
      "-----> iter 3885\n",
      "-----> iter 3886\n",
      "-----> iter 3887\n",
      "-----> iter 3888\n",
      "-----> iter 3889\n",
      "-----> iter 3890\n",
      "-----> iter 3891\n",
      "-----> iter 3892\n",
      "-----> iter 3893\n",
      "-----> iter 3894\n",
      "-----> iter 3895\n",
      "-----> iter 3896\n",
      "-----> iter 3897\n",
      "-----> iter 3898\n",
      "-----> iter 3899\n",
      "-----> iter 3900\n",
      "-----> iter 3901\n",
      "-----> iter 3902\n",
      "-----> iter 3903\n",
      "-----> iter 3904\n",
      "-----> iter 3905\n",
      "-----> iter 3906\n",
      "-----> iter 3907\n",
      "-----> iter 3908\n",
      "-----> iter 3909\n",
      "-----> iter 3910\n",
      "-----> iter 3911\n",
      "-----> iter 3912\n",
      "-----> iter 3913\n",
      "-----> iter 3914\n",
      "-----> iter 3915\n",
      "-----> iter 3916\n",
      "-----> iter 3917\n",
      "-----> iter 3918\n",
      "-----> iter 3919\n",
      "-----> iter 3920\n",
      "-----> iter 3921\n",
      "-----> iter 3922\n",
      "-----> iter 3923\n",
      "-----> iter 3924\n",
      "-----> iter 3925\n",
      "-----> iter 3926\n",
      "-----> iter 3927\n",
      "-----> iter 3928\n",
      "-----> iter 3929\n",
      "-----> iter 3930\n",
      "-----> iter 3931\n",
      "-----> iter 3932\n",
      "-----> iter 3933\n",
      "-----> iter 3934\n",
      "-----> iter 3935\n",
      "-----> iter 3936\n",
      "-----> iter 3937\n",
      "-----> iter 3938\n",
      "-----> iter 3939\n",
      "-----> iter 3940\n",
      "-----> iter 3941\n",
      "-----> iter 3942\n",
      "-----> iter 3943\n",
      "-----> iter 3944\n",
      "-----> iter 3945\n",
      "-----> iter 3946\n",
      "-----> iter 3947\n",
      "-----> iter 3948\n",
      "-----> iter 3949\n",
      "-----> iter 3950\n",
      "-----> iter 3951\n",
      "-----> iter 3952\n",
      "-----> iter 3953\n",
      "-----> iter 3954\n",
      "-----> iter 3955\n",
      "-----> iter 3956\n",
      "-----> iter 3957\n",
      "-----> iter 3958\n",
      "-----> iter 3959\n",
      "-----> iter 3960\n",
      "-----> iter 3961\n",
      "-----> iter 3962\n",
      "-----> iter 3963\n",
      "-----> iter 3964\n",
      "-----> iter 3965\n",
      "-----> iter 3966\n",
      "-----> iter 3967\n",
      "-----> iter 3968\n",
      "-----> iter 3969\n",
      "-----> iter 3970\n",
      "-----> iter 3971\n",
      "-----> iter 3972\n",
      "-----> iter 3973\n",
      "-----> iter 3974\n",
      "-----> iter 3975\n",
      "-----> iter 3976\n",
      "-----> iter 3977\n",
      "-----> iter 3978\n",
      "-----> iter 3979\n",
      "-----> iter 3980\n",
      "-----> iter 3981\n",
      "-----> iter 3982\n",
      "-----> iter 3983\n",
      "-----> iter 3984\n",
      "-----> iter 3985\n",
      "-----> iter 3986\n",
      "-----> iter 3987\n",
      "-----> iter 3988\n",
      "-----> iter 3989\n",
      "-----> iter 3990\n",
      "-----> iter 3991\n",
      "-----> iter 3992\n",
      "-----> iter 3993\n",
      "-----> iter 3994\n",
      "-----> iter 3995\n",
      "-----> iter 3996\n",
      "-----> iter 3997\n",
      "-----> iter 3998\n",
      "-----> iter 3999\n",
      "-----> iter 4000\n",
      "-----> iter 4001\n",
      "-----> iter 4002\n",
      "-----> iter 4003\n",
      "-----> iter 4004\n",
      "-----> iter 4005\n",
      "-----> iter 4006\n",
      "-----> iter 4007\n",
      "-----> iter 4008\n",
      "-----> iter 4009\n",
      "-----> iter 4010\n",
      "-----> iter 4011\n",
      "-----> iter 4012\n",
      "-----> iter 4013\n",
      "-----> iter 4014\n",
      "-----> iter 4015\n",
      "-----> iter 4016\n",
      "-----> iter 4017\n",
      "-----> iter 4018\n",
      "-----> iter 4019\n",
      "-----> iter 4020\n",
      "-----> iter 4021\n",
      "-----> iter 4022\n",
      "-----> iter 4023\n",
      "-----> iter 4024\n",
      "-----> iter 4025\n",
      "-----> iter 4026\n",
      "-----> iter 4027\n",
      "-----> iter 4028\n",
      "-----> iter 4029\n",
      "-----> iter 4030\n",
      "-----> iter 4031\n",
      "-----> iter 4032\n",
      "-----> iter 4033\n",
      "-----> iter 4034\n",
      "-----> iter 4035\n",
      "-----> iter 4036\n",
      "-----> iter 4037\n",
      "-----> iter 4038\n",
      "-----> iter 4039\n",
      "-----> iter 4040\n",
      "-----> iter 4041\n",
      "-----> iter 4042\n",
      "-----> iter 4043\n",
      "-----> iter 4044\n",
      "-----> iter 4045\n",
      "-----> iter 4046\n",
      "-----> iter 4047\n",
      "-----> iter 4048\n",
      "-----> iter 4049\n",
      "-----> iter 4050\n",
      "-----> iter 4051\n",
      "-----> iter 4052\n",
      "-----> iter 4053\n",
      "-----> iter 4054\n",
      "-----> iter 4055\n",
      "-----> iter 4056\n",
      "-----> iter 4057\n",
      "-----> iter 4058\n",
      "-----> iter 4059\n",
      "-----> iter 4060\n",
      "-----> iter 4061\n",
      "-----> iter 4062\n",
      "-----> iter 4063\n",
      "-----> iter 4064\n",
      "-----> iter 4065\n",
      "-----> iter 4066\n",
      "-----> iter 4067\n",
      "-----> iter 4068\n",
      "-----> iter 4069\n",
      "-----> iter 4070\n",
      "-----> iter 4071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 4072\n",
      "-----> iter 4073\n",
      "-----> iter 4074\n",
      "-----> iter 4075\n",
      "-----> iter 4076\n",
      "-----> iter 4077\n",
      "-----> iter 4078\n",
      "-----> iter 4079\n",
      "-----> iter 4080\n",
      "-----> iter 4081\n",
      "-----> iter 4082\n",
      "-----> iter 4083\n",
      "-----> iter 4084\n",
      "-----> iter 4085\n",
      "-----> iter 4086\n",
      "-----> iter 4087\n",
      "-----> iter 4088\n",
      "-----> iter 4089\n",
      "-----> iter 4090\n",
      "-----> iter 4091\n",
      "-----> iter 4092\n",
      "-----> iter 4093\n",
      "-----> iter 4094\n",
      "-----> iter 4095\n",
      "-----> iter 4096\n",
      "-----> iter 4097\n",
      "-----> iter 4098\n",
      "-----> iter 4099\n",
      "-----> iter 4100\n",
      "-----> iter 4101\n",
      "-----> iter 4102\n",
      "-----> iter 4103\n",
      "-----> iter 4104\n",
      "-----> iter 4105\n",
      "-----> iter 4106\n",
      "-----> iter 4107\n",
      "-----> iter 4108\n",
      "-----> iter 4109\n",
      "-----> iter 4110\n",
      "-----> iter 4111\n",
      "-----> iter 4112\n",
      "-----> iter 4113\n",
      "-----> iter 4114\n",
      "-----> iter 4115\n",
      "-----> iter 4116\n",
      "-----> iter 4117\n",
      "-----> iter 4118\n",
      "-----> iter 4119\n",
      "-----> iter 4120\n",
      "-----> iter 4121\n",
      "-----> iter 4122\n",
      "-----> iter 4123\n",
      "-----> iter 4124\n",
      "-----> iter 4125\n",
      "-----> iter 4126\n",
      "-----> iter 4127\n",
      "-----> iter 4128\n",
      "-----> iter 4129\n",
      "-----> iter 4130\n",
      "-----> iter 4131\n",
      "-----> iter 4132\n",
      "-----> iter 4133\n",
      "-----> iter 4134\n",
      "-----> iter 4135\n",
      "-----> iter 4136\n",
      "-----> iter 4137\n",
      "-----> iter 4138\n",
      "-----> iter 4139\n",
      "-----> iter 4140\n",
      "-----> iter 4141\n",
      "-----> iter 4142\n",
      "-----> iter 4143\n",
      "-----> iter 4144\n",
      "-----> iter 4145\n",
      "-----> iter 4146\n",
      "-----> iter 4147\n",
      "-----> iter 4148\n",
      "-----> iter 4149\n",
      "-----> iter 4150\n",
      "-----> iter 4151\n",
      "-----> iter 4152\n",
      "-----> iter 4153\n",
      "-----> iter 4154\n",
      "-----> iter 4155\n",
      "-----> iter 4156\n",
      "-----> iter 4157\n",
      "-----> iter 4158\n",
      "-----> iter 4159\n",
      "-----> iter 4160\n",
      "-----> iter 4161\n",
      "-----> iter 4162\n",
      "-----> iter 4163\n",
      "-----> iter 4164\n",
      "-----> iter 4165\n",
      "-----> iter 4166\n",
      "-----> iter 4167\n",
      "-----> iter 4168\n",
      "-----> iter 4169\n",
      "-----> iter 4170\n",
      "-----> iter 4171\n",
      "-----> iter 4172\n",
      "-----> iter 4173\n",
      "-----> iter 4174\n",
      "-----> iter 4175\n",
      "-----> iter 4176\n",
      "-----> iter 4177\n",
      "-----> iter 4178\n",
      "-----> iter 4179\n",
      "-----> iter 4180\n",
      "-----> iter 4181\n",
      "-----> iter 4182\n",
      "-----> iter 4183\n",
      "-----> iter 4184\n",
      "-----> iter 4185\n",
      "-----> iter 4186\n",
      "-----> iter 4187\n",
      "-----> iter 4188\n",
      "-----> iter 4189\n",
      "-----> iter 4190\n",
      "-----> iter 4191\n",
      "-----> iter 4192\n",
      "-----> iter 4193\n",
      "-----> iter 4194\n",
      "-----> iter 4195\n",
      "-----> iter 4196\n",
      "-----> iter 4197\n",
      "-----> iter 4198\n",
      "-----> iter 4199\n",
      "-----> iter 4200\n",
      "-----> iter 4201\n",
      "-----> iter 4202\n",
      "-----> iter 4203\n",
      "-----> iter 4204\n",
      "-----> iter 4205\n",
      "-----> iter 4206\n",
      "-----> iter 4207\n",
      "-----> iter 4208\n",
      "-----> iter 4209\n",
      "-----> iter 4210\n",
      "-----> iter 4211\n",
      "-----> iter 4212\n",
      "-----> iter 4213\n",
      "-----> iter 4214\n",
      "-----> iter 4215\n",
      "-----> iter 4216\n",
      "-----> iter 4217\n",
      "-----> iter 4218\n",
      "-----> iter 4219\n",
      "-----> iter 4220\n",
      "-----> iter 4221\n",
      "-----> iter 4222\n",
      "-----> iter 4223\n",
      "-----> iter 4224\n",
      "-----> iter 4225\n",
      "-----> iter 4226\n",
      "-----> iter 4227\n",
      "-----> iter 4228\n",
      "-----> iter 4229\n",
      "-----> iter 4230\n",
      "-----> iter 4231\n",
      "-----> iter 4232\n",
      "-----> iter 4233\n",
      "-----> iter 4234\n",
      "-----> iter 4235\n",
      "-----> iter 4236\n",
      "-----> iter 4237\n",
      "-----> iter 4238\n",
      "-----> iter 4239\n",
      "-----> iter 4240\n",
      "-----> iter 4241\n",
      "-----> iter 4242\n",
      "-----> iter 4243\n",
      "-----> iter 4244\n",
      "-----> iter 4245\n",
      "-----> iter 4246\n",
      "-----> iter 4247\n",
      "-----> iter 4248\n",
      "-----> iter 4249\n",
      "-----> iter 4250\n",
      "-----> iter 4251\n",
      "-----> iter 4252\n",
      "-----> iter 4253\n",
      "-----> iter 4254\n",
      "-----> iter 4255\n",
      "-----> iter 4256\n",
      "-----> iter 4257\n",
      "-----> iter 4258\n",
      "-----> iter 4259\n",
      "-----> iter 4260\n",
      "-----> iter 4261\n",
      "-----> iter 4262\n",
      "-----> iter 4263\n",
      "-----> iter 4264\n",
      "-----> iter 4265\n",
      "-----> iter 4266\n",
      "-----> iter 4267\n",
      "-----> iter 4268\n",
      "-----> iter 4269\n",
      "-----> iter 4270\n",
      "-----> iter 4271\n",
      "-----> iter 4272\n",
      "-----> iter 4273\n",
      "-----> iter 4274\n",
      "-----> iter 4275\n",
      "-----> iter 4276\n",
      "-----> iter 4277\n",
      "-----> iter 4278\n",
      "-----> iter 4279\n",
      "-----> iter 4280\n",
      "-----> iter 4281\n",
      "-----> iter 4282\n",
      "-----> iter 4283\n",
      "-----> iter 4284\n",
      "-----> iter 4285\n",
      "-----> iter 4286\n",
      "-----> iter 4287\n",
      "-----> iter 4288\n",
      "-----> iter 4289\n",
      "-----> iter 4290\n",
      "-----> iter 4291\n",
      "-----> iter 4292\n",
      "-----> iter 4293\n",
      "-----> iter 4294\n",
      "-----> iter 4295\n",
      "-----> iter 4296\n",
      "-----> iter 4297\n",
      "-----> iter 4298\n",
      "-----> iter 4299\n",
      "-----> iter 4300\n",
      "-----> iter 4301\n",
      "-----> iter 4302\n",
      "-----> iter 4303\n",
      "-----> iter 4304\n",
      "-----> iter 4305\n",
      "-----> iter 4306\n",
      "-----> iter 4307\n",
      "-----> iter 4308\n",
      "-----> iter 4309\n",
      "-----> iter 4310\n",
      "-----> iter 4311\n",
      "-----> iter 4312\n",
      "-----> iter 4313\n",
      "-----> iter 4314\n",
      "-----> iter 4315\n",
      "-----> iter 4316\n",
      "-----> iter 4317\n",
      "-----> iter 4318\n",
      "-----> iter 4319\n",
      "-----> iter 4320\n",
      "-----> iter 4321\n",
      "-----> iter 4322\n",
      "-----> iter 4323\n",
      "-----> iter 4324\n",
      "-----> iter 4325\n",
      "-----> iter 4326\n",
      "-----> iter 4327\n",
      "-----> iter 4328\n",
      "-----> iter 4329\n",
      "-----> iter 4330\n",
      "-----> iter 4331\n",
      "-----> iter 4332\n",
      "-----> iter 4333\n",
      "-----> iter 4334\n",
      "-----> iter 4335\n",
      "-----> iter 4336\n",
      "-----> iter 4337\n",
      "-----> iter 4338\n",
      "-----> iter 4339\n",
      "-----> iter 4340\n",
      "-----> iter 4341\n",
      "-----> iter 4342\n",
      "-----> iter 4343\n",
      "-----> iter 4344\n",
      "-----> iter 4345\n",
      "-----> iter 4346\n",
      "-----> iter 4347\n",
      "-----> iter 4348\n",
      "-----> iter 4349\n",
      "-----> iter 4350\n",
      "-----> iter 4351\n",
      "-----> iter 4352\n",
      "-----> iter 4353\n",
      "-----> iter 4354\n",
      "-----> iter 4355\n",
      "-----> iter 4356\n",
      "-----> iter 4357\n",
      "-----> iter 4358\n",
      "-----> iter 4359\n",
      "-----> iter 4360\n",
      "-----> iter 4361\n",
      "-----> iter 4362\n",
      "-----> iter 4363\n",
      "-----> iter 4364\n",
      "-----> iter 4365\n",
      "-----> iter 4366\n",
      "-----> iter 4367\n",
      "-----> iter 4368\n",
      "-----> iter 4369\n",
      "-----> iter 4370\n",
      "-----> iter 4371\n",
      "-----> iter 4372\n",
      "-----> iter 4373\n",
      "-----> iter 4374\n",
      "-----> iter 4375\n",
      "-----> iter 4376\n",
      "-----> iter 4377\n",
      "-----> iter 4378\n",
      "-----> iter 4379\n",
      "-----> iter 4380\n",
      "-----> iter 4381\n",
      "-----> iter 4382\n",
      "-----> iter 4383\n",
      "-----> iter 4384\n",
      "-----> iter 4385\n",
      "-----> iter 4386\n",
      "-----> iter 4387\n",
      "-----> iter 4388\n",
      "-----> iter 4389\n",
      "-----> iter 4390\n",
      "-----> iter 4391\n",
      "-----> iter 4392\n",
      "-----> iter 4393\n",
      "-----> iter 4394\n",
      "-----> iter 4395\n",
      "-----> iter 4396\n",
      "-----> iter 4397\n",
      "-----> iter 4398\n",
      "-----> iter 4399\n",
      "-----> iter 4400\n",
      "-----> iter 4401\n",
      "-----> iter 4402\n",
      "-----> iter 4403\n",
      "-----> iter 4404\n",
      "-----> iter 4405\n",
      "-----> iter 4406\n",
      "-----> iter 4407\n",
      "-----> iter 4408\n",
      "-----> iter 4409\n",
      "-----> iter 4410\n",
      "-----> iter 4411\n",
      "-----> iter 4412\n",
      "-----> iter 4413\n",
      "-----> iter 4414\n",
      "-----> iter 4415\n",
      "-----> iter 4416\n",
      "-----> iter 4417\n",
      "-----> iter 4418\n",
      "-----> iter 4419\n",
      "-----> iter 4420\n",
      "-----> iter 4421\n",
      "-----> iter 4422\n",
      "-----> iter 4423\n",
      "-----> iter 4424\n",
      "-----> iter 4425\n",
      "-----> iter 4426\n",
      "-----> iter 4427\n",
      "-----> iter 4428\n",
      "-----> iter 4429\n",
      "-----> iter 4430\n",
      "-----> iter 4431\n",
      "-----> iter 4432\n",
      "-----> iter 4433\n",
      "-----> iter 4434\n",
      "-----> iter 4435\n",
      "-----> iter 4436\n",
      "-----> iter 4437\n",
      "-----> iter 4438\n",
      "-----> iter 4439\n",
      "-----> iter 4440\n",
      "-----> iter 4441\n",
      "-----> iter 4442\n",
      "-----> iter 4443\n",
      "-----> iter 4444\n",
      "-----> iter 4445\n",
      "-----> iter 4446\n",
      "-----> iter 4447\n",
      "-----> iter 4448\n",
      "-----> iter 4449\n",
      "-----> iter 4450\n",
      "-----> iter 4451\n",
      "-----> iter 4452\n",
      "-----> iter 4453\n",
      "-----> iter 4454\n",
      "-----> iter 4455\n",
      "-----> iter 4456\n",
      "-----> iter 4457\n",
      "-----> iter 4458\n",
      "-----> iter 4459\n",
      "-----> iter 4460\n",
      "-----> iter 4461\n",
      "-----> iter 4462\n",
      "-----> iter 4463\n",
      "-----> iter 4464\n",
      "-----> iter 4465\n",
      "-----> iter 4466\n",
      "-----> iter 4467\n",
      "-----> iter 4468\n",
      "-----> iter 4469\n",
      "-----> iter 4470\n",
      "-----> iter 4471\n",
      "-----> iter 4472\n",
      "-----> iter 4473\n",
      "-----> iter 4474\n",
      "-----> iter 4475\n",
      "-----> iter 4476\n",
      "-----> iter 4477\n",
      "-----> iter 4478\n",
      "-----> iter 4479\n",
      "-----> iter 4480\n",
      "-----> iter 4481\n",
      "-----> iter 4482\n",
      "-----> iter 4483\n",
      "-----> iter 4484\n",
      "-----> iter 4485\n",
      "-----> iter 4486\n",
      "-----> iter 4487\n",
      "-----> iter 4488\n",
      "-----> iter 4489\n",
      "-----> iter 4490\n",
      "-----> iter 4491\n",
      "-----> iter 4492\n",
      "-----> iter 4493\n",
      "-----> iter 4494\n",
      "-----> iter 4495\n",
      "-----> iter 4496\n",
      "-----> iter 4497\n",
      "-----> iter 4498\n",
      "-----> iter 4499\n",
      "-----> iter 4500\n",
      "-----> iter 4501\n",
      "-----> iter 4502\n",
      "-----> iter 4503\n",
      "-----> iter 4504\n",
      "-----> iter 4505\n",
      "-----> iter 4506\n",
      "-----> iter 4507\n",
      "-----> iter 4508\n",
      "-----> iter 4509\n",
      "-----> iter 4510\n",
      "-----> iter 4511\n",
      "-----> iter 4512\n",
      "-----> iter 4513\n",
      "-----> iter 4514\n",
      "-----> iter 4515\n",
      "-----> iter 4516\n",
      "-----> iter 4517\n",
      "-----> iter 4518\n",
      "-----> iter 4519\n",
      "-----> iter 4520\n",
      "-----> iter 4521\n",
      "-----> iter 4522\n",
      "-----> iter 4523\n",
      "-----> iter 4524\n",
      "-----> iter 4525\n",
      "-----> iter 4526\n",
      "-----> iter 4527\n",
      "-----> iter 4528\n",
      "-----> iter 4529\n",
      "-----> iter 4530\n",
      "-----> iter 4531\n",
      "-----> iter 4532\n",
      "-----> iter 4533\n",
      "-----> iter 4534\n",
      "-----> iter 4535\n",
      "-----> iter 4536\n",
      "-----> iter 4537\n",
      "-----> iter 4538\n",
      "-----> iter 4539\n",
      "-----> iter 4540\n",
      "-----> iter 4541\n",
      "-----> iter 4542\n",
      "-----> iter 4543\n",
      "-----> iter 4544\n",
      "-----> iter 4545\n",
      "-----> iter 4546\n",
      "-----> iter 4547\n",
      "-----> iter 4548\n",
      "-----> iter 4549\n",
      "-----> iter 4550\n",
      "-----> iter 4551\n",
      "-----> iter 4552\n",
      "-----> iter 4553\n",
      "-----> iter 4554\n",
      "-----> iter 4555\n",
      "-----> iter 4556\n",
      "-----> iter 4557\n",
      "-----> iter 4558\n",
      "-----> iter 4559\n",
      "-----> iter 4560\n",
      "-----> iter 4561\n",
      "-----> iter 4562\n",
      "-----> iter 4563\n",
      "-----> iter 4564\n",
      "-----> iter 4565\n",
      "-----> iter 4566\n",
      "-----> iter 4567\n",
      "-----> iter 4568\n",
      "-----> iter 4569\n",
      "-----> iter 4570\n",
      "-----> iter 4571\n",
      "-----> iter 4572\n",
      "-----> iter 4573\n",
      "-----> iter 4574\n",
      "-----> iter 4575\n",
      "-----> iter 4576\n",
      "-----> iter 4577\n",
      "-----> iter 4578\n",
      "-----> iter 4579\n",
      "-----> iter 4580\n",
      "-----> iter 4581\n",
      "-----> iter 4582\n",
      "-----> iter 4583\n",
      "-----> iter 4584\n",
      "-----> iter 4585\n",
      "-----> iter 4586\n",
      "-----> iter 4587\n",
      "-----> iter 4588\n",
      "-----> iter 4589\n",
      "-----> iter 4590\n",
      "-----> iter 4591\n",
      "-----> iter 4592\n",
      "-----> iter 4593\n",
      "-----> iter 4594\n",
      "-----> iter 4595\n",
      "-----> iter 4596\n",
      "-----> iter 4597\n",
      "-----> iter 4598\n",
      "-----> iter 4599\n",
      "-----> iter 4600\n",
      "-----> iter 4601\n",
      "-----> iter 4602\n",
      "-----> iter 4603\n",
      "-----> iter 4604\n",
      "-----> iter 4605\n",
      "-----> iter 4606\n",
      "-----> iter 4607\n",
      "-----> iter 4608\n",
      "-----> iter 4609\n",
      "-----> iter 4610\n",
      "-----> iter 4611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 4612\n",
      "-----> iter 4613\n",
      "-----> iter 4614\n",
      "-----> iter 4615\n",
      "-----> iter 4616\n",
      "-----> iter 4617\n",
      "-----> iter 4618\n",
      "-----> iter 4619\n",
      "-----> iter 4620\n",
      "-----> iter 4621\n",
      "-----> iter 4622\n",
      "-----> iter 4623\n",
      "-----> iter 4624\n",
      "-----> iter 4625\n",
      "-----> iter 4626\n",
      "-----> iter 4627\n",
      "-----> iter 4628\n",
      "-----> iter 4629\n",
      "-----> iter 4630\n",
      "-----> iter 4631\n",
      "-----> iter 4632\n",
      "-----> iter 4633\n",
      "-----> iter 4634\n",
      "-----> iter 4635\n",
      "-----> iter 4636\n",
      "-----> iter 4637\n",
      "-----> iter 4638\n",
      "-----> iter 4639\n",
      "-----> iter 4640\n",
      "-----> iter 4641\n",
      "-----> iter 4642\n",
      "-----> iter 4643\n",
      "-----> iter 4644\n",
      "-----> iter 4645\n",
      "-----> iter 4646\n",
      "-----> iter 4647\n",
      "-----> iter 4648\n",
      "-----> iter 4649\n",
      "-----> iter 4650\n",
      "-----> iter 4651\n",
      "-----> iter 4652\n",
      "-----> iter 4653\n",
      "-----> iter 4654\n",
      "-----> iter 4655\n",
      "-----> iter 4656\n",
      "-----> iter 4657\n",
      "-----> iter 4658\n",
      "-----> iter 4659\n",
      "-----> iter 4660\n",
      "-----> iter 4661\n",
      "-----> iter 4662\n",
      "-----> iter 4663\n",
      "-----> iter 4664\n",
      "-----> iter 4665\n",
      "-----> iter 4666\n",
      "-----> iter 4667\n",
      "-----> iter 4668\n",
      "-----> iter 4669\n",
      "-----> iter 4670\n",
      "-----> iter 4671\n",
      "-----> iter 4672\n",
      "-----> iter 4673\n",
      "-----> iter 4674\n",
      "-----> iter 4675\n",
      "-----> iter 4676\n",
      "-----> iter 4677\n",
      "-----> iter 4678\n",
      "-----> iter 4679\n",
      "-----> iter 4680\n",
      "-----> iter 4681\n",
      "-----> iter 4682\n",
      "-----> iter 4683\n",
      "-----> iter 4684\n",
      "-----> iter 4685\n",
      "-----> iter 4686\n",
      "-----> iter 4687\n",
      "-----> iter 4688\n",
      "-----> iter 4689\n",
      "-----> iter 4690\n",
      "-----> iter 4691\n",
      "-----> iter 4692\n",
      "-----> iter 4693\n",
      "-----> iter 4694\n",
      "-----> iter 4695\n",
      "-----> iter 4696\n",
      "-----> iter 4697\n",
      "-----> iter 4698\n",
      "-----> iter 4699\n",
      "-----> iter 4700\n",
      "-----> iter 4701\n",
      "-----> iter 4702\n",
      "-----> iter 4703\n",
      "-----> iter 4704\n",
      "-----> iter 4705\n",
      "-----> iter 4706\n",
      "-----> iter 4707\n",
      "-----> iter 4708\n",
      "-----> iter 4709\n",
      "-----> iter 4710\n",
      "-----> iter 4711\n",
      "-----> iter 4712\n",
      "-----> iter 4713\n",
      "-----> iter 4714\n",
      "-----> iter 4715\n",
      "-----> iter 4716\n",
      "-----> iter 4717\n",
      "-----> iter 4718\n",
      "-----> iter 4719\n",
      "-----> iter 4720\n",
      "-----> iter 4721\n",
      "-----> iter 4722\n",
      "-----> iter 4723\n",
      "-----> iter 4724\n",
      "-----> iter 4725\n",
      "-----> iter 4726\n",
      "-----> iter 4727\n",
      "-----> iter 4728\n",
      "-----> iter 4729\n",
      "-----> iter 4730\n",
      "-----> iter 4731\n",
      "-----> iter 4732\n",
      "-----> iter 4733\n",
      "-----> iter 4734\n",
      "-----> iter 4735\n",
      "-----> iter 4736\n",
      "-----> iter 4737\n",
      "-----> iter 4738\n",
      "-----> iter 4739\n",
      "-----> iter 4740\n",
      "-----> iter 4741\n",
      "-----> iter 4742\n",
      "-----> iter 4743\n",
      "-----> iter 4744\n",
      "-----> iter 4745\n",
      "-----> iter 4746\n",
      "-----> iter 4747\n",
      "-----> iter 4748\n",
      "-----> iter 4749\n",
      "-----> iter 4750\n",
      "-----> iter 4751\n",
      "-----> iter 4752\n",
      "-----> iter 4753\n",
      "-----> iter 4754\n",
      "-----> iter 4755\n",
      "-----> iter 4756\n",
      "-----> iter 4757\n",
      "-----> iter 4758\n",
      "-----> iter 4759\n",
      "-----> iter 4760\n",
      "-----> iter 4761\n",
      "-----> iter 4762\n",
      "-----> iter 4763\n",
      "-----> iter 4764\n",
      "-----> iter 4765\n",
      "-----> iter 4766\n",
      "-----> iter 4767\n",
      "-----> iter 4768\n",
      "-----> iter 4769\n",
      "-----> iter 4770\n",
      "-----> iter 4771\n",
      "-----> iter 4772\n",
      "-----> iter 4773\n",
      "-----> iter 4774\n",
      "-----> iter 4775\n",
      "-----> iter 4776\n",
      "-----> iter 4777\n",
      "-----> iter 4778\n",
      "-----> iter 4779\n",
      "-----> iter 4780\n",
      "-----> iter 4781\n",
      "-----> iter 4782\n",
      "-----> iter 4783\n",
      "-----> iter 4784\n",
      "-----> iter 4785\n",
      "-----> iter 4786\n",
      "-----> iter 4787\n",
      "-----> iter 4788\n",
      "-----> iter 4789\n",
      "-----> iter 4790\n",
      "-----> iter 4791\n",
      "-----> iter 4792\n",
      "-----> iter 4793\n",
      "-----> iter 4794\n",
      "-----> iter 4795\n",
      "-----> iter 4796\n",
      "-----> iter 4797\n",
      "-----> iter 4798\n",
      "-----> iter 4799\n",
      "-----> iter 4800\n",
      "-----> iter 4801\n",
      "-----> iter 4802\n",
      "-----> iter 4803\n",
      "-----> iter 4804\n",
      "-----> iter 4805\n",
      "-----> iter 4806\n",
      "-----> iter 4807\n",
      "-----> iter 4808\n",
      "-----> iter 4809\n",
      "-----> iter 4810\n",
      "-----> iter 4811\n",
      "-----> iter 4812\n",
      "-----> iter 4813\n",
      "-----> iter 4814\n",
      "-----> iter 4815\n",
      "-----> iter 4816\n",
      "-----> iter 4817\n",
      "-----> iter 4818\n",
      "-----> iter 4819\n",
      "-----> iter 4820\n",
      "-----> iter 4821\n",
      "-----> iter 4822\n",
      "-----> iter 4823\n",
      "-----> iter 4824\n",
      "-----> iter 4825\n",
      "-----> iter 4826\n",
      "-----> iter 4827\n",
      "-----> iter 4828\n",
      "-----> iter 4829\n",
      "-----> iter 4830\n",
      "-----> iter 4831\n",
      "-----> iter 4832\n",
      "-----> iter 4833\n",
      "-----> iter 4834\n",
      "-----> iter 4835\n",
      "-----> iter 4836\n",
      "-----> iter 4837\n",
      "-----> iter 4838\n",
      "-----> iter 4839\n",
      "-----> iter 4840\n",
      "-----> iter 4841\n",
      "-----> iter 4842\n",
      "-----> iter 4843\n",
      "-----> iter 4844\n",
      "-----> iter 4845\n",
      "-----> iter 4846\n",
      "-----> iter 4847\n",
      "-----> iter 4848\n",
      "-----> iter 4849\n",
      "-----> iter 4850\n",
      "-----> iter 4851\n",
      "-----> iter 4852\n",
      "-----> iter 4853\n",
      "-----> iter 4854\n",
      "-----> iter 4855\n",
      "-----> iter 4856\n",
      "-----> iter 4857\n",
      "-----> iter 4858\n",
      "-----> iter 4859\n",
      "-----> iter 4860\n",
      "-----> iter 4861\n",
      "-----> iter 4862\n",
      "-----> iter 4863\n",
      "-----> iter 4864\n",
      "-----> iter 4865\n",
      "-----> iter 4866\n",
      "-----> iter 4867\n",
      "-----> iter 4868\n",
      "-----> iter 4869\n",
      "-----> iter 4870\n",
      "-----> iter 4871\n",
      "-----> iter 4872\n",
      "-----> iter 4873\n",
      "-----> iter 4874\n",
      "-----> iter 4875\n",
      "-----> iter 4876\n",
      "-----> iter 4877\n",
      "-----> iter 4878\n",
      "-----> iter 4879\n",
      "-----> iter 4880\n",
      "-----> iter 4881\n",
      "-----> iter 4882\n",
      "-----> iter 4883\n",
      "-----> iter 4884\n",
      "-----> iter 4885\n",
      "-----> iter 4886\n",
      "-----> iter 4887\n",
      "-----> iter 4888\n",
      "-----> iter 4889\n",
      "-----> iter 4890\n",
      "-----> iter 4891\n",
      "-----> iter 4892\n",
      "-----> iter 4893\n",
      "-----> iter 4894\n",
      "-----> iter 4895\n",
      "-----> iter 4896\n",
      "-----> iter 4897\n",
      "-----> iter 4898\n",
      "-----> iter 4899\n",
      "-----> iter 4900\n",
      "-----> iter 4901\n",
      "-----> iter 4902\n",
      "-----> iter 4903\n",
      "-----> iter 4904\n",
      "-----> iter 4905\n",
      "-----> iter 4906\n",
      "-----> iter 4907\n",
      "-----> iter 4908\n",
      "-----> iter 4909\n",
      "-----> iter 4910\n",
      "-----> iter 4911\n",
      "-----> iter 4912\n",
      "-----> iter 4913\n",
      "-----> iter 4914\n",
      "-----> iter 4915\n",
      "-----> iter 4916\n",
      "-----> iter 4917\n",
      "-----> iter 4918\n",
      "-----> iter 4919\n",
      "-----> iter 4920\n",
      "-----> iter 4921\n",
      "-----> iter 4922\n",
      "-----> iter 4923\n",
      "-----> iter 4924\n",
      "-----> iter 4925\n",
      "-----> iter 4926\n",
      "-----> iter 4927\n",
      "-----> iter 4928\n",
      "-----> iter 4929\n",
      "-----> iter 4930\n",
      "-----> iter 4931\n",
      "-----> iter 4932\n",
      "-----> iter 4933\n",
      "-----> iter 4934\n",
      "-----> iter 4935\n",
      "-----> iter 4936\n",
      "-----> iter 4937\n",
      "-----> iter 4938\n",
      "-----> iter 4939\n",
      "-----> iter 4940\n",
      "-----> iter 4941\n",
      "-----> iter 4942\n",
      "-----> iter 4943\n",
      "-----> iter 4944\n",
      "-----> iter 4945\n",
      "-----> iter 4946\n",
      "-----> iter 4947\n",
      "-----> iter 4948\n",
      "-----> iter 4949\n",
      "-----> iter 4950\n",
      "-----> iter 4951\n",
      "-----> iter 4952\n",
      "-----> iter 4953\n",
      "-----> iter 4954\n",
      "-----> iter 4955\n",
      "-----> iter 4956\n",
      "-----> iter 4957\n",
      "-----> iter 4958\n",
      "-----> iter 4959\n",
      "-----> iter 4960\n",
      "-----> iter 4961\n",
      "-----> iter 4962\n",
      "-----> iter 4963\n",
      "-----> iter 4964\n",
      "-----> iter 4965\n",
      "-----> iter 4966\n",
      "-----> iter 4967\n",
      "-----> iter 4968\n",
      "-----> iter 4969\n",
      "-----> iter 4970\n",
      "-----> iter 4971\n",
      "-----> iter 4972\n",
      "-----> iter 4973\n",
      "-----> iter 4974\n",
      "-----> iter 4975\n",
      "-----> iter 4976\n",
      "-----> iter 4977\n",
      "-----> iter 4978\n",
      "-----> iter 4979\n",
      "-----> iter 4980\n",
      "-----> iter 4981\n",
      "-----> iter 4982\n",
      "-----> iter 4983\n",
      "-----> iter 4984\n",
      "-----> iter 4985\n",
      "-----> iter 4986\n",
      "-----> iter 4987\n",
      "-----> iter 4988\n",
      "-----> iter 4989\n",
      "-----> iter 4990\n",
      "-----> iter 4991\n",
      "-----> iter 4992\n",
      "-----> iter 4993\n",
      "-----> iter 4994\n",
      "-----> iter 4995\n",
      "-----> iter 4996\n",
      "-----> iter 4997\n",
      "-----> iter 4998\n",
      "-----> iter 4999\n",
      "-----> iter 5000\n",
      "-----> iter 5001\n",
      "-----> iter 5002\n",
      "-----> iter 5003\n",
      "-----> iter 5004\n",
      "-----> iter 5005\n",
      "-----> iter 5006\n",
      "-----> iter 5007\n",
      "-----> iter 5008\n",
      "-----> iter 5009\n",
      "-----> iter 5010\n",
      "-----> iter 5011\n",
      "-----> iter 5012\n",
      "-----> iter 5013\n",
      "-----> iter 5014\n",
      "-----> iter 5015\n",
      "-----> iter 5016\n",
      "-----> iter 5017\n",
      "-----> iter 5018\n",
      "-----> iter 5019\n",
      "-----> iter 5020\n",
      "-----> iter 5021\n",
      "-----> iter 5022\n",
      "-----> iter 5023\n",
      "-----> iter 5024\n",
      "-----> iter 5025\n",
      "-----> iter 5026\n",
      "-----> iter 5027\n",
      "-----> iter 5028\n",
      "-----> iter 5029\n",
      "-----> iter 5030\n",
      "-----> iter 5031\n",
      "-----> iter 5032\n",
      "-----> iter 5033\n",
      "-----> iter 5034\n",
      "-----> iter 5035\n",
      "-----> iter 5036\n",
      "-----> iter 5037\n",
      "-----> iter 5038\n",
      "-----> iter 5039\n",
      "-----> iter 5040\n",
      "-----> iter 5041\n",
      "-----> iter 5042\n",
      "-----> iter 5043\n",
      "-----> iter 5044\n",
      "-----> iter 5045\n",
      "-----> iter 5046\n",
      "-----> iter 5047\n",
      "-----> iter 5048\n",
      "-----> iter 5049\n",
      "-----> iter 5050\n",
      "-----> iter 5051\n",
      "-----> iter 5052\n",
      "-----> iter 5053\n",
      "-----> iter 5054\n",
      "-----> iter 5055\n",
      "-----> iter 5056\n",
      "-----> iter 5057\n",
      "-----> iter 5058\n",
      "-----> iter 5059\n",
      "-----> iter 5060\n",
      "-----> iter 5061\n",
      "-----> iter 5062\n",
      "-----> iter 5063\n",
      "-----> iter 5064\n",
      "-----> iter 5065\n",
      "-----> iter 5066\n",
      "-----> iter 5067\n",
      "-----> iter 5068\n",
      "-----> iter 5069\n",
      "-----> iter 5070\n",
      "-----> iter 5071\n",
      "-----> iter 5072\n",
      "-----> iter 5073\n",
      "-----> iter 5074\n",
      "-----> iter 5075\n",
      "-----> iter 5076\n",
      "-----> iter 5077\n",
      "-----> iter 5078\n",
      "-----> iter 5079\n",
      "-----> iter 5080\n",
      "-----> iter 5081\n",
      "-----> iter 5082\n",
      "-----> iter 5083\n",
      "-----> iter 5084\n",
      "-----> iter 5085\n",
      "-----> iter 5086\n",
      "-----> iter 5087\n",
      "-----> iter 5088\n",
      "-----> iter 5089\n",
      "-----> iter 5090\n",
      "-----> iter 5091\n",
      "-----> iter 5092\n",
      "-----> iter 5093\n",
      "-----> iter 5094\n",
      "-----> iter 5095\n",
      "-----> iter 5096\n",
      "-----> iter 5097\n",
      "-----> iter 5098\n",
      "-----> iter 5099\n",
      "-----> iter 5100\n",
      "-----> iter 5101\n",
      "-----> iter 5102\n",
      "-----> iter 5103\n",
      "-----> iter 5104\n",
      "-----> iter 5105\n",
      "-----> iter 5106\n",
      "-----> iter 5107\n",
      "-----> iter 5108\n",
      "-----> iter 5109\n",
      "-----> iter 5110\n",
      "-----> iter 5111\n",
      "-----> iter 5112\n",
      "-----> iter 5113\n",
      "-----> iter 5114\n",
      "-----> iter 5115\n",
      "-----> iter 5116\n",
      "-----> iter 5117\n",
      "-----> iter 5118\n",
      "-----> iter 5119\n",
      "-----> iter 5120\n",
      "-----> iter 5121\n",
      "-----> iter 5122\n",
      "-----> iter 5123\n",
      "-----> iter 5124\n",
      "-----> iter 5125\n",
      "-----> iter 5126\n",
      "-----> iter 5127\n",
      "-----> iter 5128\n",
      "-----> iter 5129\n",
      "-----> iter 5130\n",
      "-----> iter 5131\n",
      "-----> iter 5132\n",
      "-----> iter 5133\n",
      "-----> iter 5134\n",
      "-----> iter 5135\n",
      "-----> iter 5136\n",
      "-----> iter 5137\n",
      "-----> iter 5138\n",
      "-----> iter 5139\n",
      "-----> iter 5140\n",
      "-----> iter 5141\n",
      "-----> iter 5142\n",
      "-----> iter 5143\n",
      "-----> iter 5144\n",
      "-----> iter 5145\n",
      "-----> iter 5146\n",
      "-----> iter 5147\n",
      "-----> iter 5148\n",
      "-----> iter 5149\n",
      "-----> iter 5150\n",
      "-----> iter 5151\n",
      "-----> iter 5152\n",
      "-----> iter 5153\n",
      "-----> iter 5154\n",
      "-----> iter 5155\n",
      "-----> iter 5156\n",
      "-----> iter 5157\n",
      "-----> iter 5158\n",
      "-----> iter 5159\n",
      "-----> iter 5160\n",
      "-----> iter 5161\n",
      "-----> iter 5162\n",
      "-----> iter 5163\n",
      "-----> iter 5164\n",
      "-----> iter 5165\n",
      "-----> iter 5166\n",
      "-----> iter 5167\n",
      "-----> iter 5168\n",
      "-----> iter 5169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 5170\n",
      "-----> iter 5171\n",
      "-----> iter 5172\n",
      "-----> iter 5173\n",
      "-----> iter 5174\n",
      "-----> iter 5175\n",
      "-----> iter 5176\n",
      "-----> iter 5177\n",
      "-----> iter 5178\n",
      "-----> iter 5179\n",
      "-----> iter 5180\n",
      "-----> iter 5181\n",
      "-----> iter 5182\n",
      "-----> iter 5183\n",
      "-----> iter 5184\n",
      "-----> iter 5185\n",
      "-----> iter 5186\n",
      "-----> iter 5187\n",
      "-----> iter 5188\n",
      "-----> iter 5189\n",
      "-----> iter 5190\n",
      "-----> iter 5191\n",
      "-----> iter 5192\n",
      "-----> iter 5193\n",
      "-----> iter 5194\n",
      "-----> iter 5195\n",
      "-----> iter 5196\n",
      "-----> iter 5197\n",
      "-----> iter 5198\n",
      "-----> iter 5199\n",
      "-----> iter 5200\n",
      "-----> iter 5201\n",
      "-----> iter 5202\n",
      "-----> iter 5203\n",
      "-----> iter 5204\n",
      "-----> iter 5205\n",
      "-----> iter 5206\n",
      "-----> iter 5207\n",
      "-----> iter 5208\n",
      "-----> iter 5209\n",
      "-----> iter 5210\n",
      "-----> iter 5211\n",
      "-----> iter 5212\n",
      "-----> iter 5213\n",
      "-----> iter 5214\n",
      "-----> iter 5215\n",
      "-----> iter 5216\n",
      "-----> iter 5217\n",
      "-----> iter 5218\n",
      "-----> iter 5219\n",
      "-----> iter 5220\n",
      "-----> iter 5221\n",
      "-----> iter 5222\n",
      "-----> iter 5223\n",
      "-----> iter 5224\n",
      "-----> iter 5225\n",
      "-----> iter 5226\n",
      "-----> iter 5227\n",
      "-----> iter 5228\n",
      "-----> iter 5229\n",
      "-----> iter 5230\n",
      "-----> iter 5231\n",
      "-----> iter 5232\n",
      "-----> iter 5233\n",
      "-----> iter 5234\n",
      "-----> iter 5235\n",
      "-----> iter 5236\n",
      "-----> iter 5237\n",
      "-----> iter 5238\n",
      "-----> iter 5239\n",
      "-----> iter 5240\n",
      "-----> iter 5241\n",
      "-----> iter 5242\n",
      "-----> iter 5243\n",
      "-----> iter 5244\n",
      "-----> iter 5245\n",
      "-----> iter 5246\n",
      "-----> iter 5247\n",
      "-----> iter 5248\n",
      "-----> iter 5249\n",
      "-----> iter 5250\n",
      "-----> iter 5251\n",
      "-----> iter 5252\n",
      "-----> iter 5253\n",
      "-----> iter 5254\n",
      "-----> iter 5255\n",
      "-----> iter 5256\n",
      "-----> iter 5257\n",
      "-----> iter 5258\n",
      "-----> iter 5259\n",
      "-----> iter 5260\n",
      "-----> iter 5261\n",
      "-----> iter 5262\n",
      "-----> iter 5263\n",
      "-----> iter 5264\n",
      "-----> iter 5265\n",
      "-----> iter 5266\n",
      "-----> iter 5267\n",
      "-----> iter 5268\n",
      "-----> iter 5269\n",
      "-----> iter 5270\n",
      "-----> iter 5271\n",
      "-----> iter 5272\n",
      "-----> iter 5273\n",
      "-----> iter 5274\n",
      "-----> iter 5275\n",
      "-----> iter 5276\n",
      "-----> iter 5277\n",
      "-----> iter 5278\n",
      "-----> iter 5279\n",
      "-----> iter 5280\n",
      "-----> iter 5281\n",
      "-----> iter 5282\n",
      "-----> iter 5283\n",
      "-----> iter 5284\n",
      "-----> iter 5285\n",
      "-----> iter 5286\n",
      "-----> iter 5287\n",
      "-----> iter 5288\n",
      "-----> iter 5289\n",
      "-----> iter 5290\n",
      "-----> iter 5291\n",
      "-----> iter 5292\n",
      "-----> iter 5293\n",
      "-----> iter 5294\n",
      "-----> iter 5295\n",
      "-----> iter 5296\n",
      "-----> iter 5297\n",
      "-----> iter 5298\n",
      "-----> iter 5299\n",
      "-----> iter 5300\n",
      "-----> iter 5301\n",
      "-----> iter 5302\n",
      "-----> iter 5303\n",
      "-----> iter 5304\n",
      "-----> iter 5305\n",
      "-----> iter 5306\n",
      "-----> iter 5307\n",
      "-----> iter 5308\n",
      "-----> iter 5309\n",
      "-----> iter 5310\n",
      "-----> iter 5311\n",
      "-----> iter 5312\n",
      "-----> iter 5313\n",
      "-----> iter 5314\n",
      "-----> iter 5315\n",
      "-----> iter 5316\n",
      "-----> iter 5317\n",
      "-----> iter 5318\n",
      "-----> iter 5319\n",
      "-----> iter 5320\n",
      "-----> iter 5321\n",
      "-----> iter 5322\n",
      "-----> iter 5323\n",
      "-----> iter 5324\n",
      "-----> iter 5325\n",
      "-----> iter 5326\n",
      "-----> iter 5327\n",
      "-----> iter 5328\n",
      "-----> iter 5329\n",
      "-----> iter 5330\n",
      "-----> iter 5331\n",
      "-----> iter 5332\n",
      "-----> iter 5333\n",
      "-----> iter 5334\n",
      "-----> iter 5335\n",
      "-----> iter 5336\n",
      "-----> iter 5337\n",
      "-----> iter 5338\n",
      "-----> iter 5339\n",
      "-----> iter 5340\n",
      "-----> iter 5341\n",
      "-----> iter 5342\n",
      "-----> iter 5343\n",
      "-----> iter 5344\n",
      "-----> iter 5345\n",
      "-----> iter 5346\n",
      "-----> iter 5347\n",
      "-----> iter 5348\n",
      "-----> iter 5349\n",
      "-----> iter 5350\n",
      "-----> iter 5351\n",
      "-----> iter 5352\n",
      "-----> iter 5353\n",
      "-----> iter 5354\n",
      "-----> iter 5355\n",
      "-----> iter 5356\n",
      "-----> iter 5357\n",
      "-----> iter 5358\n",
      "-----> iter 5359\n",
      "-----> iter 5360\n",
      "-----> iter 5361\n",
      "-----> iter 5362\n",
      "-----> iter 5363\n",
      "-----> iter 5364\n",
      "-----> iter 5365\n",
      "-----> iter 5366\n",
      "-----> iter 5367\n",
      "-----> iter 5368\n",
      "-----> iter 5369\n",
      "-----> iter 5370\n",
      "-----> iter 5371\n",
      "-----> iter 5372\n",
      "-----> iter 5373\n",
      "-----> iter 5374\n",
      "-----> iter 5375\n",
      "-----> iter 5376\n",
      "-----> iter 5377\n",
      "-----> iter 5378\n",
      "-----> iter 5379\n",
      "-----> iter 5380\n",
      "-----> iter 5381\n",
      "-----> iter 5382\n",
      "-----> iter 5383\n",
      "-----> iter 5384\n",
      "-----> iter 5385\n",
      "-----> iter 5386\n",
      "-----> iter 5387\n",
      "-----> iter 5388\n",
      "-----> iter 5389\n",
      "-----> iter 5390\n",
      "-----> iter 5391\n",
      "-----> iter 5392\n",
      "-----> iter 5393\n",
      "-----> iter 5394\n",
      "-----> iter 5395\n",
      "-----> iter 5396\n",
      "-----> iter 5397\n",
      "-----> iter 5398\n",
      "-----> iter 5399\n",
      "-----> iter 5400\n",
      "-----> iter 5401\n",
      "-----> iter 5402\n",
      "-----> iter 5403\n",
      "-----> iter 5404\n",
      "-----> iter 5405\n",
      "-----> iter 5406\n",
      "-----> iter 5407\n",
      "-----> iter 5408\n",
      "-----> iter 5409\n",
      "-----> iter 5410\n",
      "-----> iter 5411\n",
      "-----> iter 5412\n",
      "-----> iter 5413\n",
      "-----> iter 5414\n",
      "-----> iter 5415\n",
      "-----> iter 5416\n",
      "-----> iter 5417\n",
      "-----> iter 5418\n",
      "-----> iter 5419\n",
      "-----> iter 5420\n",
      "-----> iter 5421\n",
      "-----> iter 5422\n",
      "-----> iter 5423\n",
      "-----> iter 5424\n",
      "-----> iter 5425\n",
      "-----> iter 5426\n",
      "-----> iter 5427\n",
      "-----> iter 5428\n",
      "-----> iter 5429\n",
      "-----> iter 5430\n",
      "-----> iter 5431\n",
      "-----> iter 5432\n",
      "-----> iter 5433\n",
      "-----> iter 5434\n",
      "-----> iter 5435\n",
      "-----> iter 5436\n",
      "-----> iter 5437\n",
      "-----> iter 5438\n",
      "-----> iter 5439\n",
      "-----> iter 5440\n",
      "-----> iter 5441\n",
      "-----> iter 5442\n",
      "-----> iter 5443\n",
      "-----> iter 5444\n",
      "-----> iter 5445\n",
      "-----> iter 5446\n",
      "-----> iter 5447\n",
      "-----> iter 5448\n",
      "-----> iter 5449\n",
      "-----> iter 5450\n",
      "-----> iter 5451\n",
      "-----> iter 5452\n",
      "-----> iter 5453\n",
      "-----> iter 5454\n",
      "-----> iter 5455\n",
      "-----> iter 5456\n",
      "-----> iter 5457\n",
      "-----> iter 5458\n",
      "-----> iter 5459\n",
      "-----> iter 5460\n",
      "-----> iter 5461\n",
      "-----> iter 5462\n",
      "-----> iter 5463\n",
      "-----> iter 5464\n",
      "-----> iter 5465\n",
      "-----> iter 5466\n",
      "-----> iter 5467\n",
      "-----> iter 5468\n",
      "-----> iter 5469\n",
      "-----> iter 5470\n",
      "-----> iter 5471\n",
      "-----> iter 5472\n",
      "-----> iter 5473\n",
      "-----> iter 5474\n",
      "-----> iter 5475\n",
      "-----> iter 5476\n",
      "-----> iter 5477\n",
      "-----> iter 5478\n",
      "-----> iter 5479\n",
      "-----> iter 5480\n",
      "-----> iter 5481\n",
      "-----> iter 5482\n",
      "-----> iter 5483\n",
      "-----> iter 5484\n",
      "-----> iter 5485\n",
      "-----> iter 5486\n",
      "-----> iter 5487\n",
      "-----> iter 5488\n",
      "-----> iter 5489\n",
      "-----> iter 5490\n",
      "-----> iter 5491\n",
      "-----> iter 5492\n",
      "-----> iter 5493\n",
      "-----> iter 5494\n",
      "-----> iter 5495\n",
      "-----> iter 5496\n",
      "-----> iter 5497\n",
      "-----> iter 5498\n",
      "-----> iter 5499\n",
      "-----> iter 5500\n",
      "-----> iter 5501\n",
      "-----> iter 5502\n",
      "-----> iter 5503\n",
      "-----> iter 5504\n",
      "-----> iter 5505\n",
      "-----> iter 5506\n",
      "-----> iter 5507\n",
      "-----> iter 5508\n",
      "-----> iter 5509\n",
      "-----> iter 5510\n",
      "-----> iter 5511\n",
      "-----> iter 5512\n",
      "-----> iter 5513\n",
      "-----> iter 5514\n",
      "-----> iter 5515\n",
      "-----> iter 5516\n",
      "-----> iter 5517\n",
      "-----> iter 5518\n",
      "-----> iter 5519\n",
      "-----> iter 5520\n",
      "-----> iter 5521\n",
      "-----> iter 5522\n",
      "-----> iter 5523\n",
      "-----> iter 5524\n",
      "-----> iter 5525\n",
      "-----> iter 5526\n",
      "-----> iter 5527\n",
      "-----> iter 5528\n",
      "-----> iter 5529\n",
      "-----> iter 5530\n",
      "-----> iter 5531\n",
      "-----> iter 5532\n",
      "-----> iter 5533\n",
      "-----> iter 5534\n",
      "-----> iter 5535\n",
      "-----> iter 5536\n",
      "-----> iter 5537\n",
      "-----> iter 5538\n",
      "-----> iter 5539\n",
      "-----> iter 5540\n",
      "-----> iter 5541\n",
      "-----> iter 5542\n",
      "-----> iter 5543\n",
      "-----> iter 5544\n",
      "-----> iter 5545\n",
      "-----> iter 5546\n",
      "-----> iter 5547\n",
      "-----> iter 5548\n",
      "-----> iter 5549\n",
      "-----> iter 5550\n",
      "-----> iter 5551\n",
      "-----> iter 5552\n",
      "-----> iter 5553\n",
      "-----> iter 5554\n",
      "-----> iter 5555\n",
      "-----> iter 5556\n",
      "-----> iter 5557\n",
      "-----> iter 5558\n",
      "-----> iter 5559\n",
      "-----> iter 5560\n",
      "-----> iter 5561\n",
      "-----> iter 5562\n",
      "-----> iter 5563\n",
      "-----> iter 5564\n",
      "-----> iter 5565\n",
      "-----> iter 5566\n",
      "-----> iter 5567\n",
      "-----> iter 5568\n",
      "-----> iter 5569\n",
      "-----> iter 5570\n",
      "-----> iter 5571\n",
      "-----> iter 5572\n",
      "-----> iter 5573\n",
      "-----> iter 5574\n",
      "-----> iter 5575\n",
      "-----> iter 5576\n",
      "-----> iter 5577\n",
      "-----> iter 5578\n",
      "-----> iter 5579\n",
      "-----> iter 5580\n",
      "-----> iter 5581\n",
      "-----> iter 5582\n",
      "-----> iter 5583\n",
      "-----> iter 5584\n",
      "-----> iter 5585\n",
      "-----> iter 5586\n",
      "-----> iter 5587\n",
      "-----> iter 5588\n",
      "-----> iter 5589\n",
      "-----> iter 5590\n",
      "-----> iter 5591\n",
      "-----> iter 5592\n",
      "-----> iter 5593\n",
      "-----> iter 5594\n",
      "-----> iter 5595\n",
      "-----> iter 5596\n",
      "-----> iter 5597\n",
      "-----> iter 5598\n",
      "-----> iter 5599\n",
      "-----> iter 5600\n",
      "-----> iter 5601\n",
      "-----> iter 5602\n",
      "-----> iter 5603\n",
      "-----> iter 5604\n",
      "-----> iter 5605\n",
      "-----> iter 5606\n",
      "-----> iter 5607\n",
      "-----> iter 5608\n",
      "-----> iter 5609\n",
      "-----> iter 5610\n",
      "-----> iter 5611\n",
      "-----> iter 5612\n",
      "-----> iter 5613\n",
      "-----> iter 5614\n",
      "-----> iter 5615\n",
      "-----> iter 5616\n",
      "-----> iter 5617\n",
      "-----> iter 5618\n",
      "-----> iter 5619\n",
      "-----> iter 5620\n",
      "-----> iter 5621\n",
      "-----> iter 5622\n",
      "-----> iter 5623\n",
      "-----> iter 5624\n",
      "-----> iter 5625\n",
      "-----> iter 5626\n",
      "-----> iter 5627\n",
      "-----> iter 5628\n",
      "-----> iter 5629\n",
      "-----> iter 5630\n",
      "-----> iter 5631\n",
      "-----> iter 5632\n",
      "-----> iter 5633\n",
      "-----> iter 5634\n",
      "-----> iter 5635\n",
      "-----> iter 5636\n",
      "-----> iter 5637\n",
      "-----> iter 5638\n",
      "-----> iter 5639\n",
      "-----> iter 5640\n",
      "-----> iter 5641\n",
      "-----> iter 5642\n",
      "-----> iter 5643\n",
      "-----> iter 5644\n",
      "-----> iter 5645\n",
      "-----> iter 5646\n",
      "-----> iter 5647\n",
      "-----> iter 5648\n",
      "-----> iter 5649\n",
      "-----> iter 5650\n",
      "-----> iter 5651\n",
      "-----> iter 5652\n",
      "-----> iter 5653\n",
      "-----> iter 5654\n",
      "-----> iter 5655\n",
      "-----> iter 5656\n",
      "-----> iter 5657\n",
      "-----> iter 5658\n",
      "-----> iter 5659\n",
      "-----> iter 5660\n",
      "-----> iter 5661\n",
      "-----> iter 5662\n",
      "-----> iter 5663\n",
      "-----> iter 5664\n",
      "-----> iter 5665\n",
      "-----> iter 5666\n",
      "-----> iter 5667\n",
      "-----> iter 5668\n",
      "-----> iter 5669\n",
      "-----> iter 5670\n",
      "-----> iter 5671\n",
      "-----> iter 5672\n",
      "-----> iter 5673\n",
      "-----> iter 5674\n",
      "-----> iter 5675\n",
      "-----> iter 5676\n",
      "-----> iter 5677\n",
      "-----> iter 5678\n",
      "-----> iter 5679\n",
      "-----> iter 5680\n",
      "-----> iter 5681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 5682\n",
      "-----> iter 5683\n",
      "-----> iter 5684\n",
      "-----> iter 5685\n",
      "-----> iter 5686\n",
      "-----> iter 5687\n",
      "-----> iter 5688\n",
      "-----> iter 5689\n",
      "-----> iter 5690\n",
      "-----> iter 5691\n",
      "-----> iter 5692\n",
      "-----> iter 5693\n",
      "-----> iter 5694\n",
      "-----> iter 5695\n",
      "-----> iter 5696\n",
      "-----> iter 5697\n",
      "-----> iter 5698\n",
      "-----> iter 5699\n",
      "-----> iter 5700\n",
      "-----> iter 5701\n",
      "-----> iter 5702\n",
      "-----> iter 5703\n",
      "-----> iter 5704\n",
      "-----> iter 5705\n",
      "-----> iter 5706\n",
      "-----> iter 5707\n",
      "-----> iter 5708\n",
      "-----> iter 5709\n",
      "-----> iter 5710\n",
      "-----> iter 5711\n",
      "-----> iter 5712\n",
      "-----> iter 5713\n",
      "-----> iter 5714\n",
      "-----> iter 5715\n",
      "-----> iter 5716\n",
      "-----> iter 5717\n",
      "-----> iter 5718\n",
      "-----> iter 5719\n",
      "-----> iter 5720\n",
      "-----> iter 5721\n",
      "-----> iter 5722\n",
      "-----> iter 5723\n",
      "-----> iter 5724\n",
      "-----> iter 5725\n",
      "-----> iter 5726\n",
      "-----> iter 5727\n",
      "-----> iter 5728\n",
      "-----> iter 5729\n",
      "-----> iter 5730\n",
      "-----> iter 5731\n",
      "-----> iter 5732\n",
      "-----> iter 5733\n",
      "-----> iter 5734\n",
      "-----> iter 5735\n",
      "-----> iter 5736\n",
      "-----> iter 5737\n",
      "-----> iter 5738\n",
      "-----> iter 5739\n",
      "-----> iter 5740\n",
      "-----> iter 5741\n",
      "-----> iter 5742\n",
      "-----> iter 5743\n",
      "-----> iter 5744\n",
      "-----> iter 5745\n",
      "-----> iter 5746\n",
      "-----> iter 5747\n",
      "-----> iter 5748\n",
      "-----> iter 5749\n",
      "-----> iter 5750\n",
      "-----> iter 5751\n",
      "-----> iter 5752\n",
      "-----> iter 5753\n",
      "-----> iter 5754\n",
      "-----> iter 5755\n",
      "-----> iter 5756\n",
      "-----> iter 5757\n",
      "-----> iter 5758\n",
      "-----> iter 5759\n",
      "-----> iter 5760\n",
      "-----> iter 5761\n",
      "-----> iter 5762\n",
      "-----> iter 5763\n",
      "-----> iter 5764\n",
      "-----> iter 5765\n",
      "-----> iter 5766\n",
      "-----> iter 5767\n",
      "-----> iter 5768\n",
      "-----> iter 5769\n",
      "-----> iter 5770\n",
      "-----> iter 5771\n",
      "-----> iter 5772\n",
      "-----> iter 5773\n",
      "-----> iter 5774\n",
      "-----> iter 5775\n",
      "-----> iter 5776\n",
      "-----> iter 5777\n",
      "-----> iter 5778\n",
      "-----> iter 5779\n",
      "-----> iter 5780\n",
      "-----> iter 5781\n",
      "-----> iter 5782\n",
      "-----> iter 5783\n",
      "-----> iter 5784\n",
      "-----> iter 5785\n",
      "-----> iter 5786\n",
      "-----> iter 5787\n",
      "-----> iter 5788\n",
      "-----> iter 5789\n",
      "-----> iter 5790\n",
      "-----> iter 5791\n",
      "-----> iter 5792\n",
      "-----> iter 5793\n",
      "-----> iter 5794\n",
      "-----> iter 5795\n",
      "-----> iter 5796\n",
      "-----> iter 5797\n",
      "-----> iter 5798\n",
      "-----> iter 5799\n",
      "-----> iter 5800\n",
      "-----> iter 5801\n",
      "-----> iter 5802\n",
      "-----> iter 5803\n",
      "-----> iter 5804\n",
      "-----> iter 5805\n",
      "-----> iter 5806\n",
      "-----> iter 5807\n",
      "-----> iter 5808\n",
      "-----> iter 5809\n",
      "-----> iter 5810\n",
      "-----> iter 5811\n",
      "-----> iter 5812\n",
      "-----> iter 5813\n",
      "-----> iter 5814\n",
      "-----> iter 5815\n",
      "-----> iter 5816\n",
      "-----> iter 5817\n",
      "-----> iter 5818\n",
      "-----> iter 5819\n",
      "-----> iter 5820\n",
      "-----> iter 5821\n",
      "-----> iter 5822\n",
      "-----> iter 5823\n",
      "-----> iter 5824\n",
      "-----> iter 5825\n",
      "-----> iter 5826\n",
      "-----> iter 5827\n",
      "-----> iter 5828\n",
      "-----> iter 5829\n",
      "-----> iter 5830\n",
      "-----> iter 5831\n",
      "-----> iter 5832\n",
      "-----> iter 5833\n",
      "-----> iter 5834\n",
      "-----> iter 5835\n",
      "-----> iter 5836\n",
      "-----> iter 5837\n",
      "-----> iter 5838\n",
      "-----> iter 5839\n",
      "-----> iter 5840\n",
      "-----> iter 5841\n",
      "-----> iter 5842\n",
      "-----> iter 5843\n",
      "-----> iter 5844\n",
      "-----> iter 5845\n",
      "-----> iter 5846\n",
      "-----> iter 5847\n",
      "-----> iter 5848\n",
      "-----> iter 5849\n",
      "-----> iter 5850\n",
      "-----> iter 5851\n",
      "-----> iter 5852\n",
      "-----> iter 5853\n",
      "-----> iter 5854\n",
      "-----> iter 5855\n",
      "-----> iter 5856\n",
      "-----> iter 5857\n",
      "-----> iter 5858\n",
      "-----> iter 5859\n",
      "-----> iter 5860\n",
      "-----> iter 5861\n",
      "-----> iter 5862\n",
      "-----> iter 5863\n",
      "-----> iter 5864\n",
      "-----> iter 5865\n",
      "-----> iter 5866\n",
      "-----> iter 5867\n",
      "-----> iter 5868\n",
      "-----> iter 5869\n",
      "-----> iter 5870\n",
      "-----> iter 5871\n",
      "-----> iter 5872\n",
      "-----> iter 5873\n",
      "-----> iter 5874\n",
      "-----> iter 5875\n",
      "-----> iter 5876\n",
      "-----> iter 5877\n",
      "-----> iter 5878\n",
      "-----> iter 5879\n",
      "-----> iter 5880\n",
      "-----> iter 5881\n",
      "-----> iter 5882\n",
      "-----> iter 5883\n",
      "-----> iter 5884\n",
      "-----> iter 5885\n",
      "-----> iter 5886\n",
      "-----> iter 5887\n",
      "-----> iter 5888\n",
      "-----> iter 5889\n",
      "-----> iter 5890\n",
      "-----> iter 5891\n",
      "-----> iter 5892\n",
      "-----> iter 5893\n",
      "-----> iter 5894\n",
      "-----> iter 5895\n",
      "-----> iter 5896\n",
      "-----> iter 5897\n",
      "-----> iter 5898\n",
      "-----> iter 5899\n",
      "-----> iter 5900\n",
      "-----> iter 5901\n",
      "-----> iter 5902\n",
      "-----> iter 5903\n",
      "-----> iter 5904\n",
      "-----> iter 5905\n",
      "-----> iter 5906\n",
      "-----> iter 5907\n",
      "-----> iter 5908\n",
      "-----> iter 5909\n",
      "-----> iter 5910\n",
      "-----> iter 5911\n",
      "-----> iter 5912\n",
      "-----> iter 5913\n",
      "-----> iter 5914\n",
      "-----> iter 5915\n",
      "-----> iter 5916\n",
      "-----> iter 5917\n",
      "-----> iter 5918\n",
      "-----> iter 5919\n",
      "-----> iter 5920\n",
      "-----> iter 5921\n",
      "-----> iter 5922\n",
      "-----> iter 5923\n",
      "-----> iter 5924\n",
      "-----> iter 5925\n",
      "-----> iter 5926\n",
      "-----> iter 5927\n",
      "-----> iter 5928\n",
      "-----> iter 5929\n",
      "-----> iter 5930\n",
      "-----> iter 5931\n",
      "-----> iter 5932\n",
      "-----> iter 5933\n",
      "-----> iter 5934\n",
      "-----> iter 5935\n",
      "-----> iter 5936\n",
      "-----> iter 5937\n",
      "-----> iter 5938\n",
      "-----> iter 5939\n",
      "-----> iter 5940\n",
      "-----> iter 5941\n",
      "-----> iter 5942\n",
      "-----> iter 5943\n",
      "-----> iter 5944\n",
      "-----> iter 5945\n",
      "-----> iter 5946\n",
      "-----> iter 5947\n",
      "-----> iter 5948\n",
      "-----> iter 5949\n",
      "-----> iter 5950\n",
      "-----> iter 5951\n",
      "-----> iter 5952\n",
      "-----> iter 5953\n",
      "-----> iter 5954\n",
      "-----> iter 5955\n",
      "-----> iter 5956\n",
      "-----> iter 5957\n",
      "-----> iter 5958\n",
      "-----> iter 5959\n",
      "-----> iter 5960\n",
      "-----> iter 5961\n",
      "-----> iter 5962\n",
      "-----> iter 5963\n",
      "-----> iter 5964\n",
      "-----> iter 5965\n",
      "-----> iter 5966\n",
      "-----> iter 5967\n",
      "-----> iter 5968\n",
      "-----> iter 5969\n",
      "-----> iter 5970\n",
      "-----> iter 5971\n",
      "-----> iter 5972\n",
      "-----> iter 5973\n",
      "-----> iter 5974\n",
      "-----> iter 5975\n",
      "-----> iter 5976\n",
      "-----> iter 5977\n",
      "-----> iter 5978\n",
      "-----> iter 5979\n",
      "-----> iter 5980\n",
      "-----> iter 5981\n",
      "-----> iter 5982\n",
      "-----> iter 5983\n",
      "-----> iter 5984\n",
      "-----> iter 5985\n",
      "-----> iter 5986\n",
      "-----> iter 5987\n",
      "-----> iter 5988\n",
      "-----> iter 5989\n",
      "-----> iter 5990\n",
      "-----> iter 5991\n",
      "-----> iter 5992\n",
      "-----> iter 5993\n",
      "-----> iter 5994\n",
      "-----> iter 5995\n",
      "-----> iter 5996\n",
      "-----> iter 5997\n",
      "-----> iter 5998\n",
      "-----> iter 5999\n",
      "-----> iter 6000\n",
      "-----> iter 6001\n",
      "-----> iter 6002\n",
      "-----> iter 6003\n",
      "-----> iter 6004\n",
      "-----> iter 6005\n",
      "-----> iter 6006\n",
      "-----> iter 6007\n",
      "-----> iter 6008\n",
      "-----> iter 6009\n",
      "-----> iter 6010\n",
      "-----> iter 6011\n",
      "-----> iter 6012\n",
      "-----> iter 6013\n",
      "-----> iter 6014\n",
      "-----> iter 6015\n",
      "-----> iter 6016\n",
      "-----> iter 6017\n",
      "-----> iter 6018\n",
      "-----> iter 6019\n",
      "-----> iter 6020\n",
      "-----> iter 6021\n",
      "-----> iter 6022\n",
      "-----> iter 6023\n",
      "-----> iter 6024\n",
      "-----> iter 6025\n",
      "-----> iter 6026\n",
      "-----> iter 6027\n",
      "-----> iter 6028\n",
      "-----> iter 6029\n",
      "-----> iter 6030\n",
      "-----> iter 6031\n",
      "-----> iter 6032\n",
      "-----> iter 6033\n",
      "-----> iter 6034\n",
      "-----> iter 6035\n",
      "-----> iter 6036\n",
      "-----> iter 6037\n",
      "-----> iter 6038\n",
      "-----> iter 6039\n",
      "-----> iter 6040\n",
      "-----> iter 6041\n",
      "-----> iter 6042\n",
      "-----> iter 6043\n",
      "-----> iter 6044\n",
      "-----> iter 6045\n",
      "-----> iter 6046\n",
      "-----> iter 6047\n",
      "-----> iter 6048\n",
      "-----> iter 6049\n",
      "-----> iter 6050\n",
      "-----> iter 6051\n",
      "-----> iter 6052\n",
      "-----> iter 6053\n",
      "-----> iter 6054\n",
      "-----> iter 6055\n",
      "-----> iter 6056\n",
      "-----> iter 6057\n",
      "-----> iter 6058\n",
      "-----> iter 6059\n",
      "-----> iter 6060\n",
      "-----> iter 6061\n",
      "-----> iter 6062\n",
      "-----> iter 6063\n",
      "-----> iter 6064\n",
      "-----> iter 6065\n",
      "-----> iter 6066\n",
      "-----> iter 6067\n",
      "-----> iter 6068\n",
      "-----> iter 6069\n",
      "-----> iter 6070\n",
      "-----> iter 6071\n",
      "-----> iter 6072\n",
      "-----> iter 6073\n",
      "-----> iter 6074\n",
      "-----> iter 6075\n",
      "-----> iter 6076\n",
      "-----> iter 6077\n",
      "-----> iter 6078\n",
      "-----> iter 6079\n",
      "-----> iter 6080\n",
      "-----> iter 6081\n",
      "-----> iter 6082\n",
      "-----> iter 6083\n",
      "-----> iter 6084\n",
      "-----> iter 6085\n",
      "-----> iter 6086\n",
      "-----> iter 6087\n",
      "-----> iter 6088\n",
      "-----> iter 6089\n",
      "-----> iter 6090\n",
      "-----> iter 6091\n",
      "-----> iter 6092\n",
      "-----> iter 6093\n",
      "-----> iter 6094\n",
      "-----> iter 6095\n",
      "-----> iter 6096\n",
      "-----> iter 6097\n",
      "-----> iter 6098\n",
      "-----> iter 6099\n",
      "-----> iter 6100\n",
      "-----> iter 6101\n",
      "-----> iter 6102\n",
      "-----> iter 6103\n",
      "-----> iter 6104\n",
      "-----> iter 6105\n",
      "-----> iter 6106\n",
      "-----> iter 6107\n",
      "-----> iter 6108\n",
      "-----> iter 6109\n",
      "-----> iter 6110\n",
      "-----> iter 6111\n",
      "-----> iter 6112\n",
      "-----> iter 6113\n",
      "-----> iter 6114\n",
      "-----> iter 6115\n",
      "-----> iter 6116\n",
      "-----> iter 6117\n",
      "-----> iter 6118\n",
      "-----> iter 6119\n",
      "-----> iter 6120\n",
      "-----> iter 6121\n",
      "-----> iter 6122\n",
      "-----> iter 6123\n",
      "-----> iter 6124\n",
      "-----> iter 6125\n",
      "-----> iter 6126\n",
      "-----> iter 6127\n",
      "-----> iter 6128\n",
      "-----> iter 6129\n",
      "-----> iter 6130\n",
      "-----> iter 6131\n",
      "-----> iter 6132\n",
      "-----> iter 6133\n",
      "-----> iter 6134\n",
      "-----> iter 6135\n",
      "-----> iter 6136\n",
      "-----> iter 6137\n",
      "-----> iter 6138\n",
      "-----> iter 6139\n",
      "-----> iter 6140\n",
      "-----> iter 6141\n",
      "-----> iter 6142\n",
      "-----> iter 6143\n",
      "-----> iter 6144\n",
      "-----> iter 6145\n",
      "-----> iter 6146\n",
      "-----> iter 6147\n",
      "-----> iter 6148\n",
      "-----> iter 6149\n",
      "-----> iter 6150\n",
      "-----> iter 6151\n",
      "-----> iter 6152\n",
      "-----> iter 6153\n",
      "-----> iter 6154\n",
      "-----> iter 6155\n",
      "-----> iter 6156\n",
      "-----> iter 6157\n",
      "-----> iter 6158\n",
      "-----> iter 6159\n",
      "-----> iter 6160\n",
      "-----> iter 6161\n",
      "-----> iter 6162\n",
      "-----> iter 6163\n",
      "-----> iter 6164\n",
      "-----> iter 6165\n",
      "-----> iter 6166\n",
      "-----> iter 6167\n",
      "-----> iter 6168\n",
      "-----> iter 6169\n",
      "-----> iter 6170\n",
      "-----> iter 6171\n",
      "-----> iter 6172\n",
      "-----> iter 6173\n",
      "-----> iter 6174\n",
      "-----> iter 6175\n",
      "-----> iter 6176\n",
      "-----> iter 6177\n",
      "-----> iter 6178\n",
      "-----> iter 6179\n",
      "-----> iter 6180\n",
      "-----> iter 6181\n",
      "-----> iter 6182\n",
      "-----> iter 6183\n",
      "-----> iter 6184\n",
      "-----> iter 6185\n",
      "-----> iter 6186\n",
      "-----> iter 6187\n",
      "-----> iter 6188\n",
      "-----> iter 6189\n",
      "-----> iter 6190\n",
      "-----> iter 6191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 6192\n",
      "-----> iter 6193\n",
      "-----> iter 6194\n",
      "-----> iter 6195\n",
      "-----> iter 6196\n",
      "-----> iter 6197\n",
      "-----> iter 6198\n",
      "-----> iter 6199\n",
      "-----> iter 6200\n",
      "-----> iter 6201\n",
      "-----> iter 6202\n",
      "-----> iter 6203\n",
      "-----> iter 6204\n",
      "-----> iter 6205\n",
      "-----> iter 6206\n",
      "-----> iter 6207\n",
      "-----> iter 6208\n",
      "-----> iter 6209\n",
      "-----> iter 6210\n",
      "-----> iter 6211\n",
      "-----> iter 6212\n",
      "-----> iter 6213\n",
      "-----> iter 6214\n",
      "-----> iter 6215\n",
      "-----> iter 6216\n",
      "-----> iter 6217\n",
      "-----> iter 6218\n",
      "-----> iter 6219\n",
      "-----> iter 6220\n",
      "-----> iter 6221\n",
      "-----> iter 6222\n",
      "-----> iter 6223\n",
      "-----> iter 6224\n",
      "-----> iter 6225\n",
      "-----> iter 6226\n",
      "-----> iter 6227\n",
      "-----> iter 6228\n",
      "-----> iter 6229\n",
      "-----> iter 6230\n",
      "-----> iter 6231\n",
      "-----> iter 6232\n",
      "-----> iter 6233\n",
      "-----> iter 6234\n",
      "-----> iter 6235\n",
      "-----> iter 6236\n",
      "-----> iter 6237\n",
      "-----> iter 6238\n",
      "-----> iter 6239\n",
      "-----> iter 6240\n",
      "-----> iter 6241\n",
      "-----> iter 6242\n",
      "-----> iter 6243\n",
      "-----> iter 6244\n",
      "-----> iter 6245\n",
      "-----> iter 6246\n",
      "-----> iter 6247\n",
      "-----> iter 6248\n",
      "-----> iter 6249\n",
      "-----> iter 6250\n",
      "-----> iter 6251\n",
      "-----> iter 6252\n",
      "-----> iter 6253\n",
      "-----> iter 6254\n",
      "-----> iter 6255\n",
      "-----> iter 6256\n",
      "-----> iter 6257\n",
      "-----> iter 6258\n",
      "-----> iter 6259\n",
      "-----> iter 6260\n",
      "-----> iter 6261\n",
      "-----> iter 6262\n",
      "-----> iter 6263\n",
      "-----> iter 6264\n",
      "-----> iter 6265\n",
      "-----> iter 6266\n",
      "-----> iter 6267\n",
      "-----> iter 6268\n",
      "-----> iter 6269\n",
      "-----> iter 6270\n",
      "-----> iter 6271\n",
      "-----> iter 6272\n",
      "-----> iter 6273\n",
      "-----> iter 6274\n",
      "-----> iter 6275\n",
      "-----> iter 6276\n",
      "-----> iter 6277\n",
      "-----> iter 6278\n",
      "-----> iter 6279\n",
      "-----> iter 6280\n",
      "-----> iter 6281\n",
      "-----> iter 6282\n",
      "-----> iter 6283\n",
      "-----> iter 6284\n",
      "-----> iter 6285\n",
      "-----> iter 6286\n",
      "-----> iter 6287\n",
      "-----> iter 6288\n",
      "-----> iter 6289\n",
      "-----> iter 6290\n",
      "-----> iter 6291\n",
      "-----> iter 6292\n",
      "-----> iter 6293\n",
      "-----> iter 6294\n",
      "-----> iter 6295\n",
      "-----> iter 6296\n",
      "-----> iter 6297\n",
      "-----> iter 6298\n",
      "-----> iter 6299\n",
      "-----> iter 6300\n",
      "-----> iter 6301\n",
      "-----> iter 6302\n",
      "-----> iter 6303\n",
      "-----> iter 6304\n",
      "-----> iter 6305\n",
      "-----> iter 6306\n",
      "-----> iter 6307\n",
      "-----> iter 6308\n",
      "-----> iter 6309\n",
      "-----> iter 6310\n",
      "-----> iter 6311\n",
      "-----> iter 6312\n",
      "-----> iter 6313\n",
      "-----> iter 6314\n",
      "-----> iter 6315\n",
      "-----> iter 6316\n",
      "-----> iter 6317\n",
      "-----> iter 6318\n",
      "-----> iter 6319\n",
      "-----> iter 6320\n",
      "-----> iter 6321\n",
      "-----> iter 6322\n",
      "-----> iter 6323\n",
      "-----> iter 6324\n",
      "-----> iter 6325\n",
      "-----> iter 6326\n",
      "-----> iter 6327\n",
      "-----> iter 6328\n",
      "-----> iter 6329\n",
      "-----> iter 6330\n",
      "-----> iter 6331\n",
      "-----> iter 6332\n",
      "-----> iter 6333\n",
      "-----> iter 6334\n",
      "-----> iter 6335\n",
      "-----> iter 6336\n",
      "-----> iter 6337\n",
      "-----> iter 6338\n",
      "-----> iter 6339\n",
      "-----> iter 6340\n",
      "-----> iter 6341\n",
      "-----> iter 6342\n",
      "-----> iter 6343\n",
      "-----> iter 6344\n",
      "-----> iter 6345\n",
      "-----> iter 6346\n",
      "-----> iter 6347\n",
      "-----> iter 6348\n",
      "-----> iter 6349\n",
      "-----> iter 6350\n",
      "-----> iter 6351\n",
      "-----> iter 6352\n",
      "-----> iter 6353\n",
      "-----> iter 6354\n",
      "-----> iter 6355\n",
      "-----> iter 6356\n",
      "-----> iter 6357\n",
      "-----> iter 6358\n",
      "-----> iter 6359\n",
      "-----> iter 6360\n",
      "-----> iter 6361\n",
      "-----> iter 6362\n",
      "-----> iter 6363\n",
      "-----> iter 6364\n",
      "-----> iter 6365\n",
      "-----> iter 6366\n",
      "-----> iter 6367\n",
      "-----> iter 6368\n",
      "-----> iter 6369\n",
      "-----> iter 6370\n",
      "-----> iter 6371\n",
      "-----> iter 6372\n",
      "-----> iter 6373\n",
      "-----> iter 6374\n",
      "-----> iter 6375\n",
      "-----> iter 6376\n",
      "-----> iter 6377\n",
      "-----> iter 6378\n",
      "-----> iter 6379\n",
      "-----> iter 6380\n",
      "-----> iter 6381\n",
      "-----> iter 6382\n",
      "-----> iter 6383\n",
      "-----> iter 6384\n",
      "-----> iter 6385\n",
      "-----> iter 6386\n",
      "-----> iter 6387\n",
      "-----> iter 6388\n",
      "-----> iter 6389\n",
      "-----> iter 6390\n",
      "-----> iter 6391\n",
      "-----> iter 6392\n",
      "-----> iter 6393\n",
      "-----> iter 6394\n",
      "-----> iter 6395\n",
      "-----> iter 6396\n",
      "-----> iter 6397\n",
      "-----> iter 6398\n",
      "-----> iter 6399\n",
      "-----> iter 6400\n",
      "-----> iter 6401\n",
      "-----> iter 6402\n",
      "-----> iter 6403\n",
      "-----> iter 6404\n",
      "-----> iter 6405\n",
      "-----> iter 6406\n",
      "-----> iter 6407\n",
      "-----> iter 6408\n",
      "-----> iter 6409\n",
      "-----> iter 6410\n",
      "-----> iter 6411\n",
      "-----> iter 6412\n",
      "-----> iter 6413\n",
      "-----> iter 6414\n",
      "-----> iter 6415\n",
      "-----> iter 6416\n",
      "-----> iter 6417\n",
      "-----> iter 6418\n",
      "-----> iter 6419\n",
      "-----> iter 6420\n",
      "-----> iter 6421\n",
      "-----> iter 6422\n",
      "-----> iter 6423\n",
      "-----> iter 6424\n",
      "-----> iter 6425\n",
      "-----> iter 6426\n",
      "-----> iter 6427\n",
      "-----> iter 6428\n",
      "-----> iter 6429\n",
      "-----> iter 6430\n",
      "-----> iter 6431\n",
      "-----> iter 6432\n",
      "-----> iter 6433\n",
      "-----> iter 6434\n",
      "-----> iter 6435\n",
      "-----> iter 6436\n",
      "-----> iter 6437\n",
      "-----> iter 6438\n",
      "-----> iter 6439\n",
      "-----> iter 6440\n",
      "-----> iter 6441\n",
      "-----> iter 6442\n",
      "-----> iter 6443\n",
      "-----> iter 6444\n",
      "-----> iter 6445\n",
      "-----> iter 6446\n",
      "-----> iter 6447\n",
      "-----> iter 6448\n",
      "-----> iter 6449\n",
      "-----> iter 6450\n",
      "-----> iter 6451\n",
      "-----> iter 6452\n",
      "-----> iter 6453\n",
      "-----> iter 6454\n",
      "-----> iter 6455\n",
      "-----> iter 6456\n",
      "-----> iter 6457\n",
      "-----> iter 6458\n",
      "-----> iter 6459\n",
      "-----> iter 6460\n",
      "-----> iter 6461\n",
      "-----> iter 6462\n",
      "-----> iter 6463\n",
      "-----> iter 6464\n",
      "-----> iter 6465\n",
      "-----> iter 6466\n",
      "-----> iter 6467\n",
      "-----> iter 6468\n",
      "-----> iter 6469\n",
      "-----> iter 6470\n",
      "-----> iter 6471\n",
      "-----> iter 6472\n",
      "-----> iter 6473\n",
      "-----> iter 6474\n",
      "-----> iter 6475\n",
      "-----> iter 6476\n",
      "-----> iter 6477\n",
      "-----> iter 6478\n",
      "-----> iter 6479\n",
      "-----> iter 6480\n",
      "-----> iter 6481\n",
      "-----> iter 6482\n",
      "-----> iter 6483\n",
      "-----> iter 6484\n",
      "-----> iter 6485\n",
      "-----> iter 6486\n",
      "-----> iter 6487\n",
      "-----> iter 6488\n",
      "-----> iter 6489\n",
      "-----> iter 6490\n",
      "-----> iter 6491\n",
      "-----> iter 6492\n",
      "-----> iter 6493\n",
      "-----> iter 6494\n",
      "-----> iter 6495\n",
      "-----> iter 6496\n",
      "-----> iter 6497\n",
      "-----> iter 6498\n",
      "-----> iter 6499\n",
      "-----> iter 6500\n",
      "-----> iter 6501\n",
      "-----> iter 6502\n",
      "-----> iter 6503\n",
      "-----> iter 6504\n",
      "-----> iter 6505\n",
      "-----> iter 6506\n",
      "-----> iter 6507\n",
      "-----> iter 6508\n",
      "-----> iter 6509\n",
      "-----> iter 6510\n",
      "-----> iter 6511\n",
      "-----> iter 6512\n",
      "-----> iter 6513\n",
      "-----> iter 6514\n",
      "-----> iter 6515\n",
      "-----> iter 6516\n",
      "-----> iter 6517\n",
      "-----> iter 6518\n",
      "-----> iter 6519\n",
      "-----> iter 6520\n",
      "-----> iter 6521\n",
      "-----> iter 6522\n",
      "-----> iter 6523\n",
      "-----> iter 6524\n",
      "-----> iter 6525\n",
      "-----> iter 6526\n",
      "-----> iter 6527\n",
      "-----> iter 6528\n",
      "-----> iter 6529\n",
      "-----> iter 6530\n",
      "-----> iter 6531\n",
      "-----> iter 6532\n",
      "-----> iter 6533\n",
      "-----> iter 6534\n",
      "-----> iter 6535\n",
      "-----> iter 6536\n",
      "-----> iter 6537\n",
      "-----> iter 6538\n",
      "-----> iter 6539\n",
      "-----> iter 6540\n",
      "-----> iter 6541\n",
      "-----> iter 6542\n",
      "-----> iter 6543\n",
      "-----> iter 6544\n",
      "-----> iter 6545\n",
      "-----> iter 6546\n",
      "-----> iter 6547\n",
      "-----> iter 6548\n",
      "-----> iter 6549\n",
      "-----> iter 6550\n",
      "-----> iter 6551\n",
      "-----> iter 6552\n",
      "-----> iter 6553\n",
      "-----> iter 6554\n",
      "-----> iter 6555\n",
      "-----> iter 6556\n",
      "-----> iter 6557\n",
      "-----> iter 6558\n",
      "-----> iter 6559\n",
      "-----> iter 6560\n",
      "-----> iter 6561\n",
      "-----> iter 6562\n",
      "-----> iter 6563\n",
      "-----> iter 6564\n",
      "-----> iter 6565\n",
      "-----> iter 6566\n",
      "-----> iter 6567\n",
      "-----> iter 6568\n",
      "-----> iter 6569\n",
      "-----> iter 6570\n",
      "-----> iter 6571\n",
      "-----> iter 6572\n",
      "-----> iter 6573\n",
      "-----> iter 6574\n",
      "-----> iter 6575\n",
      "-----> iter 6576\n",
      "-----> iter 6577\n",
      "-----> iter 6578\n",
      "-----> iter 6579\n",
      "-----> iter 6580\n",
      "-----> iter 6581\n",
      "-----> iter 6582\n",
      "-----> iter 6583\n",
      "-----> iter 6584\n",
      "-----> iter 6585\n",
      "-----> iter 6586\n",
      "-----> iter 6587\n",
      "-----> iter 6588\n",
      "-----> iter 6589\n",
      "-----> iter 6590\n",
      "-----> iter 6591\n",
      "-----> iter 6592\n",
      "-----> iter 6593\n",
      "-----> iter 6594\n",
      "-----> iter 6595\n",
      "-----> iter 6596\n",
      "-----> iter 6597\n",
      "-----> iter 6598\n",
      "-----> iter 6599\n",
      "-----> iter 6600\n",
      "-----> iter 6601\n",
      "-----> iter 6602\n",
      "-----> iter 6603\n",
      "-----> iter 6604\n",
      "-----> iter 6605\n",
      "-----> iter 6606\n",
      "-----> iter 6607\n",
      "-----> iter 6608\n",
      "-----> iter 6609\n",
      "-----> iter 6610\n",
      "-----> iter 6611\n",
      "-----> iter 6612\n",
      "-----> iter 6613\n",
      "-----> iter 6614\n",
      "-----> iter 6615\n",
      "-----> iter 6616\n",
      "-----> iter 6617\n",
      "-----> iter 6618\n",
      "-----> iter 6619\n",
      "-----> iter 6620\n",
      "-----> iter 6621\n",
      "-----> iter 6622\n",
      "-----> iter 6623\n",
      "-----> iter 6624\n",
      "-----> iter 6625\n",
      "-----> iter 6626\n",
      "-----> iter 6627\n",
      "-----> iter 6628\n",
      "-----> iter 6629\n",
      "-----> iter 6630\n",
      "-----> iter 6631\n",
      "-----> iter 6632\n",
      "-----> iter 6633\n",
      "-----> iter 6634\n",
      "-----> iter 6635\n",
      "-----> iter 6636\n",
      "-----> iter 6637\n",
      "-----> iter 6638\n",
      "-----> iter 6639\n",
      "-----> iter 6640\n",
      "-----> iter 6641\n",
      "-----> iter 6642\n",
      "-----> iter 6643\n",
      "-----> iter 6644\n",
      "-----> iter 6645\n",
      "-----> iter 6646\n",
      "-----> iter 6647\n",
      "-----> iter 6648\n",
      "-----> iter 6649\n",
      "-----> iter 6650\n",
      "-----> iter 6651\n",
      "-----> iter 6652\n",
      "-----> iter 6653\n",
      "-----> iter 6654\n",
      "-----> iter 6655\n",
      "-----> iter 6656\n",
      "-----> iter 6657\n",
      "-----> iter 6658\n",
      "-----> iter 6659\n",
      "-----> iter 6660\n",
      "-----> iter 6661\n",
      "-----> iter 6662\n",
      "-----> iter 6663\n",
      "-----> iter 6664\n",
      "-----> iter 6665\n",
      "-----> iter 6666\n",
      "-----> iter 6667\n",
      "-----> iter 6668\n",
      "-----> iter 6669\n",
      "-----> iter 6670\n",
      "-----> iter 6671\n",
      "-----> iter 6672\n",
      "-----> iter 6673\n",
      "-----> iter 6674\n",
      "-----> iter 6675\n",
      "-----> iter 6676\n",
      "-----> iter 6677\n",
      "-----> iter 6678\n",
      "-----> iter 6679\n",
      "-----> iter 6680\n",
      "-----> iter 6681\n",
      "-----> iter 6682\n",
      "-----> iter 6683\n",
      "-----> iter 6684\n",
      "-----> iter 6685\n",
      "-----> iter 6686\n",
      "-----> iter 6687\n",
      "-----> iter 6688\n",
      "-----> iter 6689\n",
      "-----> iter 6690\n",
      "-----> iter 6691\n",
      "-----> iter 6692\n",
      "-----> iter 6693\n",
      "-----> iter 6694\n",
      "-----> iter 6695\n",
      "-----> iter 6696\n",
      "-----> iter 6697\n",
      "-----> iter 6698\n",
      "-----> iter 6699\n",
      "-----> iter 6700\n",
      "-----> iter 6701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 6702\n",
      "-----> iter 6703\n",
      "-----> iter 6704\n",
      "-----> iter 6705\n",
      "-----> iter 6706\n",
      "-----> iter 6707\n",
      "-----> iter 6708\n",
      "-----> iter 6709\n",
      "-----> iter 6710\n",
      "-----> iter 6711\n",
      "-----> iter 6712\n",
      "-----> iter 6713\n",
      "-----> iter 6714\n",
      "-----> iter 6715\n",
      "-----> iter 6716\n",
      "-----> iter 6717\n",
      "-----> iter 6718\n",
      "-----> iter 6719\n",
      "-----> iter 6720\n",
      "-----> iter 6721\n",
      "-----> iter 6722\n",
      "-----> iter 6723\n",
      "-----> iter 6724\n",
      "-----> iter 6725\n",
      "-----> iter 6726\n",
      "-----> iter 6727\n",
      "-----> iter 6728\n",
      "-----> iter 6729\n",
      "-----> iter 6730\n",
      "-----> iter 6731\n",
      "-----> iter 6732\n",
      "-----> iter 6733\n",
      "-----> iter 6734\n",
      "-----> iter 6735\n",
      "-----> iter 6736\n",
      "-----> iter 6737\n",
      "-----> iter 6738\n",
      "-----> iter 6739\n",
      "-----> iter 6740\n",
      "-----> iter 6741\n",
      "-----> iter 6742\n",
      "-----> iter 6743\n",
      "-----> iter 6744\n",
      "-----> iter 6745\n",
      "-----> iter 6746\n",
      "-----> iter 6747\n",
      "-----> iter 6748\n",
      "-----> iter 6749\n",
      "-----> iter 6750\n",
      "-----> iter 6751\n",
      "-----> iter 6752\n",
      "-----> iter 6753\n",
      "-----> iter 6754\n",
      "-----> iter 6755\n",
      "-----> iter 6756\n",
      "-----> iter 6757\n",
      "-----> iter 6758\n",
      "-----> iter 6759\n",
      "-----> iter 6760\n",
      "-----> iter 6761\n",
      "-----> iter 6762\n",
      "-----> iter 6763\n",
      "-----> iter 6764\n",
      "-----> iter 6765\n",
      "-----> iter 6766\n",
      "-----> iter 6767\n",
      "-----> iter 6768\n",
      "-----> iter 6769\n",
      "-----> iter 6770\n",
      "-----> iter 6771\n",
      "-----> iter 6772\n",
      "-----> iter 6773\n",
      "-----> iter 6774\n",
      "-----> iter 6775\n",
      "-----> iter 6776\n",
      "-----> iter 6777\n",
      "-----> iter 6778\n",
      "-----> iter 6779\n",
      "-----> iter 6780\n",
      "-----> iter 6781\n",
      "-----> iter 6782\n",
      "-----> iter 6783\n",
      "-----> iter 6784\n",
      "-----> iter 6785\n",
      "-----> iter 6786\n",
      "-----> iter 6787\n",
      "-----> iter 6788\n",
      "-----> iter 6789\n",
      "-----> iter 6790\n",
      "-----> iter 6791\n",
      "-----> iter 6792\n",
      "-----> iter 6793\n",
      "-----> iter 6794\n",
      "-----> iter 6795\n",
      "-----> iter 6796\n",
      "-----> iter 6797\n",
      "-----> iter 6798\n",
      "-----> iter 6799\n",
      "-----> iter 6800\n",
      "-----> iter 6801\n",
      "-----> iter 6802\n",
      "-----> iter 6803\n",
      "-----> iter 6804\n",
      "-----> iter 6805\n",
      "-----> iter 6806\n",
      "-----> iter 6807\n",
      "-----> iter 6808\n",
      "-----> iter 6809\n",
      "-----> iter 6810\n",
      "-----> iter 6811\n",
      "-----> iter 6812\n",
      "-----> iter 6813\n",
      "-----> iter 6814\n",
      "-----> iter 6815\n",
      "-----> iter 6816\n",
      "-----> iter 6817\n",
      "-----> iter 6818\n",
      "-----> iter 6819\n",
      "-----> iter 6820\n",
      "-----> iter 6821\n",
      "-----> iter 6822\n",
      "-----> iter 6823\n",
      "-----> iter 6824\n",
      "-----> iter 6825\n",
      "-----> iter 6826\n",
      "-----> iter 6827\n",
      "-----> iter 6828\n",
      "-----> iter 6829\n",
      "-----> iter 6830\n",
      "-----> iter 6831\n",
      "-----> iter 6832\n",
      "-----> iter 6833\n",
      "-----> iter 6834\n",
      "-----> iter 6835\n",
      "-----> iter 6836\n",
      "-----> iter 6837\n",
      "-----> iter 6838\n",
      "-----> iter 6839\n",
      "-----> iter 6840\n",
      "-----> iter 6841\n",
      "-----> iter 6842\n",
      "-----> iter 6843\n",
      "-----> iter 6844\n",
      "-----> iter 6845\n",
      "-----> iter 6846\n",
      "-----> iter 6847\n",
      "-----> iter 6848\n",
      "-----> iter 6849\n",
      "-----> iter 6850\n",
      "-----> iter 6851\n",
      "-----> iter 6852\n",
      "-----> iter 6853\n",
      "-----> iter 6854\n",
      "-----> iter 6855\n",
      "-----> iter 6856\n",
      "-----> iter 6857\n",
      "-----> iter 6858\n",
      "-----> iter 6859\n",
      "-----> iter 6860\n",
      "-----> iter 6861\n",
      "-----> iter 6862\n",
      "-----> iter 6863\n",
      "-----> iter 6864\n",
      "-----> iter 6865\n",
      "-----> iter 6866\n",
      "-----> iter 6867\n",
      "-----> iter 6868\n",
      "-----> iter 6869\n",
      "-----> iter 6870\n",
      "-----> iter 6871\n",
      "-----> iter 6872\n",
      "-----> iter 6873\n",
      "-----> iter 6874\n",
      "-----> iter 6875\n",
      "-----> iter 6876\n",
      "-----> iter 6877\n",
      "-----> iter 6878\n",
      "-----> iter 6879\n",
      "-----> iter 6880\n",
      "-----> iter 6881\n",
      "-----> iter 6882\n",
      "-----> iter 6883\n",
      "-----> iter 6884\n",
      "-----> iter 6885\n",
      "-----> iter 6886\n",
      "-----> iter 6887\n",
      "-----> iter 6888\n",
      "-----> iter 6889\n",
      "-----> iter 6890\n",
      "-----> iter 6891\n",
      "-----> iter 6892\n",
      "-----> iter 6893\n",
      "-----> iter 6894\n",
      "-----> iter 6895\n",
      "-----> iter 6896\n",
      "-----> iter 6897\n",
      "-----> iter 6898\n",
      "-----> iter 6899\n",
      "-----> iter 6900\n",
      "-----> iter 6901\n",
      "-----> iter 6902\n",
      "-----> iter 6903\n",
      "-----> iter 6904\n",
      "-----> iter 6905\n",
      "-----> iter 6906\n",
      "-----> iter 6907\n",
      "-----> iter 6908\n",
      "-----> iter 6909\n",
      "-----> iter 6910\n",
      "-----> iter 6911\n",
      "-----> iter 6912\n",
      "-----> iter 6913\n",
      "-----> iter 6914\n",
      "-----> iter 6915\n",
      "-----> iter 6916\n",
      "-----> iter 6917\n",
      "-----> iter 6918\n",
      "-----> iter 6919\n",
      "-----> iter 6920\n",
      "-----> iter 6921\n",
      "-----> iter 6922\n",
      "-----> iter 6923\n",
      "-----> iter 6924\n",
      "-----> iter 6925\n",
      "-----> iter 6926\n",
      "-----> iter 6927\n",
      "-----> iter 6928\n",
      "-----> iter 6929\n",
      "-----> iter 6930\n",
      "-----> iter 6931\n",
      "-----> iter 6932\n",
      "-----> iter 6933\n",
      "-----> iter 6934\n",
      "-----> iter 6935\n",
      "-----> iter 6936\n",
      "-----> iter 6937\n",
      "-----> iter 6938\n",
      "-----> iter 6939\n",
      "-----> iter 6940\n",
      "-----> iter 6941\n",
      "-----> iter 6942\n",
      "-----> iter 6943\n",
      "-----> iter 6944\n",
      "-----> iter 6945\n",
      "-----> iter 6946\n",
      "-----> iter 6947\n",
      "-----> iter 6948\n",
      "-----> iter 6949\n",
      "-----> iter 6950\n",
      "-----> iter 6951\n",
      "-----> iter 6952\n",
      "-----> iter 6953\n",
      "-----> iter 6954\n",
      "-----> iter 6955\n",
      "-----> iter 6956\n",
      "-----> iter 6957\n",
      "-----> iter 6958\n",
      "-----> iter 6959\n",
      "-----> iter 6960\n",
      "-----> iter 6961\n",
      "-----> iter 6962\n",
      "-----> iter 6963\n",
      "-----> iter 6964\n",
      "-----> iter 6965\n",
      "-----> iter 6966\n",
      "-----> iter 6967\n",
      "-----> iter 6968\n",
      "-----> iter 6969\n",
      "-----> iter 6970\n",
      "-----> iter 6971\n",
      "-----> iter 6972\n",
      "-----> iter 6973\n",
      "-----> iter 6974\n",
      "-----> iter 6975\n",
      "-----> iter 6976\n",
      "-----> iter 6977\n",
      "-----> iter 6978\n",
      "-----> iter 6979\n",
      "-----> iter 6980\n",
      "-----> iter 6981\n",
      "-----> iter 6982\n",
      "-----> iter 6983\n",
      "-----> iter 6984\n",
      "-----> iter 6985\n",
      "-----> iter 6986\n",
      "-----> iter 6987\n",
      "-----> iter 6988\n",
      "-----> iter 6989\n",
      "-----> iter 6990\n",
      "-----> iter 6991\n",
      "-----> iter 6992\n",
      "-----> iter 6993\n",
      "-----> iter 6994\n",
      "-----> iter 6995\n",
      "-----> iter 6996\n",
      "-----> iter 6997\n",
      "-----> iter 6998\n",
      "-----> iter 6999\n",
      "-----> iter 7000\n",
      "-----> iter 7001\n",
      "-----> iter 7002\n",
      "-----> iter 7003\n",
      "-----> iter 7004\n",
      "-----> iter 7005\n",
      "-----> iter 7006\n",
      "-----> iter 7007\n",
      "-----> iter 7008\n",
      "-----> iter 7009\n",
      "-----> iter 7010\n",
      "-----> iter 7011\n",
      "-----> iter 7012\n",
      "-----> iter 7013\n",
      "-----> iter 7014\n",
      "-----> iter 7015\n",
      "-----> iter 7016\n",
      "-----> iter 7017\n",
      "-----> iter 7018\n",
      "-----> iter 7019\n",
      "-----> iter 7020\n",
      "-----> iter 7021\n",
      "-----> iter 7022\n",
      "-----> iter 7023\n",
      "-----> iter 7024\n",
      "-----> iter 7025\n",
      "-----> iter 7026\n",
      "-----> iter 7027\n",
      "-----> iter 7028\n",
      "-----> iter 7029\n",
      "-----> iter 7030\n",
      "-----> iter 7031\n",
      "-----> iter 7032\n",
      "-----> iter 7033\n",
      "-----> iter 7034\n",
      "-----> iter 7035\n",
      "-----> iter 7036\n",
      "-----> iter 7037\n",
      "-----> iter 7038\n",
      "-----> iter 7039\n",
      "-----> iter 7040\n",
      "-----> iter 7041\n",
      "-----> iter 7042\n",
      "-----> iter 7043\n",
      "-----> iter 7044\n",
      "-----> iter 7045\n",
      "-----> iter 7046\n",
      "-----> iter 7047\n",
      "-----> iter 7048\n",
      "-----> iter 7049\n",
      "-----> iter 7050\n",
      "-----> iter 7051\n",
      "-----> iter 7052\n",
      "-----> iter 7053\n",
      "-----> iter 7054\n",
      "-----> iter 7055\n",
      "-----> iter 7056\n",
      "-----> iter 7057\n",
      "-----> iter 7058\n",
      "-----> iter 7059\n",
      "-----> iter 7060\n",
      "-----> iter 7061\n",
      "-----> iter 7062\n",
      "-----> iter 7063\n",
      "-----> iter 7064\n",
      "-----> iter 7065\n",
      "-----> iter 7066\n",
      "-----> iter 7067\n",
      "-----> iter 7068\n",
      "-----> iter 7069\n",
      "-----> iter 7070\n",
      "-----> iter 7071\n",
      "-----> iter 7072\n",
      "-----> iter 7073\n",
      "-----> iter 7074\n",
      "-----> iter 7075\n",
      "-----> iter 7076\n",
      "-----> iter 7077\n",
      "-----> iter 7078\n",
      "-----> iter 7079\n",
      "-----> iter 7080\n",
      "-----> iter 7081\n",
      "-----> iter 7082\n",
      "-----> iter 7083\n",
      "-----> iter 7084\n",
      "-----> iter 7085\n",
      "-----> iter 7086\n",
      "-----> iter 7087\n",
      "-----> iter 7088\n",
      "-----> iter 7089\n",
      "-----> iter 7090\n",
      "-----> iter 7091\n",
      "-----> iter 7092\n",
      "-----> iter 7093\n",
      "-----> iter 7094\n",
      "-----> iter 7095\n",
      "-----> iter 7096\n",
      "-----> iter 7097\n",
      "-----> iter 7098\n",
      "-----> iter 7099\n",
      "-----> iter 7100\n",
      "-----> iter 7101\n",
      "-----> iter 7102\n",
      "-----> iter 7103\n",
      "-----> iter 7104\n",
      "-----> iter 7105\n",
      "-----> iter 7106\n",
      "-----> iter 7107\n",
      "-----> iter 7108\n",
      "-----> iter 7109\n",
      "-----> iter 7110\n",
      "-----> iter 7111\n",
      "-----> iter 7112\n",
      "-----> iter 7113\n",
      "-----> iter 7114\n",
      "-----> iter 7115\n",
      "-----> iter 7116\n",
      "-----> iter 7117\n",
      "-----> iter 7118\n",
      "-----> iter 7119\n",
      "-----> iter 7120\n",
      "-----> iter 7121\n",
      "-----> iter 7122\n",
      "-----> iter 7123\n",
      "-----> iter 7124\n",
      "-----> iter 7125\n",
      "-----> iter 7126\n",
      "-----> iter 7127\n",
      "-----> iter 7128\n",
      "-----> iter 7129\n",
      "-----> iter 7130\n",
      "-----> iter 7131\n",
      "-----> iter 7132\n",
      "-----> iter 7133\n",
      "-----> iter 7134\n",
      "-----> iter 7135\n",
      "-----> iter 7136\n",
      "-----> iter 7137\n",
      "-----> iter 7138\n",
      "-----> iter 7139\n",
      "-----> iter 7140\n",
      "-----> iter 7141\n",
      "-----> iter 7142\n",
      "-----> iter 7143\n",
      "-----> iter 7144\n",
      "-----> iter 7145\n",
      "-----> iter 7146\n",
      "-----> iter 7147\n",
      "-----> iter 7148\n",
      "-----> iter 7149\n",
      "-----> iter 7150\n",
      "-----> iter 7151\n",
      "-----> iter 7152\n",
      "-----> iter 7153\n",
      "-----> iter 7154\n",
      "-----> iter 7155\n",
      "-----> iter 7156\n",
      "-----> iter 7157\n",
      "-----> iter 7158\n",
      "-----> iter 7159\n",
      "-----> iter 7160\n",
      "-----> iter 7161\n",
      "-----> iter 7162\n",
      "-----> iter 7163\n",
      "-----> iter 7164\n",
      "-----> iter 7165\n",
      "-----> iter 7166\n",
      "-----> iter 7167\n",
      "-----> iter 7168\n",
      "-----> iter 7169\n",
      "-----> iter 7170\n",
      "-----> iter 7171\n",
      "-----> iter 7172\n",
      "-----> iter 7173\n",
      "-----> iter 7174\n",
      "-----> iter 7175\n",
      "-----> iter 7176\n",
      "-----> iter 7177\n",
      "-----> iter 7178\n",
      "-----> iter 7179\n",
      "-----> iter 7180\n",
      "-----> iter 7181\n",
      "-----> iter 7182\n",
      "-----> iter 7183\n",
      "-----> iter 7184\n",
      "-----> iter 7185\n",
      "-----> iter 7186\n",
      "-----> iter 7187\n",
      "-----> iter 7188\n",
      "-----> iter 7189\n",
      "-----> iter 7190\n",
      "-----> iter 7191\n",
      "-----> iter 7192\n",
      "-----> iter 7193\n",
      "-----> iter 7194\n",
      "-----> iter 7195\n",
      "-----> iter 7196\n",
      "-----> iter 7197\n",
      "-----> iter 7198\n",
      "-----> iter 7199\n",
      "-----> iter 7200\n",
      "-----> iter 7201\n",
      "-----> iter 7202\n",
      "-----> iter 7203\n",
      "-----> iter 7204\n",
      "-----> iter 7205\n",
      "-----> iter 7206\n",
      "-----> iter 7207\n",
      "-----> iter 7208\n",
      "-----> iter 7209\n",
      "-----> iter 7210\n",
      "-----> iter 7211\n",
      "-----> iter 7212\n",
      "-----> iter 7213\n",
      "-----> iter 7214\n",
      "-----> iter 7215\n",
      "-----> iter 7216\n",
      "-----> iter 7217\n",
      "-----> iter 7218\n",
      "-----> iter 7219\n",
      "-----> iter 7220\n",
      "-----> iter 7221\n",
      "-----> iter 7222\n",
      "-----> iter 7223\n",
      "-----> iter 7224\n",
      "-----> iter 7225\n",
      "-----> iter 7226\n",
      "-----> iter 7227\n",
      "-----> iter 7228\n",
      "-----> iter 7229\n",
      "-----> iter 7230\n",
      "-----> iter 7231\n",
      "-----> iter 7232\n",
      "-----> iter 7233\n",
      "-----> iter 7234\n",
      "-----> iter 7235\n",
      "-----> iter 7236\n",
      "-----> iter 7237\n",
      "-----> iter 7238\n",
      "-----> iter 7239\n",
      "-----> iter 7240\n",
      "-----> iter 7241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 7242\n",
      "-----> iter 7243\n",
      "-----> iter 7244\n",
      "-----> iter 7245\n",
      "-----> iter 7246\n",
      "-----> iter 7247\n",
      "-----> iter 7248\n",
      "-----> iter 7249\n",
      "-----> iter 7250\n",
      "-----> iter 7251\n",
      "-----> iter 7252\n",
      "-----> iter 7253\n",
      "-----> iter 7254\n",
      "-----> iter 7255\n",
      "-----> iter 7256\n",
      "-----> iter 7257\n",
      "-----> iter 7258\n",
      "-----> iter 7259\n",
      "-----> iter 7260\n",
      "-----> iter 7261\n",
      "-----> iter 7262\n",
      "-----> iter 7263\n",
      "-----> iter 7264\n",
      "-----> iter 7265\n",
      "-----> iter 7266\n",
      "-----> iter 7267\n",
      "-----> iter 7268\n",
      "-----> iter 7269\n",
      "-----> iter 7270\n",
      "-----> iter 7271\n",
      "-----> iter 7272\n",
      "-----> iter 7273\n",
      "-----> iter 7274\n",
      "-----> iter 7275\n",
      "-----> iter 7276\n",
      "-----> iter 7277\n",
      "-----> iter 7278\n",
      "-----> iter 7279\n",
      "-----> iter 7280\n",
      "-----> iter 7281\n",
      "-----> iter 7282\n",
      "-----> iter 7283\n",
      "-----> iter 7284\n",
      "-----> iter 7285\n",
      "-----> iter 7286\n",
      "-----> iter 7287\n",
      "-----> iter 7288\n",
      "-----> iter 7289\n",
      "-----> iter 7290\n",
      "-----> iter 7291\n",
      "-----> iter 7292\n",
      "-----> iter 7293\n",
      "-----> iter 7294\n",
      "-----> iter 7295\n",
      "-----> iter 7296\n",
      "-----> iter 7297\n",
      "-----> iter 7298\n",
      "-----> iter 7299\n",
      "-----> iter 7300\n",
      "-----> iter 7301\n",
      "-----> iter 7302\n",
      "-----> iter 7303\n",
      "-----> iter 7304\n",
      "-----> iter 7305\n",
      "-----> iter 7306\n",
      "-----> iter 7307\n",
      "-----> iter 7308\n",
      "-----> iter 7309\n",
      "-----> iter 7310\n",
      "-----> iter 7311\n",
      "-----> iter 7312\n",
      "-----> iter 7313\n",
      "-----> iter 7314\n",
      "-----> iter 7315\n",
      "-----> iter 7316\n",
      "-----> iter 7317\n",
      "-----> iter 7318\n",
      "-----> iter 7319\n",
      "-----> iter 7320\n",
      "-----> iter 7321\n",
      "-----> iter 7322\n",
      "-----> iter 7323\n",
      "-----> iter 7324\n",
      "-----> iter 7325\n",
      "-----> iter 7326\n",
      "-----> iter 7327\n",
      "-----> iter 7328\n",
      "-----> iter 7329\n",
      "-----> iter 7330\n",
      "-----> iter 7331\n",
      "-----> iter 7332\n",
      "-----> iter 7333\n",
      "-----> iter 7334\n",
      "-----> iter 7335\n",
      "-----> iter 7336\n",
      "-----> iter 7337\n",
      "-----> iter 7338\n",
      "-----> iter 7339\n",
      "-----> iter 7340\n",
      "-----> iter 7341\n",
      "-----> iter 7342\n",
      "-----> iter 7343\n",
      "-----> iter 7344\n",
      "-----> iter 7345\n",
      "-----> iter 7346\n",
      "-----> iter 7347\n",
      "-----> iter 7348\n",
      "-----> iter 7349\n",
      "-----> iter 7350\n",
      "-----> iter 7351\n",
      "-----> iter 7352\n",
      "-----> iter 7353\n",
      "-----> iter 7354\n",
      "-----> iter 7355\n",
      "-----> iter 7356\n",
      "-----> iter 7357\n",
      "-----> iter 7358\n",
      "-----> iter 7359\n",
      "-----> iter 7360\n",
      "-----> iter 7361\n",
      "-----> iter 7362\n",
      "-----> iter 7363\n",
      "-----> iter 7364\n",
      "-----> iter 7365\n",
      "-----> iter 7366\n",
      "-----> iter 7367\n",
      "-----> iter 7368\n",
      "-----> iter 7369\n",
      "-----> iter 7370\n",
      "-----> iter 7371\n",
      "-----> iter 7372\n",
      "-----> iter 7373\n",
      "-----> iter 7374\n",
      "-----> iter 7375\n",
      "-----> iter 7376\n",
      "-----> iter 7377\n",
      "-----> iter 7378\n",
      "-----> iter 7379\n",
      "-----> iter 7380\n",
      "-----> iter 7381\n",
      "-----> iter 7382\n",
      "-----> iter 7383\n",
      "-----> iter 7384\n",
      "-----> iter 7385\n",
      "-----> iter 7386\n",
      "-----> iter 7387\n",
      "-----> iter 7388\n",
      "-----> iter 7389\n",
      "-----> iter 7390\n",
      "-----> iter 7391\n",
      "-----> iter 7392\n",
      "-----> iter 7393\n",
      "-----> iter 7394\n",
      "-----> iter 7395\n",
      "-----> iter 7396\n",
      "-----> iter 7397\n",
      "-----> iter 7398\n",
      "-----> iter 7399\n",
      "-----> iter 7400\n",
      "-----> iter 7401\n",
      "-----> iter 7402\n",
      "-----> iter 7403\n",
      "-----> iter 7404\n",
      "-----> iter 7405\n",
      "-----> iter 7406\n",
      "-----> iter 7407\n",
      "-----> iter 7408\n",
      "-----> iter 7409\n",
      "-----> iter 7410\n",
      "-----> iter 7411\n",
      "-----> iter 7412\n",
      "-----> iter 7413\n",
      "-----> iter 7414\n",
      "-----> iter 7415\n",
      "-----> iter 7416\n",
      "-----> iter 7417\n",
      "-----> iter 7418\n",
      "-----> iter 7419\n",
      "-----> iter 7420\n",
      "-----> iter 7421\n",
      "-----> iter 7422\n",
      "-----> iter 7423\n",
      "-----> iter 7424\n",
      "-----> iter 7425\n",
      "-----> iter 7426\n",
      "-----> iter 7427\n",
      "-----> iter 7428\n",
      "-----> iter 7429\n",
      "-----> iter 7430\n",
      "-----> iter 7431\n",
      "-----> iter 7432\n",
      "-----> iter 7433\n",
      "-----> iter 7434\n",
      "-----> iter 7435\n",
      "-----> iter 7436\n",
      "-----> iter 7437\n",
      "-----> iter 7438\n",
      "-----> iter 7439\n",
      "-----> iter 7440\n",
      "-----> iter 7441\n",
      "-----> iter 7442\n",
      "-----> iter 7443\n",
      "-----> iter 7444\n",
      "-----> iter 7445\n",
      "-----> iter 7446\n",
      "-----> iter 7447\n",
      "-----> iter 7448\n",
      "-----> iter 7449\n",
      "-----> iter 7450\n",
      "-----> iter 7451\n",
      "-----> iter 7452\n",
      "-----> iter 7453\n",
      "-----> iter 7454\n",
      "-----> iter 7455\n",
      "-----> iter 7456\n",
      "-----> iter 7457\n",
      "-----> iter 7458\n",
      "-----> iter 7459\n",
      "-----> iter 7460\n",
      "-----> iter 7461\n",
      "-----> iter 7462\n",
      "-----> iter 7463\n",
      "-----> iter 7464\n",
      "-----> iter 7465\n",
      "-----> iter 7466\n",
      "-----> iter 7467\n",
      "-----> iter 7468\n",
      "-----> iter 7469\n",
      "-----> iter 7470\n",
      "-----> iter 7471\n",
      "-----> iter 7472\n",
      "-----> iter 7473\n",
      "-----> iter 7474\n",
      "-----> iter 7475\n",
      "-----> iter 7476\n",
      "-----> iter 7477\n",
      "-----> iter 7478\n",
      "-----> iter 7479\n",
      "-----> iter 7480\n",
      "-----> iter 7481\n",
      "-----> iter 7482\n",
      "-----> iter 7483\n",
      "-----> iter 7484\n",
      "-----> iter 7485\n",
      "-----> iter 7486\n",
      "-----> iter 7487\n",
      "-----> iter 7488\n",
      "-----> iter 7489\n",
      "-----> iter 7490\n",
      "-----> iter 7491\n",
      "-----> iter 7492\n",
      "-----> iter 7493\n",
      "-----> iter 7494\n",
      "-----> iter 7495\n",
      "-----> iter 7496\n",
      "-----> iter 7497\n",
      "-----> iter 7498\n",
      "-----> iter 7499\n",
      "-----> iter 7500\n",
      "-----> iter 7501\n",
      "-----> iter 7502\n",
      "-----> iter 7503\n",
      "-----> iter 7504\n",
      "-----> iter 7505\n",
      "-----> iter 7506\n",
      "-----> iter 7507\n",
      "-----> iter 7508\n",
      "-----> iter 7509\n",
      "-----> iter 7510\n",
      "-----> iter 7511\n",
      "-----> iter 7512\n",
      "-----> iter 7513\n",
      "-----> iter 7514\n",
      "-----> iter 7515\n",
      "-----> iter 7516\n",
      "-----> iter 7517\n",
      "-----> iter 7518\n",
      "-----> iter 7519\n",
      "-----> iter 7520\n",
      "-----> iter 7521\n",
      "-----> iter 7522\n",
      "-----> iter 7523\n",
      "-----> iter 7524\n",
      "-----> iter 7525\n",
      "-----> iter 7526\n",
      "-----> iter 7527\n",
      "-----> iter 7528\n",
      "-----> iter 7529\n",
      "-----> iter 7530\n",
      "-----> iter 7531\n",
      "-----> iter 7532\n",
      "-----> iter 7533\n",
      "-----> iter 7534\n",
      "-----> iter 7535\n",
      "-----> iter 7536\n",
      "-----> iter 7537\n",
      "-----> iter 7538\n",
      "-----> iter 7539\n",
      "-----> iter 7540\n",
      "-----> iter 7541\n",
      "-----> iter 7542\n",
      "-----> iter 7543\n",
      "-----> iter 7544\n",
      "-----> iter 7545\n",
      "-----> iter 7546\n",
      "-----> iter 7547\n",
      "-----> iter 7548\n",
      "-----> iter 7549\n",
      "-----> iter 7550\n",
      "-----> iter 7551\n",
      "-----> iter 7552\n",
      "-----> iter 7553\n",
      "-----> iter 7554\n",
      "-----> iter 7555\n",
      "-----> iter 7556\n",
      "-----> iter 7557\n",
      "-----> iter 7558\n",
      "-----> iter 7559\n",
      "-----> iter 7560\n",
      "-----> iter 7561\n",
      "-----> iter 7562\n",
      "-----> iter 7563\n",
      "-----> iter 7564\n",
      "-----> iter 7565\n",
      "-----> iter 7566\n",
      "-----> iter 7567\n",
      "-----> iter 7568\n",
      "-----> iter 7569\n",
      "-----> iter 7570\n",
      "-----> iter 7571\n",
      "-----> iter 7572\n",
      "-----> iter 7573\n",
      "-----> iter 7574\n",
      "-----> iter 7575\n",
      "-----> iter 7576\n",
      "-----> iter 7577\n",
      "-----> iter 7578\n",
      "-----> iter 7579\n",
      "-----> iter 7580\n",
      "-----> iter 7581\n",
      "-----> iter 7582\n",
      "-----> iter 7583\n",
      "-----> iter 7584\n",
      "-----> iter 7585\n",
      "-----> iter 7586\n",
      "-----> iter 7587\n",
      "-----> iter 7588\n",
      "-----> iter 7589\n",
      "-----> iter 7590\n",
      "-----> iter 7591\n",
      "-----> iter 7592\n",
      "-----> iter 7593\n",
      "-----> iter 7594\n",
      "-----> iter 7595\n",
      "-----> iter 7596\n",
      "-----> iter 7597\n",
      "-----> iter 7598\n",
      "-----> iter 7599\n",
      "-----> iter 7600\n",
      "-----> iter 7601\n",
      "-----> iter 7602\n",
      "-----> iter 7603\n",
      "-----> iter 7604\n",
      "-----> iter 7605\n",
      "-----> iter 7606\n",
      "-----> iter 7607\n",
      "-----> iter 7608\n",
      "-----> iter 7609\n",
      "-----> iter 7610\n",
      "-----> iter 7611\n",
      "-----> iter 7612\n",
      "-----> iter 7613\n",
      "-----> iter 7614\n",
      "-----> iter 7615\n",
      "-----> iter 7616\n",
      "-----> iter 7617\n",
      "-----> iter 7618\n",
      "-----> iter 7619\n",
      "-----> iter 7620\n",
      "-----> iter 7621\n",
      "-----> iter 7622\n",
      "-----> iter 7623\n",
      "-----> iter 7624\n",
      "-----> iter 7625\n",
      "-----> iter 7626\n",
      "-----> iter 7627\n",
      "-----> iter 7628\n",
      "-----> iter 7629\n",
      "-----> iter 7630\n",
      "-----> iter 7631\n",
      "-----> iter 7632\n",
      "-----> iter 7633\n",
      "-----> iter 7634\n",
      "-----> iter 7635\n",
      "-----> iter 7636\n",
      "-----> iter 7637\n",
      "-----> iter 7638\n",
      "-----> iter 7639\n",
      "-----> iter 7640\n",
      "-----> iter 7641\n",
      "-----> iter 7642\n",
      "-----> iter 7643\n",
      "-----> iter 7644\n",
      "-----> iter 7645\n",
      "-----> iter 7646\n",
      "-----> iter 7647\n",
      "-----> iter 7648\n",
      "-----> iter 7649\n",
      "-----> iter 7650\n",
      "-----> iter 7651\n",
      "-----> iter 7652\n",
      "-----> iter 7653\n",
      "-----> iter 7654\n",
      "-----> iter 7655\n",
      "-----> iter 7656\n",
      "-----> iter 7657\n",
      "-----> iter 7658\n",
      "-----> iter 7659\n",
      "-----> iter 7660\n",
      "-----> iter 7661\n",
      "-----> iter 7662\n",
      "-----> iter 7663\n",
      "-----> iter 7664\n",
      "-----> iter 7665\n",
      "-----> iter 7666\n",
      "-----> iter 7667\n",
      "-----> iter 7668\n",
      "-----> iter 7669\n",
      "-----> iter 7670\n",
      "-----> iter 7671\n",
      "-----> iter 7672\n",
      "-----> iter 7673\n",
      "-----> iter 7674\n",
      "-----> iter 7675\n",
      "-----> iter 7676\n",
      "-----> iter 7677\n",
      "-----> iter 7678\n",
      "-----> iter 7679\n",
      "-----> iter 7680\n",
      "-----> iter 7681\n",
      "-----> iter 7682\n",
      "-----> iter 7683\n",
      "-----> iter 7684\n",
      "-----> iter 7685\n",
      "-----> iter 7686\n",
      "-----> iter 7687\n",
      "-----> iter 7688\n",
      "-----> iter 7689\n",
      "-----> iter 7690\n",
      "-----> iter 7691\n",
      "-----> iter 7692\n",
      "-----> iter 7693\n",
      "-----> iter 7694\n",
      "-----> iter 7695\n",
      "-----> iter 7696\n",
      "-----> iter 7697\n",
      "-----> iter 7698\n",
      "-----> iter 7699\n",
      "-----> iter 7700\n",
      "-----> iter 7701\n",
      "-----> iter 7702\n",
      "-----> iter 7703\n",
      "-----> iter 7704\n",
      "-----> iter 7705\n",
      "-----> iter 7706\n",
      "-----> iter 7707\n",
      "-----> iter 7708\n",
      "-----> iter 7709\n",
      "-----> iter 7710\n",
      "-----> iter 7711\n",
      "-----> iter 7712\n",
      "-----> iter 7713\n",
      "-----> iter 7714\n",
      "-----> iter 7715\n",
      "-----> iter 7716\n",
      "-----> iter 7717\n",
      "-----> iter 7718\n",
      "-----> iter 7719\n",
      "-----> iter 7720\n",
      "-----> iter 7721\n",
      "-----> iter 7722\n",
      "-----> iter 7723\n",
      "-----> iter 7724\n",
      "-----> iter 7725\n",
      "-----> iter 7726\n",
      "-----> iter 7727\n",
      "-----> iter 7728\n",
      "-----> iter 7729\n",
      "-----> iter 7730\n",
      "-----> iter 7731\n",
      "-----> iter 7732\n",
      "-----> iter 7733\n",
      "-----> iter 7734\n",
      "-----> iter 7735\n",
      "-----> iter 7736\n",
      "-----> iter 7737\n",
      "-----> iter 7738\n",
      "-----> iter 7739\n",
      "-----> iter 7740\n",
      "-----> iter 7741\n",
      "-----> iter 7742\n",
      "-----> iter 7743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 7744\n",
      "-----> iter 7745\n",
      "-----> iter 7746\n",
      "-----> iter 7747\n",
      "-----> iter 7748\n",
      "-----> iter 7749\n",
      "-----> iter 7750\n",
      "-----> iter 7751\n",
      "-----> iter 7752\n",
      "-----> iter 7753\n",
      "-----> iter 7754\n",
      "-----> iter 7755\n",
      "-----> iter 7756\n",
      "-----> iter 7757\n",
      "-----> iter 7758\n",
      "-----> iter 7759\n",
      "-----> iter 7760\n",
      "-----> iter 7761\n",
      "-----> iter 7762\n",
      "-----> iter 7763\n",
      "-----> iter 7764\n",
      "-----> iter 7765\n",
      "-----> iter 7766\n",
      "-----> iter 7767\n",
      "-----> iter 7768\n",
      "-----> iter 7769\n",
      "-----> iter 7770\n",
      "-----> iter 7771\n",
      "-----> iter 7772\n",
      "-----> iter 7773\n",
      "-----> iter 7774\n",
      "-----> iter 7775\n",
      "-----> iter 7776\n",
      "-----> iter 7777\n",
      "-----> iter 7778\n",
      "-----> iter 7779\n",
      "-----> iter 7780\n",
      "-----> iter 7781\n",
      "-----> iter 7782\n",
      "-----> iter 7783\n",
      "-----> iter 7784\n",
      "-----> iter 7785\n",
      "-----> iter 7786\n",
      "-----> iter 7787\n",
      "-----> iter 7788\n",
      "-----> iter 7789\n",
      "-----> iter 7790\n",
      "-----> iter 7791\n",
      "-----> iter 7792\n",
      "-----> iter 7793\n",
      "-----> iter 7794\n",
      "-----> iter 7795\n",
      "-----> iter 7796\n",
      "-----> iter 7797\n",
      "-----> iter 7798\n",
      "-----> iter 7799\n",
      "-----> iter 7800\n",
      "-----> iter 7801\n",
      "-----> iter 7802\n",
      "-----> iter 7803\n",
      "-----> iter 7804\n",
      "-----> iter 7805\n",
      "-----> iter 7806\n",
      "-----> iter 7807\n",
      "-----> iter 7808\n",
      "-----> iter 7809\n",
      "-----> iter 7810\n",
      "-----> iter 7811\n",
      "-----> iter 7812\n",
      "-----> iter 7813\n",
      "-----> iter 7814\n",
      "-----> iter 7815\n",
      "-----> iter 7816\n",
      "-----> iter 7817\n",
      "-----> iter 7818\n",
      "-----> iter 7819\n",
      "-----> iter 7820\n",
      "-----> iter 7821\n",
      "-----> iter 7822\n",
      "-----> iter 7823\n",
      "-----> iter 7824\n",
      "-----> iter 7825\n",
      "-----> iter 7826\n",
      "-----> iter 7827\n",
      "-----> iter 7828\n",
      "-----> iter 7829\n",
      "-----> iter 7830\n",
      "-----> iter 7831\n",
      "-----> iter 7832\n",
      "-----> iter 7833\n",
      "-----> iter 7834\n",
      "-----> iter 7835\n",
      "-----> iter 7836\n",
      "-----> iter 7837\n",
      "-----> iter 7838\n",
      "-----> iter 7839\n",
      "-----> iter 7840\n",
      "-----> iter 7841\n",
      "-----> iter 7842\n",
      "-----> iter 7843\n",
      "-----> iter 7844\n",
      "-----> iter 7845\n",
      "-----> iter 7846\n",
      "-----> iter 7847\n",
      "-----> iter 7848\n",
      "-----> iter 7849\n",
      "-----> iter 7850\n",
      "-----> iter 7851\n",
      "-----> iter 7852\n",
      "-----> iter 7853\n",
      "-----> iter 7854\n",
      "-----> iter 7855\n",
      "-----> iter 7856\n",
      "-----> iter 7857\n",
      "-----> iter 7858\n",
      "-----> iter 7859\n",
      "-----> iter 7860\n",
      "-----> iter 7861\n",
      "-----> iter 7862\n",
      "-----> iter 7863\n",
      "-----> iter 7864\n",
      "-----> iter 7865\n",
      "-----> iter 7866\n",
      "-----> iter 7867\n",
      "-----> iter 7868\n",
      "-----> iter 7869\n",
      "-----> iter 7870\n",
      "-----> iter 7871\n",
      "-----> iter 7872\n",
      "-----> iter 7873\n",
      "-----> iter 7874\n",
      "-----> iter 7875\n",
      "-----> iter 7876\n",
      "-----> iter 7877\n",
      "-----> iter 7878\n",
      "-----> iter 7879\n",
      "-----> iter 7880\n",
      "-----> iter 7881\n",
      "-----> iter 7882\n",
      "-----> iter 7883\n",
      "-----> iter 7884\n",
      "-----> iter 7885\n",
      "-----> iter 7886\n",
      "-----> iter 7887\n",
      "-----> iter 7888\n",
      "-----> iter 7889\n",
      "-----> iter 7890\n",
      "-----> iter 7891\n",
      "-----> iter 7892\n",
      "-----> iter 7893\n",
      "-----> iter 7894\n",
      "-----> iter 7895\n",
      "-----> iter 7896\n",
      "-----> iter 7897\n",
      "-----> iter 7898\n",
      "-----> iter 7899\n",
      "-----> iter 7900\n",
      "-----> iter 7901\n",
      "-----> iter 7902\n",
      "-----> iter 7903\n",
      "-----> iter 7904\n",
      "-----> iter 7905\n",
      "-----> iter 7906\n",
      "-----> iter 7907\n",
      "-----> iter 7908\n",
      "-----> iter 7909\n",
      "-----> iter 7910\n",
      "-----> iter 7911\n",
      "-----> iter 7912\n",
      "-----> iter 7913\n",
      "-----> iter 7914\n",
      "-----> iter 7915\n",
      "-----> iter 7916\n",
      "-----> iter 7917\n",
      "-----> iter 7918\n",
      "-----> iter 7919\n",
      "-----> iter 7920\n",
      "-----> iter 7921\n",
      "-----> iter 7922\n",
      "-----> iter 7923\n",
      "-----> iter 7924\n",
      "-----> iter 7925\n",
      "-----> iter 7926\n",
      "-----> iter 7927\n",
      "-----> iter 7928\n",
      "-----> iter 7929\n",
      "-----> iter 7930\n",
      "-----> iter 7931\n",
      "-----> iter 7932\n",
      "-----> iter 7933\n",
      "-----> iter 7934\n",
      "-----> iter 7935\n",
      "-----> iter 7936\n",
      "-----> iter 7937\n",
      "-----> iter 7938\n",
      "-----> iter 7939\n",
      "-----> iter 7940\n",
      "-----> iter 7941\n",
      "-----> iter 7942\n",
      "-----> iter 7943\n",
      "-----> iter 7944\n",
      "-----> iter 7945\n",
      "-----> iter 7946\n",
      "-----> iter 7947\n",
      "-----> iter 7948\n",
      "-----> iter 7949\n",
      "-----> iter 7950\n",
      "-----> iter 7951\n",
      "-----> iter 7952\n",
      "-----> iter 7953\n",
      "-----> iter 7954\n",
      "-----> iter 7955\n",
      "-----> iter 7956\n",
      "-----> iter 7957\n",
      "-----> iter 7958\n",
      "-----> iter 7959\n",
      "-----> iter 7960\n",
      "-----> iter 7961\n",
      "-----> iter 7962\n",
      "-----> iter 7963\n",
      "-----> iter 7964\n",
      "-----> iter 7965\n",
      "-----> iter 7966\n",
      "-----> iter 7967\n",
      "-----> iter 7968\n",
      "-----> iter 7969\n",
      "-----> iter 7970\n",
      "-----> iter 7971\n",
      "-----> iter 7972\n",
      "-----> iter 7973\n",
      "-----> iter 7974\n",
      "-----> iter 7975\n",
      "-----> iter 7976\n",
      "-----> iter 7977\n",
      "-----> iter 7978\n",
      "-----> iter 7979\n",
      "-----> iter 7980\n",
      "-----> iter 7981\n",
      "-----> iter 7982\n",
      "-----> iter 7983\n",
      "-----> iter 7984\n",
      "-----> iter 7985\n",
      "-----> iter 7986\n",
      "-----> iter 7987\n",
      "-----> iter 7988\n",
      "-----> iter 7989\n",
      "-----> iter 7990\n",
      "-----> iter 7991\n",
      "-----> iter 7992\n",
      "-----> iter 7993\n",
      "-----> iter 7994\n",
      "-----> iter 7995\n",
      "-----> iter 7996\n",
      "-----> iter 7997\n",
      "-----> iter 7998\n",
      "-----> iter 7999\n",
      "-----> iter 8000\n",
      "-----> iter 8001\n",
      "-----> iter 8002\n",
      "-----> iter 8003\n",
      "-----> iter 8004\n",
      "-----> iter 8005\n",
      "-----> iter 8006\n",
      "-----> iter 8007\n",
      "-----> iter 8008\n",
      "-----> iter 8009\n",
      "-----> iter 8010\n",
      "-----> iter 8011\n",
      "-----> iter 8012\n",
      "-----> iter 8013\n",
      "-----> iter 8014\n",
      "-----> iter 8015\n",
      "-----> iter 8016\n",
      "-----> iter 8017\n",
      "-----> iter 8018\n",
      "-----> iter 8019\n",
      "-----> iter 8020\n",
      "-----> iter 8021\n",
      "-----> iter 8022\n",
      "-----> iter 8023\n",
      "-----> iter 8024\n",
      "-----> iter 8025\n",
      "-----> iter 8026\n",
      "-----> iter 8027\n",
      "-----> iter 8028\n",
      "-----> iter 8029\n",
      "-----> iter 8030\n",
      "-----> iter 8031\n",
      "-----> iter 8032\n",
      "-----> iter 8033\n",
      "-----> iter 8034\n",
      "-----> iter 8035\n",
      "-----> iter 8036\n",
      "-----> iter 8037\n",
      "-----> iter 8038\n",
      "-----> iter 8039\n",
      "-----> iter 8040\n",
      "-----> iter 8041\n",
      "-----> iter 8042\n",
      "-----> iter 8043\n",
      "-----> iter 8044\n",
      "-----> iter 8045\n",
      "-----> iter 8046\n",
      "-----> iter 8047\n",
      "-----> iter 8048\n",
      "-----> iter 8049\n",
      "-----> iter 8050\n",
      "-----> iter 8051\n",
      "-----> iter 8052\n",
      "-----> iter 8053\n",
      "-----> iter 8054\n",
      "-----> iter 8055\n",
      "-----> iter 8056\n",
      "-----> iter 8057\n",
      "-----> iter 8058\n",
      "-----> iter 8059\n",
      "-----> iter 8060\n",
      "-----> iter 8061\n",
      "-----> iter 8062\n",
      "-----> iter 8063\n",
      "-----> iter 8064\n",
      "-----> iter 8065\n",
      "-----> iter 8066\n",
      "-----> iter 8067\n",
      "-----> iter 8068\n",
      "-----> iter 8069\n",
      "-----> iter 8070\n",
      "-----> iter 8071\n",
      "-----> iter 8072\n",
      "-----> iter 8073\n",
      "-----> iter 8074\n",
      "-----> iter 8075\n",
      "-----> iter 8076\n",
      "-----> iter 8077\n",
      "-----> iter 8078\n",
      "-----> iter 8079\n",
      "-----> iter 8080\n",
      "-----> iter 8081\n",
      "-----> iter 8082\n",
      "-----> iter 8083\n",
      "-----> iter 8084\n",
      "-----> iter 8085\n",
      "-----> iter 8086\n",
      "-----> iter 8087\n",
      "-----> iter 8088\n",
      "-----> iter 8089\n",
      "-----> iter 8090\n",
      "-----> iter 8091\n",
      "-----> iter 8092\n",
      "-----> iter 8093\n",
      "-----> iter 8094\n",
      "-----> iter 8095\n",
      "-----> iter 8096\n",
      "-----> iter 8097\n",
      "-----> iter 8098\n",
      "-----> iter 8099\n",
      "-----> iter 8100\n",
      "-----> iter 8101\n",
      "-----> iter 8102\n",
      "-----> iter 8103\n",
      "-----> iter 8104\n",
      "-----> iter 8105\n",
      "-----> iter 8106\n",
      "-----> iter 8107\n",
      "-----> iter 8108\n",
      "-----> iter 8109\n",
      "-----> iter 8110\n",
      "-----> iter 8111\n",
      "-----> iter 8112\n",
      "-----> iter 8113\n",
      "-----> iter 8114\n",
      "-----> iter 8115\n",
      "-----> iter 8116\n",
      "-----> iter 8117\n",
      "-----> iter 8118\n",
      "-----> iter 8119\n",
      "-----> iter 8120\n",
      "-----> iter 8121\n",
      "-----> iter 8122\n",
      "-----> iter 8123\n",
      "-----> iter 8124\n",
      "-----> iter 8125\n",
      "-----> iter 8126\n",
      "-----> iter 8127\n",
      "-----> iter 8128\n",
      "-----> iter 8129\n",
      "-----> iter 8130\n",
      "-----> iter 8131\n",
      "-----> iter 8132\n",
      "-----> iter 8133\n",
      "-----> iter 8134\n",
      "-----> iter 8135\n",
      "-----> iter 8136\n",
      "-----> iter 8137\n",
      "-----> iter 8138\n",
      "-----> iter 8139\n",
      "-----> iter 8140\n",
      "-----> iter 8141\n",
      "-----> iter 8142\n",
      "-----> iter 8143\n",
      "-----> iter 8144\n",
      "-----> iter 8145\n",
      "-----> iter 8146\n",
      "-----> iter 8147\n",
      "-----> iter 8148\n",
      "-----> iter 8149\n",
      "-----> iter 8150\n",
      "-----> iter 8151\n",
      "-----> iter 8152\n",
      "-----> iter 8153\n",
      "-----> iter 8154\n",
      "-----> iter 8155\n",
      "-----> iter 8156\n",
      "-----> iter 8157\n",
      "-----> iter 8158\n",
      "-----> iter 8159\n",
      "-----> iter 8160\n",
      "-----> iter 8161\n",
      "-----> iter 8162\n",
      "-----> iter 8163\n",
      "-----> iter 8164\n",
      "-----> iter 8165\n",
      "-----> iter 8166\n",
      "-----> iter 8167\n",
      "-----> iter 8168\n",
      "-----> iter 8169\n",
      "-----> iter 8170\n",
      "-----> iter 8171\n",
      "-----> iter 8172\n",
      "-----> iter 8173\n",
      "-----> iter 8174\n",
      "-----> iter 8175\n",
      "-----> iter 8176\n",
      "-----> iter 8177\n",
      "-----> iter 8178\n",
      "-----> iter 8179\n",
      "-----> iter 8180\n",
      "-----> iter 8181\n",
      "-----> iter 8182\n",
      "-----> iter 8183\n",
      "-----> iter 8184\n",
      "-----> iter 8185\n",
      "-----> iter 8186\n",
      "-----> iter 8187\n",
      "-----> iter 8188\n",
      "-----> iter 8189\n",
      "-----> iter 8190\n",
      "-----> iter 8191\n",
      "-----> iter 8192\n",
      "-----> iter 8193\n",
      "-----> iter 8194\n",
      "-----> iter 8195\n",
      "-----> iter 8196\n",
      "-----> iter 8197\n",
      "-----> iter 8198\n",
      "-----> iter 8199\n",
      "-----> iter 8200\n",
      "-----> iter 8201\n",
      "-----> iter 8202\n",
      "-----> iter 8203\n",
      "-----> iter 8204\n",
      "-----> iter 8205\n",
      "-----> iter 8206\n",
      "-----> iter 8207\n",
      "-----> iter 8208\n",
      "-----> iter 8209\n",
      "-----> iter 8210\n",
      "-----> iter 8211\n",
      "-----> iter 8212\n",
      "-----> iter 8213\n",
      "-----> iter 8214\n",
      "-----> iter 8215\n",
      "-----> iter 8216\n",
      "-----> iter 8217\n",
      "-----> iter 8218\n",
      "-----> iter 8219\n",
      "-----> iter 8220\n",
      "-----> iter 8221\n",
      "-----> iter 8222\n",
      "-----> iter 8223\n",
      "-----> iter 8224\n",
      "-----> iter 8225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 8226\n",
      "-----> iter 8227\n",
      "-----> iter 8228\n",
      "-----> iter 8229\n",
      "-----> iter 8230\n",
      "-----> iter 8231\n",
      "-----> iter 8232\n",
      "-----> iter 8233\n",
      "-----> iter 8234\n",
      "-----> iter 8235\n",
      "-----> iter 8236\n",
      "-----> iter 8237\n",
      "-----> iter 8238\n",
      "-----> iter 8239\n",
      "-----> iter 8240\n",
      "-----> iter 8241\n",
      "-----> iter 8242\n",
      "-----> iter 8243\n",
      "-----> iter 8244\n",
      "-----> iter 8245\n",
      "-----> iter 8246\n",
      "-----> iter 8247\n",
      "-----> iter 8248\n",
      "-----> iter 8249\n",
      "-----> iter 8250\n",
      "-----> iter 8251\n",
      "-----> iter 8252\n",
      "-----> iter 8253\n",
      "-----> iter 8254\n",
      "-----> iter 8255\n",
      "-----> iter 8256\n",
      "-----> iter 8257\n",
      "-----> iter 8258\n",
      "-----> iter 8259\n",
      "-----> iter 8260\n",
      "-----> iter 8261\n",
      "-----> iter 8262\n",
      "-----> iter 8263\n",
      "-----> iter 8264\n",
      "-----> iter 8265\n",
      "-----> iter 8266\n",
      "-----> iter 8267\n",
      "-----> iter 8268\n",
      "-----> iter 8269\n",
      "-----> iter 8270\n",
      "-----> iter 8271\n",
      "-----> iter 8272\n",
      "-----> iter 8273\n",
      "-----> iter 8274\n",
      "-----> iter 8275\n",
      "-----> iter 8276\n",
      "-----> iter 8277\n",
      "-----> iter 8278\n",
      "-----> iter 8279\n",
      "-----> iter 8280\n",
      "-----> iter 8281\n",
      "-----> iter 8282\n",
      "-----> iter 8283\n",
      "-----> iter 8284\n",
      "-----> iter 8285\n",
      "-----> iter 8286\n",
      "-----> iter 8287\n",
      "-----> iter 8288\n",
      "-----> iter 8289\n",
      "-----> iter 8290\n",
      "-----> iter 8291\n",
      "-----> iter 8292\n",
      "-----> iter 8293\n",
      "-----> iter 8294\n",
      "-----> iter 8295\n",
      "-----> iter 8296\n",
      "-----> iter 8297\n",
      "-----> iter 8298\n",
      "-----> iter 8299\n",
      "-----> iter 8300\n",
      "-----> iter 8301\n",
      "-----> iter 8302\n",
      "-----> iter 8303\n",
      "-----> iter 8304\n",
      "-----> iter 8305\n",
      "-----> iter 8306\n",
      "-----> iter 8307\n",
      "-----> iter 8308\n",
      "-----> iter 8309\n",
      "-----> iter 8310\n",
      "-----> iter 8311\n",
      "-----> iter 8312\n",
      "-----> iter 8313\n",
      "-----> iter 8314\n",
      "-----> iter 8315\n",
      "-----> iter 8316\n",
      "-----> iter 8317\n",
      "-----> iter 8318\n",
      "-----> iter 8319\n",
      "-----> iter 8320\n",
      "-----> iter 8321\n",
      "-----> iter 8322\n",
      "-----> iter 8323\n",
      "-----> iter 8324\n",
      "-----> iter 8325\n",
      "-----> iter 8326\n",
      "-----> iter 8327\n",
      "-----> iter 8328\n",
      "-----> iter 8329\n",
      "-----> iter 8330\n",
      "-----> iter 8331\n",
      "-----> iter 8332\n",
      "-----> iter 8333\n",
      "-----> iter 8334\n",
      "-----> iter 8335\n",
      "-----> iter 8336\n",
      "-----> iter 8337\n",
      "-----> iter 8338\n",
      "-----> iter 8339\n",
      "-----> iter 8340\n",
      "-----> iter 8341\n",
      "-----> iter 8342\n",
      "-----> iter 8343\n",
      "-----> iter 8344\n",
      "-----> iter 8345\n",
      "-----> iter 8346\n",
      "-----> iter 8347\n",
      "-----> iter 8348\n",
      "-----> iter 8349\n",
      "-----> iter 8350\n",
      "-----> iter 8351\n",
      "-----> iter 8352\n",
      "-----> iter 8353\n",
      "-----> iter 8354\n",
      "-----> iter 8355\n",
      "-----> iter 8356\n",
      "-----> iter 8357\n",
      "-----> iter 8358\n",
      "-----> iter 8359\n",
      "-----> iter 8360\n",
      "-----> iter 8361\n",
      "-----> iter 8362\n",
      "-----> iter 8363\n",
      "-----> iter 8364\n",
      "-----> iter 8365\n",
      "-----> iter 8366\n",
      "-----> iter 8367\n",
      "-----> iter 8368\n",
      "-----> iter 8369\n",
      "-----> iter 8370\n",
      "-----> iter 8371\n",
      "-----> iter 8372\n",
      "-----> iter 8373\n",
      "-----> iter 8374\n",
      "-----> iter 8375\n",
      "-----> iter 8376\n",
      "-----> iter 8377\n",
      "-----> iter 8378\n",
      "-----> iter 8379\n",
      "-----> iter 8380\n",
      "-----> iter 8381\n",
      "-----> iter 8382\n",
      "-----> iter 8383\n",
      "-----> iter 8384\n",
      "-----> iter 8385\n",
      "-----> iter 8386\n",
      "-----> iter 8387\n",
      "-----> iter 8388\n",
      "-----> iter 8389\n",
      "-----> iter 8390\n",
      "-----> iter 8391\n",
      "-----> iter 8392\n",
      "-----> iter 8393\n",
      "-----> iter 8394\n",
      "-----> iter 8395\n",
      "-----> iter 8396\n",
      "-----> iter 8397\n",
      "-----> iter 8398\n",
      "-----> iter 8399\n",
      "-----> iter 8400\n",
      "-----> iter 8401\n",
      "-----> iter 8402\n",
      "-----> iter 8403\n",
      "-----> iter 8404\n",
      "-----> iter 8405\n",
      "-----> iter 8406\n",
      "-----> iter 8407\n",
      "-----> iter 8408\n",
      "-----> iter 8409\n",
      "-----> iter 8410\n",
      "-----> iter 8411\n",
      "-----> iter 8412\n",
      "-----> iter 8413\n",
      "-----> iter 8414\n",
      "-----> iter 8415\n",
      "-----> iter 8416\n",
      "-----> iter 8417\n",
      "-----> iter 8418\n",
      "-----> iter 8419\n",
      "-----> iter 8420\n",
      "-----> iter 8421\n",
      "-----> iter 8422\n",
      "-----> iter 8423\n",
      "-----> iter 8424\n",
      "-----> iter 8425\n",
      "-----> iter 8426\n",
      "-----> iter 8427\n",
      "-----> iter 8428\n",
      "-----> iter 8429\n",
      "-----> iter 8430\n",
      "-----> iter 8431\n",
      "-----> iter 8432\n",
      "-----> iter 8433\n",
      "-----> iter 8434\n",
      "-----> iter 8435\n",
      "-----> iter 8436\n",
      "-----> iter 8437\n",
      "-----> iter 8438\n",
      "-----> iter 8439\n",
      "-----> iter 8440\n",
      "-----> iter 8441\n",
      "-----> iter 8442\n",
      "-----> iter 8443\n",
      "-----> iter 8444\n",
      "-----> iter 8445\n",
      "-----> iter 8446\n",
      "-----> iter 8447\n",
      "-----> iter 8448\n",
      "-----> iter 8449\n",
      "-----> iter 8450\n",
      "-----> iter 8451\n",
      "-----> iter 8452\n",
      "-----> iter 8453\n",
      "-----> iter 8454\n",
      "-----> iter 8455\n",
      "-----> iter 8456\n",
      "-----> iter 8457\n",
      "-----> iter 8458\n",
      "-----> iter 8459\n",
      "-----> iter 8460\n",
      "-----> iter 8461\n",
      "-----> iter 8462\n",
      "-----> iter 8463\n",
      "-----> iter 8464\n",
      "-----> iter 8465\n",
      "-----> iter 8466\n",
      "-----> iter 8467\n",
      "-----> iter 8468\n",
      "-----> iter 8469\n",
      "-----> iter 8470\n",
      "-----> iter 8471\n",
      "-----> iter 8472\n",
      "-----> iter 8473\n",
      "-----> iter 8474\n",
      "-----> iter 8475\n",
      "-----> iter 8476\n",
      "-----> iter 8477\n",
      "-----> iter 8478\n",
      "-----> iter 8479\n",
      "-----> iter 8480\n",
      "-----> iter 8481\n",
      "-----> iter 8482\n",
      "-----> iter 8483\n",
      "-----> iter 8484\n",
      "-----> iter 8485\n",
      "-----> iter 8486\n",
      "-----> iter 8487\n",
      "-----> iter 8488\n",
      "-----> iter 8489\n",
      "-----> iter 8490\n",
      "-----> iter 8491\n",
      "-----> iter 8492\n",
      "-----> iter 8493\n",
      "-----> iter 8494\n",
      "-----> iter 8495\n",
      "-----> iter 8496\n",
      "-----> iter 8497\n",
      "-----> iter 8498\n",
      "-----> iter 8499\n",
      "-----> iter 8500\n",
      "-----> iter 8501\n",
      "-----> iter 8502\n",
      "-----> iter 8503\n",
      "-----> iter 8504\n",
      "-----> iter 8505\n",
      "-----> iter 8506\n",
      "-----> iter 8507\n",
      "-----> iter 8508\n",
      "-----> iter 8509\n",
      "-----> iter 8510\n",
      "-----> iter 8511\n",
      "-----> iter 8512\n",
      "-----> iter 8513\n",
      "-----> iter 8514\n",
      "-----> iter 8515\n",
      "-----> iter 8516\n",
      "-----> iter 8517\n",
      "-----> iter 8518\n",
      "-----> iter 8519\n",
      "-----> iter 8520\n",
      "-----> iter 8521\n",
      "-----> iter 8522\n",
      "-----> iter 8523\n",
      "-----> iter 8524\n",
      "-----> iter 8525\n",
      "-----> iter 8526\n",
      "-----> iter 8527\n",
      "-----> iter 8528\n",
      "-----> iter 8529\n",
      "-----> iter 8530\n",
      "-----> iter 8531\n",
      "-----> iter 8532\n",
      "-----> iter 8533\n",
      "-----> iter 8534\n",
      "-----> iter 8535\n",
      "-----> iter 8536\n",
      "-----> iter 8537\n",
      "-----> iter 8538\n",
      "-----> iter 8539\n",
      "-----> iter 8540\n",
      "-----> iter 8541\n",
      "-----> iter 8542\n",
      "-----> iter 8543\n",
      "-----> iter 8544\n",
      "-----> iter 8545\n",
      "-----> iter 8546\n",
      "-----> iter 8547\n",
      "-----> iter 8548\n",
      "-----> iter 8549\n",
      "-----> iter 8550\n",
      "-----> iter 8551\n",
      "-----> iter 8552\n",
      "-----> iter 8553\n",
      "-----> iter 8554\n",
      "-----> iter 8555\n",
      "-----> iter 8556\n",
      "-----> iter 8557\n",
      "-----> iter 8558\n",
      "-----> iter 8559\n",
      "-----> iter 8560\n",
      "-----> iter 8561\n",
      "-----> iter 8562\n",
      "-----> iter 8563\n",
      "-----> iter 8564\n",
      "-----> iter 8565\n",
      "-----> iter 8566\n",
      "-----> iter 8567\n",
      "-----> iter 8568\n",
      "-----> iter 8569\n",
      "-----> iter 8570\n",
      "-----> iter 8571\n",
      "-----> iter 8572\n",
      "-----> iter 8573\n",
      "-----> iter 8574\n",
      "-----> iter 8575\n",
      "-----> iter 8576\n",
      "-----> iter 8577\n",
      "-----> iter 8578\n",
      "-----> iter 8579\n",
      "-----> iter 8580\n",
      "-----> iter 8581\n",
      "-----> iter 8582\n",
      "-----> iter 8583\n",
      "-----> iter 8584\n",
      "-----> iter 8585\n",
      "-----> iter 8586\n",
      "-----> iter 8587\n",
      "-----> iter 8588\n",
      "-----> iter 8589\n",
      "-----> iter 8590\n",
      "-----> iter 8591\n",
      "-----> iter 8592\n",
      "-----> iter 8593\n",
      "-----> iter 8594\n",
      "-----> iter 8595\n",
      "-----> iter 8596\n",
      "-----> iter 8597\n",
      "-----> iter 8598\n",
      "-----> iter 8599\n",
      "-----> iter 8600\n",
      "-----> iter 8601\n",
      "-----> iter 8602\n",
      "-----> iter 8603\n",
      "-----> iter 8604\n",
      "-----> iter 8605\n",
      "-----> iter 8606\n",
      "-----> iter 8607\n",
      "-----> iter 8608\n",
      "-----> iter 8609\n",
      "-----> iter 8610\n",
      "-----> iter 8611\n",
      "-----> iter 8612\n",
      "-----> iter 8613\n",
      "-----> iter 8614\n",
      "-----> iter 8615\n",
      "-----> iter 8616\n",
      "-----> iter 8617\n",
      "-----> iter 8618\n",
      "-----> iter 8619\n",
      "-----> iter 8620\n",
      "-----> iter 8621\n",
      "-----> iter 8622\n",
      "-----> iter 8623\n",
      "-----> iter 8624\n",
      "-----> iter 8625\n",
      "-----> iter 8626\n",
      "-----> iter 8627\n",
      "-----> iter 8628\n",
      "-----> iter 8629\n",
      "-----> iter 8630\n",
      "-----> iter 8631\n",
      "-----> iter 8632\n",
      "-----> iter 8633\n",
      "-----> iter 8634\n",
      "-----> iter 8635\n",
      "-----> iter 8636\n",
      "-----> iter 8637\n",
      "-----> iter 8638\n",
      "-----> iter 8639\n",
      "-----> iter 8640\n",
      "-----> iter 8641\n",
      "-----> iter 8642\n",
      "-----> iter 8643\n",
      "-----> iter 8644\n",
      "-----> iter 8645\n",
      "-----> iter 8646\n",
      "-----> iter 8647\n",
      "-----> iter 8648\n",
      "-----> iter 8649\n",
      "-----> iter 8650\n",
      "-----> iter 8651\n",
      "-----> iter 8652\n",
      "-----> iter 8653\n",
      "-----> iter 8654\n",
      "-----> iter 8655\n",
      "-----> iter 8656\n",
      "-----> iter 8657\n",
      "-----> iter 8658\n",
      "-----> iter 8659\n",
      "-----> iter 8660\n",
      "-----> iter 8661\n",
      "-----> iter 8662\n",
      "-----> iter 8663\n",
      "-----> iter 8664\n",
      "-----> iter 8665\n",
      "-----> iter 8666\n",
      "-----> iter 8667\n",
      "-----> iter 8668\n",
      "-----> iter 8669\n",
      "-----> iter 8670\n",
      "-----> iter 8671\n",
      "-----> iter 8672\n",
      "-----> iter 8673\n",
      "-----> iter 8674\n",
      "-----> iter 8675\n",
      "-----> iter 8676\n",
      "-----> iter 8677\n",
      "-----> iter 8678\n",
      "-----> iter 8679\n",
      "-----> iter 8680\n",
      "-----> iter 8681\n",
      "-----> iter 8682\n",
      "-----> iter 8683\n",
      "-----> iter 8684\n",
      "-----> iter 8685\n",
      "-----> iter 8686\n",
      "-----> iter 8687\n",
      "-----> iter 8688\n",
      "-----> iter 8689\n",
      "-----> iter 8690\n",
      "-----> iter 8691\n",
      "-----> iter 8692\n",
      "-----> iter 8693\n",
      "-----> iter 8694\n",
      "-----> iter 8695\n",
      "-----> iter 8696\n",
      "-----> iter 8697\n",
      "-----> iter 8698\n",
      "-----> iter 8699\n",
      "-----> iter 8700\n",
      "-----> iter 8701\n",
      "-----> iter 8702\n",
      "-----> iter 8703\n",
      "-----> iter 8704\n",
      "-----> iter 8705\n",
      "-----> iter 8706\n",
      "-----> iter 8707\n",
      "-----> iter 8708\n",
      "-----> iter 8709\n",
      "-----> iter 8710\n",
      "-----> iter 8711\n",
      "-----> iter 8712\n",
      "-----> iter 8713\n",
      "-----> iter 8714\n",
      "-----> iter 8715\n",
      "-----> iter 8716\n",
      "-----> iter 8717\n",
      "-----> iter 8718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 8719\n",
      "-----> iter 8720\n",
      "-----> iter 8721\n",
      "-----> iter 8722\n",
      "-----> iter 8723\n",
      "-----> iter 8724\n",
      "-----> iter 8725\n",
      "-----> iter 8726\n",
      "-----> iter 8727\n",
      "-----> iter 8728\n",
      "-----> iter 8729\n",
      "-----> iter 8730\n",
      "-----> iter 8731\n",
      "-----> iter 8732\n",
      "-----> iter 8733\n",
      "-----> iter 8734\n",
      "-----> iter 8735\n",
      "-----> iter 8736\n",
      "-----> iter 8737\n",
      "-----> iter 8738\n",
      "-----> iter 8739\n",
      "-----> iter 8740\n",
      "-----> iter 8741\n",
      "-----> iter 8742\n",
      "-----> iter 8743\n",
      "-----> iter 8744\n",
      "-----> iter 8745\n",
      "-----> iter 8746\n",
      "-----> iter 8747\n",
      "-----> iter 8748\n",
      "-----> iter 8749\n",
      "-----> iter 8750\n",
      "-----> iter 8751\n",
      "-----> iter 8752\n",
      "-----> iter 8753\n",
      "-----> iter 8754\n",
      "-----> iter 8755\n",
      "-----> iter 8756\n",
      "-----> iter 8757\n",
      "-----> iter 8758\n",
      "-----> iter 8759\n",
      "-----> iter 8760\n",
      "-----> iter 8761\n",
      "-----> iter 8762\n",
      "-----> iter 8763\n",
      "-----> iter 8764\n",
      "-----> iter 8765\n",
      "-----> iter 8766\n",
      "-----> iter 8767\n",
      "-----> iter 8768\n",
      "-----> iter 8769\n",
      "-----> iter 8770\n",
      "-----> iter 8771\n",
      "-----> iter 8772\n",
      "-----> iter 8773\n",
      "-----> iter 8774\n",
      "-----> iter 8775\n",
      "-----> iter 8776\n",
      "-----> iter 8777\n",
      "-----> iter 8778\n",
      "-----> iter 8779\n",
      "-----> iter 8780\n",
      "-----> iter 8781\n",
      "-----> iter 8782\n",
      "-----> iter 8783\n",
      "-----> iter 8784\n",
      "-----> iter 8785\n",
      "-----> iter 8786\n",
      "-----> iter 8787\n",
      "-----> iter 8788\n",
      "-----> iter 8789\n",
      "-----> iter 8790\n",
      "-----> iter 8791\n",
      "-----> iter 8792\n",
      "-----> iter 8793\n",
      "-----> iter 8794\n",
      "-----> iter 8795\n",
      "-----> iter 8796\n",
      "-----> iter 8797\n",
      "-----> iter 8798\n",
      "-----> iter 8799\n",
      "-----> iter 8800\n",
      "-----> iter 8801\n",
      "-----> iter 8802\n",
      "-----> iter 8803\n",
      "-----> iter 8804\n",
      "-----> iter 8805\n",
      "-----> iter 8806\n",
      "-----> iter 8807\n",
      "-----> iter 8808\n",
      "-----> iter 8809\n",
      "-----> iter 8810\n",
      "-----> iter 8811\n",
      "-----> iter 8812\n",
      "-----> iter 8813\n",
      "-----> iter 8814\n",
      "-----> iter 8815\n",
      "-----> iter 8816\n",
      "-----> iter 8817\n",
      "-----> iter 8818\n",
      "-----> iter 8819\n",
      "-----> iter 8820\n",
      "-----> iter 8821\n",
      "-----> iter 8822\n",
      "-----> iter 8823\n",
      "-----> iter 8824\n",
      "-----> iter 8825\n",
      "-----> iter 8826\n",
      "-----> iter 8827\n",
      "-----> iter 8828\n",
      "-----> iter 8829\n",
      "-----> iter 8830\n",
      "-----> iter 8831\n",
      "-----> iter 8832\n",
      "-----> iter 8833\n",
      "-----> iter 8834\n",
      "-----> iter 8835\n",
      "-----> iter 8836\n",
      "-----> iter 8837\n",
      "-----> iter 8838\n",
      "-----> iter 8839\n",
      "-----> iter 8840\n",
      "-----> iter 8841\n",
      "-----> iter 8842\n",
      "-----> iter 8843\n",
      "-----> iter 8844\n",
      "-----> iter 8845\n",
      "-----> iter 8846\n",
      "-----> iter 8847\n",
      "-----> iter 8848\n",
      "-----> iter 8849\n",
      "-----> iter 8850\n",
      "-----> iter 8851\n",
      "-----> iter 8852\n",
      "-----> iter 8853\n",
      "-----> iter 8854\n",
      "-----> iter 8855\n",
      "-----> iter 8856\n",
      "-----> iter 8857\n",
      "-----> iter 8858\n",
      "-----> iter 8859\n",
      "-----> iter 8860\n",
      "-----> iter 8861\n",
      "-----> iter 8862\n",
      "-----> iter 8863\n",
      "-----> iter 8864\n",
      "-----> iter 8865\n",
      "-----> iter 8866\n",
      "-----> iter 8867\n",
      "-----> iter 8868\n",
      "-----> iter 8869\n",
      "-----> iter 8870\n",
      "-----> iter 8871\n",
      "-----> iter 8872\n",
      "-----> iter 8873\n",
      "-----> iter 8874\n",
      "-----> iter 8875\n",
      "-----> iter 8876\n",
      "-----> iter 8877\n",
      "-----> iter 8878\n",
      "-----> iter 8879\n",
      "-----> iter 8880\n",
      "-----> iter 8881\n",
      "-----> iter 8882\n",
      "-----> iter 8883\n",
      "-----> iter 8884\n",
      "-----> iter 8885\n",
      "-----> iter 8886\n",
      "-----> iter 8887\n",
      "-----> iter 8888\n",
      "-----> iter 8889\n",
      "-----> iter 8890\n",
      "-----> iter 8891\n",
      "-----> iter 8892\n",
      "-----> iter 8893\n",
      "-----> iter 8894\n",
      "-----> iter 8895\n",
      "-----> iter 8896\n",
      "-----> iter 8897\n",
      "-----> iter 8898\n",
      "-----> iter 8899\n",
      "-----> iter 8900\n",
      "-----> iter 8901\n",
      "-----> iter 8902\n",
      "-----> iter 8903\n",
      "-----> iter 8904\n",
      "-----> iter 8905\n",
      "-----> iter 8906\n",
      "-----> iter 8907\n",
      "-----> iter 8908\n",
      "-----> iter 8909\n",
      "-----> iter 8910\n",
      "-----> iter 8911\n",
      "-----> iter 8912\n",
      "-----> iter 8913\n",
      "-----> iter 8914\n",
      "-----> iter 8915\n",
      "-----> iter 8916\n",
      "-----> iter 8917\n",
      "-----> iter 8918\n",
      "-----> iter 8919\n",
      "-----> iter 8920\n",
      "-----> iter 8921\n",
      "-----> iter 8922\n",
      "-----> iter 8923\n",
      "-----> iter 8924\n",
      "-----> iter 8925\n",
      "-----> iter 8926\n",
      "-----> iter 8927\n",
      "-----> iter 8928\n",
      "-----> iter 8929\n",
      "-----> iter 8930\n",
      "-----> iter 8931\n",
      "-----> iter 8932\n",
      "-----> iter 8933\n",
      "-----> iter 8934\n",
      "-----> iter 8935\n",
      "-----> iter 8936\n",
      "-----> iter 8937\n",
      "-----> iter 8938\n",
      "-----> iter 8939\n",
      "-----> iter 8940\n",
      "-----> iter 8941\n",
      "-----> iter 8942\n",
      "-----> iter 8943\n",
      "-----> iter 8944\n",
      "-----> iter 8945\n",
      "-----> iter 8946\n",
      "-----> iter 8947\n",
      "-----> iter 8948\n",
      "-----> iter 8949\n",
      "-----> iter 8950\n",
      "-----> iter 8951\n",
      "-----> iter 8952\n",
      "-----> iter 8953\n",
      "-----> iter 8954\n",
      "-----> iter 8955\n",
      "-----> iter 8956\n",
      "-----> iter 8957\n",
      "-----> iter 8958\n",
      "-----> iter 8959\n",
      "-----> iter 8960\n",
      "-----> iter 8961\n",
      "-----> iter 8962\n",
      "-----> iter 8963\n",
      "-----> iter 8964\n",
      "-----> iter 8965\n",
      "-----> iter 8966\n",
      "-----> iter 8967\n",
      "-----> iter 8968\n",
      "-----> iter 8969\n",
      "-----> iter 8970\n",
      "-----> iter 8971\n",
      "-----> iter 8972\n",
      "-----> iter 8973\n",
      "-----> iter 8974\n",
      "-----> iter 8975\n",
      "-----> iter 8976\n",
      "-----> iter 8977\n",
      "-----> iter 8978\n",
      "-----> iter 8979\n",
      "-----> iter 8980\n",
      "-----> iter 8981\n",
      "-----> iter 8982\n",
      "-----> iter 8983\n",
      "-----> iter 8984\n",
      "-----> iter 8985\n",
      "-----> iter 8986\n",
      "-----> iter 8987\n",
      "-----> iter 8988\n",
      "-----> iter 8989\n",
      "-----> iter 8990\n",
      "-----> iter 8991\n",
      "-----> iter 8992\n",
      "-----> iter 8993\n",
      "-----> iter 8994\n",
      "-----> iter 8995\n",
      "-----> iter 8996\n",
      "-----> iter 8997\n",
      "-----> iter 8998\n",
      "-----> iter 8999\n",
      "-----> iter 9000\n",
      "-----> iter 9001\n",
      "-----> iter 9002\n",
      "-----> iter 9003\n",
      "-----> iter 9004\n",
      "-----> iter 9005\n",
      "-----> iter 9006\n",
      "-----> iter 9007\n",
      "-----> iter 9008\n",
      "-----> iter 9009\n",
      "-----> iter 9010\n",
      "-----> iter 9011\n",
      "-----> iter 9012\n",
      "-----> iter 9013\n",
      "-----> iter 9014\n",
      "-----> iter 9015\n",
      "-----> iter 9016\n",
      "-----> iter 9017\n",
      "-----> iter 9018\n",
      "-----> iter 9019\n",
      "-----> iter 9020\n",
      "-----> iter 9021\n",
      "-----> iter 9022\n",
      "-----> iter 9023\n",
      "-----> iter 9024\n",
      "-----> iter 9025\n",
      "-----> iter 9026\n",
      "-----> iter 9027\n",
      "-----> iter 9028\n",
      "-----> iter 9029\n",
      "-----> iter 9030\n",
      "-----> iter 9031\n",
      "-----> iter 9032\n",
      "-----> iter 9033\n",
      "-----> iter 9034\n",
      "-----> iter 9035\n",
      "-----> iter 9036\n",
      "-----> iter 9037\n",
      "-----> iter 9038\n",
      "-----> iter 9039\n",
      "-----> iter 9040\n",
      "-----> iter 9041\n",
      "-----> iter 9042\n",
      "-----> iter 9043\n",
      "-----> iter 9044\n",
      "-----> iter 9045\n",
      "-----> iter 9046\n",
      "-----> iter 9047\n",
      "-----> iter 9048\n",
      "-----> iter 9049\n",
      "-----> iter 9050\n",
      "-----> iter 9051\n",
      "-----> iter 9052\n",
      "-----> iter 9053\n",
      "-----> iter 9054\n",
      "-----> iter 9055\n",
      "-----> iter 9056\n",
      "-----> iter 9057\n",
      "-----> iter 9058\n",
      "-----> iter 9059\n",
      "-----> iter 9060\n",
      "-----> iter 9061\n",
      "-----> iter 9062\n",
      "-----> iter 9063\n",
      "-----> iter 9064\n",
      "-----> iter 9065\n",
      "-----> iter 9066\n",
      "-----> iter 9067\n",
      "-----> iter 9068\n",
      "-----> iter 9069\n",
      "-----> iter 9070\n",
      "-----> iter 9071\n",
      "-----> iter 9072\n",
      "-----> iter 9073\n",
      "-----> iter 9074\n",
      "-----> iter 9075\n",
      "-----> iter 9076\n",
      "-----> iter 9077\n",
      "-----> iter 9078\n",
      "-----> iter 9079\n",
      "-----> iter 9080\n",
      "-----> iter 9081\n",
      "-----> iter 9082\n",
      "-----> iter 9083\n",
      "-----> iter 9084\n",
      "-----> iter 9085\n",
      "-----> iter 9086\n",
      "-----> iter 9087\n",
      "-----> iter 9088\n",
      "-----> iter 9089\n",
      "-----> iter 9090\n",
      "-----> iter 9091\n",
      "-----> iter 9092\n",
      "-----> iter 9093\n",
      "-----> iter 9094\n",
      "-----> iter 9095\n",
      "-----> iter 9096\n",
      "-----> iter 9097\n",
      "-----> iter 9098\n",
      "-----> iter 9099\n",
      "-----> iter 9100\n",
      "-----> iter 9101\n",
      "-----> iter 9102\n",
      "-----> iter 9103\n",
      "-----> iter 9104\n",
      "-----> iter 9105\n",
      "-----> iter 9106\n",
      "-----> iter 9107\n",
      "-----> iter 9108\n",
      "-----> iter 9109\n",
      "-----> iter 9110\n",
      "-----> iter 9111\n",
      "-----> iter 9112\n",
      "-----> iter 9113\n",
      "-----> iter 9114\n",
      "-----> iter 9115\n",
      "-----> iter 9116\n",
      "-----> iter 9117\n",
      "-----> iter 9118\n",
      "-----> iter 9119\n",
      "-----> iter 9120\n",
      "-----> iter 9121\n",
      "-----> iter 9122\n",
      "-----> iter 9123\n",
      "-----> iter 9124\n",
      "-----> iter 9125\n",
      "-----> iter 9126\n",
      "-----> iter 9127\n",
      "-----> iter 9128\n",
      "-----> iter 9129\n",
      "-----> iter 9130\n",
      "-----> iter 9131\n",
      "-----> iter 9132\n",
      "-----> iter 9133\n",
      "-----> iter 9134\n",
      "-----> iter 9135\n",
      "-----> iter 9136\n",
      "-----> iter 9137\n",
      "-----> iter 9138\n",
      "-----> iter 9139\n",
      "-----> iter 9140\n",
      "-----> iter 9141\n",
      "-----> iter 9142\n",
      "-----> iter 9143\n",
      "-----> iter 9144\n",
      "-----> iter 9145\n",
      "-----> iter 9146\n",
      "-----> iter 9147\n",
      "-----> iter 9148\n",
      "-----> iter 9149\n",
      "-----> iter 9150\n",
      "-----> iter 9151\n",
      "-----> iter 9152\n",
      "-----> iter 9153\n",
      "-----> iter 9154\n",
      "-----> iter 9155\n",
      "-----> iter 9156\n",
      "-----> iter 9157\n",
      "-----> iter 9158\n",
      "-----> iter 9159\n",
      "-----> iter 9160\n",
      "-----> iter 9161\n",
      "-----> iter 9162\n",
      "-----> iter 9163\n",
      "-----> iter 9164\n",
      "-----> iter 9165\n",
      "-----> iter 9166\n",
      "-----> iter 9167\n",
      "-----> iter 9168\n",
      "-----> iter 9169\n",
      "-----> iter 9170\n",
      "-----> iter 9171\n",
      "-----> iter 9172\n",
      "-----> iter 9173\n",
      "-----> iter 9174\n",
      "-----> iter 9175\n",
      "-----> iter 9176\n",
      "-----> iter 9177\n",
      "-----> iter 9178\n",
      "-----> iter 9179\n",
      "-----> iter 9180\n",
      "-----> iter 9181\n",
      "-----> iter 9182\n",
      "-----> iter 9183\n",
      "-----> iter 9184\n",
      "-----> iter 9185\n",
      "-----> iter 9186\n",
      "-----> iter 9187\n",
      "-----> iter 9188\n",
      "-----> iter 9189\n",
      "-----> iter 9190\n",
      "-----> iter 9191\n",
      "-----> iter 9192\n",
      "-----> iter 9193\n",
      "-----> iter 9194\n",
      "-----> iter 9195\n",
      "-----> iter 9196\n",
      "-----> iter 9197\n",
      "-----> iter 9198\n",
      "-----> iter 9199\n",
      "-----> iter 9200\n",
      "-----> iter 9201\n",
      "-----> iter 9202\n",
      "-----> iter 9203\n",
      "-----> iter 9204\n",
      "-----> iter 9205\n",
      "-----> iter 9206\n",
      "-----> iter 9207\n",
      "-----> iter 9208\n",
      "-----> iter 9209\n",
      "-----> iter 9210\n",
      "-----> iter 9211\n",
      "-----> iter 9212\n",
      "-----> iter 9213\n",
      "-----> iter 9214\n",
      "-----> iter 9215\n",
      "-----> iter 9216\n",
      "-----> iter 9217\n",
      "-----> iter 9218\n",
      "-----> iter 9219\n",
      "-----> iter 9220\n",
      "-----> iter 9221\n",
      "-----> iter 9222\n",
      "-----> iter 9223\n",
      "-----> iter 9224\n",
      "-----> iter 9225\n",
      "-----> iter 9226\n",
      "-----> iter 9227\n",
      "-----> iter 9228\n",
      "-----> iter 9229\n",
      "-----> iter 9230\n",
      "-----> iter 9231\n",
      "-----> iter 9232\n",
      "-----> iter 9233\n",
      "-----> iter 9234\n",
      "-----> iter 9235\n",
      "-----> iter 9236\n",
      "-----> iter 9237\n",
      "-----> iter 9238\n",
      "-----> iter 9239\n",
      "-----> iter 9240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 9241\n",
      "-----> iter 9242\n",
      "-----> iter 9243\n",
      "-----> iter 9244\n",
      "-----> iter 9245\n",
      "-----> iter 9246\n",
      "-----> iter 9247\n",
      "-----> iter 9248\n",
      "-----> iter 9249\n",
      "-----> iter 9250\n",
      "-----> iter 9251\n",
      "-----> iter 9252\n",
      "-----> iter 9253\n",
      "-----> iter 9254\n",
      "-----> iter 9255\n",
      "-----> iter 9256\n",
      "-----> iter 9257\n",
      "-----> iter 9258\n",
      "-----> iter 9259\n",
      "-----> iter 9260\n",
      "-----> iter 9261\n",
      "-----> iter 9262\n",
      "-----> iter 9263\n",
      "-----> iter 9264\n",
      "-----> iter 9265\n",
      "-----> iter 9266\n",
      "-----> iter 9267\n",
      "-----> iter 9268\n",
      "-----> iter 9269\n",
      "-----> iter 9270\n",
      "-----> iter 9271\n",
      "-----> iter 9272\n",
      "-----> iter 9273\n",
      "-----> iter 9274\n",
      "-----> iter 9275\n",
      "-----> iter 9276\n",
      "-----> iter 9277\n",
      "-----> iter 9278\n",
      "-----> iter 9279\n",
      "-----> iter 9280\n",
      "-----> iter 9281\n",
      "-----> iter 9282\n",
      "-----> iter 9283\n",
      "-----> iter 9284\n",
      "-----> iter 9285\n",
      "-----> iter 9286\n",
      "-----> iter 9287\n",
      "-----> iter 9288\n",
      "-----> iter 9289\n",
      "-----> iter 9290\n",
      "-----> iter 9291\n",
      "-----> iter 9292\n",
      "-----> iter 9293\n",
      "-----> iter 9294\n",
      "-----> iter 9295\n",
      "-----> iter 9296\n",
      "-----> iter 9297\n",
      "-----> iter 9298\n",
      "-----> iter 9299\n",
      "-----> iter 9300\n",
      "-----> iter 9301\n",
      "-----> iter 9302\n",
      "-----> iter 9303\n",
      "-----> iter 9304\n",
      "-----> iter 9305\n",
      "-----> iter 9306\n",
      "-----> iter 9307\n",
      "-----> iter 9308\n",
      "-----> iter 9309\n",
      "-----> iter 9310\n",
      "-----> iter 9311\n",
      "-----> iter 9312\n",
      "-----> iter 9313\n",
      "-----> iter 9314\n",
      "-----> iter 9315\n",
      "-----> iter 9316\n",
      "-----> iter 9317\n",
      "-----> iter 9318\n",
      "-----> iter 9319\n",
      "-----> iter 9320\n",
      "-----> iter 9321\n",
      "-----> iter 9322\n",
      "-----> iter 9323\n",
      "-----> iter 9324\n",
      "-----> iter 9325\n",
      "-----> iter 9326\n",
      "-----> iter 9327\n",
      "-----> iter 9328\n",
      "-----> iter 9329\n",
      "-----> iter 9330\n",
      "-----> iter 9331\n",
      "-----> iter 9332\n",
      "-----> iter 9333\n",
      "-----> iter 9334\n",
      "-----> iter 9335\n",
      "-----> iter 9336\n",
      "-----> iter 9337\n",
      "-----> iter 9338\n",
      "-----> iter 9339\n",
      "-----> iter 9340\n",
      "-----> iter 9341\n",
      "-----> iter 9342\n",
      "-----> iter 9343\n",
      "-----> iter 9344\n",
      "-----> iter 9345\n",
      "-----> iter 9346\n",
      "-----> iter 9347\n",
      "-----> iter 9348\n",
      "-----> iter 9349\n",
      "-----> iter 9350\n",
      "-----> iter 9351\n",
      "-----> iter 9352\n",
      "-----> iter 9353\n",
      "-----> iter 9354\n",
      "-----> iter 9355\n",
      "-----> iter 9356\n",
      "-----> iter 9357\n",
      "-----> iter 9358\n",
      "-----> iter 9359\n",
      "-----> iter 9360\n",
      "-----> iter 9361\n",
      "-----> iter 9362\n",
      "-----> iter 9363\n",
      "-----> iter 9364\n",
      "-----> iter 9365\n",
      "-----> iter 9366\n",
      "-----> iter 9367\n",
      "-----> iter 9368\n",
      "-----> iter 9369\n",
      "-----> iter 9370\n",
      "-----> iter 9371\n",
      "-----> iter 9372\n",
      "-----> iter 9373\n",
      "-----> iter 9374\n",
      "-----> iter 9375\n",
      "-----> iter 9376\n",
      "-----> iter 9377\n",
      "-----> iter 9378\n",
      "-----> iter 9379\n",
      "-----> iter 9380\n",
      "-----> iter 9381\n",
      "-----> iter 9382\n",
      "-----> iter 9383\n",
      "-----> iter 9384\n",
      "-----> iter 9385\n",
      "-----> iter 9386\n",
      "-----> iter 9387\n",
      "-----> iter 9388\n",
      "-----> iter 9389\n",
      "-----> iter 9390\n",
      "-----> iter 9391\n",
      "-----> iter 9392\n",
      "-----> iter 9393\n",
      "-----> iter 9394\n",
      "-----> iter 9395\n",
      "-----> iter 9396\n",
      "-----> iter 9397\n",
      "-----> iter 9398\n",
      "-----> iter 9399\n",
      "-----> iter 9400\n",
      "-----> iter 9401\n",
      "-----> iter 9402\n",
      "-----> iter 9403\n",
      "-----> iter 9404\n",
      "-----> iter 9405\n",
      "-----> iter 9406\n",
      "-----> iter 9407\n",
      "-----> iter 9408\n",
      "-----> iter 9409\n",
      "-----> iter 9410\n",
      "-----> iter 9411\n",
      "-----> iter 9412\n",
      "-----> iter 9413\n",
      "-----> iter 9414\n",
      "-----> iter 9415\n",
      "-----> iter 9416\n",
      "-----> iter 9417\n",
      "-----> iter 9418\n",
      "-----> iter 9419\n",
      "-----> iter 9420\n",
      "-----> iter 9421\n",
      "-----> iter 9422\n",
      "-----> iter 9423\n",
      "-----> iter 9424\n",
      "-----> iter 9425\n",
      "-----> iter 9426\n",
      "-----> iter 9427\n",
      "-----> iter 9428\n",
      "-----> iter 9429\n",
      "-----> iter 9430\n",
      "-----> iter 9431\n",
      "-----> iter 9432\n",
      "-----> iter 9433\n",
      "-----> iter 9434\n",
      "-----> iter 9435\n",
      "-----> iter 9436\n",
      "-----> iter 9437\n",
      "-----> iter 9438\n",
      "-----> iter 9439\n",
      "-----> iter 9440\n",
      "-----> iter 9441\n",
      "-----> iter 9442\n",
      "-----> iter 9443\n",
      "-----> iter 9444\n",
      "-----> iter 9445\n",
      "-----> iter 9446\n",
      "-----> iter 9447\n",
      "-----> iter 9448\n",
      "-----> iter 9449\n",
      "-----> iter 9450\n",
      "-----> iter 9451\n",
      "-----> iter 9452\n",
      "-----> iter 9453\n",
      "-----> iter 9454\n",
      "-----> iter 9455\n",
      "-----> iter 9456\n",
      "-----> iter 9457\n",
      "-----> iter 9458\n",
      "-----> iter 9459\n",
      "-----> iter 9460\n",
      "-----> iter 9461\n",
      "-----> iter 9462\n",
      "-----> iter 9463\n",
      "-----> iter 9464\n",
      "-----> iter 9465\n",
      "-----> iter 9466\n",
      "-----> iter 9467\n",
      "-----> iter 9468\n",
      "-----> iter 9469\n",
      "-----> iter 9470\n",
      "-----> iter 9471\n",
      "-----> iter 9472\n",
      "-----> iter 9473\n",
      "-----> iter 9474\n",
      "-----> iter 9475\n",
      "-----> iter 9476\n",
      "-----> iter 9477\n",
      "-----> iter 9478\n",
      "-----> iter 9479\n",
      "-----> iter 9480\n",
      "-----> iter 9481\n",
      "-----> iter 9482\n",
      "-----> iter 9483\n",
      "-----> iter 9484\n",
      "-----> iter 9485\n",
      "-----> iter 9486\n",
      "-----> iter 9487\n",
      "-----> iter 9488\n",
      "-----> iter 9489\n",
      "-----> iter 9490\n",
      "-----> iter 9491\n",
      "-----> iter 9492\n",
      "-----> iter 9493\n",
      "-----> iter 9494\n",
      "-----> iter 9495\n",
      "-----> iter 9496\n",
      "-----> iter 9497\n",
      "-----> iter 9498\n",
      "-----> iter 9499\n",
      "-----> iter 9500\n",
      "-----> iter 9501\n",
      "-----> iter 9502\n",
      "-----> iter 9503\n",
      "-----> iter 9504\n",
      "-----> iter 9505\n",
      "-----> iter 9506\n",
      "-----> iter 9507\n",
      "-----> iter 9508\n",
      "-----> iter 9509\n",
      "-----> iter 9510\n",
      "-----> iter 9511\n",
      "-----> iter 9512\n",
      "-----> iter 9513\n",
      "-----> iter 9514\n",
      "-----> iter 9515\n",
      "-----> iter 9516\n",
      "-----> iter 9517\n",
      "-----> iter 9518\n",
      "-----> iter 9519\n",
      "-----> iter 9520\n",
      "-----> iter 9521\n",
      "-----> iter 9522\n",
      "-----> iter 9523\n",
      "-----> iter 9524\n",
      "-----> iter 9525\n",
      "-----> iter 9526\n",
      "-----> iter 9527\n",
      "-----> iter 9528\n",
      "-----> iter 9529\n",
      "-----> iter 9530\n",
      "-----> iter 9531\n",
      "-----> iter 9532\n",
      "-----> iter 9533\n",
      "-----> iter 9534\n",
      "-----> iter 9535\n",
      "-----> iter 9536\n",
      "-----> iter 9537\n",
      "-----> iter 9538\n",
      "-----> iter 9539\n",
      "-----> iter 9540\n",
      "-----> iter 9541\n",
      "-----> iter 9542\n",
      "-----> iter 9543\n",
      "-----> iter 9544\n",
      "-----> iter 9545\n",
      "-----> iter 9546\n",
      "-----> iter 9547\n",
      "-----> iter 9548\n",
      "-----> iter 9549\n",
      "-----> iter 9550\n",
      "-----> iter 9551\n",
      "-----> iter 9552\n",
      "-----> iter 9553\n",
      "-----> iter 9554\n",
      "-----> iter 9555\n",
      "-----> iter 9556\n",
      "-----> iter 9557\n",
      "-----> iter 9558\n",
      "-----> iter 9559\n",
      "-----> iter 9560\n",
      "-----> iter 9561\n",
      "-----> iter 9562\n",
      "-----> iter 9563\n",
      "-----> iter 9564\n",
      "-----> iter 9565\n",
      "-----> iter 9566\n",
      "-----> iter 9567\n",
      "-----> iter 9568\n",
      "-----> iter 9569\n",
      "-----> iter 9570\n",
      "-----> iter 9571\n",
      "-----> iter 9572\n",
      "-----> iter 9573\n",
      "-----> iter 9574\n",
      "-----> iter 9575\n",
      "-----> iter 9576\n",
      "-----> iter 9577\n",
      "-----> iter 9578\n",
      "-----> iter 9579\n",
      "-----> iter 9580\n",
      "-----> iter 9581\n",
      "-----> iter 9582\n",
      "-----> iter 9583\n",
      "-----> iter 9584\n",
      "-----> iter 9585\n",
      "-----> iter 9586\n",
      "-----> iter 9587\n",
      "-----> iter 9588\n",
      "-----> iter 9589\n",
      "-----> iter 9590\n",
      "-----> iter 9591\n",
      "-----> iter 9592\n",
      "-----> iter 9593\n",
      "-----> iter 9594\n",
      "-----> iter 9595\n",
      "-----> iter 9596\n",
      "-----> iter 9597\n",
      "-----> iter 9598\n",
      "-----> iter 9599\n",
      "-----> iter 9600\n",
      "-----> iter 9601\n",
      "-----> iter 9602\n",
      "-----> iter 9603\n",
      "-----> iter 9604\n",
      "-----> iter 9605\n",
      "-----> iter 9606\n",
      "-----> iter 9607\n",
      "-----> iter 9608\n",
      "-----> iter 9609\n",
      "-----> iter 9610\n",
      "-----> iter 9611\n",
      "-----> iter 9612\n",
      "-----> iter 9613\n",
      "-----> iter 9614\n",
      "-----> iter 9615\n",
      "-----> iter 9616\n",
      "-----> iter 9617\n",
      "-----> iter 9618\n",
      "-----> iter 9619\n",
      "-----> iter 9620\n",
      "-----> iter 9621\n",
      "-----> iter 9622\n",
      "-----> iter 9623\n",
      "-----> iter 9624\n",
      "-----> iter 9625\n",
      "-----> iter 9626\n",
      "-----> iter 9627\n",
      "-----> iter 9628\n",
      "-----> iter 9629\n",
      "-----> iter 9630\n",
      "-----> iter 9631\n",
      "-----> iter 9632\n",
      "-----> iter 9633\n",
      "-----> iter 9634\n",
      "-----> iter 9635\n",
      "-----> iter 9636\n",
      "-----> iter 9637\n",
      "-----> iter 9638\n",
      "-----> iter 9639\n",
      "-----> iter 9640\n",
      "-----> iter 9641\n",
      "-----> iter 9642\n",
      "-----> iter 9643\n",
      "-----> iter 9644\n",
      "-----> iter 9645\n",
      "-----> iter 9646\n",
      "-----> iter 9647\n",
      "-----> iter 9648\n",
      "-----> iter 9649\n",
      "-----> iter 9650\n",
      "-----> iter 9651\n",
      "-----> iter 9652\n",
      "-----> iter 9653\n",
      "-----> iter 9654\n",
      "-----> iter 9655\n",
      "-----> iter 9656\n",
      "-----> iter 9657\n",
      "-----> iter 9658\n",
      "-----> iter 9659\n",
      "-----> iter 9660\n",
      "-----> iter 9661\n",
      "-----> iter 9662\n",
      "-----> iter 9663\n",
      "-----> iter 9664\n",
      "-----> iter 9665\n",
      "-----> iter 9666\n",
      "-----> iter 9667\n",
      "-----> iter 9668\n",
      "-----> iter 9669\n",
      "-----> iter 9670\n",
      "-----> iter 9671\n",
      "-----> iter 9672\n",
      "-----> iter 9673\n",
      "-----> iter 9674\n",
      "-----> iter 9675\n",
      "-----> iter 9676\n",
      "-----> iter 9677\n",
      "-----> iter 9678\n",
      "-----> iter 9679\n",
      "-----> iter 9680\n",
      "-----> iter 9681\n",
      "-----> iter 9682\n",
      "-----> iter 9683\n",
      "-----> iter 9684\n",
      "-----> iter 9685\n",
      "-----> iter 9686\n",
      "-----> iter 9687\n",
      "-----> iter 9688\n",
      "-----> iter 9689\n",
      "-----> iter 9690\n",
      "-----> iter 9691\n",
      "-----> iter 9692\n",
      "-----> iter 9693\n",
      "-----> iter 9694\n",
      "-----> iter 9695\n",
      "-----> iter 9696\n",
      "-----> iter 9697\n",
      "-----> iter 9698\n",
      "-----> iter 9699\n",
      "-----> iter 9700\n",
      "-----> iter 9701\n",
      "-----> iter 9702\n",
      "-----> iter 9703\n",
      "-----> iter 9704\n",
      "-----> iter 9705\n",
      "-----> iter 9706\n",
      "-----> iter 9707\n",
      "-----> iter 9708\n",
      "-----> iter 9709\n",
      "-----> iter 9710\n",
      "-----> iter 9711\n",
      "-----> iter 9712\n",
      "-----> iter 9713\n",
      "-----> iter 9714\n",
      "-----> iter 9715\n",
      "-----> iter 9716\n",
      "-----> iter 9717\n",
      "-----> iter 9718\n",
      "-----> iter 9719\n",
      "-----> iter 9720\n",
      "-----> iter 9721\n",
      "-----> iter 9722\n",
      "-----> iter 9723\n",
      "-----> iter 9724\n",
      "-----> iter 9725\n",
      "-----> iter 9726\n",
      "-----> iter 9727\n",
      "-----> iter 9728\n",
      "-----> iter 9729\n",
      "-----> iter 9730\n",
      "-----> iter 9731\n",
      "-----> iter 9732\n",
      "-----> iter 9733\n",
      "-----> iter 9734\n",
      "-----> iter 9735\n",
      "-----> iter 9736\n",
      "-----> iter 9737\n",
      "-----> iter 9738\n",
      "-----> iter 9739\n",
      "-----> iter 9740\n",
      "-----> iter 9741\n",
      "-----> iter 9742\n",
      "-----> iter 9743\n",
      "-----> iter 9744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 9745\n",
      "-----> iter 9746\n",
      "-----> iter 9747\n",
      "-----> iter 9748\n",
      "-----> iter 9749\n",
      "-----> iter 9750\n",
      "-----> iter 9751\n",
      "-----> iter 9752\n",
      "-----> iter 9753\n",
      "-----> iter 9754\n",
      "-----> iter 9755\n",
      "-----> iter 9756\n",
      "-----> iter 9757\n",
      "-----> iter 9758\n",
      "-----> iter 9759\n",
      "-----> iter 9760\n",
      "-----> iter 9761\n",
      "-----> iter 9762\n",
      "-----> iter 9763\n",
      "-----> iter 9764\n",
      "-----> iter 9765\n",
      "-----> iter 9766\n",
      "-----> iter 9767\n",
      "-----> iter 9768\n",
      "-----> iter 9769\n",
      "-----> iter 9770\n",
      "-----> iter 9771\n",
      "-----> iter 9772\n",
      "-----> iter 9773\n",
      "-----> iter 9774\n",
      "-----> iter 9775\n",
      "-----> iter 9776\n",
      "-----> iter 9777\n",
      "-----> iter 9778\n",
      "-----> iter 9779\n",
      "-----> iter 9780\n",
      "-----> iter 9781\n",
      "-----> iter 9782\n",
      "-----> iter 9783\n",
      "-----> iter 9784\n",
      "-----> iter 9785\n",
      "-----> iter 9786\n",
      "-----> iter 9787\n",
      "-----> iter 9788\n",
      "-----> iter 9789\n",
      "-----> iter 9790\n",
      "-----> iter 9791\n",
      "-----> iter 9792\n",
      "-----> iter 9793\n",
      "-----> iter 9794\n",
      "-----> iter 9795\n",
      "-----> iter 9796\n",
      "-----> iter 9797\n",
      "-----> iter 9798\n",
      "-----> iter 9799\n",
      "-----> iter 9800\n",
      "-----> iter 9801\n",
      "-----> iter 9802\n",
      "-----> iter 9803\n",
      "-----> iter 9804\n",
      "-----> iter 9805\n",
      "-----> iter 9806\n",
      "-----> iter 9807\n",
      "-----> iter 9808\n",
      "-----> iter 9809\n",
      "-----> iter 9810\n",
      "-----> iter 9811\n",
      "-----> iter 9812\n",
      "-----> iter 9813\n",
      "-----> iter 9814\n",
      "-----> iter 9815\n",
      "-----> iter 9816\n",
      "-----> iter 9817\n",
      "-----> iter 9818\n",
      "-----> iter 9819\n",
      "-----> iter 9820\n",
      "-----> iter 9821\n",
      "-----> iter 9822\n",
      "-----> iter 9823\n",
      "-----> iter 9824\n",
      "-----> iter 9825\n",
      "-----> iter 9826\n",
      "-----> iter 9827\n",
      "-----> iter 9828\n",
      "-----> iter 9829\n",
      "-----> iter 9830\n",
      "-----> iter 9831\n",
      "-----> iter 9832\n",
      "-----> iter 9833\n",
      "-----> iter 9834\n",
      "-----> iter 9835\n",
      "-----> iter 9836\n",
      "-----> iter 9837\n",
      "-----> iter 9838\n",
      "-----> iter 9839\n",
      "-----> iter 9840\n",
      "-----> iter 9841\n",
      "-----> iter 9842\n",
      "-----> iter 9843\n",
      "-----> iter 9844\n",
      "-----> iter 9845\n",
      "-----> iter 9846\n",
      "-----> iter 9847\n",
      "-----> iter 9848\n",
      "-----> iter 9849\n",
      "-----> iter 9850\n",
      "-----> iter 9851\n",
      "-----> iter 9852\n",
      "-----> iter 9853\n",
      "-----> iter 9854\n",
      "-----> iter 9855\n",
      "-----> iter 9856\n",
      "-----> iter 9857\n",
      "-----> iter 9858\n",
      "-----> iter 9859\n",
      "-----> iter 9860\n",
      "-----> iter 9861\n",
      "-----> iter 9862\n",
      "-----> iter 9863\n",
      "-----> iter 9864\n",
      "-----> iter 9865\n",
      "-----> iter 9866\n",
      "-----> iter 9867\n",
      "-----> iter 9868\n",
      "-----> iter 9869\n",
      "-----> iter 9870\n",
      "-----> iter 9871\n",
      "-----> iter 9872\n",
      "-----> iter 9873\n",
      "-----> iter 9874\n",
      "-----> iter 9875\n",
      "-----> iter 9876\n",
      "-----> iter 9877\n",
      "-----> iter 9878\n",
      "-----> iter 9879\n",
      "-----> iter 9880\n",
      "-----> iter 9881\n",
      "-----> iter 9882\n",
      "-----> iter 9883\n",
      "-----> iter 9884\n",
      "-----> iter 9885\n",
      "-----> iter 9886\n",
      "-----> iter 9887\n",
      "-----> iter 9888\n",
      "-----> iter 9889\n",
      "-----> iter 9890\n",
      "-----> iter 9891\n",
      "-----> iter 9892\n",
      "-----> iter 9893\n",
      "-----> iter 9894\n",
      "-----> iter 9895\n",
      "-----> iter 9896\n",
      "-----> iter 9897\n",
      "-----> iter 9898\n",
      "-----> iter 9899\n",
      "-----> iter 9900\n",
      "-----> iter 9901\n",
      "-----> iter 9902\n",
      "-----> iter 9903\n",
      "-----> iter 9904\n",
      "-----> iter 9905\n",
      "-----> iter 9906\n",
      "-----> iter 9907\n",
      "-----> iter 9908\n",
      "-----> iter 9909\n",
      "-----> iter 9910\n",
      "-----> iter 9911\n",
      "-----> iter 9912\n",
      "-----> iter 9913\n",
      "-----> iter 9914\n",
      "-----> iter 9915\n",
      "-----> iter 9916\n",
      "-----> iter 9917\n",
      "-----> iter 9918\n",
      "-----> iter 9919\n",
      "-----> iter 9920\n",
      "-----> iter 9921\n",
      "-----> iter 9922\n",
      "-----> iter 9923\n",
      "-----> iter 9924\n",
      "-----> iter 9925\n",
      "-----> iter 9926\n",
      "-----> iter 9927\n",
      "-----> iter 9928\n",
      "-----> iter 9929\n",
      "-----> iter 9930\n",
      "-----> iter 9931\n",
      "-----> iter 9932\n",
      "-----> iter 9933\n",
      "-----> iter 9934\n",
      "-----> iter 9935\n",
      "-----> iter 9936\n",
      "-----> iter 9937\n",
      "-----> iter 9938\n",
      "-----> iter 9939\n",
      "-----> iter 9940\n",
      "-----> iter 9941\n",
      "-----> iter 9942\n",
      "-----> iter 9943\n",
      "-----> iter 9944\n",
      "-----> iter 9945\n",
      "-----> iter 9946\n",
      "-----> iter 9947\n",
      "-----> iter 9948\n",
      "-----> iter 9949\n",
      "-----> iter 9950\n",
      "-----> iter 9951\n",
      "-----> iter 9952\n",
      "-----> iter 9953\n",
      "-----> iter 9954\n",
      "-----> iter 9955\n",
      "-----> iter 9956\n",
      "-----> iter 9957\n",
      "-----> iter 9958\n",
      "-----> iter 9959\n",
      "-----> iter 9960\n",
      "-----> iter 9961\n",
      "-----> iter 9962\n",
      "-----> iter 9963\n",
      "-----> iter 9964\n",
      "-----> iter 9965\n",
      "-----> iter 9966\n",
      "-----> iter 9967\n",
      "-----> iter 9968\n",
      "-----> iter 9969\n",
      "-----> iter 9970\n",
      "-----> iter 9971\n",
      "-----> iter 9972\n",
      "-----> iter 9973\n",
      "-----> iter 9974\n",
      "-----> iter 9975\n",
      "-----> iter 9976\n",
      "-----> iter 9977\n",
      "-----> iter 9978\n",
      "-----> iter 9979\n",
      "-----> iter 9980\n",
      "-----> iter 9981\n",
      "-----> iter 9982\n",
      "-----> iter 9983\n",
      "-----> iter 9984\n",
      "-----> iter 9985\n",
      "-----> iter 9986\n",
      "-----> iter 9987\n",
      "-----> iter 9988\n",
      "-----> iter 9989\n",
      "-----> iter 9990\n",
      "-----> iter 9991\n",
      "-----> iter 9992\n",
      "-----> iter 9993\n",
      "-----> iter 9994\n",
      "-----> iter 9995\n",
      "-----> iter 9996\n",
      "-----> iter 9997\n",
      "-----> iter 9998\n",
      "-----> iter 9999\n",
      "-----> iter 10000\n",
      "-----> iter 10001\n",
      "-----> iter 10002\n",
      "-----> iter 10003\n",
      "-----> iter 10004\n",
      "-----> iter 10005\n",
      "-----> iter 10006\n",
      "-----> iter 10007\n",
      "-----> iter 10008\n",
      "-----> iter 10009\n",
      "-----> iter 10010\n",
      "-----> iter 10011\n",
      "-----> iter 10012\n",
      "-----> iter 10013\n",
      "-----> iter 10014\n",
      "-----> iter 10015\n",
      "-----> iter 10016\n",
      "-----> iter 10017\n",
      "-----> iter 10018\n",
      "-----> iter 10019\n",
      "-----> iter 10020\n",
      "-----> iter 10021\n",
      "-----> iter 10022\n",
      "-----> iter 10023\n",
      "-----> iter 10024\n",
      "-----> iter 10025\n",
      "-----> iter 10026\n",
      "-----> iter 10027\n",
      "-----> iter 10028\n",
      "-----> iter 10029\n",
      "-----> iter 10030\n",
      "-----> iter 10031\n",
      "-----> iter 10032\n",
      "-----> iter 10033\n",
      "-----> iter 10034\n",
      "-----> iter 10035\n",
      "-----> iter 10036\n",
      "-----> iter 10037\n",
      "-----> iter 10038\n",
      "-----> iter 10039\n",
      "-----> iter 10040\n",
      "-----> iter 10041\n",
      "-----> iter 10042\n",
      "-----> iter 10043\n",
      "-----> iter 10044\n",
      "-----> iter 10045\n",
      "-----> iter 10046\n",
      "-----> iter 10047\n",
      "-----> iter 10048\n",
      "-----> iter 10049\n",
      "-----> iter 10050\n",
      "-----> iter 10051\n",
      "-----> iter 10052\n",
      "-----> iter 10053\n",
      "-----> iter 10054\n",
      "-----> iter 10055\n",
      "-----> iter 10056\n",
      "-----> iter 10057\n",
      "-----> iter 10058\n",
      "-----> iter 10059\n",
      "-----> iter 10060\n",
      "-----> iter 10061\n",
      "-----> iter 10062\n",
      "-----> iter 10063\n",
      "-----> iter 10064\n",
      "-----> iter 10065\n",
      "-----> iter 10066\n",
      "-----> iter 10067\n",
      "-----> iter 10068\n",
      "-----> iter 10069\n",
      "-----> iter 10070\n",
      "-----> iter 10071\n",
      "-----> iter 10072\n",
      "-----> iter 10073\n",
      "-----> iter 10074\n",
      "-----> iter 10075\n",
      "-----> iter 10076\n",
      "-----> iter 10077\n",
      "-----> iter 10078\n",
      "-----> iter 10079\n",
      "-----> iter 10080\n",
      "-----> iter 10081\n",
      "-----> iter 10082\n",
      "-----> iter 10083\n",
      "-----> iter 10084\n",
      "-----> iter 10085\n",
      "-----> iter 10086\n",
      "-----> iter 10087\n",
      "-----> iter 10088\n",
      "-----> iter 10089\n",
      "-----> iter 10090\n",
      "-----> iter 10091\n",
      "-----> iter 10092\n",
      "-----> iter 10093\n",
      "-----> iter 10094\n",
      "-----> iter 10095\n",
      "-----> iter 10096\n",
      "-----> iter 10097\n",
      "-----> iter 10098\n",
      "-----> iter 10099\n",
      "-----> iter 10100\n",
      "-----> iter 10101\n",
      "-----> iter 10102\n",
      "-----> iter 10103\n",
      "-----> iter 10104\n",
      "-----> iter 10105\n",
      "-----> iter 10106\n",
      "-----> iter 10107\n",
      "-----> iter 10108\n",
      "-----> iter 10109\n",
      "-----> iter 10110\n",
      "-----> iter 10111\n",
      "-----> iter 10112\n",
      "-----> iter 10113\n",
      "-----> iter 10114\n",
      "-----> iter 10115\n",
      "-----> iter 10116\n",
      "-----> iter 10117\n",
      "-----> iter 10118\n",
      "-----> iter 10119\n",
      "-----> iter 10120\n",
      "-----> iter 10121\n",
      "-----> iter 10122\n",
      "-----> iter 10123\n",
      "-----> iter 10124\n",
      "-----> iter 10125\n",
      "-----> iter 10126\n",
      "-----> iter 10127\n",
      "-----> iter 10128\n",
      "-----> iter 10129\n",
      "-----> iter 10130\n",
      "-----> iter 10131\n",
      "-----> iter 10132\n",
      "-----> iter 10133\n",
      "-----> iter 10134\n",
      "-----> iter 10135\n",
      "-----> iter 10136\n",
      "-----> iter 10137\n",
      "-----> iter 10138\n",
      "-----> iter 10139\n",
      "-----> iter 10140\n",
      "-----> iter 10141\n",
      "-----> iter 10142\n",
      "-----> iter 10143\n",
      "-----> iter 10144\n",
      "-----> iter 10145\n",
      "-----> iter 10146\n",
      "-----> iter 10147\n",
      "-----> iter 10148\n",
      "-----> iter 10149\n",
      "-----> iter 10150\n",
      "-----> iter 10151\n",
      "-----> iter 10152\n",
      "-----> iter 10153\n",
      "-----> iter 10154\n",
      "-----> iter 10155\n",
      "-----> iter 10156\n",
      "-----> iter 10157\n",
      "-----> iter 10158\n",
      "-----> iter 10159\n",
      "-----> iter 10160\n",
      "-----> iter 10161\n",
      "-----> iter 10162\n",
      "-----> iter 10163\n",
      "-----> iter 10164\n",
      "-----> iter 10165\n",
      "-----> iter 10166\n",
      "-----> iter 10167\n",
      "-----> iter 10168\n",
      "-----> iter 10169\n",
      "-----> iter 10170\n",
      "-----> iter 10171\n",
      "-----> iter 10172\n",
      "-----> iter 10173\n",
      "-----> iter 10174\n",
      "-----> iter 10175\n",
      "-----> iter 10176\n",
      "-----> iter 10177\n",
      "-----> iter 10178\n",
      "-----> iter 10179\n",
      "-----> iter 10180\n",
      "-----> iter 10181\n",
      "-----> iter 10182\n",
      "-----> iter 10183\n",
      "-----> iter 10184\n",
      "-----> iter 10185\n",
      "-----> iter 10186\n",
      "-----> iter 10187\n",
      "-----> iter 10188\n",
      "-----> iter 10189\n",
      "-----> iter 10190\n",
      "-----> iter 10191\n",
      "-----> iter 10192\n",
      "-----> iter 10193\n",
      "-----> iter 10194\n",
      "-----> iter 10195\n",
      "-----> iter 10196\n",
      "-----> iter 10197\n",
      "-----> iter 10198\n",
      "-----> iter 10199\n",
      "-----> iter 10200\n",
      "-----> iter 10201\n",
      "-----> iter 10202\n",
      "-----> iter 10203\n",
      "-----> iter 10204\n",
      "-----> iter 10205\n",
      "-----> iter 10206\n",
      "-----> iter 10207\n",
      "-----> iter 10208\n",
      "-----> iter 10209\n",
      "-----> iter 10210\n",
      "-----> iter 10211\n",
      "-----> iter 10212\n",
      "-----> iter 10213\n",
      "-----> iter 10214\n",
      "-----> iter 10215\n",
      "-----> iter 10216\n",
      "-----> iter 10217\n",
      "-----> iter 10218\n",
      "-----> iter 10219\n",
      "-----> iter 10220\n",
      "-----> iter 10221\n",
      "-----> iter 10222\n",
      "-----> iter 10223\n",
      "-----> iter 10224\n",
      "-----> iter 10225\n",
      "-----> iter 10226\n",
      "-----> iter 10227\n",
      "-----> iter 10228\n",
      "-----> iter 10229\n",
      "-----> iter 10230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 10231\n",
      "-----> iter 10232\n",
      "-----> iter 10233\n",
      "-----> iter 10234\n",
      "-----> iter 10235\n",
      "-----> iter 10236\n",
      "-----> iter 10237\n",
      "-----> iter 10238\n",
      "-----> iter 10239\n",
      "-----> iter 10240\n",
      "-----> iter 10241\n",
      "-----> iter 10242\n",
      "-----> iter 10243\n",
      "-----> iter 10244\n",
      "-----> iter 10245\n",
      "-----> iter 10246\n",
      "-----> iter 10247\n",
      "-----> iter 10248\n",
      "-----> iter 10249\n",
      "-----> iter 10250\n",
      "-----> iter 10251\n",
      "-----> iter 10252\n",
      "-----> iter 10253\n",
      "-----> iter 10254\n",
      "-----> iter 10255\n",
      "-----> iter 10256\n",
      "-----> iter 10257\n",
      "-----> iter 10258\n",
      "-----> iter 10259\n",
      "-----> iter 10260\n",
      "-----> iter 10261\n",
      "-----> iter 10262\n",
      "-----> iter 10263\n",
      "-----> iter 10264\n",
      "-----> iter 10265\n",
      "-----> iter 10266\n",
      "-----> iter 10267\n",
      "-----> iter 10268\n",
      "-----> iter 10269\n",
      "-----> iter 10270\n",
      "-----> iter 10271\n",
      "-----> iter 10272\n",
      "-----> iter 10273\n",
      "-----> iter 10274\n",
      "-----> iter 10275\n",
      "-----> iter 10276\n",
      "-----> iter 10277\n",
      "-----> iter 10278\n",
      "-----> iter 10279\n",
      "-----> iter 10280\n",
      "-----> iter 10281\n",
      "-----> iter 10282\n",
      "-----> iter 10283\n",
      "-----> iter 10284\n",
      "-----> iter 10285\n",
      "-----> iter 10286\n",
      "-----> iter 10287\n",
      "-----> iter 10288\n",
      "-----> iter 10289\n",
      "-----> iter 10290\n",
      "-----> iter 10291\n",
      "-----> iter 10292\n",
      "-----> iter 10293\n",
      "-----> iter 10294\n",
      "-----> iter 10295\n",
      "-----> iter 10296\n",
      "-----> iter 10297\n",
      "-----> iter 10298\n",
      "-----> iter 10299\n",
      "-----> iter 10300\n",
      "-----> iter 10301\n",
      "-----> iter 10302\n",
      "-----> iter 10303\n",
      "-----> iter 10304\n",
      "-----> iter 10305\n",
      "-----> iter 10306\n",
      "-----> iter 10307\n",
      "-----> iter 10308\n",
      "-----> iter 10309\n",
      "-----> iter 10310\n",
      "-----> iter 10311\n",
      "-----> iter 10312\n",
      "-----> iter 10313\n",
      "-----> iter 10314\n",
      "-----> iter 10315\n",
      "-----> iter 10316\n",
      "-----> iter 10317\n",
      "-----> iter 10318\n",
      "-----> iter 10319\n",
      "-----> iter 10320\n",
      "-----> iter 10321\n",
      "-----> iter 10322\n",
      "-----> iter 10323\n",
      "-----> iter 10324\n",
      "-----> iter 10325\n",
      "-----> iter 10326\n",
      "-----> iter 10327\n",
      "-----> iter 10328\n",
      "-----> iter 10329\n",
      "-----> iter 10330\n",
      "-----> iter 10331\n",
      "-----> iter 10332\n",
      "-----> iter 10333\n",
      "-----> iter 10334\n",
      "-----> iter 10335\n",
      "-----> iter 10336\n",
      "-----> iter 10337\n",
      "-----> iter 10338\n",
      "-----> iter 10339\n",
      "-----> iter 10340\n",
      "-----> iter 10341\n",
      "-----> iter 10342\n",
      "-----> iter 10343\n",
      "-----> iter 10344\n",
      "-----> iter 10345\n",
      "-----> iter 10346\n",
      "-----> iter 10347\n",
      "-----> iter 10348\n",
      "-----> iter 10349\n",
      "-----> iter 10350\n",
      "-----> iter 10351\n",
      "-----> iter 10352\n",
      "-----> iter 10353\n",
      "-----> iter 10354\n",
      "-----> iter 10355\n",
      "-----> iter 10356\n",
      "-----> iter 10357\n",
      "-----> iter 10358\n",
      "-----> iter 10359\n",
      "-----> iter 10360\n",
      "-----> iter 10361\n",
      "-----> iter 10362\n",
      "-----> iter 10363\n",
      "-----> iter 10364\n",
      "-----> iter 10365\n",
      "-----> iter 10366\n",
      "-----> iter 10367\n",
      "-----> iter 10368\n",
      "-----> iter 10369\n",
      "-----> iter 10370\n",
      "-----> iter 10371\n",
      "-----> iter 10372\n",
      "-----> iter 10373\n",
      "-----> iter 10374\n",
      "-----> iter 10375\n",
      "-----> iter 10376\n",
      "-----> iter 10377\n",
      "-----> iter 10378\n",
      "-----> iter 10379\n",
      "-----> iter 10380\n",
      "-----> iter 10381\n",
      "-----> iter 10382\n",
      "-----> iter 10383\n",
      "-----> iter 10384\n",
      "-----> iter 10385\n",
      "-----> iter 10386\n",
      "-----> iter 10387\n",
      "-----> iter 10388\n",
      "-----> iter 10389\n",
      "-----> iter 10390\n",
      "-----> iter 10391\n",
      "-----> iter 10392\n",
      "-----> iter 10393\n",
      "-----> iter 10394\n",
      "-----> iter 10395\n",
      "-----> iter 10396\n",
      "-----> iter 10397\n",
      "-----> iter 10398\n",
      "-----> iter 10399\n",
      "-----> iter 10400\n",
      "-----> iter 10401\n",
      "-----> iter 10402\n",
      "-----> iter 10403\n",
      "-----> iter 10404\n",
      "-----> iter 10405\n",
      "-----> iter 10406\n",
      "-----> iter 10407\n",
      "-----> iter 10408\n",
      "-----> iter 10409\n",
      "-----> iter 10410\n",
      "-----> iter 10411\n",
      "-----> iter 10412\n",
      "-----> iter 10413\n",
      "-----> iter 10414\n",
      "-----> iter 10415\n",
      "-----> iter 10416\n",
      "-----> iter 10417\n",
      "-----> iter 10418\n",
      "-----> iter 10419\n",
      "-----> iter 10420\n",
      "-----> iter 10421\n",
      "-----> iter 10422\n",
      "-----> iter 10423\n",
      "-----> iter 10424\n",
      "-----> iter 10425\n",
      "-----> iter 10426\n",
      "-----> iter 10427\n",
      "-----> iter 10428\n",
      "-----> iter 10429\n",
      "-----> iter 10430\n",
      "-----> iter 10431\n",
      "-----> iter 10432\n",
      "-----> iter 10433\n",
      "-----> iter 10434\n",
      "-----> iter 10435\n",
      "-----> iter 10436\n",
      "-----> iter 10437\n",
      "-----> iter 10438\n",
      "-----> iter 10439\n",
      "-----> iter 10440\n",
      "-----> iter 10441\n",
      "-----> iter 10442\n",
      "-----> iter 10443\n",
      "-----> iter 10444\n",
      "-----> iter 10445\n",
      "-----> iter 10446\n",
      "-----> iter 10447\n",
      "-----> iter 10448\n",
      "-----> iter 10449\n",
      "-----> iter 10450\n",
      "-----> iter 10451\n",
      "-----> iter 10452\n",
      "-----> iter 10453\n",
      "-----> iter 10454\n",
      "-----> iter 10455\n",
      "-----> iter 10456\n",
      "-----> iter 10457\n",
      "-----> iter 10458\n",
      "-----> iter 10459\n",
      "-----> iter 10460\n",
      "-----> iter 10461\n",
      "-----> iter 10462\n",
      "-----> iter 10463\n",
      "-----> iter 10464\n",
      "-----> iter 10465\n",
      "-----> iter 10466\n",
      "-----> iter 10467\n",
      "-----> iter 10468\n",
      "-----> iter 10469\n",
      "-----> iter 10470\n",
      "-----> iter 10471\n",
      "-----> iter 10472\n",
      "-----> iter 10473\n",
      "-----> iter 10474\n",
      "-----> iter 10475\n",
      "-----> iter 10476\n",
      "-----> iter 10477\n",
      "-----> iter 10478\n",
      "-----> iter 10479\n",
      "-----> iter 10480\n",
      "-----> iter 10481\n",
      "-----> iter 10482\n",
      "-----> iter 10483\n",
      "-----> iter 10484\n",
      "-----> iter 10485\n",
      "-----> iter 10486\n",
      "-----> iter 10487\n",
      "-----> iter 10488\n",
      "-----> iter 10489\n",
      "-----> iter 10490\n",
      "-----> iter 10491\n",
      "-----> iter 10492\n",
      "-----> iter 10493\n",
      "-----> iter 10494\n",
      "-----> iter 10495\n",
      "-----> iter 10496\n",
      "-----> iter 10497\n",
      "-----> iter 10498\n",
      "-----> iter 10499\n",
      "-----> iter 10500\n",
      "-----> iter 10501\n",
      "-----> iter 10502\n",
      "-----> iter 10503\n",
      "-----> iter 10504\n",
      "-----> iter 10505\n",
      "-----> iter 10506\n",
      "-----> iter 10507\n",
      "-----> iter 10508\n",
      "-----> iter 10509\n",
      "-----> iter 10510\n",
      "-----> iter 10511\n",
      "-----> iter 10512\n",
      "-----> iter 10513\n",
      "-----> iter 10514\n",
      "-----> iter 10515\n",
      "-----> iter 10516\n",
      "-----> iter 10517\n",
      "-----> iter 10518\n",
      "-----> iter 10519\n",
      "-----> iter 10520\n",
      "-----> iter 10521\n",
      "-----> iter 10522\n",
      "-----> iter 10523\n",
      "-----> iter 10524\n",
      "-----> iter 10525\n",
      "-----> iter 10526\n",
      "-----> iter 10527\n",
      "-----> iter 10528\n",
      "-----> iter 10529\n",
      "-----> iter 10530\n",
      "-----> iter 10531\n",
      "-----> iter 10532\n",
      "-----> iter 10533\n",
      "-----> iter 10534\n",
      "-----> iter 10535\n",
      "-----> iter 10536\n",
      "-----> iter 10537\n",
      "-----> iter 10538\n",
      "-----> iter 10539\n",
      "-----> iter 10540\n",
      "-----> iter 10541\n",
      "-----> iter 10542\n",
      "-----> iter 10543\n",
      "-----> iter 10544\n",
      "-----> iter 10545\n",
      "-----> iter 10546\n",
      "-----> iter 10547\n",
      "-----> iter 10548\n",
      "-----> iter 10549\n",
      "-----> iter 10550\n",
      "-----> iter 10551\n",
      "-----> iter 10552\n",
      "-----> iter 10553\n",
      "-----> iter 10554\n",
      "-----> iter 10555\n",
      "-----> iter 10556\n",
      "-----> iter 10557\n",
      "-----> iter 10558\n",
      "-----> iter 10559\n",
      "-----> iter 10560\n",
      "-----> iter 10561\n",
      "-----> iter 10562\n",
      "-----> iter 10563\n",
      "-----> iter 10564\n",
      "-----> iter 10565\n",
      "-----> iter 10566\n",
      "-----> iter 10567\n",
      "-----> iter 10568\n",
      "-----> iter 10569\n",
      "-----> iter 10570\n",
      "-----> iter 10571\n",
      "-----> iter 10572\n",
      "-----> iter 10573\n",
      "-----> iter 10574\n",
      "-----> iter 10575\n",
      "-----> iter 10576\n",
      "-----> iter 10577\n",
      "-----> iter 10578\n",
      "-----> iter 10579\n",
      "-----> iter 10580\n",
      "-----> iter 10581\n",
      "-----> iter 10582\n",
      "-----> iter 10583\n",
      "-----> iter 10584\n",
      "-----> iter 10585\n",
      "-----> iter 10586\n",
      "-----> iter 10587\n",
      "-----> iter 10588\n",
      "-----> iter 10589\n",
      "-----> iter 10590\n",
      "-----> iter 10591\n",
      "-----> iter 10592\n",
      "-----> iter 10593\n",
      "-----> iter 10594\n",
      "-----> iter 10595\n",
      "-----> iter 10596\n",
      "-----> iter 10597\n",
      "-----> iter 10598\n",
      "-----> iter 10599\n",
      "-----> iter 10600\n",
      "-----> iter 10601\n",
      "-----> iter 10602\n",
      "-----> iter 10603\n",
      "-----> iter 10604\n",
      "-----> iter 10605\n",
      "-----> iter 10606\n",
      "-----> iter 10607\n",
      "-----> iter 10608\n",
      "-----> iter 10609\n",
      "-----> iter 10610\n",
      "-----> iter 10611\n",
      "-----> iter 10612\n",
      "-----> iter 10613\n",
      "-----> iter 10614\n",
      "-----> iter 10615\n",
      "-----> iter 10616\n",
      "-----> iter 10617\n",
      "-----> iter 10618\n",
      "-----> iter 10619\n",
      "-----> iter 10620\n",
      "-----> iter 10621\n",
      "-----> iter 10622\n",
      "-----> iter 10623\n",
      "-----> iter 10624\n",
      "-----> iter 10625\n",
      "-----> iter 10626\n",
      "-----> iter 10627\n",
      "-----> iter 10628\n",
      "-----> iter 10629\n",
      "-----> iter 10630\n",
      "-----> iter 10631\n",
      "-----> iter 10632\n",
      "-----> iter 10633\n",
      "-----> iter 10634\n",
      "-----> iter 10635\n",
      "-----> iter 10636\n",
      "-----> iter 10637\n",
      "-----> iter 10638\n",
      "-----> iter 10639\n",
      "-----> iter 10640\n",
      "-----> iter 10641\n",
      "-----> iter 10642\n",
      "-----> iter 10643\n",
      "-----> iter 10644\n",
      "-----> iter 10645\n",
      "-----> iter 10646\n",
      "-----> iter 10647\n",
      "-----> iter 10648\n",
      "-----> iter 10649\n",
      "-----> iter 10650\n",
      "-----> iter 10651\n",
      "-----> iter 10652\n",
      "-----> iter 10653\n",
      "-----> iter 10654\n",
      "-----> iter 10655\n",
      "-----> iter 10656\n",
      "-----> iter 10657\n",
      "-----> iter 10658\n",
      "-----> iter 10659\n",
      "-----> iter 10660\n",
      "-----> iter 10661\n",
      "-----> iter 10662\n",
      "-----> iter 10663\n",
      "-----> iter 10664\n",
      "-----> iter 10665\n",
      "-----> iter 10666\n",
      "-----> iter 10667\n",
      "-----> iter 10668\n",
      "-----> iter 10669\n",
      "-----> iter 10670\n",
      "-----> iter 10671\n",
      "-----> iter 10672\n",
      "-----> iter 10673\n",
      "-----> iter 10674\n",
      "-----> iter 10675\n",
      "-----> iter 10676\n",
      "-----> iter 10677\n",
      "-----> iter 10678\n",
      "-----> iter 10679\n",
      "-----> iter 10680\n",
      "-----> iter 10681\n",
      "-----> iter 10682\n",
      "-----> iter 10683\n",
      "-----> iter 10684\n",
      "-----> iter 10685\n",
      "-----> iter 10686\n",
      "-----> iter 10687\n",
      "-----> iter 10688\n",
      "-----> iter 10689\n",
      "-----> iter 10690\n",
      "-----> iter 10691\n",
      "-----> iter 10692\n",
      "-----> iter 10693\n",
      "-----> iter 10694\n",
      "-----> iter 10695\n",
      "-----> iter 10696\n",
      "-----> iter 10697\n",
      "-----> iter 10698\n",
      "-----> iter 10699\n",
      "-----> iter 10700\n",
      "-----> iter 10701\n",
      "-----> iter 10702\n",
      "-----> iter 10703\n",
      "-----> iter 10704\n",
      "-----> iter 10705\n",
      "-----> iter 10706\n",
      "-----> iter 10707\n",
      "-----> iter 10708\n",
      "-----> iter 10709\n",
      "-----> iter 10710\n",
      "-----> iter 10711\n",
      "-----> iter 10712\n",
      "-----> iter 10713\n",
      "-----> iter 10714\n",
      "-----> iter 10715\n",
      "-----> iter 10716\n",
      "-----> iter 10717\n",
      "-----> iter 10718\n",
      "-----> iter 10719\n",
      "-----> iter 10720\n",
      "-----> iter 10721\n",
      "-----> iter 10722\n",
      "-----> iter 10723\n",
      "-----> iter 10724\n",
      "-----> iter 10725\n",
      "-----> iter 10726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 10727\n",
      "-----> iter 10728\n",
      "-----> iter 10729\n",
      "-----> iter 10730\n",
      "-----> iter 10731\n",
      "-----> iter 10732\n",
      "-----> iter 10733\n",
      "-----> iter 10734\n",
      "-----> iter 10735\n",
      "-----> iter 10736\n",
      "-----> iter 10737\n",
      "-----> iter 10738\n",
      "-----> iter 10739\n",
      "-----> iter 10740\n",
      "-----> iter 10741\n",
      "-----> iter 10742\n",
      "-----> iter 10743\n",
      "-----> iter 10744\n",
      "-----> iter 10745\n",
      "-----> iter 10746\n",
      "-----> iter 10747\n",
      "-----> iter 10748\n",
      "-----> iter 10749\n",
      "-----> iter 10750\n",
      "-----> iter 10751\n",
      "-----> iter 10752\n",
      "-----> iter 10753\n",
      "-----> iter 10754\n",
      "-----> iter 10755\n",
      "-----> iter 10756\n",
      "-----> iter 10757\n",
      "-----> iter 10758\n",
      "-----> iter 10759\n",
      "-----> iter 10760\n",
      "-----> iter 10761\n",
      "-----> iter 10762\n",
      "-----> iter 10763\n",
      "-----> iter 10764\n",
      "-----> iter 10765\n",
      "-----> iter 10766\n",
      "-----> iter 10767\n",
      "-----> iter 10768\n",
      "-----> iter 10769\n",
      "-----> iter 10770\n",
      "-----> iter 10771\n",
      "-----> iter 10772\n",
      "-----> iter 10773\n",
      "-----> iter 10774\n",
      "-----> iter 10775\n",
      "-----> iter 10776\n",
      "-----> iter 10777\n",
      "-----> iter 10778\n",
      "-----> iter 10779\n",
      "-----> iter 10780\n",
      "-----> iter 10781\n",
      "-----> iter 10782\n",
      "-----> iter 10783\n",
      "-----> iter 10784\n",
      "-----> iter 10785\n",
      "-----> iter 10786\n",
      "-----> iter 10787\n",
      "-----> iter 10788\n",
      "-----> iter 10789\n",
      "-----> iter 10790\n",
      "-----> iter 10791\n",
      "-----> iter 10792\n",
      "-----> iter 10793\n",
      "-----> iter 10794\n",
      "-----> iter 10795\n",
      "-----> iter 10796\n",
      "-----> iter 10797\n",
      "-----> iter 10798\n",
      "-----> iter 10799\n",
      "-----> iter 10800\n",
      "-----> iter 10801\n",
      "-----> iter 10802\n",
      "-----> iter 10803\n",
      "-----> iter 10804\n",
      "-----> iter 10805\n",
      "-----> iter 10806\n",
      "-----> iter 10807\n",
      "-----> iter 10808\n",
      "-----> iter 10809\n",
      "-----> iter 10810\n",
      "-----> iter 10811\n",
      "-----> iter 10812\n",
      "-----> iter 10813\n",
      "-----> iter 10814\n",
      "-----> iter 10815\n",
      "-----> iter 10816\n",
      "-----> iter 10817\n",
      "-----> iter 10818\n",
      "-----> iter 10819\n",
      "-----> iter 10820\n",
      "-----> iter 10821\n",
      "-----> iter 10822\n",
      "-----> iter 10823\n",
      "-----> iter 10824\n",
      "-----> iter 10825\n",
      "-----> iter 10826\n",
      "-----> iter 10827\n",
      "-----> iter 10828\n",
      "-----> iter 10829\n",
      "-----> iter 10830\n",
      "-----> iter 10831\n",
      "-----> iter 10832\n",
      "-----> iter 10833\n",
      "-----> iter 10834\n",
      "-----> iter 10835\n",
      "-----> iter 10836\n",
      "-----> iter 10837\n",
      "-----> iter 10838\n",
      "-----> iter 10839\n",
      "-----> iter 10840\n",
      "-----> iter 10841\n",
      "-----> iter 10842\n",
      "-----> iter 10843\n",
      "-----> iter 10844\n",
      "-----> iter 10845\n",
      "-----> iter 10846\n",
      "-----> iter 10847\n",
      "-----> iter 10848\n",
      "-----> iter 10849\n",
      "-----> iter 10850\n",
      "-----> iter 10851\n",
      "-----> iter 10852\n",
      "-----> iter 10853\n",
      "-----> iter 10854\n",
      "-----> iter 10855\n",
      "-----> iter 10856\n",
      "-----> iter 10857\n",
      "-----> iter 10858\n",
      "-----> iter 10859\n",
      "-----> iter 10860\n",
      "-----> iter 10861\n",
      "-----> iter 10862\n",
      "-----> iter 10863\n",
      "-----> iter 10864\n",
      "-----> iter 10865\n",
      "-----> iter 10866\n",
      "-----> iter 10867\n",
      "-----> iter 10868\n",
      "-----> iter 10869\n",
      "-----> iter 10870\n",
      "-----> iter 10871\n",
      "-----> iter 10872\n",
      "-----> iter 10873\n",
      "-----> iter 10874\n",
      "-----> iter 10875\n",
      "-----> iter 10876\n",
      "-----> iter 10877\n",
      "-----> iter 10878\n",
      "-----> iter 10879\n",
      "-----> iter 10880\n",
      "-----> iter 10881\n",
      "-----> iter 10882\n",
      "-----> iter 10883\n",
      "-----> iter 10884\n",
      "-----> iter 10885\n",
      "-----> iter 10886\n",
      "-----> iter 10887\n",
      "-----> iter 10888\n",
      "-----> iter 10889\n",
      "-----> iter 10890\n",
      "-----> iter 10891\n",
      "-----> iter 10892\n",
      "-----> iter 10893\n",
      "-----> iter 10894\n",
      "-----> iter 10895\n",
      "-----> iter 10896\n",
      "-----> iter 10897\n",
      "-----> iter 10898\n",
      "-----> iter 10899\n",
      "-----> iter 10900\n",
      "-----> iter 10901\n",
      "-----> iter 10902\n",
      "-----> iter 10903\n",
      "-----> iter 10904\n",
      "-----> iter 10905\n",
      "-----> iter 10906\n",
      "-----> iter 10907\n",
      "-----> iter 10908\n",
      "-----> iter 10909\n",
      "-----> iter 10910\n",
      "-----> iter 10911\n",
      "-----> iter 10912\n",
      "-----> iter 10913\n",
      "-----> iter 10914\n",
      "-----> iter 10915\n",
      "-----> iter 10916\n",
      "-----> iter 10917\n",
      "-----> iter 10918\n",
      "-----> iter 10919\n",
      "-----> iter 10920\n",
      "-----> iter 10921\n",
      "-----> iter 10922\n",
      "-----> iter 10923\n",
      "-----> iter 10924\n",
      "-----> iter 10925\n",
      "-----> iter 10926\n",
      "-----> iter 10927\n",
      "-----> iter 10928\n",
      "-----> iter 10929\n",
      "-----> iter 10930\n",
      "-----> iter 10931\n",
      "-----> iter 10932\n",
      "-----> iter 10933\n",
      "-----> iter 10934\n",
      "-----> iter 10935\n",
      "-----> iter 10936\n",
      "-----> iter 10937\n",
      "-----> iter 10938\n",
      "-----> iter 10939\n",
      "-----> iter 10940\n",
      "-----> iter 10941\n",
      "-----> iter 10942\n",
      "-----> iter 10943\n",
      "-----> iter 10944\n",
      "-----> iter 10945\n",
      "-----> iter 10946\n",
      "-----> iter 10947\n",
      "-----> iter 10948\n",
      "-----> iter 10949\n",
      "-----> iter 10950\n",
      "-----> iter 10951\n",
      "-----> iter 10952\n",
      "-----> iter 10953\n",
      "-----> iter 10954\n",
      "-----> iter 10955\n",
      "-----> iter 10956\n",
      "-----> iter 10957\n",
      "-----> iter 10958\n",
      "-----> iter 10959\n",
      "-----> iter 10960\n",
      "-----> iter 10961\n",
      "-----> iter 10962\n",
      "-----> iter 10963\n",
      "-----> iter 10964\n",
      "-----> iter 10965\n",
      "-----> iter 10966\n",
      "-----> iter 10967\n",
      "-----> iter 10968\n",
      "-----> iter 10969\n",
      "-----> iter 10970\n",
      "-----> iter 10971\n",
      "-----> iter 10972\n",
      "-----> iter 10973\n",
      "-----> iter 10974\n",
      "-----> iter 10975\n",
      "-----> iter 10976\n",
      "-----> iter 10977\n",
      "-----> iter 10978\n",
      "-----> iter 10979\n",
      "-----> iter 10980\n",
      "-----> iter 10981\n",
      "-----> iter 10982\n",
      "-----> iter 10983\n",
      "-----> iter 10984\n",
      "-----> iter 10985\n",
      "-----> iter 10986\n",
      "-----> iter 10987\n",
      "-----> iter 10988\n",
      "-----> iter 10989\n",
      "-----> iter 10990\n",
      "-----> iter 10991\n",
      "-----> iter 10992\n",
      "-----> iter 10993\n",
      "-----> iter 10994\n",
      "-----> iter 10995\n",
      "-----> iter 10996\n",
      "-----> iter 10997\n",
      "-----> iter 10998\n",
      "-----> iter 10999\n",
      "-----> iter 11000\n",
      "-----> iter 11001\n",
      "-----> iter 11002\n",
      "-----> iter 11003\n",
      "-----> iter 11004\n",
      "-----> iter 11005\n",
      "-----> iter 11006\n",
      "-----> iter 11007\n",
      "-----> iter 11008\n",
      "-----> iter 11009\n",
      "-----> iter 11010\n",
      "-----> iter 11011\n",
      "-----> iter 11012\n",
      "-----> iter 11013\n",
      "-----> iter 11014\n",
      "-----> iter 11015\n",
      "-----> iter 11016\n",
      "-----> iter 11017\n",
      "-----> iter 11018\n",
      "-----> iter 11019\n",
      "-----> iter 11020\n",
      "-----> iter 11021\n",
      "-----> iter 11022\n",
      "-----> iter 11023\n",
      "-----> iter 11024\n",
      "-----> iter 11025\n",
      "-----> iter 11026\n",
      "-----> iter 11027\n",
      "-----> iter 11028\n",
      "-----> iter 11029\n",
      "-----> iter 11030\n",
      "-----> iter 11031\n",
      "-----> iter 11032\n",
      "-----> iter 11033\n",
      "-----> iter 11034\n",
      "-----> iter 11035\n",
      "-----> iter 11036\n",
      "-----> iter 11037\n",
      "-----> iter 11038\n",
      "-----> iter 11039\n",
      "-----> iter 11040\n",
      "-----> iter 11041\n",
      "-----> iter 11042\n",
      "-----> iter 11043\n",
      "-----> iter 11044\n",
      "-----> iter 11045\n",
      "-----> iter 11046\n",
      "-----> iter 11047\n",
      "-----> iter 11048\n",
      "-----> iter 11049\n",
      "-----> iter 11050\n",
      "-----> iter 11051\n",
      "-----> iter 11052\n",
      "-----> iter 11053\n",
      "-----> iter 11054\n",
      "-----> iter 11055\n",
      "-----> iter 11056\n",
      "-----> iter 11057\n",
      "-----> iter 11058\n",
      "-----> iter 11059\n",
      "-----> iter 11060\n",
      "-----> iter 11061\n",
      "-----> iter 11062\n",
      "-----> iter 11063\n",
      "-----> iter 11064\n",
      "-----> iter 11065\n",
      "-----> iter 11066\n",
      "-----> iter 11067\n",
      "-----> iter 11068\n",
      "-----> iter 11069\n",
      "-----> iter 11070\n",
      "-----> iter 11071\n",
      "-----> iter 11072\n",
      "-----> iter 11073\n",
      "-----> iter 11074\n",
      "-----> iter 11075\n",
      "-----> iter 11076\n",
      "-----> iter 11077\n",
      "-----> iter 11078\n",
      "-----> iter 11079\n",
      "-----> iter 11080\n",
      "-----> iter 11081\n",
      "-----> iter 11082\n",
      "-----> iter 11083\n",
      "-----> iter 11084\n",
      "-----> iter 11085\n",
      "-----> iter 11086\n",
      "-----> iter 11087\n",
      "-----> iter 11088\n",
      "-----> iter 11089\n",
      "-----> iter 11090\n",
      "-----> iter 11091\n",
      "-----> iter 11092\n",
      "-----> iter 11093\n",
      "-----> iter 11094\n",
      "-----> iter 11095\n",
      "-----> iter 11096\n",
      "-----> iter 11097\n",
      "-----> iter 11098\n",
      "-----> iter 11099\n",
      "-----> iter 11100\n",
      "-----> iter 11101\n",
      "-----> iter 11102\n",
      "-----> iter 11103\n",
      "-----> iter 11104\n",
      "-----> iter 11105\n",
      "-----> iter 11106\n",
      "-----> iter 11107\n",
      "-----> iter 11108\n",
      "-----> iter 11109\n",
      "-----> iter 11110\n",
      "-----> iter 11111\n",
      "-----> iter 11112\n",
      "-----> iter 11113\n",
      "-----> iter 11114\n",
      "-----> iter 11115\n",
      "-----> iter 11116\n",
      "-----> iter 11117\n",
      "-----> iter 11118\n",
      "-----> iter 11119\n",
      "-----> iter 11120\n",
      "-----> iter 11121\n",
      "-----> iter 11122\n",
      "-----> iter 11123\n",
      "-----> iter 11124\n",
      "-----> iter 11125\n",
      "-----> iter 11126\n",
      "-----> iter 11127\n",
      "-----> iter 11128\n",
      "-----> iter 11129\n",
      "-----> iter 11130\n",
      "-----> iter 11131\n",
      "-----> iter 11132\n",
      "-----> iter 11133\n",
      "-----> iter 11134\n",
      "-----> iter 11135\n",
      "-----> iter 11136\n",
      "-----> iter 11137\n",
      "-----> iter 11138\n",
      "-----> iter 11139\n",
      "-----> iter 11140\n",
      "-----> iter 11141\n",
      "-----> iter 11142\n",
      "-----> iter 11143\n",
      "-----> iter 11144\n",
      "-----> iter 11145\n",
      "-----> iter 11146\n",
      "-----> iter 11147\n",
      "-----> iter 11148\n",
      "-----> iter 11149\n",
      "-----> iter 11150\n",
      "-----> iter 11151\n",
      "-----> iter 11152\n",
      "-----> iter 11153\n",
      "-----> iter 11154\n",
      "-----> iter 11155\n",
      "-----> iter 11156\n",
      "-----> iter 11157\n",
      "-----> iter 11158\n",
      "-----> iter 11159\n",
      "-----> iter 11160\n",
      "-----> iter 11161\n",
      "-----> iter 11162\n",
      "-----> iter 11163\n",
      "-----> iter 11164\n",
      "-----> iter 11165\n",
      "-----> iter 11166\n",
      "-----> iter 11167\n",
      "-----> iter 11168\n",
      "-----> iter 11169\n",
      "-----> iter 11170\n",
      "-----> iter 11171\n",
      "-----> iter 11172\n",
      "-----> iter 11173\n",
      "-----> iter 11174\n",
      "-----> iter 11175\n",
      "-----> iter 11176\n",
      "-----> iter 11177\n",
      "-----> iter 11178\n",
      "-----> iter 11179\n",
      "-----> iter 11180\n",
      "-----> iter 11181\n",
      "-----> iter 11182\n",
      "-----> iter 11183\n",
      "-----> iter 11184\n",
      "-----> iter 11185\n",
      "-----> iter 11186\n",
      "-----> iter 11187\n",
      "-----> iter 11188\n",
      "-----> iter 11189\n",
      "-----> iter 11190\n",
      "-----> iter 11191\n",
      "-----> iter 11192\n",
      "-----> iter 11193\n",
      "-----> iter 11194\n",
      "-----> iter 11195\n",
      "-----> iter 11196\n",
      "-----> iter 11197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 11198\n",
      "-----> iter 11199\n",
      "-----> iter 11200\n",
      "-----> iter 11201\n",
      "-----> iter 11202\n",
      "-----> iter 11203\n",
      "-----> iter 11204\n",
      "-----> iter 11205\n",
      "-----> iter 11206\n",
      "-----> iter 11207\n",
      "-----> iter 11208\n",
      "-----> iter 11209\n",
      "-----> iter 11210\n",
      "-----> iter 11211\n",
      "-----> iter 11212\n",
      "-----> iter 11213\n",
      "-----> iter 11214\n",
      "-----> iter 11215\n",
      "-----> iter 11216\n",
      "-----> iter 11217\n",
      "-----> iter 11218\n",
      "-----> iter 11219\n",
      "-----> iter 11220\n",
      "-----> iter 11221\n",
      "-----> iter 11222\n",
      "-----> iter 11223\n",
      "-----> iter 11224\n",
      "-----> iter 11225\n",
      "-----> iter 11226\n",
      "-----> iter 11227\n",
      "-----> iter 11228\n",
      "-----> iter 11229\n",
      "-----> iter 11230\n",
      "-----> iter 11231\n",
      "-----> iter 11232\n",
      "-----> iter 11233\n",
      "-----> iter 11234\n",
      "-----> iter 11235\n",
      "-----> iter 11236\n",
      "-----> iter 11237\n",
      "-----> iter 11238\n",
      "-----> iter 11239\n",
      "-----> iter 11240\n",
      "-----> iter 11241\n",
      "-----> iter 11242\n",
      "-----> iter 11243\n",
      "-----> iter 11244\n",
      "-----> iter 11245\n",
      "-----> iter 11246\n",
      "-----> iter 11247\n",
      "-----> iter 11248\n",
      "-----> iter 11249\n",
      "-----> iter 11250\n",
      "-----> iter 11251\n",
      "-----> iter 11252\n",
      "-----> iter 11253\n",
      "-----> iter 11254\n",
      "-----> iter 11255\n",
      "-----> iter 11256\n",
      "-----> iter 11257\n",
      "-----> iter 11258\n",
      "-----> iter 11259\n",
      "-----> iter 11260\n",
      "-----> iter 11261\n",
      "-----> iter 11262\n",
      "-----> iter 11263\n",
      "-----> iter 11264\n",
      "-----> iter 11265\n",
      "-----> iter 11266\n",
      "-----> iter 11267\n",
      "-----> iter 11268\n",
      "-----> iter 11269\n",
      "-----> iter 11270\n",
      "-----> iter 11271\n",
      "-----> iter 11272\n",
      "-----> iter 11273\n",
      "-----> iter 11274\n",
      "-----> iter 11275\n",
      "-----> iter 11276\n",
      "-----> iter 11277\n",
      "-----> iter 11278\n",
      "-----> iter 11279\n",
      "-----> iter 11280\n",
      "-----> iter 11281\n",
      "-----> iter 11282\n",
      "-----> iter 11283\n",
      "-----> iter 11284\n",
      "-----> iter 11285\n",
      "-----> iter 11286\n",
      "-----> iter 11287\n",
      "-----> iter 11288\n",
      "-----> iter 11289\n",
      "-----> iter 11290\n",
      "-----> iter 11291\n",
      "-----> iter 11292\n",
      "-----> iter 11293\n",
      "-----> iter 11294\n",
      "-----> iter 11295\n",
      "-----> iter 11296\n",
      "-----> iter 11297\n",
      "-----> iter 11298\n",
      "-----> iter 11299\n",
      "-----> iter 11300\n",
      "-----> iter 11301\n",
      "-----> iter 11302\n",
      "-----> iter 11303\n",
      "-----> iter 11304\n",
      "-----> iter 11305\n",
      "-----> iter 11306\n",
      "-----> iter 11307\n",
      "-----> iter 11308\n",
      "-----> iter 11309\n",
      "-----> iter 11310\n",
      "-----> iter 11311\n",
      "-----> iter 11312\n",
      "-----> iter 11313\n",
      "-----> iter 11314\n",
      "-----> iter 11315\n",
      "-----> iter 11316\n",
      "-----> iter 11317\n",
      "-----> iter 11318\n",
      "-----> iter 11319\n",
      "-----> iter 11320\n",
      "-----> iter 11321\n",
      "-----> iter 11322\n",
      "-----> iter 11323\n",
      "-----> iter 11324\n",
      "-----> iter 11325\n",
      "-----> iter 11326\n",
      "-----> iter 11327\n",
      "-----> iter 11328\n",
      "-----> iter 11329\n",
      "-----> iter 11330\n",
      "-----> iter 11331\n",
      "-----> iter 11332\n",
      "-----> iter 11333\n",
      "-----> iter 11334\n",
      "-----> iter 11335\n",
      "-----> iter 11336\n",
      "-----> iter 11337\n",
      "-----> iter 11338\n",
      "-----> iter 11339\n",
      "-----> iter 11340\n",
      "-----> iter 11341\n",
      "-----> iter 11342\n",
      "-----> iter 11343\n",
      "-----> iter 11344\n",
      "-----> iter 11345\n",
      "-----> iter 11346\n",
      "-----> iter 11347\n",
      "-----> iter 11348\n",
      "-----> iter 11349\n",
      "-----> iter 11350\n",
      "-----> iter 11351\n",
      "-----> iter 11352\n",
      "-----> iter 11353\n",
      "-----> iter 11354\n",
      "-----> iter 11355\n",
      "-----> iter 11356\n",
      "-----> iter 11357\n",
      "-----> iter 11358\n",
      "-----> iter 11359\n",
      "-----> iter 11360\n",
      "-----> iter 11361\n",
      "-----> iter 11362\n",
      "-----> iter 11363\n",
      "-----> iter 11364\n",
      "-----> iter 11365\n",
      "-----> iter 11366\n",
      "-----> iter 11367\n",
      "-----> iter 11368\n",
      "-----> iter 11369\n",
      "-----> iter 11370\n",
      "-----> iter 11371\n",
      "-----> iter 11372\n",
      "-----> iter 11373\n",
      "-----> iter 11374\n",
      "-----> iter 11375\n",
      "-----> iter 11376\n",
      "-----> iter 11377\n",
      "-----> iter 11378\n",
      "-----> iter 11379\n",
      "-----> iter 11380\n",
      "-----> iter 11381\n",
      "-----> iter 11382\n",
      "-----> iter 11383\n",
      "-----> iter 11384\n",
      "-----> iter 11385\n",
      "-----> iter 11386\n",
      "-----> iter 11387\n",
      "-----> iter 11388\n",
      "-----> iter 11389\n",
      "-----> iter 11390\n",
      "-----> iter 11391\n",
      "-----> iter 11392\n",
      "-----> iter 11393\n",
      "-----> iter 11394\n",
      "-----> iter 11395\n",
      "-----> iter 11396\n",
      "-----> iter 11397\n",
      "-----> iter 11398\n",
      "-----> iter 11399\n",
      "-----> iter 11400\n",
      "-----> iter 11401\n",
      "-----> iter 11402\n",
      "-----> iter 11403\n",
      "-----> iter 11404\n",
      "-----> iter 11405\n",
      "-----> iter 11406\n",
      "-----> iter 11407\n",
      "-----> iter 11408\n",
      "-----> iter 11409\n",
      "-----> iter 11410\n",
      "-----> iter 11411\n",
      "-----> iter 11412\n",
      "-----> iter 11413\n",
      "-----> iter 11414\n",
      "-----> iter 11415\n",
      "-----> iter 11416\n",
      "-----> iter 11417\n",
      "-----> iter 11418\n",
      "-----> iter 11419\n",
      "-----> iter 11420\n",
      "-----> iter 11421\n",
      "-----> iter 11422\n",
      "-----> iter 11423\n",
      "-----> iter 11424\n",
      "-----> iter 11425\n",
      "-----> iter 11426\n",
      "-----> iter 11427\n",
      "-----> iter 11428\n",
      "-----> iter 11429\n",
      "-----> iter 11430\n",
      "-----> iter 11431\n",
      "-----> iter 11432\n",
      "-----> iter 11433\n",
      "-----> iter 11434\n",
      "-----> iter 11435\n",
      "-----> iter 11436\n",
      "-----> iter 11437\n",
      "-----> iter 11438\n",
      "-----> iter 11439\n",
      "-----> iter 11440\n",
      "-----> iter 11441\n",
      "-----> iter 11442\n",
      "-----> iter 11443\n",
      "-----> iter 11444\n",
      "-----> iter 11445\n",
      "-----> iter 11446\n",
      "-----> iter 11447\n",
      "-----> iter 11448\n",
      "-----> iter 11449\n",
      "-----> iter 11450\n",
      "-----> iter 11451\n",
      "-----> iter 11452\n",
      "-----> iter 11453\n",
      "-----> iter 11454\n",
      "-----> iter 11455\n",
      "-----> iter 11456\n",
      "-----> iter 11457\n",
      "-----> iter 11458\n",
      "-----> iter 11459\n",
      "-----> iter 11460\n",
      "-----> iter 11461\n",
      "-----> iter 11462\n",
      "-----> iter 11463\n",
      "-----> iter 11464\n",
      "-----> iter 11465\n",
      "-----> iter 11466\n",
      "-----> iter 11467\n",
      "-----> iter 11468\n",
      "-----> iter 11469\n",
      "-----> iter 11470\n",
      "-----> iter 11471\n",
      "-----> iter 11472\n",
      "-----> iter 11473\n",
      "-----> iter 11474\n",
      "-----> iter 11475\n",
      "-----> iter 11476\n",
      "-----> iter 11477\n",
      "-----> iter 11478\n",
      "-----> iter 11479\n",
      "-----> iter 11480\n",
      "-----> iter 11481\n",
      "-----> iter 11482\n",
      "-----> iter 11483\n",
      "-----> iter 11484\n",
      "-----> iter 11485\n",
      "-----> iter 11486\n",
      "-----> iter 11487\n",
      "-----> iter 11488\n",
      "-----> iter 11489\n",
      "-----> iter 11490\n",
      "-----> iter 11491\n",
      "-----> iter 11492\n",
      "-----> iter 11493\n",
      "-----> iter 11494\n",
      "-----> iter 11495\n",
      "-----> iter 11496\n",
      "-----> iter 11497\n",
      "-----> iter 11498\n",
      "-----> iter 11499\n",
      "-----> iter 11500\n",
      "-----> iter 11501\n",
      "-----> iter 11502\n",
      "-----> iter 11503\n",
      "-----> iter 11504\n",
      "-----> iter 11505\n",
      "-----> iter 11506\n",
      "-----> iter 11507\n",
      "-----> iter 11508\n",
      "-----> iter 11509\n",
      "-----> iter 11510\n",
      "-----> iter 11511\n",
      "-----> iter 11512\n",
      "-----> iter 11513\n",
      "-----> iter 11514\n",
      "-----> iter 11515\n",
      "-----> iter 11516\n",
      "-----> iter 11517\n",
      "-----> iter 11518\n",
      "-----> iter 11519\n",
      "-----> iter 11520\n",
      "-----> iter 11521\n",
      "-----> iter 11522\n",
      "-----> iter 11523\n",
      "-----> iter 11524\n",
      "-----> iter 11525\n",
      "-----> iter 11526\n",
      "-----> iter 11527\n",
      "-----> iter 11528\n",
      "-----> iter 11529\n",
      "-----> iter 11530\n",
      "-----> iter 11531\n",
      "-----> iter 11532\n",
      "-----> iter 11533\n",
      "-----> iter 11534\n",
      "-----> iter 11535\n",
      "-----> iter 11536\n",
      "-----> iter 11537\n",
      "-----> iter 11538\n",
      "-----> iter 11539\n",
      "-----> iter 11540\n",
      "-----> iter 11541\n",
      "-----> iter 11542\n",
      "-----> iter 11543\n",
      "-----> iter 11544\n",
      "-----> iter 11545\n",
      "-----> iter 11546\n",
      "-----> iter 11547\n",
      "-----> iter 11548\n",
      "-----> iter 11549\n",
      "-----> iter 11550\n",
      "-----> iter 11551\n",
      "-----> iter 11552\n",
      "-----> iter 11553\n",
      "-----> iter 11554\n",
      "-----> iter 11555\n",
      "-----> iter 11556\n",
      "-----> iter 11557\n",
      "-----> iter 11558\n",
      "-----> iter 11559\n",
      "-----> iter 11560\n",
      "-----> iter 11561\n",
      "-----> iter 11562\n",
      "-----> iter 11563\n",
      "-----> iter 11564\n",
      "-----> iter 11565\n",
      "-----> iter 11566\n",
      "-----> iter 11567\n",
      "-----> iter 11568\n",
      "-----> iter 11569\n",
      "-----> iter 11570\n",
      "-----> iter 11571\n",
      "-----> iter 11572\n",
      "-----> iter 11573\n",
      "-----> iter 11574\n",
      "-----> iter 11575\n",
      "-----> iter 11576\n",
      "-----> iter 11577\n",
      "-----> iter 11578\n",
      "-----> iter 11579\n",
      "-----> iter 11580\n",
      "-----> iter 11581\n",
      "-----> iter 11582\n",
      "-----> iter 11583\n",
      "-----> iter 11584\n",
      "-----> iter 11585\n",
      "-----> iter 11586\n",
      "-----> iter 11587\n",
      "-----> iter 11588\n",
      "-----> iter 11589\n",
      "-----> iter 11590\n",
      "-----> iter 11591\n",
      "-----> iter 11592\n",
      "-----> iter 11593\n",
      "-----> iter 11594\n",
      "-----> iter 11595\n",
      "-----> iter 11596\n",
      "-----> iter 11597\n",
      "-----> iter 11598\n",
      "-----> iter 11599\n",
      "-----> iter 11600\n",
      "-----> iter 11601\n",
      "-----> iter 11602\n",
      "-----> iter 11603\n",
      "-----> iter 11604\n",
      "-----> iter 11605\n",
      "-----> iter 11606\n",
      "-----> iter 11607\n",
      "-----> iter 11608\n",
      "-----> iter 11609\n",
      "-----> iter 11610\n",
      "-----> iter 11611\n",
      "-----> iter 11612\n",
      "-----> iter 11613\n",
      "-----> iter 11614\n",
      "-----> iter 11615\n",
      "-----> iter 11616\n",
      "-----> iter 11617\n",
      "-----> iter 11618\n",
      "-----> iter 11619\n",
      "-----> iter 11620\n",
      "-----> iter 11621\n",
      "-----> iter 11622\n",
      "-----> iter 11623\n",
      "-----> iter 11624\n",
      "-----> iter 11625\n",
      "-----> iter 11626\n",
      "-----> iter 11627\n",
      "-----> iter 11628\n",
      "-----> iter 11629\n",
      "-----> iter 11630\n",
      "-----> iter 11631\n",
      "-----> iter 11632\n",
      "-----> iter 11633\n",
      "-----> iter 11634\n",
      "-----> iter 11635\n",
      "-----> iter 11636\n",
      "-----> iter 11637\n",
      "-----> iter 11638\n",
      "-----> iter 11639\n",
      "-----> iter 11640\n",
      "-----> iter 11641\n",
      "-----> iter 11642\n",
      "-----> iter 11643\n",
      "-----> iter 11644\n",
      "-----> iter 11645\n",
      "-----> iter 11646\n",
      "-----> iter 11647\n",
      "-----> iter 11648\n",
      "-----> iter 11649\n",
      "-----> iter 11650\n",
      "-----> iter 11651\n",
      "-----> iter 11652\n",
      "-----> iter 11653\n",
      "-----> iter 11654\n",
      "-----> iter 11655\n",
      "-----> iter 11656\n",
      "-----> iter 11657\n",
      "-----> iter 11658\n",
      "-----> iter 11659\n",
      "-----> iter 11660\n",
      "-----> iter 11661\n",
      "-----> iter 11662\n",
      "-----> iter 11663\n",
      "-----> iter 11664\n",
      "-----> iter 11665\n",
      "-----> iter 11666\n",
      "-----> iter 11667\n",
      "-----> iter 11668\n",
      "-----> iter 11669\n",
      "-----> iter 11670\n",
      "-----> iter 11671\n",
      "-----> iter 11672\n",
      "-----> iter 11673\n",
      "-----> iter 11674\n",
      "-----> iter 11675\n",
      "-----> iter 11676\n",
      "-----> iter 11677\n",
      "-----> iter 11678\n",
      "-----> iter 11679\n",
      "-----> iter 11680\n",
      "-----> iter 11681\n",
      "-----> iter 11682\n",
      "-----> iter 11683\n",
      "-----> iter 11684\n",
      "-----> iter 11685\n",
      "-----> iter 11686\n",
      "-----> iter 11687\n",
      "-----> iter 11688\n",
      "-----> iter 11689\n",
      "-----> iter 11690\n",
      "-----> iter 11691\n",
      "-----> iter 11692\n",
      "-----> iter 11693\n",
      "-----> iter 11694\n",
      "-----> iter 11695\n",
      "-----> iter 11696\n",
      "-----> iter 11697\n",
      "-----> iter 11698\n",
      "-----> iter 11699\n",
      "-----> iter 11700\n",
      "-----> iter 11701\n",
      "-----> iter 11702\n",
      "-----> iter 11703\n",
      "-----> iter 11704\n",
      "-----> iter 11705\n",
      "-----> iter 11706\n",
      "-----> iter 11707\n",
      "-----> iter 11708\n",
      "-----> iter 11709\n",
      "-----> iter 11710\n",
      "-----> iter 11711\n",
      "-----> iter 11712\n",
      "-----> iter 11713\n",
      "-----> iter 11714\n",
      "-----> iter 11715\n",
      "-----> iter 11716\n",
      "-----> iter 11717\n",
      "-----> iter 11718\n",
      "-----> iter 11719\n",
      "-----> iter 11720\n",
      "-----> iter 11721\n",
      "-----> iter 11722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 11723\n",
      "-----> iter 11724\n",
      "-----> iter 11725\n",
      "-----> iter 11726\n",
      "-----> iter 11727\n",
      "-----> iter 11728\n",
      "-----> iter 11729\n",
      "-----> iter 11730\n",
      "-----> iter 11731\n",
      "-----> iter 11732\n",
      "-----> iter 11733\n",
      "-----> iter 11734\n",
      "-----> iter 11735\n",
      "-----> iter 11736\n",
      "-----> iter 11737\n",
      "-----> iter 11738\n",
      "-----> iter 11739\n",
      "-----> iter 11740\n",
      "-----> iter 11741\n",
      "-----> iter 11742\n",
      "-----> iter 11743\n",
      "-----> iter 11744\n",
      "-----> iter 11745\n",
      "-----> iter 11746\n",
      "-----> iter 11747\n",
      "-----> iter 11748\n",
      "-----> iter 11749\n",
      "-----> iter 11750\n",
      "-----> iter 11751\n",
      "-----> iter 11752\n",
      "-----> iter 11753\n",
      "-----> iter 11754\n",
      "-----> iter 11755\n",
      "-----> iter 11756\n",
      "-----> iter 11757\n",
      "-----> iter 11758\n",
      "-----> iter 11759\n",
      "-----> iter 11760\n",
      "-----> iter 11761\n",
      "-----> iter 11762\n",
      "-----> iter 11763\n",
      "-----> iter 11764\n",
      "-----> iter 11765\n",
      "-----> iter 11766\n",
      "-----> iter 11767\n",
      "-----> iter 11768\n",
      "-----> iter 11769\n",
      "-----> iter 11770\n",
      "-----> iter 11771\n",
      "-----> iter 11772\n",
      "-----> iter 11773\n",
      "-----> iter 11774\n",
      "-----> iter 11775\n",
      "-----> iter 11776\n",
      "-----> iter 11777\n",
      "-----> iter 11778\n",
      "-----> iter 11779\n",
      "-----> iter 11780\n",
      "-----> iter 11781\n",
      "-----> iter 11782\n",
      "-----> iter 11783\n",
      "-----> iter 11784\n",
      "-----> iter 11785\n",
      "-----> iter 11786\n",
      "-----> iter 11787\n",
      "-----> iter 11788\n",
      "-----> iter 11789\n",
      "-----> iter 11790\n",
      "-----> iter 11791\n",
      "-----> iter 11792\n",
      "-----> iter 11793\n",
      "-----> iter 11794\n",
      "-----> iter 11795\n",
      "-----> iter 11796\n",
      "-----> iter 11797\n",
      "-----> iter 11798\n",
      "-----> iter 11799\n",
      "-----> iter 11800\n",
      "-----> iter 11801\n",
      "-----> iter 11802\n",
      "-----> iter 11803\n",
      "-----> iter 11804\n",
      "-----> iter 11805\n",
      "-----> iter 11806\n",
      "-----> iter 11807\n",
      "-----> iter 11808\n",
      "-----> iter 11809\n",
      "-----> iter 11810\n",
      "-----> iter 11811\n",
      "-----> iter 11812\n",
      "-----> iter 11813\n",
      "-----> iter 11814\n",
      "-----> iter 11815\n",
      "-----> iter 11816\n",
      "-----> iter 11817\n",
      "-----> iter 11818\n",
      "-----> iter 11819\n",
      "-----> iter 11820\n",
      "-----> iter 11821\n",
      "-----> iter 11822\n",
      "-----> iter 11823\n",
      "-----> iter 11824\n",
      "-----> iter 11825\n",
      "-----> iter 11826\n",
      "-----> iter 11827\n",
      "-----> iter 11828\n",
      "-----> iter 11829\n",
      "-----> iter 11830\n",
      "-----> iter 11831\n",
      "-----> iter 11832\n",
      "-----> iter 11833\n",
      "-----> iter 11834\n",
      "-----> iter 11835\n",
      "-----> iter 11836\n",
      "-----> iter 11837\n",
      "-----> iter 11838\n",
      "-----> iter 11839\n",
      "-----> iter 11840\n",
      "-----> iter 11841\n",
      "-----> iter 11842\n",
      "-----> iter 11843\n",
      "-----> iter 11844\n",
      "-----> iter 11845\n",
      "-----> iter 11846\n",
      "-----> iter 11847\n",
      "-----> iter 11848\n",
      "-----> iter 11849\n",
      "-----> iter 11850\n",
      "-----> iter 11851\n",
      "-----> iter 11852\n",
      "-----> iter 11853\n",
      "-----> iter 11854\n",
      "-----> iter 11855\n",
      "-----> iter 11856\n",
      "-----> iter 11857\n",
      "-----> iter 11858\n",
      "-----> iter 11859\n",
      "-----> iter 11860\n",
      "-----> iter 11861\n",
      "-----> iter 11862\n",
      "-----> iter 11863\n",
      "-----> iter 11864\n",
      "-----> iter 11865\n",
      "-----> iter 11866\n",
      "-----> iter 11867\n",
      "-----> iter 11868\n",
      "-----> iter 11869\n",
      "-----> iter 11870\n",
      "-----> iter 11871\n",
      "-----> iter 11872\n",
      "-----> iter 11873\n",
      "-----> iter 11874\n",
      "-----> iter 11875\n",
      "-----> iter 11876\n",
      "-----> iter 11877\n",
      "-----> iter 11878\n",
      "-----> iter 11879\n",
      "-----> iter 11880\n",
      "-----> iter 11881\n",
      "-----> iter 11882\n",
      "-----> iter 11883\n",
      "-----> iter 11884\n",
      "-----> iter 11885\n",
      "-----> iter 11886\n",
      "-----> iter 11887\n",
      "-----> iter 11888\n",
      "-----> iter 11889\n",
      "-----> iter 11890\n",
      "-----> iter 11891\n",
      "-----> iter 11892\n",
      "-----> iter 11893\n",
      "-----> iter 11894\n",
      "-----> iter 11895\n",
      "-----> iter 11896\n",
      "-----> iter 11897\n",
      "-----> iter 11898\n",
      "-----> iter 11899\n",
      "-----> iter 11900\n",
      "-----> iter 11901\n",
      "-----> iter 11902\n",
      "-----> iter 11903\n",
      "-----> iter 11904\n",
      "-----> iter 11905\n",
      "-----> iter 11906\n",
      "-----> iter 11907\n",
      "-----> iter 11908\n",
      "-----> iter 11909\n",
      "-----> iter 11910\n",
      "-----> iter 11911\n",
      "-----> iter 11912\n",
      "-----> iter 11913\n",
      "-----> iter 11914\n",
      "-----> iter 11915\n",
      "-----> iter 11916\n",
      "-----> iter 11917\n",
      "-----> iter 11918\n",
      "-----> iter 11919\n",
      "-----> iter 11920\n",
      "-----> iter 11921\n",
      "-----> iter 11922\n",
      "-----> iter 11923\n",
      "-----> iter 11924\n",
      "-----> iter 11925\n",
      "-----> iter 11926\n",
      "-----> iter 11927\n",
      "-----> iter 11928\n",
      "-----> iter 11929\n",
      "-----> iter 11930\n",
      "-----> iter 11931\n",
      "-----> iter 11932\n",
      "-----> iter 11933\n",
      "-----> iter 11934\n",
      "-----> iter 11935\n",
      "-----> iter 11936\n",
      "-----> iter 11937\n",
      "-----> iter 11938\n",
      "-----> iter 11939\n",
      "-----> iter 11940\n",
      "-----> iter 11941\n",
      "-----> iter 11942\n",
      "-----> iter 11943\n",
      "-----> iter 11944\n",
      "-----> iter 11945\n",
      "-----> iter 11946\n",
      "-----> iter 11947\n",
      "-----> iter 11948\n",
      "-----> iter 11949\n",
      "-----> iter 11950\n",
      "-----> iter 11951\n",
      "-----> iter 11952\n",
      "-----> iter 11953\n",
      "-----> iter 11954\n",
      "-----> iter 11955\n",
      "-----> iter 11956\n",
      "-----> iter 11957\n",
      "-----> iter 11958\n",
      "-----> iter 11959\n",
      "-----> iter 11960\n",
      "-----> iter 11961\n",
      "-----> iter 11962\n",
      "-----> iter 11963\n",
      "-----> iter 11964\n",
      "-----> iter 11965\n",
      "-----> iter 11966\n",
      "-----> iter 11967\n",
      "-----> iter 11968\n",
      "-----> iter 11969\n",
      "-----> iter 11970\n",
      "-----> iter 11971\n",
      "-----> iter 11972\n",
      "-----> iter 11973\n",
      "-----> iter 11974\n",
      "-----> iter 11975\n",
      "-----> iter 11976\n",
      "-----> iter 11977\n",
      "-----> iter 11978\n",
      "-----> iter 11979\n",
      "-----> iter 11980\n",
      "-----> iter 11981\n",
      "-----> iter 11982\n",
      "-----> iter 11983\n",
      "-----> iter 11984\n",
      "-----> iter 11985\n",
      "-----> iter 11986\n",
      "-----> iter 11987\n",
      "-----> iter 11988\n",
      "-----> iter 11989\n",
      "-----> iter 11990\n",
      "-----> iter 11991\n",
      "-----> iter 11992\n",
      "-----> iter 11993\n",
      "-----> iter 11994\n",
      "-----> iter 11995\n",
      "-----> iter 11996\n",
      "-----> iter 11997\n",
      "-----> iter 11998\n",
      "-----> iter 11999\n",
      "-----> iter 12000\n",
      "-----> iter 12001\n",
      "-----> iter 12002\n",
      "-----> iter 12003\n",
      "-----> iter 12004\n",
      "-----> iter 12005\n",
      "-----> iter 12006\n",
      "-----> iter 12007\n",
      "-----> iter 12008\n",
      "-----> iter 12009\n",
      "-----> iter 12010\n",
      "-----> iter 12011\n",
      "-----> iter 12012\n",
      "-----> iter 12013\n",
      "-----> iter 12014\n",
      "-----> iter 12015\n",
      "-----> iter 12016\n",
      "-----> iter 12017\n",
      "-----> iter 12018\n",
      "-----> iter 12019\n",
      "-----> iter 12020\n",
      "-----> iter 12021\n",
      "-----> iter 12022\n",
      "-----> iter 12023\n",
      "-----> iter 12024\n",
      "-----> iter 12025\n",
      "-----> iter 12026\n",
      "-----> iter 12027\n",
      "-----> iter 12028\n",
      "-----> iter 12029\n",
      "-----> iter 12030\n",
      "-----> iter 12031\n",
      "-----> iter 12032\n",
      "-----> iter 12033\n",
      "-----> iter 12034\n",
      "-----> iter 12035\n",
      "-----> iter 12036\n",
      "-----> iter 12037\n",
      "-----> iter 12038\n",
      "-----> iter 12039\n",
      "-----> iter 12040\n",
      "-----> iter 12041\n",
      "-----> iter 12042\n",
      "-----> iter 12043\n",
      "-----> iter 12044\n",
      "-----> iter 12045\n",
      "-----> iter 12046\n",
      "-----> iter 12047\n",
      "-----> iter 12048\n",
      "-----> iter 12049\n",
      "-----> iter 12050\n",
      "-----> iter 12051\n",
      "-----> iter 12052\n",
      "-----> iter 12053\n",
      "-----> iter 12054\n",
      "-----> iter 12055\n",
      "-----> iter 12056\n",
      "-----> iter 12057\n",
      "-----> iter 12058\n",
      "-----> iter 12059\n",
      "-----> iter 12060\n",
      "-----> iter 12061\n",
      "-----> iter 12062\n",
      "-----> iter 12063\n",
      "-----> iter 12064\n",
      "-----> iter 12065\n",
      "-----> iter 12066\n",
      "-----> iter 12067\n",
      "-----> iter 12068\n",
      "-----> iter 12069\n",
      "-----> iter 12070\n",
      "-----> iter 12071\n",
      "-----> iter 12072\n",
      "-----> iter 12073\n",
      "-----> iter 12074\n",
      "-----> iter 12075\n",
      "-----> iter 12076\n",
      "-----> iter 12077\n",
      "-----> iter 12078\n",
      "-----> iter 12079\n",
      "-----> iter 12080\n",
      "-----> iter 12081\n",
      "-----> iter 12082\n",
      "-----> iter 12083\n",
      "-----> iter 12084\n",
      "-----> iter 12085\n",
      "-----> iter 12086\n",
      "-----> iter 12087\n",
      "-----> iter 12088\n",
      "-----> iter 12089\n",
      "-----> iter 12090\n",
      "-----> iter 12091\n",
      "-----> iter 12092\n",
      "-----> iter 12093\n",
      "-----> iter 12094\n",
      "-----> iter 12095\n",
      "-----> iter 12096\n",
      "-----> iter 12097\n",
      "-----> iter 12098\n",
      "-----> iter 12099\n",
      "-----> iter 12100\n",
      "-----> iter 12101\n",
      "-----> iter 12102\n",
      "-----> iter 12103\n",
      "-----> iter 12104\n",
      "-----> iter 12105\n",
      "-----> iter 12106\n",
      "-----> iter 12107\n",
      "-----> iter 12108\n",
      "-----> iter 12109\n",
      "-----> iter 12110\n",
      "-----> iter 12111\n",
      "-----> iter 12112\n",
      "-----> iter 12113\n",
      "-----> iter 12114\n",
      "-----> iter 12115\n",
      "-----> iter 12116\n",
      "-----> iter 12117\n",
      "-----> iter 12118\n",
      "-----> iter 12119\n",
      "-----> iter 12120\n",
      "-----> iter 12121\n",
      "-----> iter 12122\n",
      "-----> iter 12123\n",
      "-----> iter 12124\n",
      "-----> iter 12125\n",
      "-----> iter 12126\n",
      "-----> iter 12127\n",
      "-----> iter 12128\n",
      "-----> iter 12129\n",
      "-----> iter 12130\n",
      "-----> iter 12131\n",
      "-----> iter 12132\n",
      "-----> iter 12133\n",
      "-----> iter 12134\n",
      "-----> iter 12135\n",
      "-----> iter 12136\n",
      "-----> iter 12137\n",
      "-----> iter 12138\n",
      "-----> iter 12139\n",
      "-----> iter 12140\n",
      "-----> iter 12141\n",
      "-----> iter 12142\n",
      "-----> iter 12143\n",
      "-----> iter 12144\n",
      "-----> iter 12145\n",
      "-----> iter 12146\n",
      "-----> iter 12147\n",
      "-----> iter 12148\n",
      "-----> iter 12149\n",
      "-----> iter 12150\n",
      "-----> iter 12151\n",
      "-----> iter 12152\n",
      "-----> iter 12153\n",
      "-----> iter 12154\n",
      "-----> iter 12155\n",
      "-----> iter 12156\n",
      "-----> iter 12157\n",
      "-----> iter 12158\n",
      "-----> iter 12159\n",
      "-----> iter 12160\n",
      "-----> iter 12161\n",
      "-----> iter 12162\n",
      "-----> iter 12163\n",
      "-----> iter 12164\n",
      "-----> iter 12165\n",
      "-----> iter 12166\n",
      "-----> iter 12167\n",
      "-----> iter 12168\n",
      "-----> iter 12169\n",
      "-----> iter 12170\n",
      "-----> iter 12171\n",
      "-----> iter 12172\n",
      "-----> iter 12173\n",
      "-----> iter 12174\n",
      "-----> iter 12175\n",
      "-----> iter 12176\n",
      "-----> iter 12177\n",
      "-----> iter 12178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 12179\n",
      "-----> iter 12180\n",
      "-----> iter 12181\n",
      "-----> iter 12182\n",
      "-----> iter 12183\n",
      "-----> iter 12184\n",
      "-----> iter 12185\n",
      "-----> iter 12186\n",
      "-----> iter 12187\n",
      "-----> iter 12188\n",
      "-----> iter 12189\n",
      "-----> iter 12190\n",
      "-----> iter 12191\n",
      "-----> iter 12192\n",
      "-----> iter 12193\n",
      "-----> iter 12194\n",
      "-----> iter 12195\n",
      "-----> iter 12196\n",
      "-----> iter 12197\n",
      "-----> iter 12198\n",
      "-----> iter 12199\n",
      "-----> iter 12200\n",
      "-----> iter 12201\n",
      "-----> iter 12202\n",
      "-----> iter 12203\n",
      "-----> iter 12204\n",
      "-----> iter 12205\n",
      "-----> iter 12206\n",
      "-----> iter 12207\n",
      "-----> iter 12208\n",
      "-----> iter 12209\n",
      "-----> iter 12210\n",
      "-----> iter 12211\n",
      "-----> iter 12212\n",
      "-----> iter 12213\n",
      "-----> iter 12214\n",
      "-----> iter 12215\n",
      "-----> iter 12216\n",
      "-----> iter 12217\n",
      "-----> iter 12218\n",
      "-----> iter 12219\n",
      "-----> iter 12220\n",
      "-----> iter 12221\n",
      "-----> iter 12222\n",
      "-----> iter 12223\n",
      "-----> iter 12224\n",
      "-----> iter 12225\n",
      "-----> iter 12226\n",
      "-----> iter 12227\n",
      "-----> iter 12228\n",
      "-----> iter 12229\n",
      "-----> iter 12230\n",
      "-----> iter 12231\n",
      "-----> iter 12232\n",
      "-----> iter 12233\n",
      "-----> iter 12234\n",
      "-----> iter 12235\n",
      "-----> iter 12236\n",
      "-----> iter 12237\n",
      "-----> iter 12238\n",
      "-----> iter 12239\n",
      "-----> iter 12240\n",
      "-----> iter 12241\n",
      "-----> iter 12242\n",
      "-----> iter 12243\n",
      "-----> iter 12244\n",
      "-----> iter 12245\n",
      "-----> iter 12246\n",
      "-----> iter 12247\n",
      "-----> iter 12248\n",
      "-----> iter 12249\n",
      "-----> iter 12250\n",
      "-----> iter 12251\n",
      "-----> iter 12252\n",
      "-----> iter 12253\n",
      "-----> iter 12254\n",
      "-----> iter 12255\n",
      "-----> iter 12256\n",
      "-----> iter 12257\n",
      "-----> iter 12258\n",
      "-----> iter 12259\n",
      "-----> iter 12260\n",
      "-----> iter 12261\n",
      "-----> iter 12262\n",
      "-----> iter 12263\n",
      "-----> iter 12264\n",
      "-----> iter 12265\n",
      "-----> iter 12266\n",
      "-----> iter 12267\n",
      "-----> iter 12268\n",
      "-----> iter 12269\n",
      "-----> iter 12270\n",
      "-----> iter 12271\n",
      "-----> iter 12272\n",
      "-----> iter 12273\n",
      "-----> iter 12274\n",
      "-----> iter 12275\n",
      "-----> iter 12276\n",
      "-----> iter 12277\n",
      "-----> iter 12278\n",
      "-----> iter 12279\n",
      "-----> iter 12280\n",
      "-----> iter 12281\n",
      "-----> iter 12282\n",
      "-----> iter 12283\n",
      "-----> iter 12284\n",
      "-----> iter 12285\n",
      "-----> iter 12286\n",
      "-----> iter 12287\n",
      "-----> iter 12288\n",
      "-----> iter 12289\n",
      "-----> iter 12290\n",
      "-----> iter 12291\n",
      "-----> iter 12292\n",
      "-----> iter 12293\n",
      "-----> iter 12294\n",
      "-----> iter 12295\n",
      "-----> iter 12296\n",
      "-----> iter 12297\n",
      "-----> iter 12298\n",
      "-----> iter 12299\n",
      "-----> iter 12300\n",
      "-----> iter 12301\n",
      "-----> iter 12302\n",
      "-----> iter 12303\n",
      "-----> iter 12304\n",
      "-----> iter 12305\n",
      "-----> iter 12306\n",
      "-----> iter 12307\n",
      "-----> iter 12308\n",
      "-----> iter 12309\n",
      "-----> iter 12310\n",
      "-----> iter 12311\n",
      "-----> iter 12312\n",
      "-----> iter 12313\n",
      "-----> iter 12314\n",
      "-----> iter 12315\n",
      "-----> iter 12316\n",
      "-----> iter 12317\n",
      "-----> iter 12318\n",
      "-----> iter 12319\n",
      "-----> iter 12320\n",
      "-----> iter 12321\n",
      "-----> iter 12322\n",
      "-----> iter 12323\n",
      "-----> iter 12324\n",
      "-----> iter 12325\n",
      "-----> iter 12326\n",
      "-----> iter 12327\n",
      "-----> iter 12328\n",
      "-----> iter 12329\n",
      "-----> iter 12330\n",
      "-----> iter 12331\n",
      "-----> iter 12332\n",
      "-----> iter 12333\n",
      "-----> iter 12334\n",
      "-----> iter 12335\n",
      "-----> iter 12336\n",
      "-----> iter 12337\n",
      "-----> iter 12338\n",
      "-----> iter 12339\n",
      "-----> iter 12340\n",
      "-----> iter 12341\n",
      "-----> iter 12342\n",
      "-----> iter 12343\n",
      "-----> iter 12344\n",
      "-----> iter 12345\n",
      "-----> iter 12346\n",
      "-----> iter 12347\n",
      "-----> iter 12348\n",
      "-----> iter 12349\n",
      "-----> iter 12350\n",
      "-----> iter 12351\n",
      "-----> iter 12352\n",
      "-----> iter 12353\n",
      "-----> iter 12354\n",
      "-----> iter 12355\n",
      "-----> iter 12356\n",
      "-----> iter 12357\n",
      "-----> iter 12358\n",
      "-----> iter 12359\n",
      "-----> iter 12360\n",
      "-----> iter 12361\n",
      "-----> iter 12362\n",
      "-----> iter 12363\n",
      "-----> iter 12364\n",
      "-----> iter 12365\n",
      "-----> iter 12366\n",
      "-----> iter 12367\n",
      "-----> iter 12368\n",
      "-----> iter 12369\n",
      "-----> iter 12370\n",
      "-----> iter 12371\n",
      "-----> iter 12372\n",
      "-----> iter 12373\n",
      "-----> iter 12374\n",
      "-----> iter 12375\n",
      "-----> iter 12376\n",
      "-----> iter 12377\n",
      "-----> iter 12378\n",
      "-----> iter 12379\n",
      "-----> iter 12380\n",
      "-----> iter 12381\n",
      "-----> iter 12382\n",
      "-----> iter 12383\n",
      "-----> iter 12384\n",
      "-----> iter 12385\n",
      "-----> iter 12386\n",
      "-----> iter 12387\n",
      "-----> iter 12388\n",
      "-----> iter 12389\n",
      "-----> iter 12390\n",
      "-----> iter 12391\n",
      "-----> iter 12392\n",
      "-----> iter 12393\n",
      "-----> iter 12394\n",
      "-----> iter 12395\n",
      "-----> iter 12396\n",
      "-----> iter 12397\n",
      "-----> iter 12398\n",
      "-----> iter 12399\n",
      "-----> iter 12400\n",
      "-----> iter 12401\n",
      "-----> iter 12402\n",
      "-----> iter 12403\n",
      "-----> iter 12404\n",
      "-----> iter 12405\n",
      "-----> iter 12406\n",
      "-----> iter 12407\n",
      "-----> iter 12408\n",
      "-----> iter 12409\n",
      "-----> iter 12410\n",
      "-----> iter 12411\n",
      "-----> iter 12412\n",
      "-----> iter 12413\n",
      "-----> iter 12414\n",
      "-----> iter 12415\n",
      "-----> iter 12416\n",
      "-----> iter 12417\n",
      "-----> iter 12418\n",
      "-----> iter 12419\n",
      "-----> iter 12420\n",
      "-----> iter 12421\n",
      "-----> iter 12422\n",
      "-----> iter 12423\n",
      "-----> iter 12424\n",
      "-----> iter 12425\n",
      "-----> iter 12426\n",
      "-----> iter 12427\n",
      "-----> iter 12428\n",
      "-----> iter 12429\n",
      "-----> iter 12430\n",
      "-----> iter 12431\n",
      "-----> iter 12432\n",
      "-----> iter 12433\n",
      "-----> iter 12434\n",
      "-----> iter 12435\n",
      "-----> iter 12436\n",
      "-----> iter 12437\n",
      "-----> iter 12438\n",
      "-----> iter 12439\n",
      "-----> iter 12440\n",
      "-----> iter 12441\n",
      "-----> iter 12442\n",
      "-----> iter 12443\n",
      "-----> iter 12444\n",
      "-----> iter 12445\n",
      "-----> iter 12446\n",
      "-----> iter 12447\n",
      "-----> iter 12448\n",
      "-----> iter 12449\n",
      "-----> iter 12450\n",
      "-----> iter 12451\n",
      "-----> iter 12452\n",
      "-----> iter 12453\n",
      "-----> iter 12454\n",
      "-----> iter 12455\n",
      "-----> iter 12456\n",
      "-----> iter 12457\n",
      "-----> iter 12458\n",
      "-----> iter 12459\n",
      "-----> iter 12460\n",
      "-----> iter 12461\n",
      "-----> iter 12462\n",
      "-----> iter 12463\n",
      "-----> iter 12464\n",
      "-----> iter 12465\n",
      "-----> iter 12466\n",
      "-----> iter 12467\n",
      "-----> iter 12468\n",
      "-----> iter 12469\n",
      "-----> iter 12470\n",
      "-----> iter 12471\n",
      "-----> iter 12472\n",
      "-----> iter 12473\n",
      "-----> iter 12474\n",
      "-----> iter 12475\n",
      "-----> iter 12476\n",
      "-----> iter 12477\n",
      "-----> iter 12478\n",
      "-----> iter 12479\n",
      "-----> iter 12480\n",
      "-----> iter 12481\n",
      "-----> iter 12482\n",
      "-----> iter 12483\n",
      "-----> iter 12484\n",
      "-----> iter 12485\n",
      "-----> iter 12486\n",
      "-----> iter 12487\n",
      "-----> iter 12488\n",
      "-----> iter 12489\n",
      "-----> iter 12490\n",
      "-----> iter 12491\n",
      "-----> iter 12492\n",
      "-----> iter 12493\n",
      "-----> iter 12494\n",
      "-----> iter 12495\n",
      "-----> iter 12496\n",
      "-----> iter 12497\n",
      "-----> iter 12498\n",
      "-----> iter 12499\n",
      "-----> iter 12500\n",
      "-----> iter 12501\n",
      "-----> iter 12502\n",
      "-----> iter 12503\n",
      "-----> iter 12504\n",
      "-----> iter 12505\n",
      "-----> iter 12506\n",
      "-----> iter 12507\n",
      "-----> iter 12508\n",
      "-----> iter 12509\n",
      "-----> iter 12510\n",
      "-----> iter 12511\n",
      "-----> iter 12512\n",
      "-----> iter 12513\n",
      "-----> iter 12514\n",
      "-----> iter 12515\n",
      "-----> iter 12516\n",
      "-----> iter 12517\n",
      "-----> iter 12518\n",
      "-----> iter 12519\n",
      "-----> iter 12520\n",
      "-----> iter 12521\n",
      "-----> iter 12522\n",
      "-----> iter 12523\n",
      "-----> iter 12524\n",
      "-----> iter 12525\n",
      "-----> iter 12526\n",
      "-----> iter 12527\n",
      "-----> iter 12528\n",
      "-----> iter 12529\n",
      "-----> iter 12530\n",
      "-----> iter 12531\n",
      "-----> iter 12532\n",
      "-----> iter 12533\n",
      "-----> iter 12534\n",
      "-----> iter 12535\n",
      "-----> iter 12536\n",
      "-----> iter 12537\n",
      "-----> iter 12538\n",
      "-----> iter 12539\n",
      "-----> iter 12540\n",
      "-----> iter 12541\n",
      "-----> iter 12542\n",
      "-----> iter 12543\n",
      "-----> iter 12544\n",
      "-----> iter 12545\n",
      "-----> iter 12546\n",
      "-----> iter 12547\n",
      "-----> iter 12548\n",
      "-----> iter 12549\n",
      "-----> iter 12550\n",
      "-----> iter 12551\n",
      "-----> iter 12552\n",
      "-----> iter 12553\n",
      "-----> iter 12554\n",
      "-----> iter 12555\n",
      "-----> iter 12556\n",
      "-----> iter 12557\n",
      "-----> iter 12558\n",
      "-----> iter 12559\n",
      "-----> iter 12560\n",
      "-----> iter 12561\n",
      "-----> iter 12562\n",
      "-----> iter 12563\n",
      "-----> iter 12564\n",
      "-----> iter 12565\n",
      "-----> iter 12566\n",
      "-----> iter 12567\n",
      "-----> iter 12568\n",
      "-----> iter 12569\n",
      "-----> iter 12570\n",
      "-----> iter 12571\n",
      "-----> iter 12572\n",
      "-----> iter 12573\n",
      "-----> iter 12574\n",
      "-----> iter 12575\n",
      "-----> iter 12576\n",
      "-----> iter 12577\n",
      "-----> iter 12578\n",
      "-----> iter 12579\n",
      "-----> iter 12580\n",
      "-----> iter 12581\n",
      "-----> iter 12582\n",
      "-----> iter 12583\n",
      "-----> iter 12584\n",
      "-----> iter 12585\n",
      "-----> iter 12586\n",
      "-----> iter 12587\n",
      "-----> iter 12588\n",
      "-----> iter 12589\n",
      "-----> iter 12590\n",
      "-----> iter 12591\n",
      "-----> iter 12592\n",
      "-----> iter 12593\n",
      "-----> iter 12594\n",
      "-----> iter 12595\n",
      "-----> iter 12596\n",
      "-----> iter 12597\n",
      "-----> iter 12598\n",
      "-----> iter 12599\n",
      "-----> iter 12600\n",
      "-----> iter 12601\n",
      "-----> iter 12602\n",
      "-----> iter 12603\n",
      "-----> iter 12604\n",
      "-----> iter 12605\n",
      "-----> iter 12606\n",
      "-----> iter 12607\n",
      "-----> iter 12608\n",
      "-----> iter 12609\n",
      "-----> iter 12610\n",
      "-----> iter 12611\n",
      "-----> iter 12612\n",
      "-----> iter 12613\n",
      "-----> iter 12614\n",
      "-----> iter 12615\n",
      "-----> iter 12616\n",
      "-----> iter 12617\n",
      "-----> iter 12618\n",
      "-----> iter 12619\n",
      "-----> iter 12620\n",
      "-----> iter 12621\n",
      "-----> iter 12622\n",
      "-----> iter 12623\n",
      "-----> iter 12624\n",
      "-----> iter 12625\n",
      "-----> iter 12626\n",
      "-----> iter 12627\n",
      "-----> iter 12628\n",
      "-----> iter 12629\n",
      "-----> iter 12630\n",
      "-----> iter 12631\n",
      "-----> iter 12632\n",
      "-----> iter 12633\n",
      "-----> iter 12634\n",
      "-----> iter 12635\n",
      "-----> iter 12636\n",
      "-----> iter 12637\n",
      "-----> iter 12638\n",
      "-----> iter 12639\n",
      "-----> iter 12640\n",
      "-----> iter 12641\n",
      "-----> iter 12642\n",
      "-----> iter 12643\n",
      "-----> iter 12644\n",
      "-----> iter 12645\n",
      "-----> iter 12646\n",
      "-----> iter 12647\n",
      "-----> iter 12648\n",
      "-----> iter 12649\n",
      "-----> iter 12650\n",
      "-----> iter 12651\n",
      "-----> iter 12652\n",
      "-----> iter 12653\n",
      "-----> iter 12654\n",
      "-----> iter 12655\n",
      "-----> iter 12656\n",
      "-----> iter 12657\n",
      "-----> iter 12658\n",
      "-----> iter 12659\n",
      "-----> iter 12660\n",
      "-----> iter 12661\n",
      "-----> iter 12662\n",
      "-----> iter 12663\n",
      "-----> iter 12664\n",
      "-----> iter 12665\n",
      "-----> iter 12666\n",
      "-----> iter 12667\n",
      "-----> iter 12668\n",
      "-----> iter 12669\n",
      "-----> iter 12670\n",
      "-----> iter 12671\n",
      "-----> iter 12672\n",
      "-----> iter 12673\n",
      "-----> iter 12674\n",
      "-----> iter 12675\n",
      "-----> iter 12676\n",
      "-----> iter 12677\n",
      "-----> iter 12678\n",
      "-----> iter 12679\n",
      "-----> iter 12680\n",
      "-----> iter 12681\n",
      "-----> iter 12682\n",
      "-----> iter 12683\n",
      "-----> iter 12684\n",
      "-----> iter 12685\n",
      "-----> iter 12686\n",
      "-----> iter 12687\n",
      "-----> iter 12688\n",
      "-----> iter 12689\n",
      "-----> iter 12690\n",
      "-----> iter 12691\n",
      "-----> iter 12692\n",
      "-----> iter 12693\n",
      "-----> iter 12694\n",
      "-----> iter 12695\n",
      "-----> iter 12696\n",
      "-----> iter 12697\n",
      "-----> iter 12698\n",
      "-----> iter 12699\n",
      "-----> iter 12700\n",
      "-----> iter 12701\n",
      "-----> iter 12702\n",
      "-----> iter 12703\n",
      "-----> iter 12704\n",
      "-----> iter 12705\n",
      "-----> iter 12706\n",
      "-----> iter 12707\n",
      "-----> iter 12708\n",
      "-----> iter 12709\n",
      "-----> iter 12710\n",
      "-----> iter 12711\n",
      "-----> iter 12712\n",
      "-----> iter 12713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 12714\n",
      "-----> iter 12715\n",
      "-----> iter 12716\n",
      "-----> iter 12717\n",
      "-----> iter 12718\n",
      "-----> iter 12719\n",
      "-----> iter 12720\n",
      "-----> iter 12721\n",
      "-----> iter 12722\n",
      "-----> iter 12723\n",
      "-----> iter 12724\n",
      "-----> iter 12725\n",
      "-----> iter 12726\n",
      "-----> iter 12727\n",
      "-----> iter 12728\n",
      "-----> iter 12729\n",
      "-----> iter 12730\n",
      "-----> iter 12731\n",
      "-----> iter 12732\n",
      "-----> iter 12733\n",
      "-----> iter 12734\n",
      "-----> iter 12735\n",
      "-----> iter 12736\n",
      "-----> iter 12737\n",
      "-----> iter 12738\n",
      "-----> iter 12739\n",
      "-----> iter 12740\n",
      "-----> iter 12741\n",
      "-----> iter 12742\n",
      "-----> iter 12743\n",
      "-----> iter 12744\n",
      "-----> iter 12745\n",
      "-----> iter 12746\n",
      "-----> iter 12747\n",
      "-----> iter 12748\n",
      "-----> iter 12749\n",
      "-----> iter 12750\n",
      "-----> iter 12751\n",
      "-----> iter 12752\n",
      "-----> iter 12753\n",
      "-----> iter 12754\n",
      "-----> iter 12755\n",
      "-----> iter 12756\n",
      "-----> iter 12757\n",
      "-----> iter 12758\n",
      "-----> iter 12759\n",
      "-----> iter 12760\n",
      "-----> iter 12761\n",
      "-----> iter 12762\n",
      "-----> iter 12763\n",
      "-----> iter 12764\n",
      "-----> iter 12765\n",
      "-----> iter 12766\n",
      "-----> iter 12767\n",
      "-----> iter 12768\n",
      "-----> iter 12769\n",
      "-----> iter 12770\n",
      "-----> iter 12771\n",
      "-----> iter 12772\n",
      "-----> iter 12773\n",
      "-----> iter 12774\n",
      "-----> iter 12775\n",
      "-----> iter 12776\n",
      "-----> iter 12777\n",
      "-----> iter 12778\n",
      "-----> iter 12779\n",
      "-----> iter 12780\n",
      "-----> iter 12781\n",
      "-----> iter 12782\n",
      "-----> iter 12783\n",
      "-----> iter 12784\n",
      "-----> iter 12785\n",
      "-----> iter 12786\n",
      "-----> iter 12787\n",
      "-----> iter 12788\n",
      "-----> iter 12789\n",
      "-----> iter 12790\n",
      "-----> iter 12791\n",
      "-----> iter 12792\n",
      "-----> iter 12793\n",
      "-----> iter 12794\n",
      "-----> iter 12795\n",
      "-----> iter 12796\n",
      "-----> iter 12797\n",
      "-----> iter 12798\n",
      "-----> iter 12799\n",
      "-----> iter 12800\n",
      "-----> iter 12801\n",
      "-----> iter 12802\n",
      "-----> iter 12803\n",
      "-----> iter 12804\n",
      "-----> iter 12805\n",
      "-----> iter 12806\n",
      "-----> iter 12807\n",
      "-----> iter 12808\n",
      "-----> iter 12809\n",
      "-----> iter 12810\n",
      "-----> iter 12811\n",
      "-----> iter 12812\n",
      "-----> iter 12813\n",
      "-----> iter 12814\n",
      "-----> iter 12815\n",
      "-----> iter 12816\n",
      "-----> iter 12817\n",
      "-----> iter 12818\n",
      "-----> iter 12819\n",
      "-----> iter 12820\n",
      "-----> iter 12821\n",
      "-----> iter 12822\n",
      "-----> iter 12823\n",
      "-----> iter 12824\n",
      "-----> iter 12825\n",
      "-----> iter 12826\n",
      "-----> iter 12827\n",
      "-----> iter 12828\n",
      "-----> iter 12829\n",
      "-----> iter 12830\n",
      "-----> iter 12831\n",
      "-----> iter 12832\n",
      "-----> iter 12833\n",
      "-----> iter 12834\n",
      "-----> iter 12835\n",
      "-----> iter 12836\n",
      "-----> iter 12837\n",
      "-----> iter 12838\n",
      "-----> iter 12839\n",
      "-----> iter 12840\n",
      "-----> iter 12841\n",
      "-----> iter 12842\n",
      "-----> iter 12843\n",
      "-----> iter 12844\n",
      "-----> iter 12845\n",
      "-----> iter 12846\n",
      "-----> iter 12847\n",
      "-----> iter 12848\n",
      "-----> iter 12849\n",
      "-----> iter 12850\n",
      "-----> iter 12851\n",
      "-----> iter 12852\n",
      "-----> iter 12853\n",
      "-----> iter 12854\n",
      "-----> iter 12855\n",
      "-----> iter 12856\n",
      "-----> iter 12857\n",
      "-----> iter 12858\n",
      "-----> iter 12859\n",
      "-----> iter 12860\n",
      "-----> iter 12861\n",
      "-----> iter 12862\n",
      "-----> iter 12863\n",
      "-----> iter 12864\n",
      "-----> iter 12865\n",
      "-----> iter 12866\n",
      "-----> iter 12867\n",
      "-----> iter 12868\n",
      "-----> iter 12869\n",
      "-----> iter 12870\n",
      "-----> iter 12871\n",
      "-----> iter 12872\n",
      "-----> iter 12873\n",
      "-----> iter 12874\n",
      "-----> iter 12875\n",
      "-----> iter 12876\n",
      "-----> iter 12877\n",
      "-----> iter 12878\n",
      "-----> iter 12879\n",
      "-----> iter 12880\n",
      "-----> iter 12881\n",
      "-----> iter 12882\n",
      "-----> iter 12883\n",
      "-----> iter 12884\n",
      "-----> iter 12885\n",
      "-----> iter 12886\n",
      "-----> iter 12887\n",
      "-----> iter 12888\n",
      "-----> iter 12889\n",
      "-----> iter 12890\n",
      "-----> iter 12891\n",
      "-----> iter 12892\n",
      "-----> iter 12893\n",
      "-----> iter 12894\n",
      "-----> iter 12895\n",
      "-----> iter 12896\n",
      "-----> iter 12897\n",
      "-----> iter 12898\n",
      "-----> iter 12899\n",
      "-----> iter 12900\n",
      "-----> iter 12901\n",
      "-----> iter 12902\n",
      "-----> iter 12903\n",
      "-----> iter 12904\n",
      "-----> iter 12905\n",
      "-----> iter 12906\n",
      "-----> iter 12907\n",
      "-----> iter 12908\n",
      "-----> iter 12909\n",
      "-----> iter 12910\n",
      "-----> iter 12911\n",
      "-----> iter 12912\n",
      "-----> iter 12913\n",
      "-----> iter 12914\n",
      "-----> iter 12915\n",
      "-----> iter 12916\n",
      "-----> iter 12917\n",
      "-----> iter 12918\n",
      "-----> iter 12919\n",
      "-----> iter 12920\n",
      "-----> iter 12921\n",
      "-----> iter 12922\n",
      "-----> iter 12923\n",
      "-----> iter 12924\n",
      "-----> iter 12925\n",
      "-----> iter 12926\n",
      "-----> iter 12927\n",
      "-----> iter 12928\n",
      "-----> iter 12929\n",
      "-----> iter 12930\n",
      "-----> iter 12931\n",
      "-----> iter 12932\n",
      "-----> iter 12933\n",
      "-----> iter 12934\n",
      "-----> iter 12935\n",
      "-----> iter 12936\n",
      "-----> iter 12937\n",
      "-----> iter 12938\n",
      "-----> iter 12939\n",
      "-----> iter 12940\n",
      "-----> iter 12941\n",
      "-----> iter 12942\n",
      "-----> iter 12943\n",
      "-----> iter 12944\n",
      "-----> iter 12945\n",
      "-----> iter 12946\n",
      "-----> iter 12947\n",
      "-----> iter 12948\n",
      "-----> iter 12949\n",
      "-----> iter 12950\n",
      "-----> iter 12951\n",
      "-----> iter 12952\n",
      "-----> iter 12953\n",
      "-----> iter 12954\n",
      "-----> iter 12955\n",
      "-----> iter 12956\n",
      "-----> iter 12957\n",
      "-----> iter 12958\n",
      "-----> iter 12959\n",
      "-----> iter 12960\n",
      "-----> iter 12961\n",
      "-----> iter 12962\n",
      "-----> iter 12963\n",
      "-----> iter 12964\n",
      "-----> iter 12965\n",
      "-----> iter 12966\n",
      "-----> iter 12967\n",
      "-----> iter 12968\n",
      "-----> iter 12969\n",
      "-----> iter 12970\n",
      "-----> iter 12971\n",
      "-----> iter 12972\n",
      "-----> iter 12973\n",
      "-----> iter 12974\n",
      "-----> iter 12975\n",
      "-----> iter 12976\n",
      "-----> iter 12977\n",
      "-----> iter 12978\n",
      "-----> iter 12979\n",
      "-----> iter 12980\n",
      "-----> iter 12981\n",
      "-----> iter 12982\n",
      "-----> iter 12983\n",
      "-----> iter 12984\n",
      "-----> iter 12985\n",
      "-----> iter 12986\n",
      "-----> iter 12987\n",
      "-----> iter 12988\n",
      "-----> iter 12989\n",
      "-----> iter 12990\n",
      "-----> iter 12991\n",
      "-----> iter 12992\n",
      "-----> iter 12993\n",
      "-----> iter 12994\n",
      "-----> iter 12995\n",
      "-----> iter 12996\n",
      "-----> iter 12997\n",
      "-----> iter 12998\n",
      "-----> iter 12999\n",
      "-----> iter 13000\n",
      "-----> iter 13001\n",
      "-----> iter 13002\n",
      "-----> iter 13003\n",
      "-----> iter 13004\n",
      "-----> iter 13005\n",
      "-----> iter 13006\n",
      "-----> iter 13007\n",
      "-----> iter 13008\n",
      "-----> iter 13009\n",
      "-----> iter 13010\n",
      "-----> iter 13011\n",
      "-----> iter 13012\n",
      "-----> iter 13013\n",
      "-----> iter 13014\n",
      "-----> iter 13015\n",
      "-----> iter 13016\n",
      "-----> iter 13017\n",
      "-----> iter 13018\n",
      "-----> iter 13019\n",
      "-----> iter 13020\n",
      "-----> iter 13021\n",
      "-----> iter 13022\n",
      "-----> iter 13023\n",
      "-----> iter 13024\n",
      "-----> iter 13025\n",
      "-----> iter 13026\n",
      "-----> iter 13027\n",
      "-----> iter 13028\n",
      "-----> iter 13029\n",
      "-----> iter 13030\n",
      "-----> iter 13031\n",
      "-----> iter 13032\n",
      "-----> iter 13033\n",
      "-----> iter 13034\n",
      "-----> iter 13035\n",
      "-----> iter 13036\n",
      "-----> iter 13037\n",
      "-----> iter 13038\n",
      "-----> iter 13039\n",
      "-----> iter 13040\n",
      "-----> iter 13041\n",
      "-----> iter 13042\n",
      "-----> iter 13043\n",
      "-----> iter 13044\n",
      "-----> iter 13045\n",
      "-----> iter 13046\n",
      "-----> iter 13047\n",
      "-----> iter 13048\n",
      "-----> iter 13049\n",
      "-----> iter 13050\n",
      "-----> iter 13051\n",
      "-----> iter 13052\n",
      "-----> iter 13053\n",
      "-----> iter 13054\n",
      "-----> iter 13055\n",
      "-----> iter 13056\n",
      "-----> iter 13057\n",
      "-----> iter 13058\n",
      "-----> iter 13059\n",
      "-----> iter 13060\n",
      "-----> iter 13061\n",
      "-----> iter 13062\n",
      "-----> iter 13063\n",
      "-----> iter 13064\n",
      "-----> iter 13065\n",
      "-----> iter 13066\n",
      "-----> iter 13067\n",
      "-----> iter 13068\n",
      "-----> iter 13069\n",
      "-----> iter 13070\n",
      "-----> iter 13071\n",
      "-----> iter 13072\n",
      "-----> iter 13073\n",
      "-----> iter 13074\n",
      "-----> iter 13075\n",
      "-----> iter 13076\n",
      "-----> iter 13077\n",
      "-----> iter 13078\n",
      "-----> iter 13079\n",
      "-----> iter 13080\n",
      "-----> iter 13081\n",
      "-----> iter 13082\n",
      "-----> iter 13083\n",
      "-----> iter 13084\n",
      "-----> iter 13085\n",
      "-----> iter 13086\n",
      "-----> iter 13087\n",
      "-----> iter 13088\n",
      "-----> iter 13089\n",
      "-----> iter 13090\n",
      "-----> iter 13091\n",
      "-----> iter 13092\n",
      "-----> iter 13093\n",
      "-----> iter 13094\n",
      "-----> iter 13095\n",
      "-----> iter 13096\n",
      "-----> iter 13097\n",
      "-----> iter 13098\n",
      "-----> iter 13099\n",
      "-----> iter 13100\n",
      "-----> iter 13101\n",
      "-----> iter 13102\n",
      "-----> iter 13103\n",
      "-----> iter 13104\n",
      "-----> iter 13105\n",
      "-----> iter 13106\n",
      "-----> iter 13107\n",
      "-----> iter 13108\n",
      "-----> iter 13109\n",
      "-----> iter 13110\n",
      "-----> iter 13111\n",
      "-----> iter 13112\n",
      "-----> iter 13113\n",
      "-----> iter 13114\n",
      "-----> iter 13115\n",
      "-----> iter 13116\n",
      "-----> iter 13117\n",
      "-----> iter 13118\n",
      "-----> iter 13119\n",
      "-----> iter 13120\n",
      "-----> iter 13121\n",
      "-----> iter 13122\n",
      "-----> iter 13123\n",
      "-----> iter 13124\n",
      "-----> iter 13125\n",
      "-----> iter 13126\n",
      "-----> iter 13127\n",
      "-----> iter 13128\n",
      "-----> iter 13129\n",
      "-----> iter 13130\n",
      "-----> iter 13131\n",
      "-----> iter 13132\n",
      "-----> iter 13133\n",
      "-----> iter 13134\n",
      "-----> iter 13135\n",
      "-----> iter 13136\n",
      "-----> iter 13137\n",
      "-----> iter 13138\n",
      "-----> iter 13139\n",
      "-----> iter 13140\n",
      "-----> iter 13141\n",
      "-----> iter 13142\n",
      "-----> iter 13143\n",
      "-----> iter 13144\n",
      "-----> iter 13145\n",
      "-----> iter 13146\n",
      "-----> iter 13147\n",
      "-----> iter 13148\n",
      "-----> iter 13149\n",
      "-----> iter 13150\n",
      "-----> iter 13151\n",
      "-----> iter 13152\n",
      "-----> iter 13153\n",
      "-----> iter 13154\n",
      "-----> iter 13155\n",
      "-----> iter 13156\n",
      "-----> iter 13157\n",
      "-----> iter 13158\n",
      "-----> iter 13159\n",
      "-----> iter 13160\n",
      "-----> iter 13161\n",
      "-----> iter 13162\n",
      "-----> iter 13163\n",
      "-----> iter 13164\n",
      "-----> iter 13165\n",
      "-----> iter 13166\n",
      "-----> iter 13167\n",
      "-----> iter 13168\n",
      "-----> iter 13169\n",
      "-----> iter 13170\n",
      "-----> iter 13171\n",
      "-----> iter 13172\n",
      "-----> iter 13173\n",
      "-----> iter 13174\n",
      "-----> iter 13175\n",
      "-----> iter 13176\n",
      "-----> iter 13177\n",
      "-----> iter 13178\n",
      "-----> iter 13179\n",
      "-----> iter 13180\n",
      "-----> iter 13181\n",
      "-----> iter 13182\n",
      "-----> iter 13183\n",
      "-----> iter 13184\n",
      "-----> iter 13185\n",
      "-----> iter 13186\n",
      "-----> iter 13187\n",
      "-----> iter 13188\n",
      "-----> iter 13189\n",
      "-----> iter 13190\n",
      "-----> iter 13191\n",
      "-----> iter 13192\n",
      "-----> iter 13193\n",
      "-----> iter 13194\n",
      "-----> iter 13195\n",
      "-----> iter 13196\n",
      "-----> iter 13197\n",
      "-----> iter 13198\n",
      "-----> iter 13199\n",
      "-----> iter 13200\n",
      "-----> iter 13201\n",
      "-----> iter 13202\n",
      "-----> iter 13203\n",
      "-----> iter 13204\n",
      "-----> iter 13205\n",
      "-----> iter 13206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 13207\n",
      "-----> iter 13208\n",
      "-----> iter 13209\n",
      "-----> iter 13210\n",
      "-----> iter 13211\n",
      "-----> iter 13212\n",
      "-----> iter 13213\n",
      "-----> iter 13214\n",
      "-----> iter 13215\n",
      "-----> iter 13216\n",
      "-----> iter 13217\n",
      "-----> iter 13218\n",
      "-----> iter 13219\n",
      "-----> iter 13220\n",
      "-----> iter 13221\n",
      "-----> iter 13222\n",
      "-----> iter 13223\n",
      "-----> iter 13224\n",
      "-----> iter 13225\n",
      "-----> iter 13226\n",
      "-----> iter 13227\n",
      "-----> iter 13228\n",
      "-----> iter 13229\n",
      "-----> iter 13230\n",
      "-----> iter 13231\n",
      "-----> iter 13232\n",
      "-----> iter 13233\n",
      "-----> iter 13234\n",
      "-----> iter 13235\n",
      "-----> iter 13236\n",
      "-----> iter 13237\n",
      "-----> iter 13238\n",
      "-----> iter 13239\n",
      "-----> iter 13240\n",
      "-----> iter 13241\n",
      "-----> iter 13242\n",
      "-----> iter 13243\n",
      "-----> iter 13244\n",
      "-----> iter 13245\n",
      "-----> iter 13246\n",
      "-----> iter 13247\n",
      "-----> iter 13248\n",
      "-----> iter 13249\n",
      "-----> iter 13250\n",
      "-----> iter 13251\n",
      "-----> iter 13252\n",
      "-----> iter 13253\n",
      "-----> iter 13254\n",
      "-----> iter 13255\n",
      "-----> iter 13256\n",
      "-----> iter 13257\n",
      "-----> iter 13258\n",
      "-----> iter 13259\n",
      "-----> iter 13260\n",
      "-----> iter 13261\n",
      "-----> iter 13262\n",
      "-----> iter 13263\n",
      "-----> iter 13264\n",
      "-----> iter 13265\n",
      "-----> iter 13266\n",
      "-----> iter 13267\n",
      "-----> iter 13268\n",
      "-----> iter 13269\n",
      "-----> iter 13270\n",
      "-----> iter 13271\n",
      "-----> iter 13272\n",
      "-----> iter 13273\n",
      "-----> iter 13274\n",
      "-----> iter 13275\n",
      "-----> iter 13276\n",
      "-----> iter 13277\n",
      "-----> iter 13278\n",
      "-----> iter 13279\n",
      "-----> iter 13280\n",
      "-----> iter 13281\n",
      "-----> iter 13282\n",
      "-----> iter 13283\n",
      "-----> iter 13284\n",
      "-----> iter 13285\n",
      "-----> iter 13286\n",
      "-----> iter 13287\n",
      "-----> iter 13288\n",
      "-----> iter 13289\n",
      "-----> iter 13290\n",
      "-----> iter 13291\n",
      "-----> iter 13292\n",
      "-----> iter 13293\n",
      "-----> iter 13294\n",
      "-----> iter 13295\n",
      "-----> iter 13296\n",
      "-----> iter 13297\n",
      "-----> iter 13298\n",
      "-----> iter 13299\n",
      "-----> iter 13300\n",
      "-----> iter 13301\n",
      "-----> iter 13302\n",
      "-----> iter 13303\n",
      "-----> iter 13304\n",
      "-----> iter 13305\n",
      "-----> iter 13306\n",
      "-----> iter 13307\n",
      "-----> iter 13308\n",
      "-----> iter 13309\n",
      "-----> iter 13310\n",
      "-----> iter 13311\n",
      "-----> iter 13312\n",
      "-----> iter 13313\n",
      "-----> iter 13314\n",
      "-----> iter 13315\n",
      "-----> iter 13316\n",
      "-----> iter 13317\n",
      "-----> iter 13318\n",
      "-----> iter 13319\n",
      "-----> iter 13320\n",
      "-----> iter 13321\n",
      "-----> iter 13322\n",
      "-----> iter 13323\n",
      "-----> iter 13324\n",
      "-----> iter 13325\n",
      "-----> iter 13326\n",
      "-----> iter 13327\n",
      "-----> iter 13328\n",
      "-----> iter 13329\n",
      "-----> iter 13330\n",
      "-----> iter 13331\n",
      "-----> iter 13332\n",
      "-----> iter 13333\n",
      "-----> iter 13334\n",
      "-----> iter 13335\n",
      "-----> iter 13336\n",
      "-----> iter 13337\n",
      "-----> iter 13338\n",
      "-----> iter 13339\n",
      "-----> iter 13340\n",
      "-----> iter 13341\n",
      "-----> iter 13342\n",
      "-----> iter 13343\n",
      "-----> iter 13344\n",
      "-----> iter 13345\n",
      "-----> iter 13346\n",
      "-----> iter 13347\n",
      "-----> iter 13348\n",
      "-----> iter 13349\n",
      "-----> iter 13350\n",
      "-----> iter 13351\n",
      "-----> iter 13352\n",
      "-----> iter 13353\n",
      "-----> iter 13354\n",
      "-----> iter 13355\n",
      "-----> iter 13356\n",
      "-----> iter 13357\n",
      "-----> iter 13358\n",
      "-----> iter 13359\n",
      "-----> iter 13360\n",
      "-----> iter 13361\n",
      "-----> iter 13362\n",
      "-----> iter 13363\n",
      "-----> iter 13364\n",
      "-----> iter 13365\n",
      "-----> iter 13366\n",
      "-----> iter 13367\n",
      "-----> iter 13368\n",
      "-----> iter 13369\n",
      "-----> iter 13370\n",
      "-----> iter 13371\n",
      "-----> iter 13372\n",
      "-----> iter 13373\n",
      "-----> iter 13374\n",
      "-----> iter 13375\n",
      "-----> iter 13376\n",
      "-----> iter 13377\n",
      "-----> iter 13378\n",
      "-----> iter 13379\n",
      "-----> iter 13380\n",
      "-----> iter 13381\n",
      "-----> iter 13382\n",
      "-----> iter 13383\n",
      "-----> iter 13384\n",
      "-----> iter 13385\n",
      "-----> iter 13386\n",
      "-----> iter 13387\n",
      "-----> iter 13388\n",
      "-----> iter 13389\n",
      "-----> iter 13390\n",
      "-----> iter 13391\n",
      "-----> iter 13392\n",
      "-----> iter 13393\n",
      "-----> iter 13394\n",
      "-----> iter 13395\n",
      "-----> iter 13396\n",
      "-----> iter 13397\n",
      "-----> iter 13398\n",
      "-----> iter 13399\n",
      "-----> iter 13400\n",
      "-----> iter 13401\n",
      "-----> iter 13402\n",
      "-----> iter 13403\n",
      "-----> iter 13404\n",
      "-----> iter 13405\n",
      "-----> iter 13406\n",
      "-----> iter 13407\n",
      "-----> iter 13408\n",
      "-----> iter 13409\n",
      "-----> iter 13410\n",
      "-----> iter 13411\n",
      "-----> iter 13412\n",
      "-----> iter 13413\n",
      "-----> iter 13414\n",
      "-----> iter 13415\n",
      "-----> iter 13416\n",
      "-----> iter 13417\n",
      "-----> iter 13418\n",
      "-----> iter 13419\n",
      "-----> iter 13420\n",
      "-----> iter 13421\n",
      "-----> iter 13422\n",
      "-----> iter 13423\n",
      "-----> iter 13424\n",
      "-----> iter 13425\n",
      "-----> iter 13426\n",
      "-----> iter 13427\n",
      "-----> iter 13428\n",
      "-----> iter 13429\n",
      "-----> iter 13430\n",
      "-----> iter 13431\n",
      "-----> iter 13432\n",
      "-----> iter 13433\n",
      "-----> iter 13434\n",
      "-----> iter 13435\n",
      "-----> iter 13436\n",
      "-----> iter 13437\n",
      "-----> iter 13438\n",
      "-----> iter 13439\n",
      "-----> iter 13440\n",
      "-----> iter 13441\n",
      "-----> iter 13442\n",
      "-----> iter 13443\n",
      "-----> iter 13444\n",
      "-----> iter 13445\n",
      "-----> iter 13446\n",
      "-----> iter 13447\n",
      "-----> iter 13448\n",
      "-----> iter 13449\n",
      "-----> iter 13450\n",
      "-----> iter 13451\n",
      "-----> iter 13452\n",
      "-----> iter 13453\n",
      "-----> iter 13454\n",
      "-----> iter 13455\n",
      "-----> iter 13456\n",
      "-----> iter 13457\n",
      "-----> iter 13458\n",
      "-----> iter 13459\n",
      "-----> iter 13460\n",
      "-----> iter 13461\n",
      "-----> iter 13462\n",
      "-----> iter 13463\n",
      "-----> iter 13464\n",
      "-----> iter 13465\n",
      "-----> iter 13466\n",
      "-----> iter 13467\n",
      "-----> iter 13468\n",
      "-----> iter 13469\n",
      "-----> iter 13470\n",
      "-----> iter 13471\n",
      "-----> iter 13472\n",
      "-----> iter 13473\n",
      "-----> iter 13474\n",
      "-----> iter 13475\n",
      "-----> iter 13476\n",
      "-----> iter 13477\n",
      "-----> iter 13478\n",
      "-----> iter 13479\n",
      "-----> iter 13480\n",
      "-----> iter 13481\n",
      "-----> iter 13482\n",
      "-----> iter 13483\n",
      "-----> iter 13484\n",
      "-----> iter 13485\n",
      "-----> iter 13486\n",
      "-----> iter 13487\n",
      "-----> iter 13488\n",
      "-----> iter 13489\n",
      "-----> iter 13490\n",
      "-----> iter 13491\n",
      "-----> iter 13492\n",
      "-----> iter 13493\n",
      "-----> iter 13494\n",
      "-----> iter 13495\n",
      "-----> iter 13496\n",
      "-----> iter 13497\n",
      "-----> iter 13498\n",
      "-----> iter 13499\n",
      "-----> iter 13500\n",
      "-----> iter 13501\n",
      "-----> iter 13502\n",
      "-----> iter 13503\n",
      "-----> iter 13504\n",
      "-----> iter 13505\n",
      "-----> iter 13506\n",
      "-----> iter 13507\n",
      "-----> iter 13508\n",
      "-----> iter 13509\n",
      "-----> iter 13510\n",
      "-----> iter 13511\n",
      "-----> iter 13512\n",
      "-----> iter 13513\n",
      "-----> iter 13514\n",
      "-----> iter 13515\n",
      "-----> iter 13516\n",
      "-----> iter 13517\n",
      "-----> iter 13518\n",
      "-----> iter 13519\n",
      "-----> iter 13520\n",
      "-----> iter 13521\n",
      "-----> iter 13522\n",
      "-----> iter 13523\n",
      "-----> iter 13524\n",
      "-----> iter 13525\n",
      "-----> iter 13526\n",
      "-----> iter 13527\n",
      "-----> iter 13528\n",
      "-----> iter 13529\n",
      "-----> iter 13530\n",
      "-----> iter 13531\n",
      "-----> iter 13532\n",
      "-----> iter 13533\n",
      "-----> iter 13534\n",
      "-----> iter 13535\n",
      "-----> iter 13536\n",
      "-----> iter 13537\n",
      "-----> iter 13538\n",
      "-----> iter 13539\n",
      "-----> iter 13540\n",
      "-----> iter 13541\n",
      "-----> iter 13542\n",
      "-----> iter 13543\n",
      "-----> iter 13544\n",
      "-----> iter 13545\n",
      "-----> iter 13546\n",
      "-----> iter 13547\n",
      "-----> iter 13548\n",
      "-----> iter 13549\n",
      "-----> iter 13550\n",
      "-----> iter 13551\n",
      "-----> iter 13552\n",
      "-----> iter 13553\n",
      "-----> iter 13554\n",
      "-----> iter 13555\n",
      "-----> iter 13556\n",
      "-----> iter 13557\n",
      "-----> iter 13558\n",
      "-----> iter 13559\n",
      "-----> iter 13560\n",
      "-----> iter 13561\n",
      "-----> iter 13562\n",
      "-----> iter 13563\n",
      "-----> iter 13564\n",
      "-----> iter 13565\n",
      "-----> iter 13566\n",
      "-----> iter 13567\n",
      "-----> iter 13568\n",
      "-----> iter 13569\n",
      "-----> iter 13570\n",
      "-----> iter 13571\n",
      "-----> iter 13572\n",
      "-----> iter 13573\n",
      "-----> iter 13574\n",
      "-----> iter 13575\n",
      "-----> iter 13576\n",
      "-----> iter 13577\n",
      "-----> iter 13578\n",
      "-----> iter 13579\n",
      "-----> iter 13580\n",
      "-----> iter 13581\n",
      "-----> iter 13582\n",
      "-----> iter 13583\n",
      "-----> iter 13584\n",
      "-----> iter 13585\n",
      "-----> iter 13586\n",
      "-----> iter 13587\n",
      "-----> iter 13588\n",
      "-----> iter 13589\n",
      "-----> iter 13590\n",
      "-----> iter 13591\n",
      "-----> iter 13592\n",
      "-----> iter 13593\n",
      "-----> iter 13594\n",
      "-----> iter 13595\n",
      "-----> iter 13596\n",
      "-----> iter 13597\n",
      "-----> iter 13598\n",
      "-----> iter 13599\n",
      "-----> iter 13600\n",
      "-----> iter 13601\n",
      "-----> iter 13602\n",
      "-----> iter 13603\n",
      "-----> iter 13604\n",
      "-----> iter 13605\n",
      "-----> iter 13606\n",
      "-----> iter 13607\n",
      "-----> iter 13608\n",
      "-----> iter 13609\n",
      "-----> iter 13610\n",
      "-----> iter 13611\n",
      "-----> iter 13612\n",
      "-----> iter 13613\n",
      "-----> iter 13614\n",
      "-----> iter 13615\n",
      "-----> iter 13616\n",
      "-----> iter 13617\n",
      "-----> iter 13618\n",
      "-----> iter 13619\n",
      "-----> iter 13620\n",
      "-----> iter 13621\n",
      "-----> iter 13622\n",
      "-----> iter 13623\n",
      "-----> iter 13624\n",
      "-----> iter 13625\n",
      "-----> iter 13626\n",
      "-----> iter 13627\n",
      "-----> iter 13628\n",
      "-----> iter 13629\n",
      "-----> iter 13630\n",
      "-----> iter 13631\n",
      "-----> iter 13632\n",
      "-----> iter 13633\n",
      "-----> iter 13634\n",
      "-----> iter 13635\n",
      "-----> iter 13636\n",
      "-----> iter 13637\n",
      "-----> iter 13638\n",
      "-----> iter 13639\n",
      "-----> iter 13640\n",
      "-----> iter 13641\n",
      "-----> iter 13642\n",
      "-----> iter 13643\n",
      "-----> iter 13644\n",
      "-----> iter 13645\n",
      "-----> iter 13646\n",
      "-----> iter 13647\n",
      "-----> iter 13648\n",
      "-----> iter 13649\n",
      "-----> iter 13650\n",
      "-----> iter 13651\n",
      "-----> iter 13652\n",
      "-----> iter 13653\n",
      "-----> iter 13654\n",
      "-----> iter 13655\n",
      "-----> iter 13656\n",
      "-----> iter 13657\n",
      "-----> iter 13658\n",
      "-----> iter 13659\n",
      "-----> iter 13660\n",
      "-----> iter 13661\n",
      "-----> iter 13662\n",
      "-----> iter 13663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 13664\n",
      "-----> iter 13665\n",
      "-----> iter 13666\n",
      "-----> iter 13667\n",
      "-----> iter 13668\n",
      "-----> iter 13669\n",
      "-----> iter 13670\n",
      "-----> iter 13671\n",
      "-----> iter 13672\n",
      "-----> iter 13673\n",
      "-----> iter 13674\n",
      "-----> iter 13675\n",
      "-----> iter 13676\n",
      "-----> iter 13677\n",
      "-----> iter 13678\n",
      "-----> iter 13679\n",
      "-----> iter 13680\n",
      "-----> iter 13681\n",
      "-----> iter 13682\n",
      "-----> iter 13683\n",
      "-----> iter 13684\n",
      "-----> iter 13685\n",
      "-----> iter 13686\n",
      "-----> iter 13687\n",
      "-----> iter 13688\n",
      "-----> iter 13689\n",
      "-----> iter 13690\n",
      "-----> iter 13691\n",
      "-----> iter 13692\n",
      "-----> iter 13693\n",
      "-----> iter 13694\n",
      "-----> iter 13695\n",
      "-----> iter 13696\n",
      "-----> iter 13697\n",
      "-----> iter 13698\n",
      "-----> iter 13699\n",
      "-----> iter 13700\n",
      "-----> iter 13701\n",
      "-----> iter 13702\n",
      "-----> iter 13703\n",
      "-----> iter 13704\n",
      "-----> iter 13705\n",
      "-----> iter 13706\n",
      "-----> iter 13707\n",
      "-----> iter 13708\n",
      "-----> iter 13709\n",
      "-----> iter 13710\n",
      "-----> iter 13711\n",
      "-----> iter 13712\n",
      "-----> iter 13713\n",
      "-----> iter 13714\n",
      "-----> iter 13715\n",
      "-----> iter 13716\n",
      "-----> iter 13717\n",
      "-----> iter 13718\n",
      "-----> iter 13719\n",
      "-----> iter 13720\n",
      "-----> iter 13721\n",
      "-----> iter 13722\n",
      "-----> iter 13723\n",
      "-----> iter 13724\n",
      "-----> iter 13725\n",
      "-----> iter 13726\n",
      "-----> iter 13727\n",
      "-----> iter 13728\n",
      "-----> iter 13729\n",
      "-----> iter 13730\n",
      "-----> iter 13731\n",
      "-----> iter 13732\n",
      "-----> iter 13733\n",
      "-----> iter 13734\n",
      "-----> iter 13735\n",
      "-----> iter 13736\n",
      "-----> iter 13737\n",
      "-----> iter 13738\n",
      "-----> iter 13739\n",
      "-----> iter 13740\n",
      "-----> iter 13741\n",
      "-----> iter 13742\n",
      "-----> iter 13743\n",
      "-----> iter 13744\n",
      "-----> iter 13745\n",
      "-----> iter 13746\n",
      "-----> iter 13747\n",
      "-----> iter 13748\n",
      "-----> iter 13749\n",
      "-----> iter 13750\n",
      "-----> iter 13751\n",
      "-----> iter 13752\n",
      "-----> iter 13753\n",
      "-----> iter 13754\n",
      "-----> iter 13755\n",
      "-----> iter 13756\n",
      "-----> iter 13757\n",
      "-----> iter 13758\n",
      "-----> iter 13759\n",
      "-----> iter 13760\n",
      "-----> iter 13761\n",
      "-----> iter 13762\n",
      "-----> iter 13763\n",
      "-----> iter 13764\n",
      "-----> iter 13765\n",
      "-----> iter 13766\n",
      "-----> iter 13767\n",
      "-----> iter 13768\n",
      "-----> iter 13769\n",
      "-----> iter 13770\n",
      "-----> iter 13771\n",
      "-----> iter 13772\n",
      "-----> iter 13773\n",
      "-----> iter 13774\n",
      "-----> iter 13775\n",
      "-----> iter 13776\n",
      "-----> iter 13777\n",
      "-----> iter 13778\n",
      "-----> iter 13779\n",
      "-----> iter 13780\n",
      "-----> iter 13781\n",
      "-----> iter 13782\n",
      "-----> iter 13783\n",
      "-----> iter 13784\n",
      "-----> iter 13785\n",
      "-----> iter 13786\n",
      "-----> iter 13787\n",
      "-----> iter 13788\n",
      "-----> iter 13789\n",
      "-----> iter 13790\n",
      "-----> iter 13791\n",
      "-----> iter 13792\n",
      "-----> iter 13793\n",
      "-----> iter 13794\n",
      "-----> iter 13795\n",
      "-----> iter 13796\n",
      "-----> iter 13797\n",
      "-----> iter 13798\n",
      "-----> iter 13799\n",
      "-----> iter 13800\n",
      "-----> iter 13801\n",
      "-----> iter 13802\n",
      "-----> iter 13803\n",
      "-----> iter 13804\n",
      "-----> iter 13805\n",
      "-----> iter 13806\n",
      "-----> iter 13807\n",
      "-----> iter 13808\n",
      "-----> iter 13809\n",
      "-----> iter 13810\n",
      "-----> iter 13811\n",
      "-----> iter 13812\n",
      "-----> iter 13813\n",
      "-----> iter 13814\n",
      "-----> iter 13815\n",
      "-----> iter 13816\n",
      "-----> iter 13817\n",
      "-----> iter 13818\n",
      "-----> iter 13819\n",
      "-----> iter 13820\n",
      "-----> iter 13821\n",
      "-----> iter 13822\n",
      "-----> iter 13823\n",
      "-----> iter 13824\n",
      "-----> iter 13825\n",
      "-----> iter 13826\n",
      "-----> iter 13827\n",
      "-----> iter 13828\n",
      "-----> iter 13829\n",
      "-----> iter 13830\n",
      "-----> iter 13831\n",
      "-----> iter 13832\n",
      "-----> iter 13833\n",
      "-----> iter 13834\n",
      "-----> iter 13835\n",
      "-----> iter 13836\n",
      "-----> iter 13837\n",
      "-----> iter 13838\n",
      "-----> iter 13839\n",
      "-----> iter 13840\n",
      "-----> iter 13841\n",
      "-----> iter 13842\n",
      "-----> iter 13843\n",
      "-----> iter 13844\n",
      "-----> iter 13845\n",
      "-----> iter 13846\n",
      "-----> iter 13847\n",
      "-----> iter 13848\n",
      "-----> iter 13849\n",
      "-----> iter 13850\n",
      "-----> iter 13851\n",
      "-----> iter 13852\n",
      "-----> iter 13853\n",
      "-----> iter 13854\n",
      "-----> iter 13855\n",
      "-----> iter 13856\n",
      "-----> iter 13857\n",
      "-----> iter 13858\n",
      "-----> iter 13859\n",
      "-----> iter 13860\n",
      "-----> iter 13861\n",
      "-----> iter 13862\n",
      "-----> iter 13863\n",
      "-----> iter 13864\n",
      "-----> iter 13865\n",
      "-----> iter 13866\n",
      "-----> iter 13867\n",
      "-----> iter 13868\n",
      "-----> iter 13869\n",
      "-----> iter 13870\n",
      "-----> iter 13871\n",
      "-----> iter 13872\n",
      "-----> iter 13873\n",
      "-----> iter 13874\n",
      "-----> iter 13875\n",
      "-----> iter 13876\n",
      "-----> iter 13877\n",
      "-----> iter 13878\n",
      "-----> iter 13879\n",
      "-----> iter 13880\n",
      "-----> iter 13881\n",
      "-----> iter 13882\n",
      "-----> iter 13883\n",
      "-----> iter 13884\n",
      "-----> iter 13885\n",
      "-----> iter 13886\n",
      "-----> iter 13887\n",
      "-----> iter 13888\n",
      "-----> iter 13889\n",
      "-----> iter 13890\n",
      "-----> iter 13891\n",
      "-----> iter 13892\n",
      "-----> iter 13893\n",
      "-----> iter 13894\n",
      "-----> iter 13895\n",
      "-----> iter 13896\n",
      "-----> iter 13897\n",
      "-----> iter 13898\n",
      "-----> iter 13899\n",
      "-----> iter 13900\n",
      "-----> iter 13901\n",
      "-----> iter 13902\n",
      "-----> iter 13903\n",
      "-----> iter 13904\n",
      "-----> iter 13905\n",
      "-----> iter 13906\n",
      "-----> iter 13907\n",
      "-----> iter 13908\n",
      "-----> iter 13909\n",
      "-----> iter 13910\n",
      "-----> iter 13911\n",
      "-----> iter 13912\n",
      "-----> iter 13913\n",
      "-----> iter 13914\n",
      "-----> iter 13915\n",
      "-----> iter 13916\n",
      "-----> iter 13917\n",
      "-----> iter 13918\n",
      "-----> iter 13919\n",
      "-----> iter 13920\n",
      "-----> iter 13921\n",
      "-----> iter 13922\n",
      "-----> iter 13923\n",
      "-----> iter 13924\n",
      "-----> iter 13925\n",
      "-----> iter 13926\n",
      "-----> iter 13927\n",
      "-----> iter 13928\n",
      "-----> iter 13929\n",
      "-----> iter 13930\n",
      "-----> iter 13931\n",
      "-----> iter 13932\n",
      "-----> iter 13933\n",
      "-----> iter 13934\n",
      "-----> iter 13935\n",
      "-----> iter 13936\n",
      "-----> iter 13937\n",
      "-----> iter 13938\n",
      "-----> iter 13939\n",
      "-----> iter 13940\n",
      "-----> iter 13941\n",
      "-----> iter 13942\n",
      "-----> iter 13943\n",
      "-----> iter 13944\n",
      "-----> iter 13945\n",
      "-----> iter 13946\n",
      "-----> iter 13947\n",
      "-----> iter 13948\n",
      "-----> iter 13949\n",
      "-----> iter 13950\n",
      "-----> iter 13951\n",
      "-----> iter 13952\n",
      "-----> iter 13953\n",
      "-----> iter 13954\n",
      "-----> iter 13955\n",
      "-----> iter 13956\n",
      "-----> iter 13957\n",
      "-----> iter 13958\n",
      "-----> iter 13959\n",
      "-----> iter 13960\n",
      "-----> iter 13961\n",
      "-----> iter 13962\n",
      "-----> iter 13963\n",
      "-----> iter 13964\n",
      "-----> iter 13965\n",
      "-----> iter 13966\n",
      "-----> iter 13967\n",
      "-----> iter 13968\n",
      "-----> iter 13969\n",
      "-----> iter 13970\n",
      "-----> iter 13971\n",
      "-----> iter 13972\n",
      "-----> iter 13973\n",
      "-----> iter 13974\n",
      "-----> iter 13975\n",
      "-----> iter 13976\n",
      "-----> iter 13977\n",
      "-----> iter 13978\n",
      "-----> iter 13979\n",
      "-----> iter 13980\n",
      "-----> iter 13981\n",
      "-----> iter 13982\n",
      "-----> iter 13983\n",
      "-----> iter 13984\n",
      "-----> iter 13985\n",
      "-----> iter 13986\n",
      "-----> iter 13987\n",
      "-----> iter 13988\n",
      "-----> iter 13989\n",
      "-----> iter 13990\n",
      "-----> iter 13991\n",
      "-----> iter 13992\n",
      "-----> iter 13993\n",
      "-----> iter 13994\n",
      "-----> iter 13995\n",
      "-----> iter 13996\n",
      "-----> iter 13997\n",
      "-----> iter 13998\n",
      "-----> iter 13999\n",
      "-----> iter 14000\n",
      "-----> iter 14001\n",
      "-----> iter 14002\n",
      "-----> iter 14003\n",
      "-----> iter 14004\n",
      "-----> iter 14005\n",
      "-----> iter 14006\n",
      "-----> iter 14007\n",
      "-----> iter 14008\n",
      "-----> iter 14009\n",
      "-----> iter 14010\n",
      "-----> iter 14011\n",
      "-----> iter 14012\n",
      "-----> iter 14013\n",
      "-----> iter 14014\n",
      "-----> iter 14015\n",
      "-----> iter 14016\n",
      "-----> iter 14017\n",
      "-----> iter 14018\n",
      "-----> iter 14019\n",
      "-----> iter 14020\n",
      "-----> iter 14021\n",
      "-----> iter 14022\n",
      "-----> iter 14023\n",
      "-----> iter 14024\n",
      "-----> iter 14025\n",
      "-----> iter 14026\n",
      "-----> iter 14027\n",
      "-----> iter 14028\n",
      "-----> iter 14029\n",
      "-----> iter 14030\n",
      "-----> iter 14031\n",
      "-----> iter 14032\n",
      "-----> iter 14033\n",
      "-----> iter 14034\n",
      "-----> iter 14035\n",
      "-----> iter 14036\n",
      "-----> iter 14037\n",
      "-----> iter 14038\n",
      "-----> iter 14039\n",
      "-----> iter 14040\n",
      "-----> iter 14041\n",
      "-----> iter 14042\n",
      "-----> iter 14043\n",
      "-----> iter 14044\n",
      "-----> iter 14045\n",
      "-----> iter 14046\n",
      "-----> iter 14047\n",
      "-----> iter 14048\n",
      "-----> iter 14049\n",
      "-----> iter 14050\n",
      "-----> iter 14051\n",
      "-----> iter 14052\n",
      "-----> iter 14053\n",
      "-----> iter 14054\n",
      "-----> iter 14055\n",
      "-----> iter 14056\n",
      "-----> iter 14057\n",
      "-----> iter 14058\n",
      "-----> iter 14059\n",
      "-----> iter 14060\n",
      "-----> iter 14061\n",
      "-----> iter 14062\n",
      "-----> iter 14063\n",
      "-----> iter 14064\n",
      "-----> iter 14065\n",
      "-----> iter 14066\n",
      "-----> iter 14067\n",
      "-----> iter 14068\n",
      "-----> iter 14069\n",
      "-----> iter 14070\n",
      "-----> iter 14071\n",
      "-----> iter 14072\n",
      "-----> iter 14073\n",
      "-----> iter 14074\n",
      "-----> iter 14075\n",
      "-----> iter 14076\n",
      "-----> iter 14077\n",
      "-----> iter 14078\n",
      "-----> iter 14079\n",
      "-----> iter 14080\n",
      "-----> iter 14081\n",
      "-----> iter 14082\n",
      "-----> iter 14083\n",
      "-----> iter 14084\n",
      "-----> iter 14085\n",
      "-----> iter 14086\n",
      "-----> iter 14087\n",
      "-----> iter 14088\n",
      "-----> iter 14089\n",
      "-----> iter 14090\n",
      "-----> iter 14091\n",
      "-----> iter 14092\n",
      "-----> iter 14093\n",
      "-----> iter 14094\n",
      "-----> iter 14095\n",
      "-----> iter 14096\n",
      "-----> iter 14097\n",
      "-----> iter 14098\n",
      "-----> iter 14099\n",
      "-----> iter 14100\n",
      "-----> iter 14101\n",
      "-----> iter 14102\n",
      "-----> iter 14103\n",
      "-----> iter 14104\n",
      "-----> iter 14105\n",
      "-----> iter 14106\n",
      "-----> iter 14107\n",
      "-----> iter 14108\n",
      "-----> iter 14109\n",
      "-----> iter 14110\n",
      "-----> iter 14111\n",
      "-----> iter 14112\n",
      "-----> iter 14113\n",
      "-----> iter 14114\n",
      "-----> iter 14115\n",
      "-----> iter 14116\n",
      "-----> iter 14117\n",
      "-----> iter 14118\n",
      "-----> iter 14119\n",
      "-----> iter 14120\n",
      "-----> iter 14121\n",
      "-----> iter 14122\n",
      "-----> iter 14123\n",
      "-----> iter 14124\n",
      "-----> iter 14125\n",
      "-----> iter 14126\n",
      "-----> iter 14127\n",
      "-----> iter 14128\n",
      "-----> iter 14129\n",
      "-----> iter 14130\n",
      "-----> iter 14131\n",
      "-----> iter 14132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> iter 14133\n",
      "-----> iter 14134\n",
      "-----> iter 14135\n",
      "-----> iter 14136\n",
      "-----> iter 14137\n",
      "-----> iter 14138\n",
      "-----> iter 14139\n",
      "-----> iter 14140\n",
      "-----> iter 14141\n",
      "-----> iter 14142\n",
      "-----> iter 14143\n",
      "-----> iter 14144\n",
      "-----> iter 14145\n",
      "-----> iter 14146\n",
      "-----> iter 14147\n",
      "-----> iter 14148\n",
      "-----> iter 14149\n",
      "-----> iter 14150\n",
      "-----> iter 14151\n",
      "-----> iter 14152\n",
      "-----> iter 14153\n",
      "-----> iter 14154\n",
      "-----> iter 14155\n",
      "-----> iter 14156\n",
      "-----> iter 14157\n",
      "-----> iter 14158\n",
      "-----> iter 14159\n",
      "-----> iter 14160\n",
      "-----> iter 14161\n",
      "-----> iter 14162\n",
      "-----> iter 14163\n",
      "-----> iter 14164\n",
      "-----> iter 14165\n",
      "-----> iter 14166\n",
      "-----> iter 14167\n",
      "-----> iter 14168\n",
      "-----> iter 14169\n",
      "-----> iter 14170\n",
      "-----> iter 14171\n",
      "-----> iter 14172\n",
      "-----> iter 14173\n",
      "-----> iter 14174\n",
      "-----> iter 14175\n",
      "-----> iter 14176\n",
      "-----> iter 14177\n",
      "-----> iter 14178\n",
      "-----> iter 14179\n",
      "-----> iter 14180\n",
      "-----> iter 14181\n",
      "-----> iter 14182\n",
      "-----> iter 14183\n",
      "-----> iter 14184\n",
      "-----> iter 14185\n",
      "-----> iter 14186\n",
      "-----> iter 14187\n",
      "-----> iter 14188\n",
      "-----> iter 14189\n",
      "-----> iter 14190\n",
      "-----> iter 14191\n",
      "-----> iter 14192\n",
      "-----> iter 14193\n",
      "-----> iter 14194\n",
      "-----> iter 14195\n",
      "-----> iter 14196\n",
      "-----> iter 14197\n",
      "-----> iter 14198\n",
      "-----> iter 14199\n",
      "-----> iter 14200\n",
      "-----> iter 14201\n",
      "-----> iter 14202\n",
      "-----> iter 14203\n",
      "-----> iter 14204\n",
      "-----> iter 14205\n",
      "-----> iter 14206\n",
      "-----> iter 14207\n",
      "-----> iter 14208\n",
      "-----> iter 14209\n",
      "-----> iter 14210\n",
      "-----> iter 14211\n",
      "-----> iter 14212\n",
      "-----> iter 14213\n",
      "-----> iter 14214\n",
      "-----> iter 14215\n",
      "-----> iter 14216\n",
      "-----> iter 14217\n",
      "-----> iter 14218\n",
      "-----> iter 14219\n",
      "-----> iter 14220\n",
      "-----> iter 14221\n",
      "-----> iter 14222\n",
      "-----> iter 14223\n",
      "-----> iter 14224\n",
      "-----> iter 14225\n",
      "-----> iter 14226\n",
      "-----> iter 14227\n",
      "-----> iter 14228\n",
      "-----> iter 14229\n",
      "-----> iter 14230\n",
      "-----> iter 14231\n",
      "-----> iter 14232\n",
      "-----> iter 14233\n",
      "-----> iter 14234\n",
      "-----> iter 14235\n",
      "-----> iter 14236\n",
      "-----> iter 14237\n",
      "-----> iter 14238\n",
      "-----> iter 14239\n",
      "-----> iter 14240\n",
      "-----> iter 14241\n",
      "-----> iter 14242\n",
      "-----> iter 14243\n",
      "-----> iter 14244\n",
      "-----> iter 14245\n",
      "-----> iter 14246\n",
      "-----> iter 14247\n",
      "-----> iter 14248\n",
      "-----> iter 14249\n",
      "-----> iter 14250\n",
      "-----> iter 14251\n",
      "-----> iter 14252\n",
      "-----> iter 14253\n",
      "-----> iter 14254\n",
      "-----> iter 14255\n",
      "-----> iter 14256\n",
      "-----> iter 14257\n",
      "-----> iter 14258\n",
      "-----> iter 14259\n",
      "-----> iter 14260\n",
      "-----> iter 14261\n",
      "-----> iter 14262\n",
      "-----> iter 14263\n",
      "-----> iter 14264\n",
      "-----> iter 14265\n",
      "-----> iter 14266\n",
      "-----> iter 14267\n",
      "-----> iter 14268\n",
      "-----> iter 14269\n",
      "-----> iter 14270\n",
      "-----> iter 14271\n",
      "-----> iter 14272\n",
      "-----> iter 14273\n",
      "-----> iter 14274\n",
      "-----> iter 14275\n",
      "-----> iter 14276\n",
      "-----> iter 14277\n",
      "-----> iter 14278\n",
      "-----> iter 14279\n",
      "-----> iter 14280\n",
      "-----> iter 14281\n",
      "-----> iter 14282\n",
      "-----> iter 14283\n",
      "-----> iter 14284\n",
      "-----> iter 14285\n",
      "-----> iter 14286\n",
      "-----> iter 14287\n",
      "-----> iter 14288\n",
      "-----> iter 14289\n",
      "-----> iter 14290\n",
      "-----> iter 14291\n",
      "-----> iter 14292\n",
      "-----> iter 14293\n",
      "-----> iter 14294\n",
      "-----> iter 14295\n",
      "-----> iter 14296\n",
      "-----> iter 14297\n",
      "-----> iter 14298\n",
      "-----> iter 14299\n",
      "-----> iter 14300\n",
      "-----> iter 14301\n",
      "-----> iter 14302\n",
      "-----> iter 14303\n",
      "-----> iter 14304\n",
      "-----> iter 14305\n",
      "-----> iter 14306\n",
      "-----> iter 14307\n",
      "-----> iter 14308\n",
      "-----> iter 14309\n",
      "-----> iter 14310\n",
      "-----> iter 14311\n",
      "-----> iter 14312\n",
      "-----> iter 14313\n",
      "-----> iter 14314\n",
      "-----> iter 14315\n",
      "-----> iter 14316\n",
      "-----> iter 14317\n",
      "-----> iter 14318\n",
      "-----> iter 14319\n",
      "-----> iter 14320\n",
      "-----> iter 14321\n",
      "-----> iter 14322\n",
      "-----> iter 14323\n",
      "-----> iter 14324\n",
      "-----> iter 14325\n",
      "-----> iter 14326\n",
      "-----> iter 14327\n",
      "-----> iter 14328\n",
      "-----> iter 14329\n",
      "-----> iter 14330\n",
      "-----> iter 14331\n",
      "-----> iter 14332\n",
      "-----> iter 14333\n",
      "-----> iter 14334\n",
      "-----> iter 14335\n",
      "-----> iter 14336\n",
      "-----> iter 14337\n",
      "-----> iter 14338\n",
      "-----> iter 14339\n",
      "-----> iter 14340\n",
      "-----> iter 14341\n",
      "-----> iter 14342\n",
      "-----> iter 14343\n",
      "-----> iter 14344\n",
      "-----> iter 14345\n",
      "-----> iter 14346\n",
      "-----> iter 14347\n",
      "-----> iter 14348\n",
      "-----> iter 14349\n",
      "-----> iter 14350\n",
      "-----> iter 14351\n",
      "-----> iter 14352\n",
      "-----> iter 14353\n",
      "-----> iter 14354\n",
      "-----> iter 14355\n",
      "-----> iter 14356\n",
      "-----> iter 14357\n",
      "-----> iter 14358\n",
      "-----> iter 14359\n",
      "-----> iter 14360\n",
      "-----> iter 14361\n",
      "-----> iter 14362\n",
      "-----> iter 14363\n",
      "-----> iter 14364\n",
      "-----> iter 14365\n",
      "-----> iter 14366\n",
      "-----> iter 14367\n",
      "-----> iter 14368\n",
      "-----> iter 14369\n",
      "-----> iter 14370\n",
      "-----> iter 14371\n",
      "-----> iter 14372\n",
      "-----> iter 14373\n",
      "-----> iter 14374\n",
      "-----> iter 14375\n",
      "-----> iter 14376\n",
      "-----> iter 14377\n",
      "-----> iter 14378\n",
      "-----> iter 14379\n",
      "-----> iter 14380\n",
      "-----> iter 14381\n",
      "-----> iter 14382\n",
      "-----> iter 14383\n",
      "-----> iter 14384\n",
      "-----> iter 14385\n",
      "-----> iter 14386\n",
      "-----> iter 14387\n",
      "-----> iter 14388\n",
      "-----> iter 14389\n",
      "-----> iter 14390\n",
      "-----> iter 14391\n",
      "-----> iter 14392\n",
      "-----> iter 14393\n",
      "-----> iter 14394\n",
      "-----> iter 14395\n",
      "-----> iter 14396\n"
     ]
    }
   ],
   "source": [
    "new_expert_data = expert_with_obs(immit_data[\"observations\"],expert_policy_file = \"experts/Humanoid-v1.pkl\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14397, 376)\n",
      "(14397, 1, 17)\n"
     ]
    }
   ],
   "source": [
    "print(new_expert_data[\"observations\"].shape)\n",
    "print(new_expert_data[\"actions\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:38:45,357] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:38:45,423] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:38:53,408] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:38:53,426] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:38:54,732] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 2.34143\n",
      "epoch 1,training loss nan ,test loss 2.89087\n",
      "epoch 2,training loss nan ,test loss 6.26349\n",
      "epoch 3,training loss nan ,test loss 4.42506\n",
      "epoch 4,training loss nan ,test loss 3.21356\n",
      "\n",
      "----> iteration 0, time 19.235613 sec\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:04,592] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:39:04,639] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:05,199] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:05,217] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:05,333] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 1.65694\n",
      "epoch 1,training loss nan ,test loss 1.39069\n",
      "epoch 2,training loss nan ,test loss 0.885734\n",
      "epoch 3,training loss nan ,test loss 1.05758\n",
      "epoch 4,training loss nan ,test loss 0.919822\n",
      "\n",
      "----> iteration 1, time 12.143011 sec\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:16,735] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:39:16,790] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:19,353] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:19,374] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:19,737] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 1.14558\n",
      "epoch 1,training loss nan ,test loss 2.23803\n",
      "epoch 2,training loss nan ,test loss 12.2514\n",
      "epoch 3,training loss nan ,test loss 3.94635\n",
      "epoch 4,training loss nan ,test loss 39.9388\n",
      "\n",
      "----> iteration 2, time 12.700926 sec\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:29,436] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:39:29,479] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:30,573] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:30,598] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:30,776] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 1.22384\n",
      "epoch 1,training loss nan ,test loss 1.15417\n",
      "epoch 2,training loss nan ,test loss 1.16053\n",
      "epoch 3,training loss nan ,test loss 1.17784\n",
      "epoch 4,training loss nan ,test loss 1.19286\n",
      "\n",
      "----> iteration 3, time 14.234363 sec\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:43,671] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:39:43,728] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:46,114] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:46,138] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:46,447] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 1.21286\n",
      "epoch 1,training loss nan ,test loss 1.28868\n",
      "epoch 2,training loss nan ,test loss 5.30166\n",
      "epoch 3,training loss nan ,test loss 1.21712\n",
      "epoch 4,training loss nan ,test loss 2.05346\n",
      "\n",
      "----> iteration 4, time 13.829556 sec\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:57,500] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:39:57,541] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:58,865] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:58,892] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:39:59,110] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 1.32523\n",
      "epoch 1,training loss nan ,test loss 1.39965\n",
      "epoch 2,training loss nan ,test loss 1.33049\n",
      "epoch 3,training loss nan ,test loss 1.36001\n",
      "epoch 4,training loss nan ,test loss 1.38957\n",
      "\n",
      "----> iteration 5, time 14.881896 sec\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:12,382] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:40:12,430] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:14,806] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:14,831] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:15,154] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 1.38533\n",
      "epoch 1,training loss nan ,test loss 1.38924\n",
      "epoch 2,training loss nan ,test loss 1.39868\n",
      "epoch 3,training loss nan ,test loss 1.41204\n",
      "epoch 4,training loss nan ,test loss 1.50084\n",
      "\n",
      "----> iteration 6, time 14.069234 sec\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:26,452] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:40:26,506] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:28,710] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:28,742] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:29,112] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 1.4744\n",
      "epoch 1,training loss nan ,test loss 1.44875\n",
      "epoch 2,training loss nan ,test loss 1.59278\n",
      "epoch 3,training loss nan ,test loss 1.45676\n",
      "epoch 4,training loss nan ,test loss 1.6525\n",
      "\n",
      "----> iteration 7, time 14.343657 sec\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:40,796] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:40:40,910] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:42,996] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:43,031] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:43,440] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 1.36545\n",
      "epoch 1,training loss nan ,test loss 1.44566\n",
      "epoch 2,training loss nan ,test loss 1.43038\n",
      "epoch 3,training loss nan ,test loss 1.49855\n",
      "epoch 4,training loss nan ,test loss 1.43579\n",
      "\n",
      "----> iteration 8, time 14.755958 sec\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:55,552] Restoring parameters from ./tmp/model.ckpt\n",
      "[2017-10-23 19:40:55,596] Making new env: Humanoid-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs (1, 376) (1, 376)\n",
      "WARNING:tensorflow:From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:57,156] From /media/afakharany93/Common/Online_courses/CS294-berkeleydeeprlcourse/homework/hw1/tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:57,180] From /home/afakharany93/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-23 19:40:57,444] Restoring parameters from ./tmp/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,training loss nan ,test loss 1.72604\n",
      "epoch 1,training loss nan ,test loss 1.51866\n",
      "epoch 2,training loss nan ,test loss 1.558\n",
      "epoch 3,training loss nan ,test loss 1.62117\n",
      "epoch 4,training loss nan ,test loss 1.61917\n",
      "\n",
      "----> iteration 9, time 14.381261 sec\n"
     ]
    }
   ],
   "source": [
    "num_of_inter = 10\n",
    "for i in range(num_of_inter):\n",
    "    start = time.time()\n",
    "    immit_data = try_policy(num_rollouts = 10, render=True)\n",
    "    new_expert_data = expert_with_obs(immit_data[\"observations\"],expert_policy_file = \"experts/Humanoid-v1.pkl\", verbose = False)\n",
    "    train(new_expert_data, batch_size = 100, epochs = 5, train_from_start = False, verbose = False)\n",
    "    end = time.time()\n",
    "    print(\"\\n----> iteration %i, time %f sec\" %(i, end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
